<!DOCTYPE html>
<html lang="zh-CN">
    
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <meta name="description" content="Jane Streetæ—¥å†…é«˜é¢‘äº¤æ˜“é¢„æµ‹" />
    <meta name="hexo-theme-A4" content="v1.9.8" />
    <link rel="alternate icon" type="image/webp" href="/img/20250602032635.jpg">
    <title>Yann | æˆ‘æ„¿åšä½ å…‰åä¸­æ·¡æ·¡çš„ä¸€ç¬”</title>

    
        
<link rel="stylesheet" href="/css/highlight/style1.css">

        
<link rel="stylesheet" href="/css/reset.css">

        
<link rel="stylesheet" href="/css/markdown.css">

        
<link rel="stylesheet" href="/css/fonts.css">
 
         <!--æ³¨æ„ï¼šé¦–é¡µæ—¢ä¸æ˜¯postä¹Ÿä¸æ˜¯page-->
        
        
        
<link rel="stylesheet" href="/css/ui.css">
 
        
<link rel="stylesheet" href="/css/style.css">


        
            <!--è¿”å›é¡¶éƒ¨css-->
            
<link rel="stylesheet" href="/css/returnToTop.css">

            
<link rel="stylesheet" href="/css/unicons.css">

        
        
            <!--ç›®å½•-->
            
<link rel="stylesheet" href="/css/toc.css">

        
    

    
        
<link rel="stylesheet" href="/css/returnToLastPage.css">

    
    
   
<link rel="stylesheet" href="/css/lightgallery-bundle.min.css">


   
        
<link rel="stylesheet" href="/css/custom.css">

    

<meta name="generator" content="Hexo 5.4.2"></head>
    
    

    
    



    
        <style>
            .paper-main{
                max-width:  1200px;
            }
        </style>

    
    




    
    <style>
        :root {
            --waline-theme-color: #323e74; 
            --waline-color: #323e74; 
            --waline-border-color: #323e74; 
            --waline-white: #323e74; 
            --waline-bgcolor-light: #f2fafc;  
        }
        body {
            color: #323e74;
            background: #eaeae8;
        }
        .post-md code {
            background: #e7f7f3;
            color: #7f688d; 
        }
        .post-md pre, .post-md .highlight {
            background: #e7f7f3;
            color: #7f688d; 
        }
        pre .string, pre .value, pre .inheritance, pre .header, pre .ruby .symbol, pre .xml .cdata {
            color: #323e74;
        }
        pre .number, pre .preprocessor, pre .built_in, pre .literal, pre .params, pre .constant {
            color: #323e74;
        }
        .year-font-color {
            color: #323e74 !important;
        }
        .wl-card span.wl-nick {
            color: #323e74; 
        }
        .wl-card .wl-badge {
            border: 1px solid #323e74;
            color: #323e74; 
        }
        .wl-btn {
            border: 1px solid #323e74; 
            color:  #323e74;  
        }
        .wl-btn.primary {
            color: #f2fafc; 
        }
        .wl-header label {
            color: #323e74;
        }
        a {
            color: #7f688d;
        }

        .post-md a {
            color: #7f688d;
        }

        .nav li a {
            color: #7f688d;
        }

        .archive-main a:link {
            color: #7f688d;
        }
        .archive-main a:visited {
            color: #767c7c; 
        }

        .archive li span {
            color: #323e74;
        }

        .post-main-title {
            color: #323e74;
        }

        .post-md h1,
        .post-md h2,
        .post-md h3,
        .post-md h4,
        .post-md h5,
        .post-md h6 {
            color: #323e74;
        }

        [data-waline] p {
            color: #323e74;
        }
        [data-waline] a {
            color: #323e74;
        } 
        .wl-sort li.active {
            color: #323e74;
        }

        .wl-card .wl-meta>span {
            background: #f2fafc;
        }

        .paper {
            background: #eaeae8;
        }

        .index-main {
            background: #f2fafc;
        }

        .paper-main {
            background: #f2fafc;
        }

        .wl-panel {
            background: #f2fafc;
        }

        .archive li:nth-child(odd) {
            background: #f2fafc;
            ;
        }

        .archive li:nth-child(even) {
            background: #f2fafc;
        }

        .post-md table tr:nth-child(odd) td {
            background: #f2fafc;
        }

        .post-md table tr:nth-child(even) td {
            background: #f2fafc;
        }

    
        .progress-wrap::after {
            color: #323e74; /* ç®­å¤´çš„é¢œè‰² */
        }
        .progress-wrap svg.progress-circle path {
	        stroke: #323e74; /* è¾¹æ¡†çš„é¢œè‰² */
        }
        .progress-wrap::before {
	        background-image: linear-gradient(298deg, #7f688d, #7f688d); /* é¼ æ ‡æ»‘è¿‡çš„ç®­å¤´é¢œè‰² */
         }

        .return-to-last-progress-wrap::after {
            color: #323e74; /* ç®­å¤´çš„é¢œè‰² */
        }
        .return-to-last-progress-wrap svg.progress-circle path {
	        stroke: #323e74; /* è¾¹æ¡†çš„é¢œè‰² */
        }
        .return-to-last-progress-wrap::before {
	        background-image: linear-gradient(298deg, #7f688d, #7f688d); /* é¼ æ ‡æ»‘è¿‡çš„ç®­å¤´é¢œè‰² */
         }

         .left-toc-container::-webkit-scrollbar-thumb {
            background-color: #323e74; /* è®¾ç½®æ»šåŠ¨æ¡æ‹–åŠ¨å—çš„é¢œè‰² */
        }

        .bs-docs-sidebar .nav>.active>a,
        .bs-docs-sidebar .nav>li>a:hover,
        .bs-docs-sidebar .nav>li>a:focus {
            color: #7f688d;
            border-left-color: #7f688d;
        }
        .bs-docs-sidebar .nav>li>a {
            color:  #323e74;
        }
    </style>

    
    <style>
        body {
            background-image: url(/img/3.jpg);
            background-attachment: fixed;  /* èƒŒæ™¯å›ºå®šï¼Œä¸éšé¡µé¢æ»šåŠ¨ */
            background-repeat: no-repeat;  /* é˜²æ­¢èƒŒæ™¯å›¾ç‰‡é‡å¤ */
            background-size: cover;       /* èƒŒæ™¯è‡ªé€‚åº”å¤§å°ï¼Œè¦†ç›–æ•´ä¸ªèƒŒæ™¯ */
            background-position: center;   /* èƒŒæ™¯å±…ä¸­æ˜¾ç¤º */
        }
        .paper {
            background-image: url(/img/3.jpg);
            background-attachment: fixed;  /* èƒŒæ™¯å›ºå®šï¼Œä¸éšé¡µé¢æ»šåŠ¨ */
            background-repeat: no-repeat;  /* é˜²æ­¢èƒŒæ™¯å›¾ç‰‡é‡å¤ */
            background-size: cover;       /* èƒŒæ™¯è‡ªé€‚åº”å¤§å°ï¼Œè¦†ç›–æ•´ä¸ªèƒŒæ™¯ */
            background-position: center;   /* èƒŒæ™¯å±…ä¸­æ˜¾ç¤º */
        }  
    </style>


    
    <body>
        <script src="/js/darkmode-js.min.js"></script>
        
        <script>
            const options = {
                bottom: '40px', // default: '32px'
                right: 'unset', // default: '32px'
                left: '42px', // default: 'unset'
                time: '0.3s', // default: '0.3s'
                mixColor: '#fff', // default: '#fff'
                backgroundColor: ' #eaeae8  ',  // default: '#fff'
                buttonColorDark: '#100f2c',  // default: '#100f2c'
                buttonColorLight: '#fff', // default: '#fff'
                saveInCookies: true, // default: true,
                label: 'ğŸŒ“', // default: ''
                autoMatchOsTheme: true // default: true
            }
            const darkmode = new Darkmode(options);
            darkmode.showWidget();
        </script>
        
        
            
                <div class="left-toc-container">
                    <nav id="toc" class="bs-docs-sidebar"></nav>
                </div>
            
        
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    


<div class="header">
    <div class="header-container">
        <style>
            .header-img {
                width: 56px;
                height: auto;
                object-fit: cover; /* ä¿æŒå›¾ç‰‡æ¯”ä¾‹ */
                transition: transform 0.3s ease-in-out; 
                border-radius: 0; 
            }
            
        </style>
        <img 
            alt="^-^" 
            cache-control="max-age=86400" 
            class="header-img" 
            src="/img/20250602032635.jpg" 
        />
        <div class="header-content">
            <a class="logo" href="/">Yann</a> 
            <span class="description">äººå·¥æ™ºèƒ½ã€è®¡ç®—æœºã€æœºå™¨å­¦ä¹ ã€linuxã€ç¨‹åºå‘˜</span> 
        </div>
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="/">é¦–é¡µ</a></li>
            
        
            
                <li><a href="/list/">æ–‡ç« </a></li>
            
        
            
                <li><a href="/about/">å…³äº</a></li>
            
        
            
                <li><a href="/tags/">æ ‡ç­¾</a></li>
            
        
            
                <li><a href="/categories/">åˆ†ç±»</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--è¯´æ˜æ˜¯æ–‡ç« posté¡µé¢-->
                    
                        <div class="post-main">
    

    
        
            
                <div class="post-main-title" style="text-align: center;">
                    Jane Streetæ—¥å†…é«˜é¢‘äº¤æ˜“é¢„æµ‹
                </div>
            
        
      
    

    

        
            <div class="post-head-meta-center">
        
                
                    <span>æœ€è¿‘æ›´æ–°ï¼š2021-02-05</span> 
                
                
                    
                        &nbsp; | &nbsp;
                    
                     <span>å­—æ•°æ€»è®¡ï¼š2.4k</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span>é˜…è¯»ä¼°æ—¶ï¼š15åˆ†é’Ÿ</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span id="busuanzi_container_page_pv">
                        é˜…è¯»é‡ï¼š<span id="busuanzi_value_page_pv"></span>æ¬¡
                    </span>
                
            </div>
    

    <div class="post-md">
        
            
                <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-Overview"><span class="post-toc-text">1. Overview</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-1-Description"><span class="post-toc-text">1.1 Description</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-2-Evaluation"><span class="post-toc-text">1.2 Evaluation</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-Implementing"><span class="post-toc-text">2. Implementing.</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-0-Preprocessing"><span class="post-toc-text">2.0 Preprocessing</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-1-EDA"><span class="post-toc-text">2.1. EDA</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-Using-XGBoost-Algorithm"><span class="post-toc-text">2.2 Using XGBoost Algorithm</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#XGBoost"><span class="post-toc-text">XGBoost?</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-1-Ensemble-learning"><span class="post-toc-text">2.2.1 Ensemble learning</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-1-1-Bagging"><span class="post-toc-text">2.2.1.1 Bagging</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-1-2-Boosting"><span class="post-toc-text">2.2.1.2 Boosting</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-2-Different-of-Bagging-and-Boosting"><span class="post-toc-text">2.2.2 Different of Bagging and Boosting</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting"><span class="post-toc-text">2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-1-Split-finding-Algorithm"><span class="post-toc-text">2.2.1 Split finding Algorithm</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-2-Sparsity-Aware-Split-Finding"><span class="post-toc-text">2.2.2 Sparsity-Aware Split Finding</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-3-System-Design-for-Efficient-Computing"><span class="post-toc-text">2.2.3 System Design for Efficient Computing</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Importing-dataset"><span class="post-toc-text">Importing dataset</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-import-library"><span class="post-toc-text">1) import library</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-load-and-clean-dataset"><span class="post-toc-text">2) load and clean dataset</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Cleaning-data"><span class="post-toc-text">Cleaning data</span></a></li></ol></li></ol>
            
        
        <div class=".article-gallery"><blockquote>
<p>è®°å½•ä¸€æ¬¡kaggleæ¯”èµ›ï¼Œè¿™æ¬¡æ¯”èµ›çš„åœºæ™¯ä¸»è¦æ˜¯Jane Streetæ—¥å†…é«˜é¢‘äº¤æ˜“ï¼Œæ˜¯çŸ­çº¿æ¨¡å‹ï¼Œä½¿ç”¨å¤æ™®ç‡è¡¡é‡æ¨¡å‹æ•ˆæœï¼Œä½†æ˜¯æä¾›çš„æ•°æ®å¹¶éåŸå§‹æ•°æ®ï¼Œä¸æ¸…æ¥šfeatureå«ä¹‰ã€‚</p>
</blockquote>
<span id="more"></span> 
<h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><h3 id="1-1-Description"><a href="#1-1-Description" class="headerlink" title="1.1 Description"></a><strong>1.1 Description</strong></h3><p>In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their â€œfair valuesâ€ and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.</p>
<p>Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. </p>
<h3 id="1-2-Evaluation"><a href="#1-2-Evaluation" class="headerlink" title="1.2 Evaluation"></a><strong>1.2 Evaluation</strong></h3><ul>
<li><p>Utility score</p>
<p>Each row in the test set represents a trading opportunity for which <strong>you will be predicting an <code>action</code> value, 1 to make the trade and 0 to pass on it.</strong> Each trade <code>j</code> has an associated <code>weight</code> and <code>resp</code>, which represents a return.</p>
</li>
</ul>
<p>$$<br>p_i = \sum_j(weight_{ij} <em> resp_{ij} </em> action_{ij}),<br>$$</p>
<p>$$<br>t = \frac{\sum p_i }{\sqrt{\sum p_i^2}} * \sqrt{\frac{250}{|i|}},<br>$$<br>                where |i| is the number of unique dates in the test set. The utility is                 then defined as:<br>$$<br>u = min(max(t,0), 6)  \sum p_i.<br>$$</p>
<blockquote>
<p> <a target="_blank" rel="noopener" href="https://www.kaggle.com/renataghisloti/understanding-the-utility-score-function">https://www.kaggle.com/renataghisloti/understanding-the-utility-score-function</a></p>
</blockquote>
<ul>
<li><p>_Pi_ </p>
<p>Each row or trading opportunity can be chosen (action == 1) or not (action == 0). </p>
<p>The variable _pi_ is a indicator for each day _i_, showing how much return we got for that day.</p>
<p>Since we want to maximize u, we also want to maximize _pi_. To do that, we have to select the least amount of negative <em>resp</em> values as possible (since this is the only negative value in my equation and only value that would make the total sum of p going down) and maximize the positive number of positive <em>resp</em> transactions we select.</p>
</li>
<li><p><em><code>t</code></em> </p>
<p><strong>_t_</strong> is <strong>larger</strong> when the return for <strong>each day is better distributed and has lower variation.</strong> It is better to have returns uniformly divided among days than have all of your returns concentrated in just one day. It reminds me a little of a <strong>_L1_</strong> over <strong>_L2_</strong> situation, where the <strong>_L2_</strong> norm penalizes outliers more than <strong>_L1_</strong>.</p>
</li>
</ul>
<p>  Basically, we want to select uniformly distributed distributed returns over days, maximizing our return but giving a penalty on choosing too many dates.</p>
<ul>
<li>t is simply the annualized sharpe ratio assuming that there are 250 trading days in a year, an important risk adjusted performance measure in investing. If sharpe ratio is negative, utility is zero. A sharpe ratio higher than 6 is very unlikely, so it is capped at 6. The utility function overall try to maximize the product of sharpe ratio and total return.</li>
</ul>
<hr>
<h2 id="2-Implementing"><a href="#2-Implementing" class="headerlink" title="2. Implementing."></a>2. Implementing.</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning">https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning</a></p>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners">https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners</a></p>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/smilewithme/jane-street-eda-of-day-0-and-feature-importance/edit">https://www.kaggle.com/smilewithme/jane-street-eda-of-day-0-and-feature-importance/edit</a></p>
</blockquote>
<h3 id="2-0-Preprocessing"><a href="#2-0-Preprocessing" class="headerlink" title="2.0 Preprocessing"></a>2.0 Preprocessing</h3><h3 id="2-1-EDA"><a href="#2-1-EDA" class="headerlink" title="2.1. EDA"></a>2.1. EDA</h3><p><strong>Market Basics:</strong> Financial market is a dynamic world where investors, speculators, traders, hedgers understand the market by different strategies and use the opportunities to make profit. They may use fundamental, technical analysis, sentimental analysis,etc. to place their bet. As data is growing, many professionals use data to understand and analyze previous trends and predict the future prices to book profit.</p>
<p><strong>Competition Description:</strong> The dataset provided contains set of features, <strong>feature_{0â€¦129}</strong>,representing real stock market data. Each row in the dataset represents a trading opportunity, for which we will be predicting an action value: <strong>1</strong> to make the trade and <strong>0</strong> to pass on it. </p>
<p>Each trade has an associated weight and resp, which together represents a return on the trade. In the training set, <strong>train.csv</strong>, you are provided a <strong>resp</strong> value, as well as several other <strong>resp_{1,2,3,4}</strong> values that represent returns over different time horizons.</p>
<p>In <strong>Test set</strong> we donâ€™t have <strong>resp</strong> value, and other <strong>resp_{1,2,3,4}</strong> data, so we have to use only <strong>feature_{0â€¦129}</strong> to make prediction.</p>
<p>Trades with <strong>weight = 0</strong> were intentionally included in the dataset for completeness, although such trades <strong>will not</strong> contribute towards the scoring evaluation. So we will ignore it.</p>
<h3 id="2-2-Using-XGBoost-Algorithm"><a href="#2-2-Using-XGBoost-Algorithm" class="headerlink" title="2.2 Using XGBoost Algorithm"></a>2.2 Using XGBoost Algorithm</h3><h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost?"></a>XGBoost?</h1><p>it is contents of ensemble learning.</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=VHky3d_qZ_E">https://www.youtube.com/watch?v=VHky3d_qZ_E</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=1OEeguDBsLU&amp;list=PL23__qkpFtnPHVbPnOm-9Br6119NMkEDE&amp;index=4">https://www.youtube.com/watch?v=1OEeguDBsLU&amp;list=PL23__qkpFtnPHVbPnOm-9Br6119NMkEDE&amp;index=4</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4Jz4_IOgS4c">https://www.youtube.com/watch?v=4Jz4_IOgS4c</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=VkaZXGknN3g&amp;feature=youtu.be">https://www.youtube.com/watch?v=VkaZXGknN3g&amp;feature=youtu.be</a></p>
<p><a target="_blank" rel="noopener" href="https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost">https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost</a></p>
</blockquote>
<h2 id="2-2-1-Ensemble-learning"><a href="#2-2-1-Ensemble-learning" class="headerlink" title="2.2.1 Ensemble learning"></a>2.2.1 Ensemble learning</h2><p>x - dataset, y - error rate, each color is algorithm.</p>
<ul>
<li>No Free Lunch Theorem.<ul>
<li>any classification method cannot be superior or inferior overall</li>
</ul>
</li>
</ul>
<p>Ensemble means harmony or unity.</p>
<p>When we predict the value of a data, we use one model. But if we learn several models in harmony and use their predictions, weâ€™ll get a more accurate estimate.</p>
<p>Ensemble learning is a machine learning technique that combines multiple decision trees to perform better than a single decision tree. The key to ensemble learning is to combine several weak classifiers to create a Strong Classifier. This improves the accuracy of the model.</p>
<h3 id="2-2-1-1-Bagging"><a href="#2-2-1-1-Bagging" class="headerlink" title="2.2.1.1 Bagging"></a>2.2.1.1 Bagging</h3><p>Bagging is Bootstrap Aggregation. Bagging is a method of aggregating results by taking samples multiple times (Bootstrap) each model.</p>
<p><a target="_blank" rel="noopener" href="https://i.loli.net/2021/02/05/m4nEZyg3qUCu1Da.png" title="bagging" class="gallery-item" style="box-shadow: none;"> <img src="https://i.loli.net/2021/02/05/m4nEZyg3qUCu1Da.png" alt="bagging"></a></p>
<p>First, bootstrap from the data. (Restore random sampling) Examine the bootstrap data to learn the model. It aggregates the results of the learned model to obtain the final result value.</p>
<p>Categorical data aggregates results in Voting, and Continuous data is averaged.</p>
<p>When itâ€™s categorical data, voting means that the highest number of values predicted by the overall model is chosen as the final prediction. Letâ€™s say there are six crystal tree models. If you predicted four as A, and two as B, four models will predict A as the final result by voting.</p>
<p>Aggregating by means literally means that each decision tree model averages the predicted values to determine the predicted values of the final bagging model.</p>
<p>Bagging is a simple yet powerful method. Random Forest is representative model of using bagging.</p>
<h3 id="2-2-1-2-Boosting"><a href="#2-2-1-2-Boosting" class="headerlink" title="2.2.1.2 Boosting"></a>2.2.1.2 Boosting</h3><p>Boosting is a method of making weak classifiers into strong classifiers using weights. The Bagging predicts results independently of the Deicison Tree1 and Decision Tree2. This is how multiple independent decision trees predict the values, and then aggregate the resulting values to predict the final outcome. </p>
<p>Boosting, however, takes place between models. When the first model predicts, the data is weighted according to its prediction results, and the weights given affect the next model. Repeat the steps for creating new classification rules by focusing on misclassified data.</p>
<p><a target="_blank" rel="noopener" href="https://blog.kakaocdn.net/dn/kCejr/btqyghvqEZB/9o3rKTEsuSIDHEfelYFJlk/img.png" class="gallery-item" style="box-shadow: none;"> <img src="https://blog.kakaocdn.net/dn/kCejr/btqyghvqEZB/9o3rKTEsuSIDHEfelYFJlk/img.png" alt=""></a></p>
<h2 id="2-2-2-Different-of-Bagging-and-Boosting"><a href="#2-2-2-Different-of-Bagging-and-Boosting" class="headerlink" title="2.2.2 Different of Bagging and Boosting"></a>2.2.2 Different of Bagging and Boosting</h2><p><a target="_blank" rel="noopener" href="https://i.loli.net/2021/02/05/slDmWHtPY3quxfp.png" class="gallery-item" style="box-shadow: none;"> <img src="https://i.loli.net/2021/02/05/slDmWHtPY3quxfp.png" alt=""></a></p>
<p>Bagging is learned in parallel, while boosting is learned sequentially. After learning once, weights are given according to the results. Such weights affect the prediction of the results of the following models.</p>
<p>High weights are given for incorrect answers and low weights for correct answers. This allows you to focus more on the wrong answers in order to get them right.</p>
<p>Boosting has fewer errors compared to bagging. That is, performance is good performance. However, the speed is slow and there is a possibility of over fitting. So, which one should you choose between bagging or boosting when you actually use it? It depends on the situation. If the low performance of the individual decision tree is a problem, boosting is a good idea, or over-fitting problem bagging is a good idea.</p>
<h1 id="2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting"><a href="#2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting" class="headerlink" title="2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)"></a>2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)</h1><p><a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/">https://xgboost.readthedocs.io/en/latest/</a></p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d">https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d</a></p>
<p>Optimized Gradient Boosting algorithm through parallel processing, tree-pruning, handling missing values and regularization to avoid over-fitting/bias</p>
<p><strong>XGBoost = GBM + Regularization</strong></p>
<p>Red part is Regularization term.</p>
<p>T : weak learner(node)</p>
<p>w : node score</p>
<p>Therefore, it can be seen that the regularization term of XGboost prevents overfitting by giving penalty to loss as the tree complexity increases.</p>
<h2 id="2-2-1-Split-finding-Algorithm"><a href="#2-2-1-Split-finding-Algorithm" class="headerlink" title="2.2.1 Split finding Algorithm"></a>2.2.1 Split finding Algorithm</h2><ul>
<li><p>Basing exact greedy algorithm</p>
<ul>
<li>Pros: <strong>Always find the optimal split</strong> point because it enumerates over <em>all possible</em> splitting points greedily</li>
<li>Cons: <ul>
<li>Impossible to efficiently do so when the data does not fit entirely into memory</li>
<li>Cannot be done under a distributed setting</li>
</ul>
</li>
</ul>
</li>
<li><p>Approximate algorithm</p>
<ul>
<li>Example<ul>
<li>Assume that the value is sorted in an ascending order</li>
<li>Divide the dataset into 10 buckets</li>
<li>Global variant(Per tree) vs Local variant(per split)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-2-2-Sparsity-Aware-Split-Finding"><a href="#2-2-2-Sparsity-Aware-Split-Finding" class="headerlink" title="2.2.2 Sparsity-Aware Split Finding"></a>2.2.2 Sparsity-Aware Split Finding</h2><ul>
<li>In many real-world Problems, it is quite common for the input x to be sparse<ul>
<li>presence of missing values in the data</li>
<li>frequent zero entries in the statistics</li>
<li>artifacts of feature engineering such as one-hot encoding</li>
</ul>
</li>
<li>Solution : Set the default direction that is learned from the data</li>
</ul>
<h2 id="2-2-3-System-Design-for-Efficient-Computing"><a href="#2-2-3-System-Design-for-Efficient-Computing" class="headerlink" title="2.2.3 System Design for Efficient Computing"></a>2.2.3 System Design for Efficient Computing</h2><ul>
<li>The most time-consuming part of tree learning<ul>
<li>to get the data into sorted order</li>
</ul>
</li>
<li>XGBoost propose to store the data in in-memory units called block<ul>
<li>Data in each block is stored in the compressed column(CSC) format, with each column sorted by the corresponding feature value</li>
<li>This input data layout only needs to be computed once before training and can be reused in later iterations.</li>
</ul>
</li>
<li>Cache-aware access<ul>
<li>For the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm</li>
<li>For approximate algorithms, we solve the problem by choosing a correct block size</li>
</ul>
</li>
<li>Out-of-core computing<ul>
<li>Besides processors and memory, it is important to utilize disk space to handle data that does not fit into main memory</li>
<li>To enable out-of-core computation, the data is divided into multiple blocks and store each block on disk</li>
<li>To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk</li>
<li>It is important to reduce the overhead and increase the throughout of disk IO</li>
</ul>
</li>
<li>Block Compression<ul>
<li>The block is compressed by columns and decompressed on the fly by an independent thread when loading into main memory</li>
<li>This helps to trade some of the computation in decompression with the disk reading cost</li>
</ul>
</li>
<li>Block Sharding<ul>
<li>A pre-fetcher thread is assigned to each disk and fetches the data into an in-memory buffer</li>
<li>The training thread then alternatively reads the data from each buffer.</li>
<li>This helps to increase the throughput of disk reading when multiple disks are available.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&#x27;/kaggle/input/jane-street-market-prediction/train.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_memory_usage</span>(<span class="params">df</span>):</span></span><br><span class="line">    </span><br><span class="line">    start_memory = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Memory usage of dataframe is <span class="subst">&#123;start_memory&#125;</span> MB&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtype</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="string">&#x27;object&#x27;</span>:</span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    end_memory = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Memory usage of dataframe after reduction <span class="subst">&#123;end_memory&#125;</span> MB&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Reduced by <span class="subst">&#123;<span class="number">100</span> * (start_memory - end_memory) / start_memory&#125;</span> % &quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>
<h3 id="Importing-dataset"><a href="#Importing-dataset" class="headerlink" title="Importing dataset"></a>Importing dataset</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = reduce_memory_usage(train)</span><br></pre></td></tr></table></figure>
<h2 id="1-import-library"><a href="#1-import-library" class="headerlink" title="1) import library"></a>1) import library</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># This Python 3 environment comes with many helpful analytics libraries installed</span><br><span class="line"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span><br><span class="line"># For example, here&#x27;s several helpful packages to load</span><br><span class="line"></span><br><span class="line">#import numpy as np # linear algebra</span><br><span class="line">#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import sklearn</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">import xgboost as xgb</span><br><span class="line">import optuna</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"># Input data files are available in the read-only &quot;../input/&quot; directory</span><br><span class="line"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">for dirname, _, filenames in os.walk(&#x27;/kaggle/input&#x27;):</span><br><span class="line">    for filename in filenames:</span><br><span class="line">        print(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span><br><span class="line"># You can also write temporary files to /kaggle/temp/, but they won&#x27;t be saved outside of the current session</span><br></pre></td></tr></table></figure>
<h2 id="2-load-and-clean-dataset"><a href="#2-load-and-clean-dataset" class="headerlink" title="2) load and clean dataset"></a>2) load and clean dataset</h2><h3 id="Cleaning-data"><a href="#Cleaning-data" class="headerlink" title="Cleaning data"></a>Cleaning data</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># This Python 3 environment comes with many helpful analytics libraries installed</span><br><span class="line"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span><br><span class="line"># For example, here&#x27;s several helpful packages to load</span><br><span class="line"></span><br><span class="line">#import numpy as np # linear algebra</span><br><span class="line">#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import sklearn</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">import xgboost as xgb</span><br><span class="line">import optuna</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"># Input data files are available in the read-only &quot;../input/&quot; directory</span><br><span class="line"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">for dirname, _, filenames in os.walk(&#x27;/kaggle/input&#x27;):</span><br><span class="line">    for filename in filenames:</span><br><span class="line">        print(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span><br><span class="line"># You can also write temporary files to /kaggle/temp/, but they won&#x27;t be saved outside of the current session</span><br></pre></td></tr></table></figure>
</div>
    </div>

    <div class="post-meta">
        <i>
        
            <span>2021-02-05</span>
            
                <span>è¯¥ç¯‡æ–‡ç« è¢« Yann</span>
            
            
                <span>æ‰“ä¸Šæ ‡ç­¾:
                    
                    
                        <a href='/tags/%E9%87%8F%E5%8C%96/'>
                            é‡åŒ–
                        </a>
                    
                        <a href='/tags/kaggle/'>
                            kaggle
                        </a>
                    
                </span>
             
             
        
        </i>
    </div>
    <br>
    
    
        
            
    
            <div class="post-footer-pre-next">
                
                    <span>ä¸Šä¸€ç¯‡ï¼š<a href='/2021/20210314/'>ã€Šç»„åˆä¼˜åŒ–ã€‹éƒ¨åˆ†ç»“æœæ•´ç†-1</a></span>
                

                
                    <span class="post-footer-pre-next-last-span-right">ä¸‹ä¸€ç¯‡ï¼š<a href="/2020/20201221/">Autoware</a>
                    </span>
                
            </div>
    
        
    

    
        

     
</div>



                                      
                    
                    
                    <div class="footer">
    
        <span> 
            Â© 1949-2025 China 

            
                

            
        </span>
       
    
</div>



<!--è¿™æ˜¯æŒ‡ä¸€æ¡çº¿å¾€ä¸‹çš„å†…å®¹-->
<div class="footer-last">
    
            <span>ğŸŒŠçœ‹è¿‡å¤§æµ·çš„äººä¸ä¼šå¿˜è®°æµ·çš„å¹¿é˜”ğŸŒŠ</span>
            
                <span class="footer-last-span-right"><i>æœ¬ç«™ç”±<a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/index.html">Hexo</a>é©±åŠ¨ï½œä½¿ç”¨<a target="_blank" rel="noopener" href="https://github.com/HiNinoJay/hexo-theme-A4">Hexo-theme-A4</a>ä¸»é¢˜</i></span>
            
    
</div>


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

    <!--ç›®å½•-->
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tocify/1.9.0/javascripts/jquery.tocify.min.js" type="text/javascript" ></script>
        
<script src="/js/toc.js"></script>

    

    
<script src="/js/randomHeaderContent.js"></script>

    <!--å›åˆ°é¡¶éƒ¨æŒ‰é’®-->
    
        
<script src="/js/returnToTop.js"></script>

    

    
        
<script src="/js/returnToLastPage.js"></script>

    





<script src="/js/lightgallery/lightgallery.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-thumbnail.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-fullscreen.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-autoplay.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-zoom.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-rotate.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-paper.umd.min.js"></script>




<script type="text/javascript">
     
    if (typeof lightGallery !== "undefined") {
        var options1 = {
            selector: '.gallery-item',
            plugins: [lgThumbnail, lgFullscreen, lgAutoplay, lgZoom, lgRotate, lgPager], // å¯ç”¨æ’ä»¶
            thumbnail: true,          // æ˜¾ç¤ºç¼©ç•¥å›¾
            zoom: true,               // å¯ç”¨ç¼©æ”¾åŠŸ
            rotate: true,             // å¯ç”¨æ—‹è½¬åŠŸèƒ½èƒ½
            autoplay: true,        // å¯ç”¨è‡ªåŠ¨æ’­æ”¾åŠŸèƒ½
            fullScreen: true,      // å¯ç”¨å…¨å±åŠŸèƒ½
            pager: false, //é¡µç ,
            zoomFromOrigin: true,   // ä»åŸå§‹ä½ç½®ç¼©æ”¾
            actualSize: true,       // å¯ç”¨æŸ¥çœ‹å®é™…å¤§å°çš„åŠŸèƒ½
            enableZoomAfter: 300,    // å»¶è¿Ÿç¼©æ”¾ï¼Œç¡®ä¿å›¾ç‰‡åŠ è½½å®Œæˆåå¯ç¼©æ”¾
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options1); // ä¿®å¤é€‰æ‹©å™¨
    }
    
</script>


    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> 

                </div>
            
            
                <!-- å›åˆ°é¡¶éƒ¨çš„æŒ‰é’®-->  
                <div class="progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
            
                <!-- è¿”å›çš„æŒ‰é’®-->  
                <div class="return-to-last-progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
    </body>
</html>
<script src="/js/emojiHandler.js"></script>
<script>
  document.addEventListener('DOMContentLoaded', () => {
    wrapEmojis('.paper');
  });
</script>