<!DOCTYPE html>
<html lang="zh-CN">
    
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <meta name="description" content="Jane Street日内高频交易预测" />
    <meta name="hexo-theme-A4" content="v1.9.8" />
    <link rel="alternate icon" type="image/webp" href="/img/20250602032635.jpg">
    <title>Yann | 我愿做你光华中淡淡的一笔</title>

    
        
<link rel="stylesheet" href="/css/highlight/style1.css">

        
<link rel="stylesheet" href="/css/reset.css">

        
<link rel="stylesheet" href="/css/markdown.css">

        
<link rel="stylesheet" href="/css/fonts.css">
 
         <!--注意：首页既不是post也不是page-->
        
        
        
<link rel="stylesheet" href="/css/ui.css">
 
        
<link rel="stylesheet" href="/css/style.css">


        
            <!--返回顶部css-->
            
<link rel="stylesheet" href="/css/returnToTop.css">

            
<link rel="stylesheet" href="/css/unicons.css">

        
        
            <!--目录-->
            
<link rel="stylesheet" href="/css/toc.css">

        
    

    
        
<link rel="stylesheet" href="/css/returnToLastPage.css">

    
    
   
<link rel="stylesheet" href="/css/lightgallery-bundle.min.css">


   
        
<link rel="stylesheet" href="/css/custom.css">

    

<meta name="generator" content="Hexo 5.4.2"></head>
    
    

    
    



    
        <style>
            .paper-main{
                max-width:  1200px;
            }
        </style>

    
    




    
    <style>
        :root {
            --waline-theme-color: #323e74; 
            --waline-color: #323e74; 
            --waline-border-color: #323e74; 
            --waline-white: #323e74; 
            --waline-bgcolor-light: #f2fafc;  
        }
        body {
            color: #323e74;
            background: #eaeae8;
        }
        .post-md code {
            background: #e7f7f3;
            color: #7f688d; 
        }
        .post-md pre, .post-md .highlight {
            background: #e7f7f3;
            color: #7f688d; 
        }
        pre .string, pre .value, pre .inheritance, pre .header, pre .ruby .symbol, pre .xml .cdata {
            color: #323e74;
        }
        pre .number, pre .preprocessor, pre .built_in, pre .literal, pre .params, pre .constant {
            color: #323e74;
        }
        .year-font-color {
            color: #323e74 !important;
        }
        .wl-card span.wl-nick {
            color: #323e74; 
        }
        .wl-card .wl-badge {
            border: 1px solid #323e74;
            color: #323e74; 
        }
        .wl-btn {
            border: 1px solid #323e74; 
            color:  #323e74;  
        }
        .wl-btn.primary {
            color: #f2fafc; 
        }
        .wl-header label {
            color: #323e74;
        }
        a {
            color: #7f688d;
        }

        .post-md a {
            color: #7f688d;
        }

        .nav li a {
            color: #7f688d;
        }

        .archive-main a:link {
            color: #7f688d;
        }
        .archive-main a:visited {
            color: #767c7c; 
        }

        .archive li span {
            color: #323e74;
        }

        .post-main-title {
            color: #323e74;
        }

        .post-md h1,
        .post-md h2,
        .post-md h3,
        .post-md h4,
        .post-md h5,
        .post-md h6 {
            color: #323e74;
        }

        [data-waline] p {
            color: #323e74;
        }
        [data-waline] a {
            color: #323e74;
        } 
        .wl-sort li.active {
            color: #323e74;
        }

        .wl-card .wl-meta>span {
            background: #f2fafc;
        }

        .paper {
            background: #eaeae8;
        }

        .index-main {
            background: #f2fafc;
        }

        .paper-main {
            background: #f2fafc;
        }

        .wl-panel {
            background: #f2fafc;
        }

        .archive li:nth-child(odd) {
            background: #f2fafc;
            ;
        }

        .archive li:nth-child(even) {
            background: #f2fafc;
        }

        .post-md table tr:nth-child(odd) td {
            background: #f2fafc;
        }

        .post-md table tr:nth-child(even) td {
            background: #f2fafc;
        }

    
        .progress-wrap::after {
            color: #323e74; /* 箭头的颜色 */
        }
        .progress-wrap svg.progress-circle path {
	        stroke: #323e74; /* 边框的颜色 */
        }
        .progress-wrap::before {
	        background-image: linear-gradient(298deg, #7f688d, #7f688d); /* 鼠标滑过的箭头颜色 */
         }

        .return-to-last-progress-wrap::after {
            color: #323e74; /* 箭头的颜色 */
        }
        .return-to-last-progress-wrap svg.progress-circle path {
	        stroke: #323e74; /* 边框的颜色 */
        }
        .return-to-last-progress-wrap::before {
	        background-image: linear-gradient(298deg, #7f688d, #7f688d); /* 鼠标滑过的箭头颜色 */
         }

         .left-toc-container::-webkit-scrollbar-thumb {
            background-color: #323e74; /* 设置滚动条拖动块的颜色 */
        }

        .bs-docs-sidebar .nav>.active>a,
        .bs-docs-sidebar .nav>li>a:hover,
        .bs-docs-sidebar .nav>li>a:focus {
            color: #7f688d;
            border-left-color: #7f688d;
        }
        .bs-docs-sidebar .nav>li>a {
            color:  #323e74;
        }
    </style>

    
    <style>
        body {
            background-image: url(/img/3.jpg);
            background-attachment: fixed;  /* 背景固定，不随页面滚动 */
            background-repeat: no-repeat;  /* 防止背景图片重复 */
            background-size: cover;       /* 背景自适应大小，覆盖整个背景 */
            background-position: center;   /* 背景居中显示 */
        }
        .paper {
            background-image: url(/img/3.jpg);
            background-attachment: fixed;  /* 背景固定，不随页面滚动 */
            background-repeat: no-repeat;  /* 防止背景图片重复 */
            background-size: cover;       /* 背景自适应大小，覆盖整个背景 */
            background-position: center;   /* 背景居中显示 */
        }  
    </style>


    
    <body>
        <script src="/js/darkmode-js.min.js"></script>
        
        <script>
            const options = {
                bottom: '40px', // default: '32px'
                right: 'unset', // default: '32px'
                left: '42px', // default: 'unset'
                time: '0.3s', // default: '0.3s'
                mixColor: '#fff', // default: '#fff'
                backgroundColor: ' #eaeae8  ',  // default: '#fff'
                buttonColorDark: '#100f2c',  // default: '#100f2c'
                buttonColorLight: '#fff', // default: '#fff'
                saveInCookies: true, // default: true,
                label: '🌓', // default: ''
                autoMatchOsTheme: true // default: true
            }
            const darkmode = new Darkmode(options);
            darkmode.showWidget();
        </script>
        
        
            
                <div class="left-toc-container">
                    <nav id="toc" class="bs-docs-sidebar"></nav>
                </div>
            
        
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    


<div class="header">
    <div class="header-container">
        <style>
            .header-img {
                width: 56px;
                height: auto;
                object-fit: cover; /* 保持图片比例 */
                transition: transform 0.3s ease-in-out; 
                border-radius: 0; 
            }
            
        </style>
        <img 
            alt="^-^" 
            cache-control="max-age=86400" 
            class="header-img" 
            src="/img/20250602032635.jpg" 
        />
        <div class="header-content">
            <a class="logo" href="/">Yann</a> 
            <span class="description">人工智能、计算机、机器学习、linux、程序员</span> 
        </div>
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="/">首页</a></li>
            
        
            
                <li><a href="/list/">文章</a></li>
            
        
            
                <li><a href="/about/">关于</a></li>
            
        
            
                <li><a href="/tags/">标签</a></li>
            
        
            
                <li><a href="/categories/">分类</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--说明是文章post页面-->
                    
                        <div class="post-main">
    

    
        
            
                <div class="post-main-title" style="text-align: center;">
                    Jane Street日内高频交易预测
                </div>
            
        
      
    

    

        
            <div class="post-head-meta-center">
        
                
                    <span>最近更新：2021-02-05</span> 
                
                
                    
                        &nbsp; | &nbsp;
                    
                     <span>字数总计：2.4k</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span>阅读估时：15分钟</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span id="busuanzi_container_page_pv">
                        阅读量：<span id="busuanzi_value_page_pv"></span>次
                    </span>
                
            </div>
    

    <div class="post-md">
        
            
                <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-Overview"><span class="post-toc-text">1. Overview</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-1-Description"><span class="post-toc-text">1.1 Description</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-2-Evaluation"><span class="post-toc-text">1.2 Evaluation</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-Implementing"><span class="post-toc-text">2. Implementing.</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-0-Preprocessing"><span class="post-toc-text">2.0 Preprocessing</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-1-EDA"><span class="post-toc-text">2.1. EDA</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-Using-XGBoost-Algorithm"><span class="post-toc-text">2.2 Using XGBoost Algorithm</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#XGBoost"><span class="post-toc-text">XGBoost?</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-1-Ensemble-learning"><span class="post-toc-text">2.2.1 Ensemble learning</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-1-1-Bagging"><span class="post-toc-text">2.2.1.1 Bagging</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-1-2-Boosting"><span class="post-toc-text">2.2.1.2 Boosting</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-2-Different-of-Bagging-and-Boosting"><span class="post-toc-text">2.2.2 Different of Bagging and Boosting</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting"><span class="post-toc-text">2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-1-Split-finding-Algorithm"><span class="post-toc-text">2.2.1 Split finding Algorithm</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-2-Sparsity-Aware-Split-Finding"><span class="post-toc-text">2.2.2 Sparsity-Aware Split Finding</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-2-3-System-Design-for-Efficient-Computing"><span class="post-toc-text">2.2.3 System Design for Efficient Computing</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Importing-dataset"><span class="post-toc-text">Importing dataset</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-import-library"><span class="post-toc-text">1) import library</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-load-and-clean-dataset"><span class="post-toc-text">2) load and clean dataset</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Cleaning-data"><span class="post-toc-text">Cleaning data</span></a></li></ol></li></ol>
            
        
        <div class=".article-gallery"><blockquote>
<p>记录一次kaggle比赛，这次比赛的场景主要是Jane Street日内高频交易，是短线模型，使用夏普率衡量模型效果，但是提供的数据并非原始数据，不清楚feature含义。</p>
</blockquote>
<span id="more"></span> 
<h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><h3 id="1-1-Description"><a href="#1-1-Description" class="headerlink" title="1.1 Description"></a><strong>1.1 Description</strong></h3><p>In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their “fair values” and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.</p>
<p>Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. </p>
<h3 id="1-2-Evaluation"><a href="#1-2-Evaluation" class="headerlink" title="1.2 Evaluation"></a><strong>1.2 Evaluation</strong></h3><ul>
<li><p>Utility score</p>
<p>Each row in the test set represents a trading opportunity for which <strong>you will be predicting an <code>action</code> value, 1 to make the trade and 0 to pass on it.</strong> Each trade <code>j</code> has an associated <code>weight</code> and <code>resp</code>, which represents a return.</p>
</li>
</ul>
<p>$$<br>p_i = \sum_j(weight_{ij} <em> resp_{ij} </em> action_{ij}),<br>$$</p>
<p>$$<br>t = \frac{\sum p_i }{\sqrt{\sum p_i^2}} * \sqrt{\frac{250}{|i|}},<br>$$<br>                where |i| is the number of unique dates in the test set. The utility is                 then defined as:<br>$$<br>u = min(max(t,0), 6)  \sum p_i.<br>$$</p>
<blockquote>
<p> <a target="_blank" rel="noopener" href="https://www.kaggle.com/renataghisloti/understanding-the-utility-score-function">https://www.kaggle.com/renataghisloti/understanding-the-utility-score-function</a></p>
</blockquote>
<ul>
<li><p>_Pi_ </p>
<p>Each row or trading opportunity can be chosen (action == 1) or not (action == 0). </p>
<p>The variable _pi_ is a indicator for each day _i_, showing how much return we got for that day.</p>
<p>Since we want to maximize u, we also want to maximize _pi_. To do that, we have to select the least amount of negative <em>resp</em> values as possible (since this is the only negative value in my equation and only value that would make the total sum of p going down) and maximize the positive number of positive <em>resp</em> transactions we select.</p>
</li>
<li><p><em><code>t</code></em> </p>
<p><strong>_t_</strong> is <strong>larger</strong> when the return for <strong>each day is better distributed and has lower variation.</strong> It is better to have returns uniformly divided among days than have all of your returns concentrated in just one day. It reminds me a little of a <strong>_L1_</strong> over <strong>_L2_</strong> situation, where the <strong>_L2_</strong> norm penalizes outliers more than <strong>_L1_</strong>.</p>
</li>
</ul>
<p>  Basically, we want to select uniformly distributed distributed returns over days, maximizing our return but giving a penalty on choosing too many dates.</p>
<ul>
<li>t is simply the annualized sharpe ratio assuming that there are 250 trading days in a year, an important risk adjusted performance measure in investing. If sharpe ratio is negative, utility is zero. A sharpe ratio higher than 6 is very unlikely, so it is capped at 6. The utility function overall try to maximize the product of sharpe ratio and total return.</li>
</ul>
<hr>
<h2 id="2-Implementing"><a href="#2-Implementing" class="headerlink" title="2. Implementing."></a>2. Implementing.</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning">https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning</a></p>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners">https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners</a></p>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/smilewithme/jane-street-eda-of-day-0-and-feature-importance/edit">https://www.kaggle.com/smilewithme/jane-street-eda-of-day-0-and-feature-importance/edit</a></p>
</blockquote>
<h3 id="2-0-Preprocessing"><a href="#2-0-Preprocessing" class="headerlink" title="2.0 Preprocessing"></a>2.0 Preprocessing</h3><h3 id="2-1-EDA"><a href="#2-1-EDA" class="headerlink" title="2.1. EDA"></a>2.1. EDA</h3><p><strong>Market Basics:</strong> Financial market is a dynamic world where investors, speculators, traders, hedgers understand the market by different strategies and use the opportunities to make profit. They may use fundamental, technical analysis, sentimental analysis,etc. to place their bet. As data is growing, many professionals use data to understand and analyze previous trends and predict the future prices to book profit.</p>
<p><strong>Competition Description:</strong> The dataset provided contains set of features, <strong>feature_{0…129}</strong>,representing real stock market data. Each row in the dataset represents a trading opportunity, for which we will be predicting an action value: <strong>1</strong> to make the trade and <strong>0</strong> to pass on it. </p>
<p>Each trade has an associated weight and resp, which together represents a return on the trade. In the training set, <strong>train.csv</strong>, you are provided a <strong>resp</strong> value, as well as several other <strong>resp_{1,2,3,4}</strong> values that represent returns over different time horizons.</p>
<p>In <strong>Test set</strong> we don’t have <strong>resp</strong> value, and other <strong>resp_{1,2,3,4}</strong> data, so we have to use only <strong>feature_{0…129}</strong> to make prediction.</p>
<p>Trades with <strong>weight = 0</strong> were intentionally included in the dataset for completeness, although such trades <strong>will not</strong> contribute towards the scoring evaluation. So we will ignore it.</p>
<h3 id="2-2-Using-XGBoost-Algorithm"><a href="#2-2-Using-XGBoost-Algorithm" class="headerlink" title="2.2 Using XGBoost Algorithm"></a>2.2 Using XGBoost Algorithm</h3><h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost?"></a>XGBoost?</h1><p>it is contents of ensemble learning.</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=VHky3d_qZ_E">https://www.youtube.com/watch?v=VHky3d_qZ_E</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=1OEeguDBsLU&amp;list=PL23__qkpFtnPHVbPnOm-9Br6119NMkEDE&amp;index=4">https://www.youtube.com/watch?v=1OEeguDBsLU&amp;list=PL23__qkpFtnPHVbPnOm-9Br6119NMkEDE&amp;index=4</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4Jz4_IOgS4c">https://www.youtube.com/watch?v=4Jz4_IOgS4c</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=VkaZXGknN3g&amp;feature=youtu.be">https://www.youtube.com/watch?v=VkaZXGknN3g&amp;feature=youtu.be</a></p>
<p><a target="_blank" rel="noopener" href="https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost">https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost</a></p>
</blockquote>
<h2 id="2-2-1-Ensemble-learning"><a href="#2-2-1-Ensemble-learning" class="headerlink" title="2.2.1 Ensemble learning"></a>2.2.1 Ensemble learning</h2><p>x - dataset, y - error rate, each color is algorithm.</p>
<ul>
<li>No Free Lunch Theorem.<ul>
<li>any classification method cannot be superior or inferior overall</li>
</ul>
</li>
</ul>
<p>Ensemble means harmony or unity.</p>
<p>When we predict the value of a data, we use one model. But if we learn several models in harmony and use their predictions, we’ll get a more accurate estimate.</p>
<p>Ensemble learning is a machine learning technique that combines multiple decision trees to perform better than a single decision tree. The key to ensemble learning is to combine several weak classifiers to create a Strong Classifier. This improves the accuracy of the model.</p>
<h3 id="2-2-1-1-Bagging"><a href="#2-2-1-1-Bagging" class="headerlink" title="2.2.1.1 Bagging"></a>2.2.1.1 Bagging</h3><p>Bagging is Bootstrap Aggregation. Bagging is a method of aggregating results by taking samples multiple times (Bootstrap) each model.</p>
<p><a target="_blank" rel="noopener" href="https://i.loli.net/2021/02/05/m4nEZyg3qUCu1Da.png" title="bagging" class="gallery-item" style="box-shadow: none;"> <img src="https://i.loli.net/2021/02/05/m4nEZyg3qUCu1Da.png" alt="bagging"></a></p>
<p>First, bootstrap from the data. (Restore random sampling) Examine the bootstrap data to learn the model. It aggregates the results of the learned model to obtain the final result value.</p>
<p>Categorical data aggregates results in Voting, and Continuous data is averaged.</p>
<p>When it’s categorical data, voting means that the highest number of values predicted by the overall model is chosen as the final prediction. Let’s say there are six crystal tree models. If you predicted four as A, and two as B, four models will predict A as the final result by voting.</p>
<p>Aggregating by means literally means that each decision tree model averages the predicted values to determine the predicted values of the final bagging model.</p>
<p>Bagging is a simple yet powerful method. Random Forest is representative model of using bagging.</p>
<h3 id="2-2-1-2-Boosting"><a href="#2-2-1-2-Boosting" class="headerlink" title="2.2.1.2 Boosting"></a>2.2.1.2 Boosting</h3><p>Boosting is a method of making weak classifiers into strong classifiers using weights. The Bagging predicts results independently of the Deicison Tree1 and Decision Tree2. This is how multiple independent decision trees predict the values, and then aggregate the resulting values to predict the final outcome. </p>
<p>Boosting, however, takes place between models. When the first model predicts, the data is weighted according to its prediction results, and the weights given affect the next model. Repeat the steps for creating new classification rules by focusing on misclassified data.</p>
<p><a target="_blank" rel="noopener" href="https://blog.kakaocdn.net/dn/kCejr/btqyghvqEZB/9o3rKTEsuSIDHEfelYFJlk/img.png" class="gallery-item" style="box-shadow: none;"> <img src="https://blog.kakaocdn.net/dn/kCejr/btqyghvqEZB/9o3rKTEsuSIDHEfelYFJlk/img.png" alt=""></a></p>
<h2 id="2-2-2-Different-of-Bagging-and-Boosting"><a href="#2-2-2-Different-of-Bagging-and-Boosting" class="headerlink" title="2.2.2 Different of Bagging and Boosting"></a>2.2.2 Different of Bagging and Boosting</h2><p><a target="_blank" rel="noopener" href="https://i.loli.net/2021/02/05/slDmWHtPY3quxfp.png" class="gallery-item" style="box-shadow: none;"> <img src="https://i.loli.net/2021/02/05/slDmWHtPY3quxfp.png" alt=""></a></p>
<p>Bagging is learned in parallel, while boosting is learned sequentially. After learning once, weights are given according to the results. Such weights affect the prediction of the results of the following models.</p>
<p>High weights are given for incorrect answers and low weights for correct answers. This allows you to focus more on the wrong answers in order to get them right.</p>
<p>Boosting has fewer errors compared to bagging. That is, performance is good performance. However, the speed is slow and there is a possibility of over fitting. So, which one should you choose between bagging or boosting when you actually use it? It depends on the situation. If the low performance of the individual decision tree is a problem, boosting is a good idea, or over-fitting problem bagging is a good idea.</p>
<h1 id="2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting"><a href="#2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting" class="headerlink" title="2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)"></a>2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)</h1><p><a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/">https://xgboost.readthedocs.io/en/latest/</a></p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d">https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d</a></p>
<p>Optimized Gradient Boosting algorithm through parallel processing, tree-pruning, handling missing values and regularization to avoid over-fitting/bias</p>
<p><strong>XGBoost = GBM + Regularization</strong></p>
<p>Red part is Regularization term.</p>
<p>T : weak learner(node)</p>
<p>w : node score</p>
<p>Therefore, it can be seen that the regularization term of XGboost prevents overfitting by giving penalty to loss as the tree complexity increases.</p>
<h2 id="2-2-1-Split-finding-Algorithm"><a href="#2-2-1-Split-finding-Algorithm" class="headerlink" title="2.2.1 Split finding Algorithm"></a>2.2.1 Split finding Algorithm</h2><ul>
<li><p>Basing exact greedy algorithm</p>
<ul>
<li>Pros: <strong>Always find the optimal split</strong> point because it enumerates over <em>all possible</em> splitting points greedily</li>
<li>Cons: <ul>
<li>Impossible to efficiently do so when the data does not fit entirely into memory</li>
<li>Cannot be done under a distributed setting</li>
</ul>
</li>
</ul>
</li>
<li><p>Approximate algorithm</p>
<ul>
<li>Example<ul>
<li>Assume that the value is sorted in an ascending order</li>
<li>Divide the dataset into 10 buckets</li>
<li>Global variant(Per tree) vs Local variant(per split)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-2-2-Sparsity-Aware-Split-Finding"><a href="#2-2-2-Sparsity-Aware-Split-Finding" class="headerlink" title="2.2.2 Sparsity-Aware Split Finding"></a>2.2.2 Sparsity-Aware Split Finding</h2><ul>
<li>In many real-world Problems, it is quite common for the input x to be sparse<ul>
<li>presence of missing values in the data</li>
<li>frequent zero entries in the statistics</li>
<li>artifacts of feature engineering such as one-hot encoding</li>
</ul>
</li>
<li>Solution : Set the default direction that is learned from the data</li>
</ul>
<h2 id="2-2-3-System-Design-for-Efficient-Computing"><a href="#2-2-3-System-Design-for-Efficient-Computing" class="headerlink" title="2.2.3 System Design for Efficient Computing"></a>2.2.3 System Design for Efficient Computing</h2><ul>
<li>The most time-consuming part of tree learning<ul>
<li>to get the data into sorted order</li>
</ul>
</li>
<li>XGBoost propose to store the data in in-memory units called block<ul>
<li>Data in each block is stored in the compressed column(CSC) format, with each column sorted by the corresponding feature value</li>
<li>This input data layout only needs to be computed once before training and can be reused in later iterations.</li>
</ul>
</li>
<li>Cache-aware access<ul>
<li>For the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm</li>
<li>For approximate algorithms, we solve the problem by choosing a correct block size</li>
</ul>
</li>
<li>Out-of-core computing<ul>
<li>Besides processors and memory, it is important to utilize disk space to handle data that does not fit into main memory</li>
<li>To enable out-of-core computation, the data is divided into multiple blocks and store each block on disk</li>
<li>To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk</li>
<li>It is important to reduce the overhead and increase the throughout of disk IO</li>
</ul>
</li>
<li>Block Compression<ul>
<li>The block is compressed by columns and decompressed on the fly by an independent thread when loading into main memory</li>
<li>This helps to trade some of the computation in decompression with the disk reading cost</li>
</ul>
</li>
<li>Block Sharding<ul>
<li>A pre-fetcher thread is assigned to each disk and fetches the data into an in-memory buffer</li>
<li>The training thread then alternatively reads the data from each buffer.</li>
<li>This helps to increase the throughput of disk reading when multiple disks are available.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&#x27;/kaggle/input/jane-street-market-prediction/train.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_memory_usage</span>(<span class="params">df</span>):</span></span><br><span class="line">    </span><br><span class="line">    start_memory = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Memory usage of dataframe is <span class="subst">&#123;start_memory&#125;</span> MB&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtype</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="string">&#x27;object&#x27;</span>:</span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    end_memory = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Memory usage of dataframe after reduction <span class="subst">&#123;end_memory&#125;</span> MB&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Reduced by <span class="subst">&#123;<span class="number">100</span> * (start_memory - end_memory) / start_memory&#125;</span> % &quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>
<h3 id="Importing-dataset"><a href="#Importing-dataset" class="headerlink" title="Importing dataset"></a>Importing dataset</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = reduce_memory_usage(train)</span><br></pre></td></tr></table></figure>
<h2 id="1-import-library"><a href="#1-import-library" class="headerlink" title="1) import library"></a>1) import library</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># This Python 3 environment comes with many helpful analytics libraries installed</span><br><span class="line"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span><br><span class="line"># For example, here&#x27;s several helpful packages to load</span><br><span class="line"></span><br><span class="line">#import numpy as np # linear algebra</span><br><span class="line">#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import sklearn</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">import xgboost as xgb</span><br><span class="line">import optuna</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"># Input data files are available in the read-only &quot;../input/&quot; directory</span><br><span class="line"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">for dirname, _, filenames in os.walk(&#x27;/kaggle/input&#x27;):</span><br><span class="line">    for filename in filenames:</span><br><span class="line">        print(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span><br><span class="line"># You can also write temporary files to /kaggle/temp/, but they won&#x27;t be saved outside of the current session</span><br></pre></td></tr></table></figure>
<h2 id="2-load-and-clean-dataset"><a href="#2-load-and-clean-dataset" class="headerlink" title="2) load and clean dataset"></a>2) load and clean dataset</h2><h3 id="Cleaning-data"><a href="#Cleaning-data" class="headerlink" title="Cleaning data"></a>Cleaning data</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># This Python 3 environment comes with many helpful analytics libraries installed</span><br><span class="line"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span><br><span class="line"># For example, here&#x27;s several helpful packages to load</span><br><span class="line"></span><br><span class="line">#import numpy as np # linear algebra</span><br><span class="line">#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import sklearn</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">import xgboost as xgb</span><br><span class="line">import optuna</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"># Input data files are available in the read-only &quot;../input/&quot; directory</span><br><span class="line"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">for dirname, _, filenames in os.walk(&#x27;/kaggle/input&#x27;):</span><br><span class="line">    for filename in filenames:</span><br><span class="line">        print(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span><br><span class="line"># You can also write temporary files to /kaggle/temp/, but they won&#x27;t be saved outside of the current session</span><br></pre></td></tr></table></figure>
</div>
    </div>

    <div class="post-meta">
        <i>
        
            <span>2021-02-05</span>
            
                <span>该篇文章被 Yann</span>
            
            
                <span>打上标签:
                    
                    
                        <a href='/tags/%E9%87%8F%E5%8C%96/'>
                            量化
                        </a>
                    
                        <a href='/tags/kaggle/'>
                            kaggle
                        </a>
                    
                </span>
             
             
        
        </i>
    </div>
    <br>
    
    
        
            
    
            <div class="post-footer-pre-next">
                
                    <span>上一篇：<a href='/2021/20210314/'>《组合优化》部分结果整理-1</a></span>
                

                
                    <span class="post-footer-pre-next-last-span-right">下一篇：<a href="/2020/20201221/">Autoware</a>
                    </span>
                
            </div>
    
        
    

    
        

     
</div>



                                      
                    
                    
                    <div class="footer">
    
        <span> 
            © 1949-2025 China 

            
                

            
        </span>
       
    
</div>



<!--这是指一条线往下的内容-->
<div class="footer-last">
    
            <span>🌊看过大海的人不会忘记海的广阔🌊</span>
            
                <span class="footer-last-span-right"><i>本站由<a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/index.html">Hexo</a>驱动｜使用<a target="_blank" rel="noopener" href="https://github.com/HiNinoJay/hexo-theme-A4">Hexo-theme-A4</a>主题</i></span>
            
    
</div>


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

    <!--目录-->
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tocify/1.9.0/javascripts/jquery.tocify.min.js" type="text/javascript" ></script>
        
<script src="/js/toc.js"></script>

    

    
<script src="/js/randomHeaderContent.js"></script>

    <!--回到顶部按钮-->
    
        
<script src="/js/returnToTop.js"></script>

    

    
        
<script src="/js/returnToLastPage.js"></script>

    





<script src="/js/lightgallery/lightgallery.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-thumbnail.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-fullscreen.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-autoplay.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-zoom.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-rotate.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-paper.umd.min.js"></script>




<script type="text/javascript">
     
    if (typeof lightGallery !== "undefined") {
        var options1 = {
            selector: '.gallery-item',
            plugins: [lgThumbnail, lgFullscreen, lgAutoplay, lgZoom, lgRotate, lgPager], // 启用插件
            thumbnail: true,          // 显示缩略图
            zoom: true,               // 启用缩放功
            rotate: true,             // 启用旋转功能能
            autoplay: true,        // 启用自动播放功能
            fullScreen: true,      // 启用全屏功能
            pager: false, //页码,
            zoomFromOrigin: true,   // 从原始位置缩放
            actualSize: true,       // 启用查看实际大小的功能
            enableZoomAfter: 300,    // 延迟缩放，确保图片加载完成后可缩放
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options1); // 修复选择器
    }
    
</script>


    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> 

                </div>
            
            
                <!-- 回到顶部的按钮-->  
                <div class="progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
            
                <!-- 返回的按钮-->  
                <div class="return-to-last-progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
    </body>
</html>
<script src="/js/emojiHandler.js"></script>
<script>
  document.addEventListener('DOMContentLoaded', () => {
    wrapEmojis('.paper');
  });
</script>