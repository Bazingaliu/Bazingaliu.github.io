<!DOCTYPE html>
<html lang="zh-CN">
    
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <meta name="description" content="Some Virtual Try-on (VTON) Research" />
    <meta name="hexo-theme-A4" content="v1.9.8" />
    <link rel="alternate icon" type="image/webp" href="/img/20250602032635.jpg">
    <title>Yann | æˆ‘æ„¿åšä½ å…‰åä¸­æ·¡æ·¡çš„ä¸€ç¬”</title>

    
        
<link rel="stylesheet" href="/css/highlight/style1.css">

        
<link rel="stylesheet" href="/css/reset.css">

        
<link rel="stylesheet" href="/css/markdown.css">

        
<link rel="stylesheet" href="/css/fonts.css">
 
         <!--æ³¨æ„ï¼šé¦–é¡µæ—¢ä¸æ˜¯postä¹Ÿä¸æ˜¯page-->
        
        
        
<link rel="stylesheet" href="/css/ui.css">
 
        
<link rel="stylesheet" href="/css/style.css">


        
            <!--è¿”å›é¡¶éƒ¨css-->
            
<link rel="stylesheet" href="/css/returnToTop.css">

            
<link rel="stylesheet" href="/css/unicons.css">

        
        
            <!--ç›®å½•-->
            
<link rel="stylesheet" href="/css/toc.css">

        
    

    
        
<link rel="stylesheet" href="/css/returnToLastPage.css">

    
    
   
<link rel="stylesheet" href="/css/lightgallery-bundle.min.css">


   
        
<link rel="stylesheet" href="/css/custom.css">

    

<meta name="generator" content="Hexo 5.4.2"></head>
    
    

    
    



    

    
    




    
    <style>
        :root {
            --waline-theme-color: #323e74; 
            --waline-color: #323e74; 
            --waline-border-color: #323e74; 
            --waline-white: #323e74; 
            --waline-bgcolor-light: #f2fafc;  
        }
        body {
            color: #323e74;
            background: #eaeae8;
        }
        .post-md code {
            background: #e7f7f3;
            color: #7f688d; 
        }
        .post-md pre, .post-md .highlight {
            background: #e7f7f3;
            color: #7f688d; 
        }
        pre .string, pre .value, pre .inheritance, pre .header, pre .ruby .symbol, pre .xml .cdata {
            color: #323e74;
        }
        pre .number, pre .preprocessor, pre .built_in, pre .literal, pre .params, pre .constant {
            color: #323e74;
        }
        .year-font-color {
            color: #323e74 !important;
        }
        .wl-card span.wl-nick {
            color: #323e74; 
        }
        .wl-card .wl-badge {
            border: 1px solid #323e74;
            color: #323e74; 
        }
        .wl-btn {
            border: 1px solid #323e74; 
            color:  #323e74;  
        }
        .wl-btn.primary {
            color: #f2fafc; 
        }
        .wl-header label {
            color: #323e74;
        }
        a {
            color: #7f688d;
        }

        .post-md a {
            color: #7f688d;
        }

        .nav li a {
            color: #7f688d;
        }

        .archive-main a:link {
            color: #7f688d;
        }
        .archive-main a:visited {
            color: #767c7c; 
        }

        .archive li span {
            color: #323e74;
        }

        .post-main-title {
            color: #323e74;
        }

        .post-md h1,
        .post-md h2,
        .post-md h3,
        .post-md h4,
        .post-md h5,
        .post-md h6 {
            color: #323e74;
        }

        [data-waline] p {
            color: #323e74;
        }
        [data-waline] a {
            color: #323e74;
        } 
        .wl-sort li.active {
            color: #323e74;
        }

        .wl-card .wl-meta>span {
            background: #f2fafc;
        }

        .paper {
            background: #eaeae8;
        }

        .index-main {
            background: #f2fafc;
        }

        .paper-main {
            background: #f2fafc;
        }

        .wl-panel {
            background: #f2fafc;
        }

        .archive li:nth-child(odd) {
            background: #f2fafc;
            ;
        }

        .archive li:nth-child(even) {
            background: #f2fafc;
        }

        .post-md table tr:nth-child(odd) td {
            background: #f2fafc;
        }

        .post-md table tr:nth-child(even) td {
            background: #f2fafc;
        }

    
        .progress-wrap::after {
            color: #323e74; /* ç®­å¤´çš„é¢œè‰² */
        }
        .progress-wrap svg.progress-circle path {
	        stroke: #323e74; /* è¾¹æ¡†çš„é¢œè‰² */
        }
        .progress-wrap::before {
	        background-image: linear-gradient(298deg, #7f688d, #7f688d); /* é¼ æ ‡æ»‘è¿‡çš„ç®­å¤´é¢œè‰² */
         }

        .return-to-last-progress-wrap::after {
            color: #323e74; /* ç®­å¤´çš„é¢œè‰² */
        }
        .return-to-last-progress-wrap svg.progress-circle path {
	        stroke: #323e74; /* è¾¹æ¡†çš„é¢œè‰² */
        }
        .return-to-last-progress-wrap::before {
	        background-image: linear-gradient(298deg, #7f688d, #7f688d); /* é¼ æ ‡æ»‘è¿‡çš„ç®­å¤´é¢œè‰² */
         }

         .left-toc-container::-webkit-scrollbar-thumb {
            background-color: #323e74; /* è®¾ç½®æ»šåŠ¨æ¡æ‹–åŠ¨å—çš„é¢œè‰² */
        }

        .bs-docs-sidebar .nav>.active>a,
        .bs-docs-sidebar .nav>li>a:hover,
        .bs-docs-sidebar .nav>li>a:focus {
            color: #7f688d;
            border-left-color: #7f688d;
        }
        .bs-docs-sidebar .nav>li>a {
            color:  #323e74;
        }
    </style>

    
    <style>
        body {
            background-image: url(/img/3.jpg);
            background-attachment: fixed;  /* èƒŒæ™¯å›ºå®šï¼Œä¸éšé¡µé¢æ»šåŠ¨ */
            background-repeat: no-repeat;  /* é˜²æ­¢èƒŒæ™¯å›¾ç‰‡é‡å¤ */
            background-size: cover;       /* èƒŒæ™¯è‡ªé€‚åº”å¤§å°ï¼Œè¦†ç›–æ•´ä¸ªèƒŒæ™¯ */
            background-position: center;   /* èƒŒæ™¯å±…ä¸­æ˜¾ç¤º */
        }
        .paper {
            background-image: url(/img/3.jpg);
            background-attachment: fixed;  /* èƒŒæ™¯å›ºå®šï¼Œä¸éšé¡µé¢æ»šåŠ¨ */
            background-repeat: no-repeat;  /* é˜²æ­¢èƒŒæ™¯å›¾ç‰‡é‡å¤ */
            background-size: cover;       /* èƒŒæ™¯è‡ªé€‚åº”å¤§å°ï¼Œè¦†ç›–æ•´ä¸ªèƒŒæ™¯ */
            background-position: center;   /* èƒŒæ™¯å±…ä¸­æ˜¾ç¤º */
        }  
    </style>


    
    <body>
        <script src="/js/darkmode-js.min.js"></script>
        
        <script>
            const options = {
                bottom: '40px', // default: '32px'
                right: 'unset', // default: '32px'
                left: '42px', // default: 'unset'
                time: '0.3s', // default: '0.3s'
                mixColor: '#fff', // default: '#fff'
                backgroundColor: ' #eaeae8  ',  // default: '#fff'
                buttonColorDark: '#100f2c',  // default: '#100f2c'
                buttonColorLight: '#fff', // default: '#fff'
                saveInCookies: true, // default: true,
                label: 'ğŸŒ“', // default: ''
                autoMatchOsTheme: true // default: true
            }
            const darkmode = new Darkmode(options);
            darkmode.showWidget();
        </script>
        
        
            
                <div class="left-toc-container">
                    <nav id="toc" class="bs-docs-sidebar"></nav>
                </div>
            
        
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    


<div class="header">
    <div class="header-container">
        <style>
            .header-img {
                width: 56px;
                height: auto;
                object-fit: cover; /* ä¿æŒå›¾ç‰‡æ¯”ä¾‹ */
                transition: transform 0.3s ease-in-out; 
                border-radius: 0; 
            }
            
        </style>
        <img 
            alt="^-^" 
            cache-control="max-age=86400" 
            class="header-img" 
            src="/img/20250602032635.jpg" 
        />
        <div class="header-content">
            <a class="logo" href="/">Yann</a> 
            <span class="description">äººå·¥æ™ºèƒ½ã€è®¡ç®—æœºã€æœºå™¨å­¦ä¹ ã€linuxã€ç¨‹åºå‘˜</span> 
        </div>
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="/">é¦–é¡µ</a></li>
            
        
            
                <li><a href="/list/">æ–‡ç« </a></li>
            
        
            
                <li><a href="/about/">å…³äº</a></li>
            
        
            
                <li><a href="/tags/">æ ‡ç­¾</a></li>
            
        
            
                <li><a href="/categories/">åˆ†ç±»</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--è¯´æ˜æ˜¯æ–‡ç« posté¡µé¢-->
                    
                        <div class="post-main">
    

    
        
            
                <div class="post-main-title" style="text-align: center;">
                    Some Virtual Try-on (VTON) Research
                </div>
            
        
      
    

    

        
            <div class="post-head-meta-center">
        
                
                    <span>æœ€è¿‘æ›´æ–°ï¼š2020-10-29</span> 
                
                
                    
                        &nbsp; | &nbsp;
                    
                     <span>å­—æ•°æ€»è®¡ï¼š1.7k</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span>é˜…è¯»ä¼°æ—¶ï¼š10åˆ†é’Ÿ</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span id="busuanzi_container_page_pv">
                        é˜…è¯»é‡ï¼š<span id="busuanzi_value_page_pv"></span>æ¬¡
                    </span>
                
            </div>
    

    <div class="post-md">
        
            
                <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Clothing-dataset"><span class="post-toc-text">Clothing dataset</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Data"><span class="post-toc-text">Data</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Links"><span class="post-toc-text">Links</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Top-10-subset"><span class="post-toc-text">Top-10 subset</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Acknowledgements"><span class="post-toc-text">Acknowledgements</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Image-based-2D-Virtual-Try-on"><span class="post-toc-text">Image-based (2D) Virtual Try-on</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ACCV-2020"><span class="post-toc-text">ACCV 2020</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ECCV-2020"><span class="post-toc-text">ECCV 2020</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CVPR-2020"><span class="post-toc-text">CVPR 2020</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CVPRW-2020"><span class="post-toc-text">CVPRW 2020</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ICCV-2019"><span class="post-toc-text">ICCV 2019</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ICCVW-2019"><span class="post-toc-text">ICCVW 2019</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ECCV-2018"><span class="post-toc-text">ECCV 2018</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CVPR-2018"><span class="post-toc-text">CVPR 2018</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Others"><span class="post-toc-text">Others</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3D-Virtual-Try-on"><span class="post-toc-text">3D Virtual Try-on</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ECCV-2020-1"><span class="post-toc-text">ECCV 2020</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CVPR-2020-1"><span class="post-toc-text">CVPR 2020</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ICCV-2019-1"><span class="post-toc-text">ICCV 2019</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ECCV-2018-1"><span class="post-toc-text">ECCV 2018</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CVPR-2018-1"><span class="post-toc-text">CVPR 2018</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Others-1"><span class="post-toc-text">Others</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Multi-Pose-Guided-Virtual-Try-on"><span class="post-toc-text">Multi-Pose Guided Virtual Try-on</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Video-Virtual-Try-on"><span class="post-toc-text">Video Virtual Try-on</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Non-clothing-Virtual-Try-on"><span class="post-toc-text">Non-clothing Virtual Try-on</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Pose-Guided-Human-Synthesis"><span class="post-toc-text">Pose-Guided Human Synthesis</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Datasets-for-Virtual-Try-on"><span class="post-toc-text">Datasets for Virtual Try-on</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Related-Conference-Workshops"><span class="post-toc-text">Related Conference Workshops</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Related-Repositories"><span class="post-toc-text">Related Repositories</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Pull-requests-are-welcome"><span class="post-toc-text">Pull requests are welcome!</span></a></li></ol></li></ol></li></ol>
            
        
        <div class=".article-gallery"><blockquote>
<p>æœ€è¿‘è·Ÿé¡¹ç›®åœ¨åšä¸€äº›ï¼Œè™šæ‹Ÿè¯•è¡£Virtual Try-on (VTONï¼‰çš„å·¥ä½œï¼Œè®°å½•ä¸€ä¸‹è°ƒç ”çš„æ•°æ®ä»¥åŠå¼€æºçš„è®ºæ–‡ä»¥åŠæ¨¡å‹ã€‚</p>
</blockquote>
<span id="more"></span>
<h2 id="Clothing-dataset"><a href="#Clothing-dataset" class="headerlink" title="Clothing dataset"></a>Clothing dataset</h2><p>Over 5,000 images of 20 different classes.</p>
<p>This dataset can be freely used for any purpose, including commercial:</p>
<p>For example:</p>
<ul>
<li>Creating a tutorial or a course (free or paid)</li>
<li>Writing a book</li>
<li>Kaggle competitions (as an external dataset)</li>
<li>Training an internal model at any company</li>
</ul>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>The <code>images.csv</code> file contains:</p>
<ul>
<li><code>image</code> - the ID of the image (use it to load the image from <code>images/&lt;ID&gt;.jpg</code>)</li>
<li><code>sender_id</code> - the ID of a person who contributed the image</li>
<li><code>label</code> - the class of the image</li>
<li><code>kids</code> - flag, <code>True</code> if itâ€™s clothes for kids</li>
</ul>
<h3 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h3><ul>
<li>If youâ€™re looking for a subset of the clothing dataset, check here: <a target="_blank" rel="noopener" href="https://github.com/alexeygrigorev/clothing-dataset-small">https://github.com/alexeygrigorev/clothing-dataset-small</a></li>
<li>You can read more about this dataset here: <a target="_blank" rel="noopener" href="https://medium.com/data-science-insider/clothing-dataset-5b72cd7c3f1f">https://medium.com/data-science-insider/clothing-dataset-5b72cd7c3f1f</a></li>
<li>This dataset is also awailable on Kaggle (with images in higher resolution): </li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.kaggle.com/agrigorev/clothing-dataset-full/  </span><br></pre></td></tr></table></figure>
<h3 id="Top-10-subset"><a href="#Top-10-subset" class="headerlink" title="Top-10 subset"></a>Top-10 subset</h3><p>Images of some classes donâ€™t appear very often. Training a neural network to predict these classes is quite difficult â€” we need at least 100-200 images of each class to make a meaningful model.</p>
<p>Thatâ€™s why, for educational purposes, we created a subset of the full dataset that covers only the top-10 classes.</p>
<p>Check it here: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/alexeygrigorev/clothing-dataset-small</span><br></pre></td></tr></table></figure>
<p>Examples</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.kaggle.com/agrigorev/collage</span><br></pre></td></tr></table></figure>
<p>Do you use this dataset somewhere? Please submit a PR with a link</p>
<h3 id="Acknowledgements"><a href="#Acknowledgements" class="headerlink" title="Acknowledgements"></a>Acknowledgements</h3><p>Weâ€™d like to thank</p>
<ul>
<li><p>Kenes Shangereyev and Tagias.com for helping with 3000 images</p>
</li>
<li><p>All the 32 people who contributed their images to the dataset via the forms:</p>
</li>
<li><ul>
<li>Patricia Goldberg</li>
</ul>
</li>
<li><pre><code>https://www.linkedin.com/in/patricia-goldberg/
</code></pre></li>
<li><p>Everyone who supported the initiative by engaging with the announcements on social media</p>
</li>
</ul>
<p>It wouldnâ€™t be possible to collect this dataset without your help!</p>
<p>A curated list of awesome research papers, projects, code, dataset, workshops etc. related to virtual try-on (VTON).</p>
<ul>
<li><a href="#Image-based-2D-Virtual-Try-on">Image-based (2D) Virtual Try-on</a></li>
<li><a href="#3D-virtual-try-on">3D Virtual Try-on</a></li>
<li><a href="#Multi-Pose-Guided-Virtual-Try-on">Multi-Pose Guided Virtual Try-on</a></li>
<li><a href="#Video-Virtual-Try-on">Video Virtual Try-on</a></li>
<li><a href="#non-clothing-virtual-try-on">Non-clothing Virtual Try-on</a></li>
<li><a href="#pose-guided-human-synthesis">Pose-Guided Human Synthesis</a></li>
<li><a href="#Datasets-for-Virtual-Try-on">Datasets for Virtual Try-on</a></li>
<li><a href="#Related-Conference-Workshops">Related Conference Workshops</a></li>
<li><a href="#Related-Repositories">Related Repositories</a></li>
</ul>
<h2 id="Image-based-2D-Virtual-Try-on"><a href="#Image-based-2D-Virtual-Try-on" class="headerlink" title="Image-based (2D) Virtual Try-on"></a>Image-based (2D) Virtual Try-on</h2><h4 id="ACCV-2020"><a href="#ACCV-2020" class="headerlink" title="ACCV 2020"></a>ACCV 2020</h4><ul>
<li><p>CloTH-VTON: Clothing Three-dimensional reconstruction for Hybrid image-based Virtual Try-ON - <a target="_blank" rel="noopener" href="https://minar09.github.io/clothvton/">Project</a></p>
<h4 id="ECCV-2020"><a href="#ECCV-2020" class="headerlink" title="ECCV 2020"></a>ECCV 2020</h4></li>
<li><p>Do Not Mask What You Do Not Need to Mask: a Parser Free Virtual Try-On - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.02721.pdf">Paper</a></p>
<h4 id="CVPR-2020"><a href="#CVPR-2020" class="headerlink" title="CVPR 2020"></a>CVPR 2020</h4></li>
<li><p>Towards Photo-Realistic Virtual Try-On by Adaptively Generatingâ†”Preserving Image Content - <a target="_blank" rel="noopener" href="https://github.com/switchablenorms/DeepFashion_Try_On">Paper/Code/Data</a></p>
</li>
<li>Image Based Virtual Try-On Network From Unpaired Data - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2020/html/Neuberger_Image_Based_Virtual_Try-On_Network_From_Unpaired_Data_CVPR_2020_paper.html">Paper</a></li>
<li><p>Semantically Multi-modal Image Synthesis - <a target="_blank" rel="noopener" href="https://seanseattle.github.io/SMIS/">Paper/Code/Model</a></p>
<h4 id="CVPRW-2020"><a href="#CVPRW-2020" class="headerlink" title="CVPRW 2020"></a>CVPRW 2020</h4></li>
<li><p>CP-VTON+: Clothing Shape and Texture Preserving Image-Based Virtual Try-On - <a target="_blank" rel="noopener" href="https://minar09.github.io/cpvtonplus/">Paper/Code/Data/Model</a></p>
</li>
<li><p>3D Reconstruction of Clothes using a Human Body Model and its Application to Image-based Virtual Try-On - <a target="_blank" rel="noopener" href="https://minar09.github.io/c3dvton/">Paper/Project</a></p>
<h4 id="ICCV-2019"><a href="#ICCV-2019" class="headerlink" title="ICCV 2019"></a>ICCV 2019</h4></li>
<li><p>VTNFP: An Image-Based Virtual Try-On Network With Body and Clothing Feature Preservation - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_VTNFP_An_Image-Based_Virtual_Try-On_Network_With_Body_and_Clothing_ICCV_2019_paper.html">Paper</a></p>
</li>
<li><p>ClothFlow: A Flow-Based Model for Clothed Person Generation - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.html">Paper</a></p>
<h4 id="ICCVW-2019"><a href="#ICCVW-2019" class="headerlink" title="ICCVW 2019"></a>ICCVW 2019</h4></li>
<li><p>UVTON: UV Mapping to Consider the 3D Structure of a Human in Image-Based Virtual Try-On Network, <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Kubo_UVTON_UV_Mapping_to_Consider_the_3D_Structure_of_a_ICCVW_2019_paper.html">Paper</a></p>
</li>
<li>LA-VITON: A Network for Looking-Attractive Virtual Try-On - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Lee_LA-VITON_A_Network_for_Looking-Attractive_Virtual_Try-On_ICCVW_2019_paper.html">Paper</a></li>
<li>Robust Cloth Warping via Multi-Scale Patch Adversarial Loss for Virtual Try-On Framework - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCVW_2019/html/HBU/Ayush_Robust_Cloth_Warping_via_Multi-Scale_Patch_Adversarial_Loss_for_Virtual_ICCVW_2019_paper.html">Paper</a></li>
<li>Powering Virtual Try-On via Auxiliary Human Segmentation Learning - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Ayush_Powering_Virtual_Try-On_via_Auxiliary_Human_Segmentation_Learning_ICCVW_2019_paper.html">Paper</a></li>
<li><p>Generating High-Resolution Fashion Model Images Wearing Custom Outfits - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Yildirim_Generating_High-Resolution_Fashion_Model_Images_Wearing_Custom_Outfits_ICCVW_2019_paper.html">Paper</a></p>
<h4 id="ECCV-2018"><a href="#ECCV-2018" class="headerlink" title="ECCV 2018"></a>ECCV 2018</h4></li>
<li><p>Toward Characteristic-Preserving Image-based Virtual Try-On Network - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/sergeywong/cp-vton">Code</a></p>
</li>
<li><p>SwapNet: Garment Transfer in Single View Images - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Amit_Raj_SwapNet_Garment_Transfer_ECCV_2018_paper.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/andrewjong/SwapNet">Code (unofficial)</a></p>
<h4 id="CVPR-2018"><a href="#CVPR-2018" class="headerlink" title="CVPR 2018"></a>CVPR 2018</h4></li>
<li><p>VITON: An Image-based Virtual Try-on Network - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.08447">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/xthan/VITON">Code/Model</a></p>
<h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4></li>
<li><p>Keypoints-Based 2D Virtual Try-on Network System, JAKO 2020 - <a target="_blank" rel="noopener" href="https://www.koreascience.or.kr/article/JAKO202010163508810.pdf">Paper</a></p>
</li>
<li>Virtual Try-On With Generative Adversarial Networks: A Taxonomical Survey - <a target="_blank" rel="noopener" href="https://www.igi-global.com/chapter/virtual-try-on-with-generative-adversarial-networks/260791">Book chapter</a></li>
<li>LGVTON: A Landmark Guided Approach to Virtual Try-On - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.00562v1">Paper</a></li>
<li>SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On, WACV 2020 - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_WACV_2020/html/Jandial_SieveNet_A_Unified_Framework_for_Robust_Image-Based_Virtual_Try-On_WACV_2020_paper.html">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/levindabhi/SieveNet">Code/Model (unofficial)</a></li>
<li>GarmentGAN: Photo-realistic Adversarial Fashion Transfer - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.01894">Paper</a></li>
<li>Toward Accurate and Realistic Virtual Try-on Through Shape Matching and Multiple Warps - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.10817">Paper</a></li>
<li>FashionFit Analysis of Mapping 3D Pose and neural body fit for custom virtual try-on, IEEE Access 2020 - <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091008">Paper</a></li>
<li>SP-VITON: shape-preserving image-based virtual try-on network, Multimedia Tools and Applications 2019 - <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s11042-019-08363-w">Paper</a></li>
<li>VITON-GAN: Virtual Try-on Image Generator Trained with Adversarial Loss, Eurographics 2019 Posters - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.07926v1">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/shionhonda/viton-gan">Code/Model</a></li>
<li>Image-Based Virtual Try-on Network with Structural Coherence, ICIP 2019 - <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8803811">Paper</a></li>
<li>End-to-End Learning of Geometric Deformations of Feature Maps for Virtual Try-On - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.01347v2">Paper</a></li>
<li>M2E-Try On Net: Fashion from Model to Everyone - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.08599v1">Paper</a></li>
</ul>
<h2 id="3D-Virtual-Try-on"><a href="#3D-Virtual-Try-on" class="headerlink" title="3D Virtual Try-on"></a>3D Virtual Try-on</h2><h4 id="ECCV-2020-1"><a href="#ECCV-2020-1" class="headerlink" title="ECCV 2020"></a>ECCV 2020</h4><ul>
<li>BCNet: Learning Body and Cloth Shape from A Single Image - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.00214.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/jby1993/BCNet">Code/Data</a></li>
<li>GAN-based Garment Generation Using Sewing Pattern Images - <a target="_blank" rel="noopener" href="https://gamma.umd.edu/researchdirections/virtualtryon/garmentgeneration/">Paper/Code/Model/Data</a></li>
<li>Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images - <a target="_blank" rel="noopener" href="https://kv2000.github.io/2020/03/25/deepFashion3DRevisited/">Paper/Data</a></li>
<li>SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing - <a target="_blank" rel="noopener" href="https://virtualhumans.mpi-inf.mpg.de/sizer/">Paper/Code/Data</a></li>
<li><p>CLOTH3D: Clothed 3D Humans - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.02792.pdf">Paper</a></p>
<h4 id="CVPR-2020-1"><a href="#CVPR-2020-1" class="headerlink" title="CVPR 2020"></a>CVPR 2020</h4></li>
<li><p>Learning to Transfer Texture from Clothing Images to 3D Humans - <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/pix2surf/">Paper/Code</a></p>
</li>
<li>TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style - <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/tailornet/">Paper/Code/Data</a></li>
<li><p>Learning to Dress 3D People in Generative Clothing - <a target="_blank" rel="noopener" href="https://cape.is.tue.mpg.de/">Paper/Code/Data</a></p>
<h4 id="ICCV-2019-1"><a href="#ICCV-2019-1" class="headerlink" title="ICCV 2019"></a>ICCV 2019</h4></li>
<li><p>Multi-Garment Net: Learning to Dress 3D People from Images - <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/mgn/">Paper/Code/Data</a></p>
</li>
<li>3DPeople: Modeling the Geometry of Dressed Humans - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04571">Paper</a></li>
<li><p>GarNet: A Two-Stream Network for Fast and Accurate 3D Cloth Draping - <a target="_blank" rel="noopener" href="https://www.epfl.ch/labs/cvlab/research/garment-simulation/garnet/">Paper/Data</a></p>
<h4 id="ECCV-2018-1"><a href="#ECCV-2018-1" class="headerlink" title="ECCV 2018"></a>ECCV 2018</h4></li>
<li><p>DeepWrinkles: Accurate and Realistic Clothing Modeling - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.03417">Paper</a></p>
<h4 id="CVPR-2018-1"><a href="#CVPR-2018-1" class="headerlink" title="CVPR 2018"></a>CVPR 2018</h4></li>
<li><p>Video Based Reconstruction of 3D People Models - <a target="_blank" rel="noopener" href="http://gvv.mpi-inf.mpg.de/projects/wxu/VideoAvatar/">Paper/Code/Data</a></p>
<h4 id="Others-1"><a href="#Others-1" class="headerlink" title="Others"></a>Others</h4></li>
<li><p>CloTH-VTON: Clothing Three-dimensional reconstruction for Hybrid image-based Virtual Try-ON, ACCV 2020 - <a target="_blank" rel="noopener" href="https://minar09.github.io/clothvton/">Project</a></p>
</li>
<li>Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On, ACM SCA 2020 - <a target="_blank" rel="noopener" href="http://mslab.es/projects/FullyConvolutionalGraphVirtualTryOn">Paper/Project</a></li>
<li>DeePSD: Automatic Deep Skinning And Pose Space Deformation For 3D Garment Animation - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.02715.pdf">Paper</a></li>
<li>3D Reconstruction of Clothes using a Human Body Model and its Application to Image-based Virtual Try-On, CVPRW 2020 - <a target="_blank" rel="noopener" href="https://minar09.github.io/c3dvton/">Paper/Project</a></li>
<li>Learning-Based Animation of Clothing for Virtual Try-On, Eurographics 2019 - <a target="_blank" rel="noopener" href="http://dancasas.github.io/projects/LearningBasedVirtualTryOn/index.html">Paper/Project</a></li>
<li>Learning an Intrinsic Garment Space for Interactive Authoring of Garment Animation, SIGGRAPH Asia 2019 - <a target="_blank" rel="noopener" href="http://geometry.cs.ucl.ac.uk/projects/2019/garment_authoring/">Paper/Code</a></li>
<li>3D Virtual Garment Modeling from RGB Images, ISMAR 2019 - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.00114">Paper</a></li>
<li>Deep Garment Image Matting for a Virtual Try-on System, ICCVW 2019 - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Shin_Deep_Garment_Image_Matting_for_a_Virtual_Try-on_System_ICCVW_2019_paper.html">Paper</a></li>
<li>Learning a Shared Shape Space for Multimodal Garment Design, SIGGRAPH Asia 2018 - <a target="_blank" rel="noopener" href="http://geometry.cs.ucl.ac.uk/projects/2018/garment_design/">Paper/Code/Data</a></li>
<li>Detailed Garment Recovery from a Single-View Image, ACM TOG 2018 - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.01250">Paper</a></li>
<li>ClothCap: Seamless 4D Clothing Capture and Retargeting, SIGGRAPH 2017 - <a target="_blank" rel="noopener" href="http://clothcap.is.tue.mpg.de/">Paper</a></li>
<li>Virtual Try-On through Image-Based Rendering, IEEE T-VCG 2013 - <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/6487501">Paper</a></li>
<li>Markerless Garment Capture, ACM TOG 2008 - <a target="_blank" rel="noopener" href="http://www.cs.ubc.ca/labs/imager/tr/2008/MarkerlessGarmentCapture/">Paper/Data</a></li>
</ul>
<h2 id="Multi-Pose-Guided-Virtual-Try-on"><a href="#Multi-Pose-Guided-Virtual-Try-on" class="headerlink" title="Multi-Pose Guided Virtual Try-on"></a>Multi-Pose Guided Virtual Try-on</h2><ul>
<li><p>Down to the Last Detail: Virtual Try-on with Detail Carving - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.06324v2">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/JDAI-CV/Down-to-the-Last-Detail-Virtual-Try-on-with-Detail-Carving">Code/Model</a></p>
</li>
<li><p>Towards Multi-pose Guided Virtual Try-on Network, ICCV 2019 - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.11026">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/thaithanhtuan/MyMGVTON">Code</a></p>
</li>
<li><p>FIT-ME: IMAGE-BASED VIRTUAL TRY-ON WITH ARBITRARY POSES, ICIP 2019 - <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803681&amp;casa_token=2CL5K9pwy1IAAAAA:OTa5P-h6RWj9BdQVvkxQURR8tDy4Eg1BZynYOizMyQACnE-zL_EHu2xRzyXBOWijP_cItaO4">Paper</a></p>
</li>
<li><p>Virtually Trying on New Clothing with Arbitrary Poses, ACM MM 2019 - <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3343031.3350946?casa_token=w7EzejnZIaEAAAAA:KvDBsi1xYswuQuzEdJO-rsTDvysnSLYlAYi1J2st5lf8lnyotm5-umPKQupGaMEPUGxyBzijUkA9">Paper</a></p>
</li>
<li><p>FashionOn: Semantic-guided Image-based Virtual Try-on with Detailed Human and Clothing Information, ACM MM 2019 - <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3343031.3351075?casa_token=7y85FCo6B-QAAAAA:diZbVYmcSK13bMQ94MzrMG_-VvVG_oNFoGpI8wCBFJ_dHEzYnLBAPn2ZwbAgj_pmOWFMD6_1hOuk">Paper</a></p>
</li>
</ul>
<h2 id="Video-Virtual-Try-on"><a href="#Video-Virtual-Try-on" class="headerlink" title="Video Virtual Try-on"></a>Video Virtual Try-on</h2><ul>
<li>FW-GAN: Flow-Navigated Warping GAN for Video Virtual Try-On, ICCV 2019 - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_FW-GAN_Flow-Navigated_Warping_GAN_for_Video_Virtual_Try-On_ICCV_2019_paper.html">Paper</a></li>
<li>Unsupervised Image-to-Video Clothing Transfer, ICCVW 2019 - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Pumarola_Unsupervised_Image-to-Video_Clothing_Transfer_ICCVW_2019_paper.html">Paper</a></li>
</ul>
<h2 id="Non-clothing-Virtual-Try-on"><a href="#Non-clothing-Virtual-Try-on" class="headerlink" title="Non-clothing Virtual Try-on"></a>Non-clothing Virtual Try-on</h2><ul>
<li>CA-GAN: Weakly Supervised Color Aware GAN for Controllable Makeup Transfer - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.10298.pdf">Paper</a></li>
<li>Regularized Adversarial Training for Single-Shot Virtual Try-On, ICCVW 2019 - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Kikuchi_Regularized_Adversarial_Training_for_Single-Shot_Virtual_Try-On_ICCVW_2019_paper.html">Paper</a></li>
<li>Disentangled Makeup Transfer with Generative Adversarial Network - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.01144v1.pdf">Paper</a></li>
<li>PIVTONS: Pose Invariant Virtual Try-On Shoe with Conditional Image Completion, ACCV 2018 - <a target="_blank" rel="noopener" href="https://winstonhsu.info/wp-content/uploads/2018/09/chou18PIVTONS.pdf">Paper</a></li>
<li>Virtual Try-on of Eyeglasses using 3D Model of the Head - <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/2087756.2087838">Paper</a></li>
<li>A MIXED REALITY SYSTEM FOR VIRTUAL GLASSES TRY-ON - <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/2087756.2087816">Paper</a></li>
<li>A virtual try-on system in augmented reality using RGB-D cameras for footwear personalization - <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0278612514000594">Paper</a></li>
</ul>
<h2 id="Pose-Guided-Human-Synthesis"><a href="#Pose-Guided-Human-Synthesis" class="headerlink" title="Pose-Guided Human Synthesis"></a>Pose-Guided Human Synthesis</h2><ul>
<li>PoNA: Pose-Guided Non-Local Attention for Human Pose Transfer, IEEE T-IP 2020 - <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9222550">Paper</a></li>
<li>Pose-Guided High-Resolution Appearance Transfer via Progressive Training - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.11898.pdf">Paper</a></li>
<li>Recapture as You Want - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.01435.pdf">Paper</a></li>
<li>Generating Person Images with Appearance-aware Pose Stylizer, IJCAI 2020 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.09077.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/siyuhuang/PoseStylizer">Code</a></li>
<li>Controllable Person Image Synthesis with Attribute-Decomposed GAN, CVPR 2020 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.12267.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/menyifang/ADGAN">Code</a></li>
<li>Deep Image Spatial Transformation for Person Image Generation, CVPR 2020 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.00696v2.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/RenYurui/Global-Flow-Local-Attention">Code</a></li>
<li>Neural Pose Transfer by Spatially Adaptive Instance Normalization, CVPR 2020 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.07254v2.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/jiashunwang/Neural-Pose-Transfer">Code</a></li>
<li>Guided Image-to-Image Translation with Bi-Directional Feature Transformation, ICCV 2019 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.11328v1.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/vt-vl-lab/Guided-pix2pix">Code</a></li>
<li>Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis, ICCV 2019 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.12224.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/svip-lab/impersonator">Code</a></li>
<li>ClothFlow: A Flow-Based Model for Clothed Person Generation, ICCV 2019 - <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/html/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.html">Paper</a></li>
<li>Progressive Pose Attention for Person Image Generation, CVPR 2019 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03349.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/tengteng95/Pose-Transfer">Code</a></li>
<li>Dense Intrinsic Appearance Flow for Human Pose Transfer, CVPR 2019 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.11326v1.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/ly015/intrinsic_flow">Code</a></li>
<li>Unsupervised Person Image Generation with Semantic Parsing Transformation, CVPR 2019, TPAMI 2020 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03379.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/SijieSong/person_generation_spt">Code</a></li>
<li>Pose Guided Fashion Image Synthesis Using Deep Generative Model - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.07251.pdf">Paper</a></li>
<li>Synthesizing Images of Humans in Unseen Poses, CVPR 2018 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.07739.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/balakg/posewarp-cvpr2018">Code</a></li>
<li>Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis, NeurIPS 2018 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.11610.pdf">Paper</a></li>
<li>Deformable GANs for Pose-based Human Image Generation, CVPR 2018 - <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Siarohin_Deformable_GANs_for_CVPR_2018_paper.pdf">Paper</a></li>
<li>Pose-Normalized Image Generation for Person Re-identification, ECCV 2018 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1712.02225.pdf">Paper</a></li>
<li>Disentangled Person Image Generation, CVPR 2018 - <a target="_blank" rel="noopener" href="https://homes.esat.kuleuven.be/~liqianma/CVPR18_DPIG/index.html">Paper/Code/Data</a></li>
<li>A Variational U-Net for Conditional Appearance and Shape Generation, CVPR 2018 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.04694.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/CompVis/vunet">Code</a></li>
<li>Human Appearance Transfer, CVPR 2018 - <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Human_Appearance_Transfer_CVPR_2018_paper.pdf">Paper</a></li>
<li>Pose guided person image generation, NeurIPS 2017 - <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.09368.pdf">Paper</a>, <a target="_blank" rel="noopener" href="https://github.com/charliememory/Pose-Guided-Person-Image-Generation">Code</a></li>
</ul>
<h2 id="Datasets-for-Virtual-Try-on"><a href="#Datasets-for-Virtual-Try-on" class="headerlink" title="Datasets for Virtual Try-on"></a>Datasets for Virtual Try-on</h2><ul>
<li>VITON - <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1MxCUvKxejnwWnoZ-KoCyMCXo3TLhRuTo/view">Download</a>, <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Han_VITON_An_Image-Based_CVPR_2018_paper.pdf">Paper</a></li>
<li>MPV - <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1e3ThRpSj8j9PaCUw8IrqzKPDVJK_grcA">Download</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.11026">Paper</a></li>
<li>Deep Fashion3D - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.12753">Paper</a></li>
<li>Digital Wardrobe - <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/mgn/">Download/Paper/Project</a></li>
<li>TailorNet Dataset - <a target="_blank" rel="noopener" href="https://github.com/zycliao/TailorNet_dataset">Download</a>, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/tailornet/">Project</a></li>
<li>CLOTH3D - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.02792">Paper</a></li>
<li>3DPeople - <a target="_blank" rel="noopener" href="https://www.albertpumarola.com/research/3DPeople/index.html">Project</a></li>
<li>THUman Dataset - <a target="_blank" rel="noopener" href="http://www.liuyebin.com/deephuman/deephuman.html">Project</a></li>
<li>Garment Dataset, Wang et al. 2018 - <a target="_blank" rel="noopener" href="http://geometry.cs.ucl.ac.uk/projects/2018/garment_design/">Project</a></li>
</ul>
<h2 id="Related-Conference-Workshops"><a href="#Related-Conference-Workshops" class="headerlink" title="Related Conference Workshops"></a>Related Conference Workshops</h2><ul>
<li>Workshop on Computer Vision for Fashion, Art and Design: <a target="_blank" rel="noopener" href="https://sites.google.com/view/cvcreative2020">CVPR 2020</a>, <a target="_blank" rel="noopener" href="https://sites.google.com/view/cvcreative">ICCV 2019</a>, <a target="_blank" rel="noopener" href="https://sites.google.com/view/eccvfashion">ECCV 2018</a></li>
<li>Workshop on Towards Human-Centric Image/Video Synthesis: <a target="_blank" rel="noopener" href="https://vuhcs.github.io/">CVPR 2020</a>, <a target="_blank" rel="noopener" href="https://vuhcs.github.io/vuhcs-2019/index.html">CVPR 2019</a></li>
</ul>
<h2 id="Related-Repositories"><a href="#Related-Repositories" class="headerlink" title="Related Repositories"></a>Related Repositories</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ayushidalmia/awesome-fashion-ai">awesome-fashion-ai</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lzhbrian/Cool-Fashion-Papers">Cool Fashion Papers</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lzhbrian/Clothes-3D">Clothes-3D</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/lijiaman/awesome-3d-human">Awesome 3D Human</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/openMVG/awesome_3DReconstruction_list">Awesome 3D reconstruction list</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/chenweikai/Body_Reconstruction_References">Human Body Reconstruction</a></li>
</ul>
<h4 id="Pull-requests-are-welcome"><a href="#Pull-requests-are-welcome" class="headerlink" title="Pull requests are welcome!"></a>Pull requests are welcome!</h4></div>
    </div>

    <div class="post-meta">
        <i>
        
            <span>2020-10-30</span>
            
                <span>è¯¥ç¯‡æ–‡ç« è¢« Yann</span>
            
            
                <span>æ‰“ä¸Šæ ‡ç­¾:
                    
                    
                        <a href='/tags/VTON/'>
                            VTON
                        </a>
                    
                </span>
             
             
        
        </i>
    </div>
    <br>
    
    
        
            
    
            <div class="post-footer-pre-next">
                
                    <span>ä¸Šä¸€ç¯‡ï¼š<a href='/2020/20201103/'>Effective AER Object Classification Using Segmented Probability-Maximization Learning in Spiking Neural Networks</a></span>
                

                
                    <span class="post-footer-pre-next-last-span-right">ä¸‹ä¸€ç¯‡ï¼š<a href="/2020/20201029/">æ ‘è“æ´¾4B+è‹±ç‰¹å°”ç¥ç»è®¡ç®—æ£’(Intel Movidius Neural Computing Stick)-YOLOV3ç›®æ ‡æ£€æµ‹(3)</a>
                    </span>
                
            </div>
    
        
    

    
        

     
</div>



                                      
                    
                    
                    <div class="footer">
    
        <span> 
            Â© 1949-2025 China 

            
                

            
        </span>
       
    
</div>



<!--è¿™æ˜¯æŒ‡ä¸€æ¡çº¿å¾€ä¸‹çš„å†…å®¹-->
<div class="footer-last">
    
            <span>ğŸŒŠçœ‹è¿‡å¤§æµ·çš„äººä¸ä¼šå¿˜è®°æµ·çš„å¹¿é˜”ğŸŒŠ</span>
            
                <span class="footer-last-span-right"><i>æœ¬ç«™ç”±<a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/index.html">Hexo</a>é©±åŠ¨ï½œä½¿ç”¨<a target="_blank" rel="noopener" href="https://github.com/HiNinoJay/hexo-theme-A4">Hexo-theme-A4</a>ä¸»é¢˜</i></span>
            
    
</div>


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

    <!--ç›®å½•-->
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tocify/1.9.0/javascripts/jquery.tocify.min.js" type="text/javascript" ></script>
        
<script src="/js/toc.js"></script>

    

    
<script src="/js/randomHeaderContent.js"></script>

    <!--å›åˆ°é¡¶éƒ¨æŒ‰é’®-->
    
        
<script src="/js/returnToTop.js"></script>

    

    
        
<script src="/js/returnToLastPage.js"></script>

    





<script src="/js/lightgallery/lightgallery.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-thumbnail.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-fullscreen.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-autoplay.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-zoom.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-rotate.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-paper.umd.min.js"></script>




<script type="text/javascript">
     
    if (typeof lightGallery !== "undefined") {
        var options1 = {
            selector: '.gallery-item',
            plugins: [lgThumbnail, lgFullscreen, lgAutoplay, lgZoom, lgRotate, lgPager], // å¯ç”¨æ’ä»¶
            thumbnail: true,          // æ˜¾ç¤ºç¼©ç•¥å›¾
            zoom: true,               // å¯ç”¨ç¼©æ”¾åŠŸ
            rotate: true,             // å¯ç”¨æ—‹è½¬åŠŸèƒ½èƒ½
            autoplay: true,        // å¯ç”¨è‡ªåŠ¨æ’­æ”¾åŠŸèƒ½
            fullScreen: true,      // å¯ç”¨å…¨å±åŠŸèƒ½
            pager: false, //é¡µç ,
            zoomFromOrigin: true,   // ä»åŸå§‹ä½ç½®ç¼©æ”¾
            actualSize: true,       // å¯ç”¨æŸ¥çœ‹å®é™…å¤§å°çš„åŠŸèƒ½
            enableZoomAfter: 300,    // å»¶è¿Ÿç¼©æ”¾ï¼Œç¡®ä¿å›¾ç‰‡åŠ è½½å®Œæˆåå¯ç¼©æ”¾
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options1); // ä¿®å¤é€‰æ‹©å™¨
    }
    
</script>


    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> 

                </div>
            
            
                <!-- å›åˆ°é¡¶éƒ¨çš„æŒ‰é’®-->  
                <div class="progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
            
                <!-- è¿”å›çš„æŒ‰é’®-->  
                <div class="return-to-last-progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
    </body>
</html>
<script src="/js/emojiHandler.js"></script>
<script>
  document.addEventListener('DOMContentLoaded', () => {
    wrapEmojis('.paper');
  });
</script>