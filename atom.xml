<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yuanquanquan的个人博客 | 我愿做你光华中淡淡的一笔</title>
  
  <subtitle>我愿做你光华中淡淡的一笔</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanquanquan.top/"/>
  <updated>2021-08-30T11:11:32.061Z</updated>
  <id>http://yuanquanquan.top/</id>
  
  <author>
    <name>Yann</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Moving Deep Learning into Web Browser:How Far Can We Go</title>
    <link href="http://yuanquanquan.top/2021/20210830/"/>
    <id>http://yuanquanquan.top/2021/20210830/</id>
    <published>2021-08-30T10:06:38.000Z</published>
    <updated>2021-08-30T11:11:32.061Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文是对发表于WWW 2019的论文《Moving Deep Learning into Web Browser: How Far Can We Go? 》的回顾。论文第一作者为中心的马郓助理教授，通信作者为中心的刘譞哲副教授，其余作者为中心本科毕业生向东伟、硕士生郑舒宇以及博士生田得雨。本文针对在浏览器中运行深度学习任务的新趋势，调研和测试了最热门的7个基于JavaScript的深度学习框架，以评估这些框架的功能和性能，并与传统的深度学习框架进行了性能比较。本文的发现能够帮助应用开发者、深度学习框架开发者、浏览器厂商对浏览器上的深度学习效率进行优化。</p></blockquote><span id="more"></span><div class="row">    <embed src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding/1901.09388.pdf" width="100%" height="550" type="application/pdf"></div><p>​    深度学习技术在过去十年极大地拓展了人工智能的边界，在图像处理、语音识别、自然语言处理等领域得到了广泛的应用。学术和产业界也提出和研发了多个深度学习框架如TensorFlow、Caffe等。然而，人工智能应用需要运行于Windows、iOS、Android等多种类型的平台上，将人工智能应用向多种平台移植极具挑战。Web应用具有较强的可移植性，因此基于Web的人工智能应用开发成为了研究热点。从2015年起，ConvNetJS 、TensorFlow.js 等一系列基于JavaScript的深度学习库和框架被提出。然而这些框架在功能和性能上能否很好地支持深度学习应用仍然存疑，本文所进行的实证研究回答了以下三个问题：现有框架提供了哪些功能和特性支持多种类型的深度学习任务？现有框架在不同的深度学习任务下性能表现如何？在浏览器中进行深度学习与在原生平台上运行相比有多大的性能差距？本文选择了7个基于JavaScript的深度学习框架进行研究，并将Tensorflow.js与基于Python的原生Tensorflow的性能进行了比较，得出了一系列结论并给出了建议。</p><h2 id="1、现有框架的功能与特性"><a href="#1、现有框架的功能与特性" class="headerlink" title="1、现有框架的功能与特性"></a>1、现有框架的功能与特性</h2><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210830184828621.png" alt="表1：基于JavaScript的深度学习框架的特性"></p><p>​        为了选择最新的浏览器支持的 DL 框架，作者在 GitHub 上搜索关键字“deep learning framework”，并用 JavaScript 语言过滤结果。然后<strong>选择了 GitHub 上星数超过 1000 的前 7 个框架[1]</strong>。对每个框架的具体介绍如下：</p><p><strong>TensorFlow.js[2]</strong> ：2018 年 3 月由 Google 发布，是一个 inbrowser 机器学习库，支持使用 JavaScript 在浏览器中定义、训练和运行模型。TensorFlow.js 由 WebGL 提供支持，并提供用于定义模型的高级 API。TensorFlow.js 支持所有 Keras 层（包括 Dense、CNN、LSTM 等）。因此，很容易将原生 TensorFlow 和 Keras 预先训练的模型导入到浏览器中并使用 Tensorflow.js。</p><p><strong>ConvNetJS[3]</strong> ：是一个 Javascript 库，最初由斯坦福大学的 Andrej Karpathy 编写。ConvNetJS 目前支持用于分类和回归的常用神经网络模型和代价函数。此外，它还支持卷积网络和强化学习。然而遗憾的是，尽管 ConvNetJS 可能是在 TensorFlow.js 之前最著名的框架，但其在 2016 年 11 月后已经不再维护了。</p><p><strong>Keras.js[4]</strong>：抽象出许多框架作为后端支撑，包括 TensorFlow、CNTK 等。它支持导入 Keras 预先训练的模型进行推理。在 GPU 模式下，Keras.js 的计算由 WebGL 执行。然而，这个项目也已经不再活跃。</p><p><strong>WebDNN[5]</strong>：由东京大学发布的 WebDNN 号称是浏览器中最快的 DNN 执行框架。它只支持推理（训练）任务。该框架支持 4 个执行后端：WebGPU、WebGL、WebAssembly 和 Fallback pure JavaScript 实现。WebDNN 通过压缩模型数据来优化 DNN 模型，以加快执行速度。</p><p><strong>brain.js[6]</strong>：是一个用于神经网络的 JavaScript 库，它取代了不推荐使用的 “brain” 库。它为训练任务提供 DNN、RNN、LSTM 和 GRU。该库支持将训练好的 DL 模型的状态用 JSON 序列化和加载。</p><p><strong>synaptic[7]</strong>：这是一个不依赖于 JavaScript 架构的神经网络库，基本上支持任何类型的一阶甚至二阶 RNN。该库还包括一些内置的 DL 架构，包括多层感知器、LSTM、液态机（Liquid state machines）和 Hopfield 网络。</p><p><strong>Mind[8]</strong>：这是一个灵活的神经网络库。核心框架只有 247 行代码，它使用矩阵实现来处理训练数据。它支持自定义网络拓扑和插件，以导入 mind 社区创建的预训练模型。然而，这个项目也已经不再活跃。</p><p>从提供的功能性来看，多数框架支持训练和推断两类任务；不同的框架支持的神经网络类型及操作有较大差异，仅有TensorFlow.js等3个框架支持了DNN、CNN、RNN三类神经网络；多数框架都支持以层为单位构建神经网络，TensorFlow.js相比其他框架支持了更多的层类别；TensorFlow.js也支持了更多种类的激活函数和优化器；7个框架中仅有TensorFlow.js支持使用GPU加速训练，TensorFlow.js等3个框架支持使用GPU加速推断。</p><p>从开发者支持来看，TensorFlow.js在开发文档、演示等多个方面优于其他框架，但因其支持较丰富的功能，其软件包也是较大的。</p><h2 id="2、现有框架的性能"><a href="#2、现有框架的性能" class="headerlink" title="2、现有框架的性能"></a>2、现有框架的性能</h2><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210830184912849.png" alt="图1：平均训练时间"></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210830184950411.png" alt="图2：平均推断时间"></p><p>测试结果显示，ConvNetJS在训练和推断上性能均为最佳；Tensorflow.js是唯一支持GPU加速的框架，且性能与ConvNetJS可比。ConvNetJS性能更佳的原因可能是其在实现上与其他框架存在较大区别。</p><h2 id="3、浏览器框架与原生框架性能对比"><a href="#3、浏览器框架与原生框架性能对比" class="headerlink" title="3、浏览器框架与原生框架性能对比"></a>3、浏览器框架与原生框架性能对比</h2><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210830185032610.png" alt="图3：Keras预训练模型的推断时间"></p><p>在推断任务的实验中，Tensorflow.js在nGPU上的表现弱于原生Tensorflow，但也仅慢1-2倍。Tensorflow.js在iGPU上的表现则优于原生Tensorflow在CPU上的表现。</p><p>在决策树的实验中，尽管在任何配置下Tensorflow.js几乎都慢于原生Tensorflow。但两个重要发现是，第一，较多使用CPU还是GPU对性能差异有极大的影响；第二，任务类型对性能差异有极大影响，在训练任务中Tensorflow.js平均慢33.9倍，而在推断任务中则只慢5.8倍。</p><h2 id="4、主要研究发现"><a href="#4、主要研究发现" class="headerlink" title="4、主要研究发现"></a>4、主要研究发现</h2><p>第一，浏览器端的深度学习框架仍在早期阶段，仅有Tensorflow.js提供了较全面的功能和支持了较多种类的深度学习任务。</p><p>第二，浏览器端对深度学习的支持主要仍然集中于推断任务，训练任务仍然受到较大局限。</p><p>第三，模型加载是推断任务中最耗时的部分，因此对于较小的模型，使用CPU的表现优于GPU。</p><p>第四，在没有独立显卡时，集成显卡对浏览器端深度学习框架性能有较大帮助。相比原生Tensorflow仅在CPU上运行，Tensorflow.js利用集成显卡表现出了更好的性能。</p><p>第五，系统资源的利用有待进一步优化。对于Tensorflow.js，CPU没有得到充分的利用；对于WebGL，分配的内存受限于浏览器，导致部分深度学习任务崩溃。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是对发表于WWW 2019的论文《Moving Deep Learning into Web Browser: How Far Can We Go? 》的回顾。论文第一作者为中心的马郓助理教授，通信作者为中心的刘譞哲副教授，其余作者为中心本科毕业生向东伟、硕士生郑舒宇以及博士生田得雨。本文针对在浏览器中运行深度学习任务的新趋势，调研和测试了最热门的7个基于JavaScript的深度学习框架，以评估这些框架的功能和性能，并与传统的深度学习框架进行了性能比较。本文的发现能够帮助应用开发者、深度学习框架开发者、浏览器厂商对浏览器上的深度学习效率进行优化。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>SETI Breakthrough Listen - E.T. Signal Search</title>
    <link href="http://yuanquanquan.top/2021/20210817/"/>
    <id>http://yuanquanquan.top/2021/20210817/</id>
    <published>2021-08-17T13:43:40.000Z</published>
    <updated>2021-08-17T15:29:55.013Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>比赛链接：<a href="https://www.kaggle.com/c/seti-breakthrough-listen">https://www.kaggle.com/c/seti-breakthrough-listen</a></p><p>比赛名称：SETI Breakthrough Listen - E.T. Signal Search</p><p>比赛内容：利用算法来识别异常的信号，CV类型赛题</p></blockquote><span id="more"></span> <hr><h3 id="比赛背景："><a href="#比赛背景：" class="headerlink" title="比赛背景："></a>比赛背景：</h3><p>“我们一个人在宇宙里吗？”这是最深刻且长期存在的人类问题之一。随着技术的进步，我们正在寻找新的和更强大的方法来寻求答案。加州大学伯克利大学使用世界上最强大的望远镜对数百万颗恒星进行技术扫描。现在希望Kaggle社区能够帮助解释他们收到的信号。</p><p><strong>加州大学伯克利分校的Breakthrough Listen 团队使用世界上最强大的望远镜扫描数百万颗恒星以寻找技术迹象。</strong></p><p><strong>现在它希望Kaggle社区帮助解释他们接收到的信号。Listen团队是外星智慧搜索(SETI) 的一部分，使用地球上最大的可操纵天线，即直径100 米的绿岸望远镜。</strong>与任何SETI搜索一样，交流的动机也是主要挑战。</p><p>人类已经建造了大量的无线电设备。很难在现代技术的巨大检测结果中寻找微弱的外星传播针。当前的方法使用两个过滤器来搜索大海捞针。</p><p><strong>首先，</strong>Listen团队将目标恒星的扫描与天空其他区域的扫描穿插在一起。两组扫描中出现的任何信号都可能不是来自目标恒星的方向。<strong>其次，</strong>管道会丢弃不会改变其频率的信号，因为这意味着它们可能在望远镜附近。</p><p><strong>运动中的源应该有一个暗示运动的信号，类似于路过的消防车警报器的音调变化。这两个过滤器非常有效，但我们知道它们可以改进。</strong></p><p>管道无疑会错过有趣的信号，尤其是那些具有复杂时间或频率结构的信号，以及那些在有大量干扰的频谱区域中的信号。<strong>在本次比赛中，利用您的数据科学技能帮助识别Breakthrough Listen 目标扫描中的异常信号。</strong></p><p>由于没有确认的用于训练机器学习算法的外星信号示例，<strong>该团队在来自望远镜的海量数据中加入了一些模拟信号（他们称之为“针”）。</strong>他们已经确定了一些隐藏的针，以便您可以训练您的模型以找到更多。</p><p><strong>数据由二维数组组成，因此可能存在有前景的计算机视觉方法，以及数字信号处理、异常检测等。</strong>成功识别最多针的算法将赢得现金奖励，但也有可能帮助回答科学中最大的问题之一。</p><h3 id="赛题目标"><a href="#赛题目标" class="headerlink" title="赛题目标"></a>赛题目标</h3><p>比赛的任务就是通过给定的频谱图预测对应的标签：0或者1在这场比赛中，利用算法来识别异常的信号。数据由二维数组组成，因此计算机视觉中可能会有一些方法，可能涉及的知识包括数字信号处理，异常检测等。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210817224014119.png" alt=""></p><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p>Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.</p><h3 id="赛题赛程"><a href="#赛题赛程" class="headerlink" title="赛题赛程"></a>赛题赛程</h3><ul><li>July 21, 2021 - Entry Deadline.</li><li>July 21, 2021 - Team Merger Deadline.</li><li>July 28, 2021 - Final Submission Deadline.</li></ul><h3 id="赛题数据"><a href="#赛题数据" class="headerlink" title="赛题数据"></a>赛题数据</h3><p>赛题数据与频谱图类似，但通常跨越几个GHz的无线电频谱。数据存储为滤波器组格式或HDF5格式文件。</p><p>通过交替观测来做到这一点：在恒星“A”上观察 5 分钟，然后在恒星“B”上观察 5 分钟，然后回到恒星“A”上 5 分钟，然后是“C” ”，然后回到“A”，然后在“D”星上用 5 分钟结束。一组六个观察值 (ABACAD) 被称为“节奏”。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210817224118204.png" alt="异常样本"></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210817232838744.png" alt="正常样本"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;比赛链接：&lt;a href=&quot;https://www.kaggle.com/c/seti-breakthrough-listen&quot;&gt;https://www.kaggle.com/c/seti-breakthrough-listen&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;比赛名称：SETI Breakthrough Listen - E.T. Signal Search&lt;/p&gt;
&lt;p&gt;比赛内容：利用算法来识别异常的信号，CV类型赛题&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="kaggle" scheme="http://yuanquanquan.top/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>MLB Player Digital Engagement Forecasting</title>
    <link href="http://yuanquanquan.top/2021/2202210803/"/>
    <id>http://yuanquanquan.top/2021/2202210803/</id>
    <published>2021-08-03T12:16:23.000Z</published>
    <updated>2021-08-17T15:22:35.036Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一次kaggle比赛，这次比赛的场景主要通过MLB球员的历史表现数据、社交媒体数据以及市场规模等团队因素来预测在未来MLB 球员的数字内容互动趋势（社交媒体互动）。建立的模型将预测出MLB球员在未来的数字内容互动趋势指数（target1- target4）。</p></blockquote><span id="more"></span> <hr><p><img src="https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F603584%2Ff7c0669c09db26bd45f76ade61b2a91f%2FGoogleCloud_MLB_Lockup.jpg?generation=1623359034713516&amp;alt=media" alt=""></p><h1 id="MLB-Player-Digital-Engagement-Forecasting"><a href="#MLB-Player-Digital-Engagement-Forecasting" class="headerlink" title="MLB Player Digital Engagement Forecasting"></a>MLB Player Digital Engagement Forecasting</h1><h3 id="比赛背景与任务："><a href="#比赛背景与任务：" class="headerlink" title="比赛背景与任务："></a>比赛背景与任务：</h3><p>A player hits a walk-off home run. A pitcher throws a no-hitter. A team gets red hot going into the Postseason. We know some of the catalysts that increase baseball fan interest. Now Major League Baseball (MLB) and Google Cloud want the Kaggle community’s help to identify the many other factors which pique supporter engagement and create deeper relationships betweens players and fans.</p><p>The sport has a long history of being numbers-driven. Nearly every day from at least April through October, baseball fans watch, read, and search for information about players. Which individuals they seek can depend on player performance, team standings, popularity, among other, currently unknown factors—which could be better understood thanks to data science.</p><p>Since at least the early 1990s, MLB has led the sports world in the use of data, showing fans, players, coaches, and media what’s possible when you combine data with human performance. MLB continues its leadership using technology to engage fans and provide new fans innovative ways to experience America’s Favorite Pastime.</p><p>MLB has teamed up with Google Cloud to transform the fan experience through data. Google Cloud proudly supports this Kaggle contest to celebrate the launch of Vertex AI: Google Cloud’s new platform to unify your ML workflows.</p><p>In this competition, you’ll predict how fans engage with MLB players’ digital content on a daily basis for a future date range. You’ll have access to player performance data, social media data, and team factors like market size. Successful models will provide new insights into what signals most strongly correlate with and influence engagement.</p><p>Imagine if you could predict MLB All Stars all season long or when each of a team’s 25 players has his moment in the spotlight. These insights are possible when you dive deeper into the fandom of America’s pastime. Be part of the first method of its kind to try to understand digital engagement at the player level in this granular, day-to-day fashion. Simultaneously help MLB build innovation more easily using Google Cloud’s data analytics, Vertex AI and MLOps tools. You could play a part in shaping the future of MLB fan and player engagement.</p><p>该赛题为<strong>时间序列</strong>任务，通过MLB球员的历史表现数据、社交媒体数据以及市场规模等团队因素来预测在未来MLB 球员的数字内容互动趋势（社交媒体互动）。建立的模型将预测出MLB球员在未来的数字内容互动趋势指数（target1- target4）。旨在为MLB 球迷和球员的未来社交媒体互动参与度挖掘价值。</p><p> 至少从 1990 年代初期开始，美国职业棒球大联盟就在使用数据方面领先于体育界，向球迷、球员、教练和媒体展示了将数据与人类表现相结合的可能性。MLB使用创新技术吸引球迷，并为新球迷提供体验美国最受欢迎的消遣的创新方式。 </p><p><strong>评价指标</strong>：MCMAE 计算四个目标变量中的每一个的平均绝对误差，得分是这四个MAE值的平均值</p><h3 id="方案简述"><a href="#方案简述" class="headerlink" title="方案简述"></a>方案简述</h3><p>通过竞赛提供的在2021赛季活跃的2055位MLB球员的四种不同的数字内容参与度 ( target1- target4)和对应的球员团队、历史比赛、历史得分情况、所获奖项、比赛事件等累计7.9G的历史数据信息(2021年1-4月)来结合机器学习构建MLB球员未来（2021年5月）数字内容互动趋势指数预测模型。通过季节性EDA、MLB球员历史信息统计后进行特征工程，分别使用ANN（人工神经网络）和LightGBM、CatBoost（集成学习）进行模型融合并对各模型的超参数进行了网格优化后在排行榜取得了铜牌的成绩。</p><h3 id="方案流程："><a href="#方案流程：" class="headerlink" title="方案流程："></a>方案流程：</h3><ol><li>mlb-ann-training：ANN模型训练代码</li><li>mlb-lightgbm-training：LightGBM模型训练代码</li><li>mlb-catboost-training：：CatBoost模型训练代码</li><li>全流程推理代码（特征提取、超参数调优、模型融合）</li></ol><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><p>I used ~440 features. In addition to joins and asof merge of basic tables, the following features were used:</p><ul><li>lag features per player<ul><li>Average of the last 7/28/70/360/720 days</li><li>Average over the on-seasons</li><li>Average for the same period in the previous year</li><li>Average of days with/without a game</li></ul></li><li>number of events, pitch events, action events</li><li>days from last rosters, awards, transactions and box scores</li><li>sum of box scores in the last 7/30/90 days</li><li>number of games and events in the day</li><li>event-level meta feature<ul><li>aggregation of predictions of model trained on event table</li><li>group by (date, playerId), (date, teamId) and (date)</li></ul></li></ul><h3 id="Cumcount-Leakage"><a href="#Cumcount-Leakage" class="headerlink" title="Cumcount Leakage"></a>Cumcount Leakage</h3><p>There is a strange correlation between the cumcount of the dataframe retrieved from the Time-Series API and the target.</p><p>I noticed this problem 3 days before the competition ended. I did not post it in the discussion as it might confuse the participants, but contacted the host immediately.<br>Adding this cumcount to the features only improves the CV a little bit, so it’s probably some kind of artifact or something, but even if it doesn’t improve the CV much, it’s better to shuffle the test data since it’s nonsense that the order of the rows makes sense.</p><p>I did not end up using this leak for final submission.</p><h3 id="Implementation-Note"><a href="#Implementation-Note" class="headerlink" title="Implementation Note"></a>Implementation Note</h3><p>Building a complex data pipeline in Jupyter Notebook with the Time Series API can be a big pain. I’ll share some of my efforts.</p><ul><li>Maintain the source code on GitHub and paste the BASE64-encoded code into the jupyter notebook<ul><li>see: <a href="https://github.com/lopuhin/kaggle-imet-2019">https://github.com/lopuhin/kaggle-imet-2019</a></li></ul></li><li>The inference notebook is also maintained on GitHub and automatically uploaded as the Kaggle Kernel through GitHub Actions</li><li>Avoid the use of pandas and instead use a dictionary of numpy arrays to manage state updates</li><li>Use the same feature generation function for training data and inference<ul><li>Both training and test are treated as streaming data, and features were generated using for-loop.</li><li>This is the most important point to get a stable and bug-free data pipeline</li><li>see: <a href="https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/196942">https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/196942</a></li></ul></li><li>Debug code locally using the API emulator<ul><li>Test the robustness of my inference pipeline by “dropout” some of the data returned by the emulator (a kind of “Chaos Engineering”)</li></ul></li><li>Catch exceptions in various functions and convert them to appropriate “default” values</li></ul><iframe src="https://nbviewer.jupyter.org/github/Bazingaliu/MLB-Player-Digital-Engagement-Forecasting/blob/main/4.%E5%85%A8%E6%B5%81%E7%A8%8Binference.ipynb" width="570" height="2000"></iframe> ]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;记录一次kaggle比赛，这次比赛的场景主要通过MLB球员的历史表现数据、社交媒体数据以及市场规模等团队因素来预测在未来MLB 球员的数字内容互动趋势（社交媒体互动）。建立的模型将预测出MLB球员在未来的数字内容互动趋势指数（target1- target4）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="kaggle" scheme="http://yuanquanquan.top/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>网络单纯形法</title>
    <link href="http://yuanquanquan.top/2021/20210716/"/>
    <id>http://yuanquanquan.top/2021/20210716/</id>
    <published>2021-07-16T11:55:41.000Z</published>
    <updated>2021-08-17T15:31:13.185Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在实际应用中。迄今为止求解线性规划问题著名的仍是<strong>单纯形法</strong>。通过专业的实现，它具有卓越的性能：≈1000变量和≈1000约束的问题可以在0.1到0.5秒内得到解决。那么，是否可以尝试将该算法应用于图论中的问题呢？</p><p>​    事实上，最重要的网络优化问题都可以用线性规划来表示，比如确定最短路、最大流、最小费用流等。然而，直接应用通常的单纯形算法是没有意义的，因为所产生的程序将是<strong>笨拙(unwiedy)</strong>和<strong>高度退化的(highly degenerate)</strong>。这两个问题通过使用单纯形法的适当图论特殊化来避免：<strong>网络单纯形法</strong>。</p></blockquote><span id="more"></span><p>在此之前，我们先从<strong>最短路</strong>的故事说起。</p><p>   我们知道所有解决最短路问题的方法的基础是下面这样一个简单的想法：假设已知对每个v存在一条费用为π(v)的从r到v的有向路，并且我们找到一条满足π(v) + C(vw) &lt; π(w)的弧vw。由于把vw附加到从r到v的有向路可以得到一条到w的有向路，因此存在一条到w的费用为π(v) + C(vw)的更便宜的有向路。基于此，定义<strong>可行势（feasible potential）</strong>的概念是自然的：</p><ul><li><p><strong>Def1.</strong>如果π(v)是到v的有向路的最小费用，那么π满足π(v) + C(vw) ≥ π(w)，称π是一个可行势。</p><p> 可行势对最短路的费用给出了下界，我们有如下命题：</p></li><li><p><strong>Prop1.</strong>令π是可行势且P是从s到v的有向路，那么c(P)≥π(v).</p><p> 事实上，我们通过<strong>Ford算法</strong>在终止时得到了一个可行势和若干有向路，对它们来说上述命题中的等号成立。即有下面的定理陈述这个事实：</p></li><li><p><strong>Thm1.</strong> min{c(P):P是从s到t的有向路} = max{π(t):π可行势}</p><p> 我们可以通过这个陈述看到它与线性规划对偶性之间的联系。定理陈述中的最大化显然是一个线性规划问题：</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040907249.png" alt=""><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040923491.png" alt=""></p><p><strong>对偶最优值定理</strong>告诉我们(P)(D)中有一个最优值存在，则两个最优值都存在且相等。注意到任何r到s的有向路P为(D)提供了一个可行解，则(D)的目标函数值恰好是P的费用，因此，定理1意味着，当最短路存在时，(D)有一个最优解，它是一条简单有向路的特征向量。这个结果将等价于结论：</p><ul><li><strong>Prop2</strong>. (D)的可行解多面体的顶点是简单有向路的特征向量.</li></ul><p>​    因为我们已经可以用Ford算法解决线性规划问题(D)，那么这个算法和单纯形法之间的联系是什么呢？单纯形法保存了一个“基”弧（对应于(D)中的基变量）的集合T，(D)的一个可行解x及向量y∈R^{V}，它们满足</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040954255.png" alt=""></p><p>在每次迭代中，通过用集合外的一条弧替代集合中的一条，得到一个新的这样的集合。基弧的集合T必须对应于(D)的等式约束矩阵A的列极大线性无关组（列基）。事实上，这个矩阵就是图G的<strong>关联矩阵(incidence matrix)</strong>，它的列基可以用很好的方法来刻画。</p><ul><li><p><strong>Prop3.</strong>G:有向、连通，$A=\left{a_{e}: e \in E\right}$.集合$\left{a_{e}: e \in E\right}$是$A$的列基，当且仅当T是G的一颗生成树的弧集（<strong>ex.</strong>如果T不包含任何圈的弧集，那么它对应的列是线性无关的）</p><p>所以在这样的观察下，我们可以知道单纯形法是从生成树到生成树，每一步都可以看作是一序列的Ford算法步骤，她是一个被若干个不改变树的步骤所跟随着的通常步骤，直到y“追赶”为树。</p></li></ul><p>​    <strong>最大流问题</strong>同样是一个线性规划问题，所以通过线性规划对偶可以给出最大流的一个好的刻画。那么，我们的一个自然的问题就是这种刻画与我们通过割所给出的刻画有什么联系。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041025672.png" alt=""><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041043901.png" alt=""></p><p>同样由对偶最优定理，我们可以给出经典的<strong>最大流-最小割定理</strong>的如下刻画：<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041946529.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041954142.png" alt=""></p><p>现在故事来到了最小费用流问题（MCFP），它的一个标准陈述如下：<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041311274.png" alt=""></p><p>我们·可以自然的写出其对应的线性规划形式：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210724005804511.png" alt=""></p><p>作为一个重要的特例，我们想让其中每个u(e)都是无穷的，这样的问题称为<strong>转运问题(transshipment problem)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210724010218929.png" alt=""></p><p>设x是任意的b-流，即(P)的任意可行解，则由<strong>对偶最优定理</strong>，我们有如下结论：（互补松紧性条件给出了我们想要的最优性刻画）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210724010253027.png" alt=""></p><p>另外，下面两个观察是重要的：</p><ol><li>给定G中的一条路P。 定义P(关于c)的费用为∑(c(e) :e是P的正向弧)-∑ (c(e) :e是P的反向弧)。这样定义的原因是：假设我们在G中沿一条x-可扩路P发送ε单位的流，即xe在P的正向弧上提高了ε，在P的反向弧上降低了ε。那么cTx增加了∑(εce :e是P的正向弧)-∑ (εce :e是P的反向弧) ，即增加了ε倍的P的费用。特别的，如果x是(P)的可行解，那么某个负费用的x-可扩圈将给出一个费用更低的解。</li></ol><ol start="2"><li>对满足0≤x≤u的向量x，如同处理最大流时一样定义一个辅助有向图Gx，则G中每条x-可扩路对应于Gx中具有相同费用的一条有向路，特别的，G中一个负费用的x-可扩圈对应于Gx中一个负费用有向圈。</li></ol><p>于是我们就可以用最短路方法来确定Gx是否包含负费用有向圈。这便引出了下面对(P)的最优性的刻画：</p><ul><li><strong>Thm3</strong>.(P)的可行解x是最优的当且仅当不存在具有负费用c的x-可扩圈。</li><li><strong>(Klein 1967)</strong>(G,u,b,c). A b-flow f is of minimum cost iff there is no f-augmenting cycle with negative total weight iff there exists a feasible potential for (Gf , c).</li></ul><p>这是一条最重要的<strong>最优性准则</strong>。以上基本讲清了线性规划与最小费用流问题的联系。下面在我们正式将单纯形法应用于此之前，我们首先陈述这部分内容所需要的一些基本想法：</p><ul><li>假设问题定义在连通有向图G上（否则，限制在G的每个连通分支上处理即可）</li><li>可先对转运问题这一特殊情况进行分析（即假设对每条弧e有u(e)=∞）</li><li>定义转运问题的树解（支撑树解）</li><li>网络单纯形法保持可行的树解并且寻找一种特殊的负费用圈。(如果每个C(T,e)都具有非负费用，那么由T确定的树解x满足最优值定理的条件)</li><li>增加辅助边来找初始的树和流</li><li>避免循环：从一颗强可行树开始并使用离开弧规则(选择h是C(T,e)中第一条满足 x(h)=θ的反向弧)</li><li>网络单纯形法在有限步后会终止。</li><li>一般的，网络单纯形法将从树解移动到树解，只使用通过增加一条弧到T上形成的圈。</li></ul><p>我们首选对转运问题来定义树解，转运问题的支撑树解(spanning tree solution)是一个向量x∈R^{E}满足条件：对某棵树T，<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041834923.png" alt=""></p><p>树T和T的一条弧h=pq确定了将顶点氛围两部分R(T,h)和V\R(T,h)的一个划分如下：R(T,h)是在T中从r到v的简单路没有用到h的那些顶点v的集合。显然如图，r∈R(T,h)，h是T中唯一的一个端点在R(T,h)中，另一个端点不在R(T,h)中的弧。所以在与T相关的任何树解x中，流入R(T,h)的净流量一定是完全由h携带的。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725015500600.png" alt=""></p><p>这个观察将给我们下面的结果：</p><ul><li><p><strong>Prop4</strong>.一棵树T唯一地确定了它的树解.</p></li><li><p><strong>Prop5</strong>.(G:连通) 一个可行解是树解，当且仅当不存在每条弧都具有正流的圈。</p></li></ul><p>我们已经指出网络单纯形法是保持可行的树解并且寻找一种特殊的负费用圈，现在就给出这样的圈：<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725015539193.png" alt=""></p><p>那么这种特殊类型的圈有什么优点呢？</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725015603770.png" alt=""></p><p><strong>Prop6</strong>.如果树T确定了可行树解x且对每个e不属于T，C(T,e)都具有非负费用。那么x是最优的。</p><p>有了上面的陈述，我们已经可以叙述我们心目中算法的初步形式：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040144633.png" alt=""></p><p>这里立刻出现的一个问题是：<strong>如何找到初始的树和流。</strong>我们直接针对一般的最小费用流问题给出答案。</p><p>下面的命题演示了我们是如何从找到的圈中替换原来树上的一条弧从而实现“换基”操作的。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040244874.png" alt=""></p><ul><li><strong>Def2</strong>.如果一棵树T确定了一个满足T中至少有一条弧携带0流的流x，那么我们说x是退化的。</li></ul><p>这直接引发了一个重要的理论问题：</p><p><strong>Q</strong>. 在一系列退化的迭代(改变树而不改变流的迭代只可能出现在流是退化的情况下)后，该算法会不会返回同一棵树？（循环，导致算法不能终止/另一方面，如果循环不发生，由于不同的树的数目是有限的，那么算法一定在有限步内终止。）</p><p>(注：循环是可能发生的[Cunningham]，但Cook提到“尽管据我们所知，迄今为止它从未在实际问题的解决中发生)</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040310879.png" alt=""></p><p>令h=pq是T中的一条弧，我们说h在T中是远离(away from)r的，如果p∈R(T,h)；否则h在T中是朝向(toward)r的。上面由一棵树和一条弧导出的划分的图中弧h是远离r的。</p><ul><li><strong>Def3</strong>.一棵树T称为强可行的(strongly feasible)，如果它确定了一个可行流x，使得对T的每条满足xh=0的弧h，h在T中是远离r的。</li></ul><p>注意到如果流不是退化的，那么树平凡的满足这个条件，还注意到我们用来初始化算法的树是强可行的。现在假设我们从一棵强可行的树开始算法，并在算法的每一步，我们按如下规则选择弧h：</p><ul><li><strong>离开弧规则：</strong>选择h是C(T,e)中第一条满足x(h)=θ的反向弧。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041729373.png" alt=""></p><ul><li><strong>Thm4.</strong> 如从一棵强可行树开始并使用离开弧规则的网络单纯形法在有限步后会终止。</li></ul><p>下面我们将转运问题的上述讨论直接扩展到解决一般的最小费用流问题的网络单纯形法，它也可以看作是对线性规划的有界变量单纯形法的一个解释。</p><ol><li><p>定义树解是一个向量x∈R^{E}，s.t.对某棵树T以及E\T的划分(L,U)我们有</p><p>fx(v)=b(v)，对所有v∈V；</p><p>x(e)=0，对所有e∈L；</p><p>x(e)=u(e)，对所有e∈U.</p></li><li><p>添加一条弧形成的圈C(T,L,U,e)满足以下性质：</p><p>C(T,L,U,e)的每条弧都是T∪{e}的元素；</p><p>如果e∈L，那么e是C(T,L,U,e)的正向弧，否则是反向弧；</p><p>C(T,L,U,e)的起点s是T中从v和w到r的简单路上的第一个公共顶点。</p></li><li><p>我们可以将之前得到的最优性定理拓展如下：</p></li></ol><ul><li><strong>Prop8</strong>.如果树(T,L,U)确定了可行树解x并且对每个e∉T,C(T,L,U,e)都有非负的费用，那么x是最优的。</li></ul><p>好，现在我们可以叙述本算法了。<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040336007.png" alt=""></p><p>参考文献： Bernhard Korte and Jens Vygen《Combinatorial Optimization Theory and Algorithms(6th,2018)》</p><p>​         Cook等《Combinatorial Optimization》</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在实际应用中。迄今为止求解线性规划问题著名的仍是&lt;strong&gt;单纯形法&lt;/strong&gt;。通过专业的实现，它具有卓越的性能：≈1000变量和≈1000约束的问题可以在0.1到0.5秒内得到解决。那么，是否可以尝试将该算法应用于图论中的问题呢？&lt;/p&gt;
&lt;p&gt;​    事实上，最重要的网络优化问题都可以用线性规划来表示，比如确定最短路、最大流、最小费用流等。然而，直接应用通常的单纯形算法是没有意义的，因为所产生的程序将是&lt;strong&gt;笨拙(unwiedy)&lt;/strong&gt;和&lt;strong&gt;高度退化的(highly degenerate)&lt;/strong&gt;。这两个问题通过使用单纯形法的适当图论特殊化来避免：&lt;strong&gt;网络单纯形法&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="组合优化" scheme="http://yuanquanquan.top/tags/%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>FFT的CUDA实现</title>
    <link href="http://yuanquanquan.top/2021/20210605/"/>
    <id>http://yuanquanquan.top/2021/20210605/</id>
    <published>2021-06-05T03:10:10.000Z</published>
    <updated>2021-06-05T06:15:35.196Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一维FFT算法在Maxwell架构上，归为访存密集算法。<br>即，在足够优化的情况下，可在一次memory copy的耗时内完成计算。</p><p>本文实现的FFT算法达到与官方库cuFFT一致的速度，通过整合kernel，可实现比调用CUFFT更快的算法整体执行速度。在处理65536*4以上大点数一维FFT+IFFT计算时（一个大核心共享内存放不下完整的一维FFT数据），组合算法可以实现比CUFFT少2个kernel调用的时间（减少两次显存数据交换），主要说明4096点FFT算法设计的思路及实现。大点数仅说明方法和测试结果。</p></blockquote><span id="more"></span><h2 id="算法原理及设计思路"><a href="#算法原理及设计思路" class="headerlink" title="算法原理及设计思路"></a>算法原理及设计思路</h2><p>本节说明快速傅里叶变换（Fast Fourier Transform）的原理和数值计算过程，重介绍能发挥GPU架构优势的算法类型。</p><h3 id="常规FFT实现（Cooley-Tukey）"><a href="#常规FFT实现（Cooley-Tukey）" class="headerlink" title="常规FFT实现（Cooley-Tukey）"></a>常规FFT实现（Cooley-Tukey）</h3><h4 id="公式推导与计算结构"><a href="#公式推导与计算结构" class="headerlink" title="公式推导与计算结构"></a>公式推导与计算结构</h4><h4 id=""><a href="#" class="headerlink" title=""></a><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210605141428000.png" alt=""></h4><p>旋转因子W具有对称、周期、可约的特点：<br>$$<br>\begin{array}{c}<br>W_{N}^{k+\frac{N}{2}}=-W_{N}^{k} \<br>W_{N}^{2 k r}=e^{-j \frac{2 \pi}{N} 2 k r}=e^{-j \frac{2 \pi}{N / 2} k r}=W_{N / 2}^{k r} \<br>W_{N}^{2}=e^{-j \frac{2 \pi}{N} 2}=e^{-j \frac{2 \pi}{N / 2}}=W_{N / 2}^{1}<br>\end{array}<br>$$<br>Cooley和Tukey利用旋转因子的特性，将DFT进行奇偶分解：<br>$$<br>\begin{aligned}<br>X(k) &amp;=D F T[x(n)]=\sum_{n=0}^{N-1} x(n) W_{N}^{n k}=\sum_{r=0}^{N / 2-1} x(2 r) W_{N}^{2 r k}+\sum_{r=0}^{N / 2-1} x(2 r+1) W_{N}^{(2 r+1) k} \<br>&amp;=\sum_{r=0}^{N / 2-1} x_{1}(r) W_{N}^{2 r k}+W_{N}^{k} \sum_{r=0}^{N / 2-1} x_{2}(r) W_{N}^{2 r k}=\sum_{r=0}^{N / 2-1} x_{1}(r) W_{N / 2}^{r k}+W_{N}^{k} \sum_{r=0}^{N / 2-1} x_{2}(r) W_{N / 2}^{r k} \<br>&amp;=X_{1}(k)+W_{N}^{k} X_{2}(k)<br>\end{aligned}<br>$$<br>式中，$X_1(k)$ 和$X_2(k)$ 分别是$X_1(r)$和$X_2(r)$的N/2点DFT。</p><p>分解之后只得到N/2点的序列，而$X(k)$有N点，还要计算另一半项数的结果，利用旋转因子的周期性：<br>$$<br>X_{1}\left(\frac{N}{2}+k\right)=\sum_{r=0}^{N / 2-1} x_{1}(r) W_{N / 2}^{r(N / 2+k)}=\sum_{r=0}^{N / 2-1} x_{1}(r) W_{N / 2}^{r k}=X_{1}(k)<br>$$</p><p>前半部分：<br>$$<br>X(k)=X_{1}(k)+W_{N}^{k} X_{2}(k) \quad, \quad k=0,1, \ldots, \frac{N}{2}-1<br>$$<br>后半部分：<br>$$<br>X\left(\frac{N}{2}+k\right)=X_{1}\left(\frac{N}{2}+k\right)+W_{N}^{(k+N / 2)} X_{2}\left(\frac{N}{2}+k\right)=X_{1}(k)-W_{N}^{k} X_{2}(k) \quad, \quad k=0,1, \ldots, \frac{N}{2}-1<br>$$<br>因此，只要求出(0,N/2)区间内所有$X_1(k)$ 和$X_2(k)$ 值，即可求出(0,N-1)区间内所有$X(k)$值，<strong>计算量“减半”</strong>。</p><p>FFT算法有很多结构，但通常可归为两类，时域抽取法FFT（Decimation-In-Time FFT，简称DIT-FFT）和频域抽取法FFT（Decimation-In-Frequency FFT，简称DIF-FFT）。Cooley-Tukey方法的8点基2 FFT计算流程图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbip3hfrba1d9h11uu13da1hmq2n-20210605141027100.png" alt=""><br>图1. 8点基2 DIT-FFT计算流程图</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbip3o5v1lb91ml77n6pnsc6734-20210605141030680.png" alt=""><br>图2. 8点基2 DIF-FFT计算流程图</p><p>从图1和图2可以清晰地看出Cooley-Tukey方法的特点：</p><ul><li>从输入输出来看，DIT-FFT需要先把顺序数据倒序（Bit-reverse），再进行计算，可以得到顺序的计算结果。DIF-FFT直接用顺序数据计算，但是得到的结果是倒序的，如果需要顺序结果，还要进行处理。</li><li>从计算结构来看，DIT-FFT和DIF-FFT具有相反的计算结构，DIT-FFT的计算跨度（stride）是2L ，即1,2,4…2L-1 ，DIF-FFT的计算跨度相反。</li><li>从计算过程来看，Cooley-Tukey方法具有原址（In-place）计算的特点，一个蝶形单元的计算结果还是放到原来的位置，不需要分配暂存空间。</li></ul><p>DFT与基-2 Cooley-Tukey DIT-FFT的程序如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">void DFTImag(Complex *Input, Complex *Output, int Amount)</span><br><span class="line">&#123;</span><br><span class="line">    int k, n;</span><br><span class="line">    Complex tmp;</span><br><span class="line">    float SumRe, SumIm, ReFactor, ImFactor, TAOPTK;</span><br><span class="line">    float TAOP = 2 * my_Pi / Amount;</span><br><span class="line">    for (k = 0; k &lt; Amount; k++)</span><br><span class="line">    &#123;</span><br><span class="line">        SumRe = 0; SumIm = 0;</span><br><span class="line">        TAOPTK = TAOP*k;</span><br><span class="line">        for (n = 0; n &lt; Amount; n++)</span><br><span class="line">        &#123;</span><br><span class="line">            ReFactor = cos(TAOPTK * n);</span><br><span class="line">            ImFactor = -sin(TAOPTK * n);</span><br><span class="line">            tmp = Input[n];</span><br><span class="line">            SumRe += tmp.x * ReFactor - tmp.y * ImFactor;   </span><br><span class="line">            SumIm += tmp.x * ImFactor + tmp.y * ReFactor;</span><br><span class="line">        &#125;</span><br><span class="line">        tmp.x = SumRe; tmp.y = SumIm;</span><br><span class="line">        Output[k] = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">void CTFFT_R2(Complex *array, Complex *array_out, unsigned int Power)</span><br><span class="line">&#123;</span><br><span class="line">    //---------------------init------------------</span><br><span class="line">    int len = pow((double)2, (int)Power);</span><br><span class="line">    bit_reverse(array, array_out, Power);            //倒序，数据放到array_out</span><br><span class="line">    //--------------------calcu------------------</span><br><span class="line">    for (int m = 2; m &lt;= len; m &lt;&lt;= 1)               //第一层计算单元：2  第二层：4   ...  第L层：2^L </span><br><span class="line">    &#123;</span><br><span class="line">        int mh = m &gt;&gt; 1;                             //单元计算跨度 1 2 4 </span><br><span class="line">        for (int i = 0; i &lt; mh; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            Complex wi = exp_calcu(-i*(my_Pi / mh)); //本来(2*pi/m) &gt;&gt; pi/(mh)  </span><br><span class="line">            for (int j = i; j &lt; len; j += m)         //j  第一层0 2 4 6   第二层0 4 (8&gt;=len 跳出)</span><br><span class="line">            &#123;                                                </span><br><span class="line">                Complex u = array_out[j];</span><br><span class="line">                Complex t = complx_mul(wi, array_out[j + mh]);</span><br><span class="line">                array_out[j] = complx_add(u, t);</span><br><span class="line">                array_out[j + mh] = complx_sub(u, t);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>本机CPU端计算4096点结果：<br>DFT的时间在2000ms左右，而FFT是4ms （Debug模式）。<br>DFT的时间在800ms左右，而FFT在1ms以内 （Release模式）。</p><p><strong>考虑到GPU的硬件架构，并行化Cooley-Tukey FFT算法存在以下问题：</strong></p><h4 id="倒序计算"><a href="#倒序计算" class="headerlink" title="倒序计算"></a><strong>倒序计算</strong></h4><p>​    数据倒序是对序列进行奇偶抽取的结果，倒序（bit reverse）是按位进行的，把数据的MSB和LSB反向，一个简单例子是0010 0000 =&gt; 0000 0100。下面分别是8bit倒序的较快算法，本文实现的根据数据位宽倒序的算法和CUDA MATH API中的32bit数据倒序程序调用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">unsigned char reverse_8bit(unsigned char x)</span><br><span class="line">&#123;</span><br><span class="line">    x = (((x &amp; 0xaa) &gt;&gt; 1) | ((x &amp; 0x55) &lt;&lt; 1));  // 交换每两位</span><br><span class="line">    x = (((x &amp; 0xcc) &gt;&gt; 2) | ((x &amp; 0x33) &lt;&lt; 2));  // 交换每四位中的前两位和后两位</span><br><span class="line">    return((x &gt;&gt; 4) | (x &lt;&lt; 4));                  // 交换前后两个4bit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">unsigned int </span><br><span class="line">index_rv(unsigned int index, unsigned int Power)</span><br><span class="line">&#123;</span><br><span class="line">    unsigned int tmp = 0;</span><br><span class="line">    unsigned int bit_value = 0;</span><br><span class="line">    //power*6的计算量</span><br><span class="line">    for (int i = 0; i &lt; Power; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        bit_value = index &amp; 1;</span><br><span class="line">        tmp += (bit_value &lt;&lt; ((Power - 1) - i));</span><br><span class="line">        index = index &gt;&gt; 1;</span><br><span class="line">    &#125;</span><br><span class="line">    return tmp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    可以看出，通常情况下要对数据进行几十次操作才能得出倒序结果。测试中使用了CUDA的库函数进行倒序，并没有比reverse_8bit快。另一个问题是，FFT kernel占用较多硬件资源，一个大核心只能保证2到3个block占用。<br>大量倒序计算在一些情况下会使访存密度降低（<strong>这里的意思是，在做到足够优化的情况下，算法是访存密集的，如果计算量再增加，就是计算密集的了，相同数据量的算法执行时间会增加</strong>），测试结果如下，左边表格是全带宽占用结果，红色箭头处带宽占用不满：<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiphbb91p9g1qskdoj77q1nki3h.png" alt=""><br>图3. 倒序与否kernel带宽占用测试对比</p><p>​    本文想到的解决办法是，使用查找表，通过纹理缓冲从显存中读取倒序索引（不占用共享内存），24KB的L1 cache基本满足小点数查找表需求，大点数会由于片上存不下，需要多次访问显存。</p><p>但倒序的计算还不是影响算法速度的关键因素。</p><h4 id="倒序存储"><a href="#倒序存储" class="headerlink" title="倒序存储"></a><strong>倒序存储</strong></h4><p>《Maxwell硬件架构与编程方法》中详细说明了GPU存储结构和使用注意事项，从算法局部性考虑，FFT算法的输入数据需要从显存顺序读到片上，在共享内存中进行倒序。而共享内存分为32个32bit的存储体（bank），在这一级倒序读取或存储都会导致存储体冲突（bank conflict）。根据倒序特点和单精度浮点的复数数据结构（两个float），实际地共享内存带宽占用仅有1/16。实测结果与理论分析一致：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiqa74thds13skf671bo11jsj3u.png" alt=""><br>图4. 无冲突共享内存访问测试</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiqaft7pq7fub1t4cg51nhv4b.png" alt=""><br>图5. 倒序冲突共享内存访问测试</p><p>​    注意，图4和图5中的数据量不同，表格第二列是传输数据量，在无冲突访存时，加载数据量=存储数据量=实际数据量。在有冲突时，数据需要排队，而每次传输数据都是32<em>32bit，所以浪费了很多带宽。图9中，加载数据量=实际数据量，倒序存储量=16</em>实际数据量，即有效带宽占用仅为全带宽的1/16。</p><p>​    本文还做了一个无冲突共享内存访问基准测试。Maxwell架构下，数据从显存读入片上，在保证完整占用显存带宽的情况下，可以在片上进行12次无冲突的数据交换，即24次存储或读取。而一次数据倒序存储就使用了16/24的时间，那么留给计算的时间就远远不够了。<br>所以，要高效地利用GPU计算和存储资源，就应尽量避免倒序。</p><h4 id="非向量化的数据抽取方式"><a href="#非向量化的数据抽取方式" class="headerlink" title="非向量化的数据抽取方式"></a><strong>非向量化的数据抽取方式</strong></h4><p>​    使用CUDA并行编程，数据是按1个warp，32个线程为基本单位读取的，再加上GPU存储器特点，并行化的算法数据存取结构尽可能实现连续无间隔地访存。先不考虑倒序，以32点DIT-FFT为例：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiqfa8ulvg1psnas1dp51mut4o-20210605135223962.png" alt=""><br>图6. 32点DIT-FFT 数据抽取流程示意</p><p>​    因为是浮点复数，占用64bit，相当于有16个64bit的bank。假设用16个线程来计算，那么第一级数据的第一次读取是0-15号线程读取数据0 , 2 , … , 30。其中0,2,4,6,8,10,12,14与16,18,20,22,24,26,28,30存在bank conflict，第二次读取同理。即第一次读取有一半的bank存在冲突，共享内存带宽利用率只有一半，第一次存储同理。第二级数据可以也类似，访问索引0,4,8,12,1,5,9,13的数据被合并到一起访问，与16,20,24,28,17,21,25,29冲突……直到最后一级（32点基-2运算共5级，图中值标了前4级，最后一级类似图4的stage3）才能分别连续访问0-15与16-31，利用完整带宽。前4级的数据交换共8次，实际消耗了16次无冲突数据交换的时间，浪费了1/2的带宽。</p><p>采用基-4结构的话，只有前两级有存储冲突，但每级带宽利用率是1/4，与基-2是等效的。基-8，基-16的计算结构同理，好处是后续计算级数更少。</p><p><strong>理论和基准测试结果表明，采用传统的FFT计算结构不利于在GPU架构上高效地进行计算。</strong></p><h3 id="向量化的DFT分解方法（Stockham-Autosort-Framework）"><a href="#向量化的DFT分解方法（Stockham-Autosort-Framework）" class="headerlink" title="向量化的DFT分解方法（Stockham Autosort Framework）"></a>向量化的DFT分解方法（Stockham Autosort Framework）</h3><h4 id="计算分解与结构"><a href="#计算分解与结构" class="headerlink" title="计算分解与结构"></a><strong>计算分解与结构</strong></h4><p>​    离散傅里叶变换可以非常多形式的分解，Cooley-Tukey FFT很好的减少了算法的空间占用（原址计算），但也存在数据倒序的问题。本小节说明Stockham结构的FFT实现，Stockham 方法以空间占用为代价避免了Cooley-Tukey过程的倒序计算。它的自动倒序结构背后的思想是把数据倒序和蝶形计算结合起来。</p><p>​    上一小节提到过DFT的矩阵形式, 可以简记为 $F(N)=W^{k n} f(k)$ 和 $f(k)=\frac{1}{N} W^{-k n} F(N)$ 。从矩阵分解的角度看, Cooley-Tukey FFT做了以下处理：倒序步骤用矩阵表示是一个置换矩阵 $P_{n}^{T} ;$ 包含旋转因子大矩阵 $W^{k n}$ 被分解为许多级<br>Tukey FFT可用矩阵表示为 $X(N)=F_{n} x(k)=A_{t} \cdots A_{1} P_{n}^{T} x(k)$ 。即数据经倒序和t级基-2运算，最终得到与原始 DFT计算方法相同的傅里叶变换结果。<br>​    用同样的方式说明Stockham的方法。与Cooley-Tukey显式的置换 $F_{n}=A_{t} \cdots A_{1} P_{n}^{T}$ 不同，Stockham通过置换矩陈 $\Gamma_{0} \cdots \Gamma_{t-1}$ 使得 $F_{n}=A_{t} \Gamma_{t-1} \cdots A_{2} \Gamma_{1} A_{1} \Gamma_{0}$, 即 $X(N)=F_{n} x(k)=A_{t} \Gamma_{t-1} \cdots A_{2} \Gamma_{1} A_{1} \Gamma_{0} x(k)$</p><p>对比两者的计算过程：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirag192o5dihvum10748sh5i.png" alt=""></p><p>Cooley-Tukey 方法首先把输入序列x完整地倒序，再进行各级蝶形计算。Stockham 方法把倒序结构和蝶形计算结合起来，在进行蝶形计算之前将序列x进行部分倒序，经过多级倒序和计算之后，得到与和Cooley-Tukey 方法相同的计算结果。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirbbse2h31t7a10e2lpamhd5v.png" alt=""><br>图7. 8点基-2 Stockham FFT计算流程图（DIT）<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirbgnt12t71c3g15ka1oi31ul16c.png" alt=""><br>图8. 8点基-2 Stockham FFT计算流程图（DIF）</p><p>图7和图8画出了DIT 和DIF两种Stockham FFT的计算流程，可以总结出以下特点：</p><ul><li>输入输出都是顺序的，没有显式的倒序步骤。</li><li>（级间有倒序处理）计算是非原址的（Out-place），蝶形单元的计算结果可能会覆盖还没计算的数据，这就要求分配和序列长度相等的空间用来存储。</li><li>Stockham DIT-FFT的数据读取索引是不变的，蝶形单元的数据存储跨度（stride）根据所在级数（stage）由1，2，一直到2L-1；DIF-FFT的数据存储索引是不变的，蝶形单元的数据读取跨度（stride）根据所在级数（stage）由1，2，一直到2L-1。这里，DIF的计算结构基本上与DIT相反，但计算结果一致。</li></ul><h4 id="算法实现与数据抽取"><a href="#算法实现与数据抽取" class="headerlink" title="算法实现与数据抽取"></a><strong>算法实现与数据抽取</strong></h4><p>把《Computational Frameworks for the Fast Fourier Transform》书中1.7节的Stockham 基-2算法（书P57，PDF的第75页）用C语言实现如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbire059msc1o931bl61sth14k76p.png" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">void stockham_fft_R2(Complex* x, Complex* y, int len, int power)</span><br><span class="line">&#123;</span><br><span class="line">    Complex wi, tau;</span><br><span class="line">    int L, L_d2, stride, stride_m2;</span><br><span class="line">    for (int q = 1; q &lt;= power; q++)</span><br><span class="line">    &#123;</span><br><span class="line">                                             //L*r=len</span><br><span class="line">        L         = pow((double)2, q);       //L      : 2 4 8 ... 计算组大小</span><br><span class="line">        stride    = len / L;                 //stride ：...4 2 1 计算组内的跨度</span><br><span class="line">        L_d2      = L / 2;</span><br><span class="line">        stride_m2 = len / L_d2;</span><br><span class="line">        memcpy(y, x, len * sizeof(Complex));</span><br><span class="line">        for (int j = 0; j &lt; L_d2; j++)       //j只有L的一半（R-2蝶形计算 两个一组）</span><br><span class="line">        &#123;</span><br><span class="line">            wi = exp_calcu(-j*(2 * my_Pi / L));</span><br><span class="line">            for (int k = 0; k &lt; stride; k++)</span><br><span class="line">            &#123;</span><br><span class="line">                tau = complx_mul(wi, y[j*stride_m2 + k + stride]);</span><br><span class="line">                x[j*stride + k] = complx_add(y[j*stride_m2 + k], tau);</span><br><span class="line">                x[(j + L_d2)*stride + k] = complx_sub(y[j*stride_m2 + k], tau);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以32点Stockham 基-2 DIT-FFT计算过程的数据抽取为例。由图9可知：前4级都可实现向量化数据读取，向量化长度分别为16、8、4、2，最后一级跨度为2，不是连续的访问；所有级的数据存储都是长度为16的向量化访问。数据读取的向量化长度是和计算点数有关的，比如256点的计算，向量化长度就会是128、64、32、16、8、4、2，以此类推。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiriqhu151m1f042juaeu1d2676.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirj163rvm1o3rv0lj681k0i7j.png" alt=""><br>图9. 32点Stockham 基-2 DIT-FFT算法的数据抽取示意（共5级）</p><p>结合GPU硬件，共享内存有16个64bit的bank，基-2计算在向量化访问长度为8、4、2、1（为表述方便，把最后一级也加进来，后同）的计算级，带宽利用率都是1/2。如果用基-4作为基本蝶形单元的话，256点计算级数为4级，访存的向量化长度是64、16、4、1，最后两级的带宽利用率为1/4，与基-2计算的4级1/2带宽利用率等效，但计算级数与基-2相比减少一半。</p><p>相比Cooley-Tukey方法Stockham FFT在GPU上实现，可以避免倒序计算和倒序存储两个重要问题。而且CUDA的线程以32个为最小单位同时执行，是向量化的处理机制，Stockham FFT中各级数据的向量化访问，非常适合用GPU来实现。虽然也有存储冲突问题，但在当前架构下基-4计算的(8-2)/24的冲突是可以接受的，剩下的时间可以完成FFT计算。GPU拥有寄存器、共享内存、片外显存的存储层次，Stockham计算的非原址问题并没有影响。在两级计算过程中，线程有足够的寄存器资源把共享内存中的数据全部读出，并行计算，再放回共享内存。</p><p>所以本文在GPU上的FFT算法实现，采用Stockham方法。下一节将会叙述Stockham FFT的CUDA实现。</p><hr><h2 id="4096点STFFT基本实现"><a href="#4096点STFFT基本实现" class="headerlink" title="4096点STFFT基本实现"></a>4096点STFFT基本实现</h2><h3 id="计算框架与工程方法"><a href="#计算框架与工程方法" class="headerlink" title="计算框架与工程方法"></a><strong>计算框架与工程方法</strong></h3><p>《Maxwell硬件架构与编程方法》中介绍过Maxwell架构下，每个大核心（SMM）内部有128个小核心（Core）、65536个32bit的寄存器文件（Register File）和96KB的共享内存空间（Shared Memory）。在软件层面，每个线程（thread）最多使用256个寄存器，每个线程块（blcok）最多分配1024个线程，最多占用65536个寄存器和49152bytes的共享内存。</p><p>49152bytes共享内存可存储12288个单精度浮点数据（FP32，4bytes），6144个复数（两个FP32）。由于FFT有多级数据交换，为了保证算法的运行时间，数据要放到片上计算，所以考虑到存储能力，理论上一次可以计算4096点。故从共享内存看，每个block需要32768bytes空间，一个大核心可以保证3个block的占用。</p><p>寄存器方面，为了4096点计算同时进行，需要8192个32bit寄存器用于数据存储，数值计算过程也会占用寄存器资源，后续还会说到寄存器占用情况和优化方法。实际上寄存器还有盈余，所以8192点的计算也可以勉强在片上一次性完成计算。</p><p>由上述架构特点和前一节的理论分析，采用如下计算框架：</p><ul><li>基-4作为基本蝶形计算单元，6级基-4完成4096点的Stockham FFT计算。</li><li>每个block分配4096*8bytes = 32768bytes的共享内存空间用于级间数据交换。</li><li>每个block分配256个线程，为同时计算4096点数据，每个线程一次读取16点数据进行处理。</li></ul><p>《Computational Frameworks for the Fast Fourier Transform》书中2.4.4节有Stockham 的基-4算法（书P105，PDF的第123页）：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirv979a311sr1qpnnkn33180.png" alt=""></p><p>//201710回忆<br>//L*=L/4，应该是一个R4在L的组内抽取跨度<br>//r=n/L，是大组数<br>//W是按组内算的，所以中循环j</p><p>用C语言实现的代码较长，本文简介方法。</p><p>在C语言验证工程中，首先用递增数给序列赋值，再新建一个txt文件，调用stockham_fft_R4子程序完成计算后，<br>把结果放到txt文件中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> for (int i = 0; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        array[i].x = i;</span><br><span class="line">        array[i].y = i;</span><br><span class="line">    &#125;</span><br><span class="line">    char txtdataFileName[1024];</span><br><span class="line">    FILE * TxtWriter1;</span><br><span class="line">    sprintf(txtdataFileName, &quot;StockhamFFTdata.txt&quot;);</span><br><span class="line">    TxtWriter1 = fopen(txtdataFileName, &quot;wb&quot;);</span><br><span class="line">    stockham_fft_R4(array, FFTarray, len, power4, -1);</span><br><span class="line">    for (int i = 0; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        fprintf(TxtWriter1, &quot;%f \r\n&quot;, array[i].x);</span><br><span class="line">        fprintf(TxtWriter1, &quot;%f \r\n&quot;, array[i].y);</span><br><span class="line">    &#125;</span><br><span class="line">//---------------------------------------------------------------------------</span><br><span class="line">    for (int i = 0; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        array[i].x = i;</span><br><span class="line">        array[i].y = i;</span><br><span class="line">    &#125;</span><br><span class="line">    sprintf(txtdataFileName, &quot;StockhamFFTindex.txt&quot;);</span><br><span class="line">    TxtWriter1 = fopen(txtdataFileName, &quot;wb&quot;);</span><br><span class="line">    stockham_fft_R4_index(array, FFTarray, len, power4, -1, TxtWriter1);</span><br></pre></td></tr></table></figure><p>在MATLAB中用同样的方法调用库函数fft()得到结果，把C语言工程的结果导入MATLAB，对比两个序列的相关系数、标准差和序列的插值，进行计算结果验证。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">clear all;</span><br><span class="line">n=<span class="number">0</span>:<span class="number">4095</span>;</span><br><span class="line">x=n+n*<span class="built_in">i</span>;</span><br><span class="line">Y=fft(x,<span class="number">4096</span>);</span><br><span class="line">Y=Y.&#x27;;       <span class="comment">%转置用了 共轭转置  应该用.&#x27;</span></span><br><span class="line">data_index=[<span class="string">&#x27;E:/StockhamFFTdata.txt&#x27;</span>];</span><br><span class="line">FFT4096 =(importdata(data_index));</span><br><span class="line">FFT4096_c=FFT4096(<span class="number">1</span>:<span class="number">2</span>:<span class="number">8192</span>)+<span class="built_in">j</span>*FFT4096(<span class="number">2</span>:<span class="number">2</span>:<span class="number">8192</span>);</span><br><span class="line">corrcoef(<span class="built_in">real</span>(FFT4096_c),<span class="built_in">real</span>(Y))</span><br><span class="line">corrcoef(<span class="built_in">imag</span>(FFT4096_c),<span class="built_in">imag</span>(Y))</span><br><span class="line">std(FFT4096_c)</span><br><span class="line">std(Y)</span><br><span class="line">err=Y-FFT4096_c;</span><br><span class="line"><span class="built_in">max</span>(err)</span><br></pre></td></tr></table></figure><p>本文在子程序stockham_fft_R4()的基础上设计了一个把各级FFT计算中的数据抽取索引，旋转因子计算索引打印到txt文档中的子程序stockham_fft_R4_index()。这样可以直观快速地查看数值计算过程，本文的后续实现也大量使用这种方法。</p><h3 id="CUDA实现"><a href="#CUDA实现" class="headerlink" title="CUDA实现"></a><strong>CUDA实现</strong></h3><h4 id="C语言验证"><a href="#C语言验证" class="headerlink" title="C语言验证"></a>C语言验证</h4><p>GPU端分配一个256线程的block进行计算，每个线程每次负责16个数据的计算，占用32768bytes的共享内存空间。每一级计算的数据抽取方式都不同。为尽快尝试和验证算法，本文首先在CPU端用的C语言模拟GPU的多线程并行情况，得到正确结果后再移植到CUDA C工程中。</p><p>输入序列首地址data_in，相同大小的暂存空间首地址data_out和计算方向direction（正向FFT计算，-1）。首先声明自动变量，index用于保存访存下标索引，wi_c和wi保存旋转因子的浮点和复数数据，16个Complex变量用于保存每个线程读取的16个数据。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">StockhamR4_FFT_4096</span><span class="params">(Complex *data_in, Complex *data_tmp, <span class="keyword">int</span> direction)</span></span></span><br><span class="line"><span class="function"></span>&#123;   </span><br><span class="line">    <span class="comment">//---------------------init-------------------------</span></span><br><span class="line">    <span class="keyword">int</span>  index = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> wi_c = <span class="number">0</span>;</span><br><span class="line">    Complex wi;</span><br><span class="line">    Complex r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12, r13, r14, r15, r16;</span><br></pre></td></tr></table></figure><p>下面是4096点FFT的第1层计算代码，一共6级计算，基本保持这样的形式。</p><p>通过256次for循环模拟256个线程的并行计算，变量tid代表线程号。</p><p>从序列data_in中抽取数据，由于非原址的结构，计算完成后为避免覆盖其他位置的数据，将结果存到data_tmp中，下一次再从data_tmp取数，计算后存到data_in……以此往复，直到计算完成。</p><p>寄存器r1,r2,r3,r4中保存的是一个基-4蝶形单元的数据，在第一级，4096点按基-4抽取，跨度4096/4=1024。所以同一个tid的r1,r2,r3,r4地址偏移按1024递增。第一组的4个数据抽取，在并行情况下256线程实际抽取了4096点数据的4个1024数据段中的前256个数据。所以之后的r5,r6,r7,r8在读取数据时，索引要加上256，以此类推。</p><p>读取数据后，每个线程计算4组基-4单元。这里调用了基础模块StockhamR4_block()，传入4个数据和1个旋转因子的地址，传入计算旋转因子需要的两个常数wi_c和index（根据线程不同，index不同；根据计算层级不同，wi_c同），图19中第一层旋转因子的基本常数wi_c是 -2π/4 ，而索引index都为0。</p><p>计算层级基本保持这一形式，由于数据存储方式不变，之后的示例仅截取for循环的前半部分进行说明。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////Level in 4////////////////////////////////////////////////////////////////</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> tid = <span class="number">0</span>; tid &lt; <span class="number">256</span>; tid++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//本层 in out index 相同</span></span><br><span class="line">    r1  = data_in[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">    r2  = data_in[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">    r3  = data_in[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">    r4  = data_in[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">    r5  = data_in[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">    r6  = data_in[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">    r7  = data_in[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">    r8  = data_in[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">    r9  = data_in[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">    r10 = data_in[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">    r11 = data_in[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">    r12 = data_in[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">    r13 = data_in[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">    r14 = data_in[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">    r15 = data_in[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">    r16 = data_in[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">    wi_c = <span class="number">2</span> * my_Pi / <span class="number">4</span> * direction;</span><br><span class="line">    StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">0</span>] = r1;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">0</span>] = r2;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">0</span>] = r3;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">0</span>] = r4;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">1</span>] = r5;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">1</span>] = r6;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">1</span>] = r7;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">1</span>] = r8;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">2</span>] = r9;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">2</span>] = r10;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">2</span>] = r11;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">2</span>] = r12;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">3</span>] = r13;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">3</span>] = r14;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">3</span>] = r15;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">3</span>] = r16;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>按理论分析，第二层级以1024点为计算单元，还是基-4抽取，1024除以4，跨度是256。同理，第三层级计算单元为256点，内部跨度64；第四层级计算单元为64点，内部跨度16；第五层级计算单元为16点，内部跨度4；第六层级计算单元为4点，内部跨度1。</p><p>本文的设计是256线程并行，所以下面第二层级的数据抽取恰好是256*4个变量存一个1024计算单元，变量1-4、5-8、9-12、13-16分别是第1、2、3、4个1024计算单元内的基-4一组蝶形运算，对应旋转因子计算索引0、1、2、3。层级的旋转因子常数是-2π/16。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">r1  = data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">r2  = data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">r3  = data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">r4  = data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">r5  = data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">r6  = data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">r7  = data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">r8  = data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">r9  = data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">r10 = data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">r11 = data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">r12 = data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">r13 = data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">r14 = data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">r15 = data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">r16 = data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">wi_c = <span class="number">2</span> * my_Pi / <span class="number">16</span> * direction;</span><br><span class="line">StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, <span class="number">1</span>);</span><br><span class="line">StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, <span class="number">2</span>);</span><br><span class="line">StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, <span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>第三层计算单元为256点，内部跨度为64点。并行的线程数是256，如果还是一次读取连续的256点，每个线程只能得到单元内的一个数据，不能计算，所以需要根据线程号来划分各自的计算区域。256点（计算单元长度）除以4（基-4计算变量个数）得到64，所以连续的64个线程负责一个256单元的计算。由于一共有16个变量，所以64个线程计算了4组256单元。整体情况是256个线程前4次读取了1024点数据，4组连续的64线程分别负责4块256单元的计算，后面3组4变量读取按照这种形式分别进行了1024点的计算。</p><p>如下面程序所示，与上两层计算不同，第三层首先计算了数据抽取索引index。线程号tid的范围0-255，tid&amp;(256 - 64)可以得到0、64、128、192四个数，即以64为单位将线程分为4组。每组线程负责256点的计算，所以 (tid&amp;(256 - 64)) <em> 4得到0、256、512、768。然后加上tid%64得到线程的组内序号。在读取数据时，以索引index为基地址，进行偏移即可，第三层级计算跨度为64，可以从图21看到读取方式符合这一形式。<br>第三层有16个256计算单元，所以基-4计算索引根据64线程组和4变量组得到：index = tid/64 + 4</em>(0 or 1 or 2 or 3)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//64threads 一组</span></span><br><span class="line">index = (tid&amp;(<span class="number">256</span> - <span class="number">64</span>)) * <span class="number">4</span> + tid % <span class="number">64</span>;</span><br><span class="line">r1 = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">64</span> * <span class="number">0</span>];</span><br><span class="line">r2 = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">64</span> * <span class="number">1</span>];</span><br><span class="line">r3 = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">64</span> * <span class="number">2</span>];</span><br><span class="line">r4 = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">64</span> * <span class="number">3</span>];</span><br><span class="line">r5 = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">64</span> * <span class="number">0</span>];</span><br><span class="line">r6 = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">64</span> * <span class="number">1</span>];</span><br><span class="line">r7 = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">64</span> * <span class="number">2</span>];</span><br><span class="line">r8 = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">64</span> * <span class="number">3</span>];</span><br><span class="line">r9 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">64</span> * <span class="number">0</span>];</span><br><span class="line">r10 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">64</span> * <span class="number">1</span>];</span><br><span class="line">r11 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">64</span> * <span class="number">2</span>];</span><br><span class="line">r12 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">64</span> * <span class="number">3</span>];</span><br><span class="line">r13 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">64</span> * <span class="number">0</span>];</span><br><span class="line">r14 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">64</span> * <span class="number">1</span>];</span><br><span class="line">r15 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">64</span> * <span class="number">2</span>];</span><br><span class="line">r16 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">64</span> * <span class="number">3</span>];</span><br><span class="line">wi_c  = <span class="number">2</span> * my_Pi / <span class="number">64</span> * direction;</span><br><span class="line">index = tid / <span class="number">64</span>;</span><br><span class="line">StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, index + <span class="number">4</span>*<span class="number">0</span>);</span><br><span class="line">StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, index + <span class="number">4</span>*<span class="number">1</span>);</span><br><span class="line">StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, index + <span class="number">4</span>*<span class="number">2</span>);</span><br><span class="line">StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, index + <span class="number">4</span>*<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>根据之前的分析和第三层级的规律，容易得到之后层级的计算结构。要注意的是，共享内存有32个32bit的bank，到第四层级计算单元长度是64，基-4的计算跨度是16（16个64bit数据），恰好还能满足全带宽访问。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">index = (tid&amp;(<span class="number">256</span> - <span class="number">16</span>)) * <span class="number">4</span> + tid % <span class="number">16</span>;</span><br><span class="line">r1  = data_tmp[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">16</span> * <span class="number">0</span>];</span><br><span class="line">r2  = data_tmp[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">16</span> * <span class="number">1</span>];</span><br><span class="line">r3  = data_tmp[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">16</span> * <span class="number">2</span>];</span><br><span class="line">r4  = data_tmp[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">16</span> * <span class="number">3</span>];</span><br><span class="line">r5  = data_tmp[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">16</span> * <span class="number">0</span>];</span><br><span class="line">r6  = data_tmp[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">16</span> * <span class="number">1</span>];</span><br><span class="line">r7  = data_tmp[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">16</span> * <span class="number">2</span>];</span><br><span class="line">r8  = data_tmp[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">16</span> * <span class="number">3</span>];</span><br><span class="line">r9  = data_tmp[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">16</span> * <span class="number">0</span>];</span><br><span class="line">r10 = data_tmp[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">16</span> * <span class="number">1</span>];</span><br><span class="line">r11 = data_tmp[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">16</span> * <span class="number">2</span>];</span><br><span class="line">r12 = data_tmp[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">16</span> * <span class="number">3</span>];</span><br><span class="line">r13 = data_tmp[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">16</span> * <span class="number">0</span>];</span><br><span class="line">r14 = data_tmp[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">16</span> * <span class="number">1</span>];</span><br><span class="line">r15 = data_tmp[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">16</span> * <span class="number">2</span>];</span><br><span class="line">r16 = data_tmp[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">16</span> * <span class="number">3</span>];</span><br><span class="line">wi_c  = <span class="number">2</span> * my_Pi / <span class="number">256</span> * direction;</span><br><span class="line">index = tid / <span class="number">16</span>;</span><br><span class="line">StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, index + <span class="number">16</span>*<span class="number">0</span>);</span><br><span class="line">StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, index + <span class="number">16</span>*<span class="number">1</span>);</span><br><span class="line">StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, index + <span class="number">16</span>*<span class="number">2</span>);</span><br><span class="line">StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, index + <span class="number">16</span>*<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>第五、六层的16点计算单元、内部跨度4和4点计算单元、内部跨度1都只能占用1/4的带宽（3/4 bank conflict）。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////Level 1024////////////////////////////////////////////////////////////////</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> tid = <span class="number">0</span>; tid &lt; <span class="number">256</span>; tid++)</span><br><span class="line">&#123;</span><br><span class="line">    index = (tid&amp;(<span class="number">256</span> - <span class="number">4</span>)) * <span class="number">4</span> + tid % <span class="number">4</span>;</span><br><span class="line">    r1  = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">4</span> * <span class="number">0</span>];</span><br><span class="line">    r2  = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">4</span> * <span class="number">1</span>];</span><br><span class="line">    r3  = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">4</span> * <span class="number">2</span>];</span><br><span class="line">    r4  = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">4</span> * <span class="number">3</span>];</span><br><span class="line">    r5  = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">4</span> * <span class="number">0</span>];</span><br><span class="line">    r6  = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">4</span> * <span class="number">1</span>];</span><br><span class="line">    r7  = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">4</span> * <span class="number">2</span>];</span><br><span class="line">    r8  = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">4</span> * <span class="number">3</span>];</span><br><span class="line">    r9  = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">4</span> * <span class="number">0</span>];</span><br><span class="line">    r10 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">4</span> * <span class="number">1</span>];</span><br><span class="line">    r11 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">4</span> * <span class="number">2</span>];</span><br><span class="line">    r12 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">4</span> * <span class="number">3</span>];</span><br><span class="line">    r13 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">4</span> * <span class="number">0</span>];</span><br><span class="line">    r14 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">4</span> * <span class="number">1</span>];</span><br><span class="line">    r15 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">4</span> * <span class="number">2</span>];</span><br><span class="line">    r16 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">4</span> * <span class="number">3</span>];</span><br><span class="line">    wi_c  = <span class="number">2</span> * my_Pi / <span class="number">1024</span> * direction;</span><br><span class="line">    index = tid / <span class="number">4</span>;</span><br><span class="line">    StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, index + <span class="number">64</span> * <span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, index + <span class="number">64</span> * <span class="number">1</span>);</span><br><span class="line">    StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, index + <span class="number">64</span> * <span class="number">2</span>);</span><br><span class="line">    StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, index + <span class="number">64</span> * <span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>还要注意，第六层的计算结构与前面几层不同，一个线程负责连续4点数据的计算，所以读取索引是tid<em>4+1024</em>(0 or 1 or 2 or 3)+(0 or 1 or 2 or 3)。其他部分还是延续规律。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">r1  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">0</span>];</span><br><span class="line">r2  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">1</span>];</span><br><span class="line">r3  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">2</span>];</span><br><span class="line">r4  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">3</span>];</span><br><span class="line">r5  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">0</span>];</span><br><span class="line">r6  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">1</span>];</span><br><span class="line">r7  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">2</span>];</span><br><span class="line">r8  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">3</span>];</span><br><span class="line">r9  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">0</span>];</span><br><span class="line">r10 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">1</span>];</span><br><span class="line">r11 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">2</span>];</span><br><span class="line">r12 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">3</span>];</span><br><span class="line">r13 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">0</span>];</span><br><span class="line">r14 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">1</span>];</span><br><span class="line">r15 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">2</span>];</span><br><span class="line">r16 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">3</span>];</span><br><span class="line">wi_c  = <span class="number">2</span> * my_Pi / <span class="number">4096</span> * direction;</span><br><span class="line">index = tid;</span><br><span class="line">StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, index + <span class="number">256</span>*<span class="number">0</span>);</span><br><span class="line">StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, index + <span class="number">256</span>*<span class="number">1</span>);</span><br><span class="line">StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, index + <span class="number">256</span>*<span class="number">2</span>);</span><br><span class="line">StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, index + <span class="number">256</span>*<span class="number">3</span>);</span><br></pre></td></tr></table></figure><h4 id="CUDA-C实现"><a href="#CUDA-C实现" class="headerlink" title="CUDA C实现"></a>CUDA C实现</h4><p>根据C语言验证结果，把程序移植到GPU端，用CUDA实现。考虑到复用，将4096点的计算封装为一个inline模块，在编译时，模块会“嵌入” 到被调用的地方，而不是跳转调用。函数及参数声明如下，传递的是寄存器地址和共享内存首地址data_shared：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> __device__ <span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">Stockham_4096_block</span></span></span><br><span class="line"><span class="function"><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *r1,  Complex *r2,  Complex *r3,  Complex *r4,</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *r5,  Complex *r6,  Complex *r7,  Complex *r8,</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *r9,  Complex *r10, Complex *r11, Complex *r12,</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *r13, Complex *r14, Complex *r15, Complex *r16,</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *data_shared</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span></span><br></pre></td></tr></table></figure><p>总结第2、3、4、5层的数据抽取和计算规律，可以把这几层用一个for循环实现，如图26所示。要注意的是，数据的读取和存储都在共享内存上操作，所以在计算完成后需要调用线程同步API __syncthreads();（第30行），等所有线程都计算完成后，才一起把数据放回共享内存，以免覆盖了其他线程的计算数据。为保证for循环下次读取数据的正确性，在数据放回后后也需要进行线程同步。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////Level 16 64 256 1024///////////////////////////////////////////////////</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=<span class="number">64</span>;i=i*<span class="number">4</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> inexd_tmp = <span class="number">256</span>/i;</span><br><span class="line">    index = (tid&amp;(<span class="number">256</span> - inexd_tmp))*<span class="number">4</span> + tid%inexd_tmp;</span><br><span class="line">    *r1  = data_shared[index + <span class="number">1024</span>*<span class="number">0</span> + inexd_tmp*<span class="number">0</span>];</span><br><span class="line">    *r2  = data_shared[index + <span class="number">1024</span>*<span class="number">0</span> + inexd_tmp*<span class="number">1</span>];</span><br><span class="line">    *r3  = data_shared[index + <span class="number">1024</span>*<span class="number">0</span> + inexd_tmp*<span class="number">2</span>];</span><br><span class="line">    *r4  = data_shared[index + <span class="number">1024</span>*<span class="number">0</span> + inexd_tmp*<span class="number">3</span>];</span><br><span class="line">    *r5  = data_shared[index + <span class="number">1024</span>*<span class="number">1</span> + inexd_tmp*<span class="number">0</span>];</span><br><span class="line">    *r6  = data_shared[index + <span class="number">1024</span>*<span class="number">1</span> + inexd_tmp*<span class="number">1</span>];</span><br><span class="line">    *r7  = data_shared[index + <span class="number">1024</span>*<span class="number">1</span> + inexd_tmp*<span class="number">2</span>];</span><br><span class="line">    *r8  = data_shared[index + <span class="number">1024</span>*<span class="number">1</span> + inexd_tmp*<span class="number">3</span>];</span><br><span class="line">    *r9  = data_shared[index + <span class="number">1024</span>*<span class="number">2</span> + inexd_tmp*<span class="number">0</span>];</span><br><span class="line">    *r10 = data_shared[index + <span class="number">1024</span>*<span class="number">2</span> + inexd_tmp*<span class="number">1</span>];</span><br><span class="line">    *r11 = data_shared[index + <span class="number">1024</span>*<span class="number">2</span> + inexd_tmp*<span class="number">2</span>];</span><br><span class="line">    *r12 = data_shared[index + <span class="number">1024</span>*<span class="number">2</span> + inexd_tmp*<span class="number">3</span>];</span><br><span class="line">    *r13 = data_shared[index + <span class="number">1024</span>*<span class="number">3</span> + inexd_tmp*<span class="number">0</span>];</span><br><span class="line">    *r14 = data_shared[index + <span class="number">1024</span>*<span class="number">3</span> + inexd_tmp*<span class="number">1</span>];</span><br><span class="line">    *r15 = data_shared[index + <span class="number">1024</span>*<span class="number">3</span> + inexd_tmp*<span class="number">2</span>];</span><br><span class="line">    *r16 = data_shared[index + <span class="number">1024</span>*<span class="number">3</span> + inexd_tmp*<span class="number">3</span>];</span><br><span class="line">    wi_c = <span class="number">-2</span>*FFT_Pi/(<span class="number">16</span>*i);</span><br><span class="line">    index = tid / inexd_tmp;</span><br><span class="line">    StockhamR4_block(r1,  r2,  r3,  r4,  &amp;wi, wi_c, index + i*<span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(r5,  r6,  r7,  r8,  &amp;wi, wi_c, index + i*<span class="number">1</span>);</span><br><span class="line">    StockhamR4_block(r9,  r10, r11, r12, &amp;wi, wi_c, index + i*<span class="number">2</span>);</span><br><span class="line">    StockhamR4_block(r13, r14, r15, r16, &amp;wi, wi_c, index + i*<span class="number">3</span>);</span><br><span class="line">    __syncthreads();</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">0</span> + <span class="number">256</span>*<span class="number">0</span>] = *r1;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">1</span> + <span class="number">256</span>*<span class="number">0</span>] = *r2;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">2</span> + <span class="number">256</span>*<span class="number">0</span>] = *r3;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">3</span> + <span class="number">256</span>*<span class="number">0</span>] = *r4;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">0</span> + <span class="number">256</span>*<span class="number">1</span>] = *r5;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">1</span> + <span class="number">256</span>*<span class="number">1</span>] = *r6;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">2</span> + <span class="number">256</span>*<span class="number">1</span>] = *r7;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">3</span> + <span class="number">256</span>*<span class="number">1</span>] = *r8;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">0</span> + <span class="number">256</span>*<span class="number">2</span>] = *r9;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">1</span> + <span class="number">256</span>*<span class="number">2</span>] = *r10;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">2</span> + <span class="number">256</span>*<span class="number">2</span>] = *r11;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">3</span> + <span class="number">256</span>*<span class="number">2</span>] = *r12;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">0</span> + <span class="number">256</span>*<span class="number">3</span>] = *r13;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">1</span> + <span class="number">256</span>*<span class="number">3</span>] = *r14;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">2</span> + <span class="number">256</span>*<span class="number">3</span>] = *r15;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">3</span> + <span class="number">256</span>*<span class="number">3</span>] = *r16;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此外，将计算模块进行<strong>for循环优化</strong>，可以<strong>减少寄存器资源占用</strong>。从目前的实际情况来看，即使不存在依赖，代码编译后寄存器的占用和代码长度成正相关。所以应尽可能把层级计算整合到for循环里，以减少占用。 只要单个线程的寄存器占用在80左右，在本文的情况下，就可达到每个大核心3个block的最大占用。</p><p>优化前寄存器占用在100个左右，只能达到2个block的占用，优化后寄存器占用降到46个，提升明显。</p><p>最后，本文把大量使用的Stockham基-4蝶形计算单元封装为inline模块，也就是将stockham_fft_R4()中的核心循环：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; L_d4; j++)</span><br><span class="line">&#123;</span><br><span class="line">    wi1 = exp_calcu(direction* j*(<span class="number">2</span> * my_Pi / L));</span><br><span class="line">    wi2 = complx_mul(wi1, wi1);</span><br><span class="line">    wi3 = complx_mul(wi1, wi2);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; stride; k++)</span><br><span class="line">    &#123;</span><br><span class="line">        alpha = y[j*stride_m4 + k];</span><br><span class="line">        beta = complx_mul(wi1, y[j*stride_m4 + <span class="number">1</span> * stride + k]);</span><br><span class="line">        gamma = complx_mul(wi2, y[j*stride_m4 + <span class="number">2</span> * stride + k]);</span><br><span class="line">        delta = complx_mul(wi3, y[j*stride_m4 + <span class="number">3</span> * stride + k]);</span><br><span class="line">        tau[<span class="number">0</span>] = complx_add(alpha, gamma);</span><br><span class="line">        tau[<span class="number">1</span>] = complx_sub(alpha, gamma);</span><br><span class="line">        tau[<span class="number">2</span>] = complx_add(beta, delta);</span><br><span class="line">        tau[<span class="number">3</span>] = complx_sub(beta, delta);</span><br><span class="line">        <span class="comment">//tau[3]*wi(0,1)</span></span><br><span class="line">        tmp = tau[<span class="number">3</span>].x;</span><br><span class="line">        tau[<span class="number">3</span>].x = -tau[<span class="number">3</span>].y;</span><br><span class="line">        tau[<span class="number">3</span>].y = tmp;</span><br><span class="line">        x[(j + L_d4 * <span class="number">0</span>)*stride + k] = complx_add(tau[<span class="number">0</span>], tau[<span class="number">2</span>]);</span><br><span class="line">        x[(j + L_d4 * <span class="number">1</span>)*stride + k] = complx_sub(tau[<span class="number">1</span>], tau[<span class="number">3</span>]);</span><br><span class="line">        x[(j + L_d4 * <span class="number">2</span>)*stride + k] = complx_sub(tau[<span class="number">0</span>], tau[<span class="number">2</span>]);</span><br><span class="line">        x[(j + L_d4 * <span class="number">3</span>)*stride + k] = complx_add(tau[<span class="number">1</span>], tau[<span class="number">3</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>模块化为：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> __device__ <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">StockhamR4_block</span></span></span><br><span class="line"><span class="function"><span class="params">(Complex *a, Complex *b, Complex *c, Complex *d, Complex *wi, <span class="keyword">float</span> wi_c, <span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Complex tmp, alpha, beta, gamma, delta;</span><br><span class="line">    alpha = *a;                     *wi = exp_calcu(index * wi_c);  tmp = *wi;</span><br><span class="line">    beta  = ComplexMul(*wi, *b);    *wi = ComplexMul(*wi, *wi);</span><br><span class="line">    gamma = ComplexMul(*wi, *c);    *wi = ComplexMul(tmp, *wi);</span><br><span class="line">    delta = ComplexMul(*wi, *d);</span><br><span class="line">    tmp   = alpha;</span><br><span class="line">    alpha = ComplexAdd(alpha, gamma);</span><br><span class="line">    gamma = ComplexSub(tmp,   gamma);</span><br><span class="line">    tmp   = beta;</span><br><span class="line">    beta  = ComplexAdd(beta, delta);</span><br><span class="line">    delta = ComplexSub(tmp,  delta);</span><br><span class="line">    <span class="comment">//tau3*wi(0,1)</span></span><br><span class="line">    tmp.x   = delta.x;</span><br><span class="line">    delta.x = -delta.y;</span><br><span class="line">    delta.y = tmp.x;</span><br><span class="line">    *a = ComplexAdd(alpha,  beta);</span><br><span class="line">    *b = ComplexSub(gamma, delta);</span><br><span class="line">    *c = ComplexSub(alpha,  beta);</span><br><span class="line">    *d = ComplexAdd(gamma, delta);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基-4蝶形单元具有很好的计算特性，涉及的旋转因子复数乘法比基-8和基-16单元少很多，每个线程负责16个数据，也就是4个基-4单元的计算，实际测试结果表明，计算很好地掩盖了访问共享内存的延迟，本文实现的4096点FFT计算时间与cuFFT一致（这里是8192组4096点1维FFT计算时间测试）。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiue2qu1u58fd69dt1thb1ma88d-20210605135115483.png" alt=""></p><p>cuFFT结果 ： 3.614ms<br>本文实现结果 ： 3.600ms</p><p>本文运行环境是WIN7 x64 + CUDA 7.5。</p><hr><h2 id="大点数FFT的计算方法"><a href="#大点数FFT的计算方法" class="headerlink" title="大点数FFT的计算方法"></a>大点数FFT的计算方法</h2><p>根据GPU硬件和FFT多级数据交换的特点，为了保证算法的运行时间，不能把FFT的数据交换放到显存里，尽量通过片上的共享内存进行。由于存储资源限制，理论上一次可以计算4096点（详见前述）。所以在实现大点数FFT时，首先想到的是把序列分解为多个4096点DFT，再用Stockham方法实现DFT的快速计算。</p><p>当序列长度n为复合数时，可以分解为一些因子的乘积，即混合基算法的基本原理。比如，128点的序列，用基-2算法需要7级计算，用基-2/4混合基的话需要3级基-4和1级基-2计算，减少了数据交换次数。在此，本文已经高效地实现了4096点FFT计算，所以可以将大点数序列分解成n=n0*4096的形式计算。</p><p>下面先说明混合基算法原理：</p><p>一维离散傅里叶变换被描述为: $\quad X_{k}=\sum_{j=0}^{n-1} x_{j} \omega_{n}^{j k}, 0 \leq k \leq n-1$<br>其中 $\omega_{n}=e^{-2 \pi i / n}, i=\sqrt{-1}, \mathrm{n}$ 是 $\mathrm{FFT}$ 的序列长度，且 $\mathrm{n}$ 具有因子 $n_{0}$ 和 $n_{1}\left(n=n_{0} \times n_{1}\right)$.</p><p>$\mathrm{j}$ 和 $\mathrm{k}$ 可以表示为:$j=j_{0} \times n_{1}+j_{1}, k=k_{1} \times n_{0}+k_{0}$<br>$x$ 和 $X$ 可用二维数组来描述:<br>$x_{j}=x\left(j_{0}, j_{1}\right), 0 \leq j_{0} \leq n_{0}-1,0 \leq j_{1} \leq n_{1}-1$<br>$X_{k}=X\left(k_{1}, k_{0}\right), 0 \leq k_{1} \leq n_{1}-1,0 \leq k_{0} \leq n_{0}-$<br>将上述式子带入离散傅里叶变换公式: $\quad x\left(k_{1}, k_{0}\right)=\sum_{j=0}^{n-1} \sum_{j_{0}=0}^{n_{0}-1} x\left(j_{0}, j_{1}\right) \omega_{n_{0}}^{j k_{0}} \omega_{n}^{j k_{0}} \omega_{n_{1}}^{j k_{1}}$<br>式(4-1)包含两步多重 FFT 计算。<br>第一步先计算 $n_{1}$ 次 $n_{0}$ 点 $\mathrm{FFT}$, 并将结果乘以一组旋转因子 $\omega_{n}^{j k_{0}}: \quad X_{1}\left(k_{0}, j_{1}\right)=\omega_{n}^{j / 1 / 0} \sum_{j_{0}=0}^{n_{0}-1} x\left(j_{0}, j_{1}\right) \omega_{n_{0}}^{j / 6_{0}}$<br>第二步计算 $n_{0}$ 次 $n_{1}$ 点 FFT: $\quad X_{2}\left(k_{0}, k_{1}\right)=\sum_{j=0}^{n-1} X_{1}\left(k_{0}, j_{1}\right) \omega_{n_{1}}^{j / 1 / 1}$<br>最终得到:</p><p>$$<br>X_{k}=X\left(k_{1}, k_{0}\right)=X_{2}\left(k_{0}, k_{1}\right)<br>$$</p><h3 id="以16384点DIF-FFT为例分析计算结构"><a href="#以16384点DIF-FFT为例分析计算结构" class="headerlink" title="以16384点DIF-FFT为例分析计算结构"></a>以16384点DIF-FFT为例分析计算结构</h3><p>16384=4*4096，n=16384，n0=4，n1=4096；j0=(0,3)，j1=(0,4095)；k0=(0,3)，k1=(0,4095)。<br>先按基-4抽取（每隔4096点抽一个数据）做4096组4点DIF-FFT，按所在序列乘以对应的，即的。<br>再计算4组4096点FFT得到结果。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bnfad4n95fa7c7cokkirn0m.png" alt=""></p><p>从数值计算的角度分析运算过程：</p><p>4096组4点FFT计算时，要注意，数据抽取是在16384点数据中以4096为跨度进行的（基-4抽取）。<br>由于是DIF-FFT计算结构，这里的基-4计算要按DIF的方式进行。<br>DIF-FFT的数据是顺序进，倒序出，所以在12行乘以旋转因子的时候是乘以倒序的旋转因子。<br>为了得到顺序的4块4096序列，计算结果的需要按照倒序放回（17行）。</p><p>由于在基-4计算中序列按MOD4抽取，所以在进行4组4096点FFT计算后，<br>要把4块数据进行合并才能得到数据的16384点FFT计算结果（25-29行）。</p><p>计算可以分为3个部分：4096组4点FFT计算，4组4096FFT计算和数据的抽取合并。用CUDA C实现的话，考虑到片上存储能力，每部分计算都要做成一个核函数（kernel），耗时是小点数的3倍，和官方库cuFFT两倍左右的耗时有较大的差距。</p><p>3个部分中包含大量计算的是中间的4组4096点FFT处理，最后一部分仅改变了数据位置。问题的关键在于，用DIF分解方式可以2步得到FFT计算结果，但这个结果的是分散在n0个数据块中的。如果要降低算法时间，一个可行的办法是结合第2和第3步，将第2步的数据跨越式地存存储。但用GPU实现时，跨越式地把数据存到显存中，会大大降低带宽利用率。要解决这个问题，就要构造出可以在与显存进行数据交换时可以连续访存的算法结构。经测试，使用单精度复数作为数据类型时，还需要保证数据的首地址对齐（128bytes的倍数）。<br>128bytes/8bytes = 16，即一个warp线程与显存数据交换的最小数量时16个单精度复数，那么底线就是32个线程（一个warp）中前16个线程和后16个线程访问的显存位置可以不一样，但在16个线程中，它们访问的数据必须是连续的，且首地址对齐128bytes。为达到这一要求，在FFT框架下即每次至少要计算16个数据块（因为，在DIF计算完成后，16个线程的数据是从不同数据块的对应位置抽取的），而当前GPU架构每次可以计算4096点数据，所以需要把大点数序列划分为以256为计算单元的多个数据块。比如16384点，就要划分为64组256点FFT，每次抽取16组到片上计算。</p><p>受到这些约束，16384点FFT计算的实际分解方式是：</p><p>16384=64*256，n=16384，n0=64，n1=256；j0=(0,63)，j1=(0,255)；k0=(0,63)，k1=(0,255)。<br>先按基-64抽取（每隔256点抽一个数据）做256组64点DIF-FFT，按所在序列乘以对应的，即的。<br>再计算64组256点FFT得到结果（在GPU上实现，有更细的划分）。</p><p>以上是理论说明。</p><h3 id="65536-4以上大点数的处理方法"><a href="#65536-4以上大点数的处理方法" class="headerlink" title="65536*4以上大点数的处理方法"></a>65536*4以上大点数的处理方法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">本节作为附加介绍</span><br></pre></td></tr></table></figure><p>为了实现更快速度，在FFT+点乘+IFFT。这样的频域算法计算中。<br>结合Cooley-Tukey和Stockham结构，中间结果不要求完全“顺序”：</p><p>顺序输入 &gt;&gt; Cooley-Tukey &gt;&gt; 倒序输出<br>65536*4/4096=64，即256K点数据可以分为64组4096点FFT计算。<br>先用DIF Cooley-Tukey 拆分4096点数据（见上一节理论和CTFFT计算结构示意图）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbip3o5v1lb91ml77n6pnsc6734.png" alt=""></p><p>可以看出DIF第一级计算以后，8点分治为两个4点计算（即后续的计算两个4点之间没有依赖）</p><p>按照4096点Stockham结构，计算64组数据，即可得到FFT计算结果。<br>注意，此时的数据是按64块倒序存放的（注意描述，按分块倒序，块内数据是“顺序”的）<br>因为，已经得到结果，为了节省时间，不再用一个kernel得到顺序结果。</p><p>点乘。完成频域计算。</p><p>进行64组4096点IFFT</p><p>利用DIT Cooley-Tukey结构处理64块4096点数据<br>（在stage0不用倒序，因为数据已经按块倒序了）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbip3hfrba1d9h11uu13da1hmq2n.png" alt=""></p><p>所以计算步奏如下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1、DIF Cooley-Tukey 拆分数据（顺序输入，按块倒序输出）</span><br><span class="line">2、计算64组4096点Stockham FFT</span><br><span class="line">3、频域点乘</span><br><span class="line">4、计算64组4096点IFFT</span><br><span class="line">5、DIT Cooley-Tukey 合并数据（按块倒序输入，顺序输出）</span><br></pre></td></tr></table></figure><p>利用Cooley-Tukey FFT的计算结构，拆分大点数计算。<br>利用Stockham FFT 向量化的计算结构，进行大部分计算。<br>计算步骤2、3、4可以合并为一个kernel。</p><p><strong>整体时间为4个访存密集kernel时间</strong>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">而调用cuFFT的话，从这个数据长度开始 FFT，IFFT都需要3个访存密集kernel时间</span><br><span class="line">再加上点乘调用，共需要7个kernel时间。</span><br></pre></td></tr></table></figure><p><strong>所以实际工程中，可以加速2倍左右。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这一方法在本文框架中，可以处理65536*4到65536*256的计算规模：</span><br><span class="line">65536*4=4096*16</span><br><span class="line">65536*256=4096*4096</span><br></pre></td></tr></table></figure><hr><h4 id="时间测试"><a href="#时间测试" class="headerlink" title="时间测试"></a>时间测试</h4><p>16384点<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bnfbvd6hq7t1kq527mluj1dsf13.png" alt=""></p><p>cuFFT结果 ： 3.642 + 3.665 = 7.307ms<br>本文实现结果 ： 4.215 + 3.740 = 7.955ms （稍慢，实际整合频域算法的话，快1/3的时间）</p><p>…</p><p>65536*4点<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bnfcb2h79vm1c4h1ldp1ojuhqu1g.png" alt=""></p><p>cuFFT结果 ： 3.65*6 + 3.65 = 25.55ms （3FFT 3IFFT 1频域计算）<br>本文实现结果 ： 3.85 + 6.54 + 3.85 = 14.24ms （注意，234步骤已整合到第二个kernel）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">补充一点，在这一维数据长度，调用cuFFT，库自己拆分为三个kernel进行计算，非用户操纵。</span><br></pre></td></tr></table></figure><p>补充公式</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210605141333528.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;一维FFT算法在Maxwell架构上，归为访存密集算法。&lt;br&gt;即，在足够优化的情况下，可在一次memory copy的耗时内完成计算。&lt;/p&gt;
&lt;p&gt;本文实现的FFT算法达到与官方库cuFFT一致的速度，通过整合kernel，可实现比调用CUFFT更快的算法整体执行速度。在处理65536*4以上大点数一维FFT+IFFT计算时（一个大核心共享内存放不下完整的一维FFT数据），组合算法可以实现比CUFFT少2个kernel调用的时间（减少两次显存数据交换），主要说明4096点FFT算法设计的思路及实现。大点数仅说明方法和测试结果。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CUDA" scheme="http://yuanquanquan.top/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>一维卷积的SASS实现</title>
    <link href="http://yuanquanquan.top/2021/20210604/"/>
    <id>http://yuanquanquan.top/2021/20210604/</id>
    <published>2021-06-03T20:51:23.000Z</published>
    <updated>2021-06-04T16:19:49.391Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本项目使用MaxAs提供的汇编器编写GPU一维卷积算法</p><p>有16点和1024点卷积核两种实现，SASS代码在目录下，两个VS2013工程在文件夹中</p><p>实验环境：WIN7 + VS2013 + CUDA 6.5 + MaxAs (SASS代码已注入cubin，运行不需要MaxAs)</p><hr><p>16点卷积和1024点卷积恰好可划分在访存密集和计算密集这两个类型中，是比较好的练手项目。</p><p>1024点卷积在Maxwell架构下达到硬件75%的峰值算力，还在找未达到90%以上的原因。</p></blockquote><span id="more"></span><h2 id="1、一维卷积计算过程"><a href="#1、一维卷积计算过程" class="headerlink" title="1、一维卷积计算过程"></a>1、一维卷积计算过程</h2><p>线性卷积: $y(n)=x(n) * h(n)$</p><p>设x(n)长度为N1，h(n)长度为N2 ， 则y(n)长为 N=N1+N2-1<br>为方便表示，在序列x(n)后添N2-1个0 ，使x(n)的长度变为 N</p><p>卷积公式 ： $y(n)=\sum_{i=0}^{N-1} x(i) \times h(n-i), \quad 0 \leq n \leq N-1$</p><p>可用矩阵表示为：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avo8jtg41p6e5egvii1vt9uec1t.png" alt=""></p><p>一个简单的计算过程示意：</p><p>x(n)=[4,3,2,1]<br>h(x)=[3,2,1]<br>y(n)=x(n)*h(n)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x  |        4  3  2  1        | </span><br><span class="line">   |                          | </span><br><span class="line">   y0 |  1  2  3                 | = 4*3              = 12</span><br><span class="line">   y1 |     1  2  3              | = 4*2 + 3*3        = 17</span><br><span class="line">   y2 |        1  2  3           | = 4*1 + 3*2 + 2*3  = 16</span><br><span class="line">   y3 |           1  2  3        | = 3*1 + 2*2 + 1*3  = 10</span><br><span class="line">   y4 |              1  2  3     | = 2*1 + 1*2        = 4</span><br><span class="line">   y5 |                 1  2  3  | = 1*1              = 1</span><br></pre></td></tr></table></figure><p>为保证局部性：</p><p>在实现时，卷积核H的数据先放到片上<br>（H点数较小时直接放到寄存器，点数稍大可放在共享内存，过大时分批从片外内存读取）</p><p>实际的原始数据X，点数较大，分块（blocking）处理。</p><p>Y的计算不按照上例中的一次性计算$y^2=4 <em>1+3</em>2+2*3=16$ 。<br>而是 多次乘加 得到结果 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//Y初始化为0</span><br><span class="line">regX = shareadX[0];      //共享内存读取x</span><br><span class="line">Y0  += regX*H1;</span><br><span class="line">Y1  += regX*H2;</span><br><span class="line">Y2  += regX*H3;</span><br><span class="line">regX = shareadX[1];      </span><br><span class="line">Y1  += regX*H1;   </span><br><span class="line">Y2  += regX*H2;</span><br><span class="line">Y3  += regX*H3;</span><br><span class="line">...</span><br><span class="line">//计算完成后把Y 在共享内存reshape 然后放回片外</span><br></pre></td></tr></table></figure><p>上例每次读一个X。实现时，X批量读入，计算掩盖访存延迟。</p><h2 id="2、两个实现：16点卷积与1024点卷积"><a href="#2、两个实现：16点卷积与1024点卷积" class="headerlink" title="2、两个实现：16点卷积与1024点卷积"></a>2、两个实现：16点卷积与1024点卷积</h2><h3 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h3><table><thead><tr><th style="text-align:center">算法</th><th style="text-align:center">卷积</th></tr></thead><tbody><tr><td style="text-align:center">数据量</td><td style="text-align:center">N*2 (输入X / 输出Y / H较小不计入)</td></tr><tr><td style="text-align:center">计算量</td><td style="text-align:center">N*N2*2 (乘加按两个指令计算)</td></tr><tr><td style="text-align:center"><strong>硬件</strong></td><td style="text-align:center">GTX970</td></tr><tr><td style="text-align:center">峰值计算能力</td><td style="text-align:center">1.25GHz <em> 1664cores </em> 2 = 4160 GFLOPS（实测达到）</td></tr><tr><td style="text-align:center">访存带宽：</td><td style="text-align:center">224GB/S （实测连续访存140GB/s ，即35G/s的FP32数据）</td></tr></tbody></table><p>算法计算强度 $=\frac{\text { 计算量 }}{\text { 数据量 }}=\frac{2 <em> N </em> N 2}{2 N}=N 2$</p><p>(即一维卷积计算算法强度与卷积核长度直接相关： 16点卷积计算强度为16， 1024点卷积计算强度为1024)</p><p>硬件计算强度 $=\frac{\text { 浮点计算能力 }}{\text { 浮点带宽 }}=\frac{4160 \mathrm{GFLOPS}}{35 \mathrm{GFloatData}}=118.8$</p><p>所以16点卷积计算 ：受限于带宽 （16 &lt; 116.8 访存密集）<br>1024点卷积计算 ： 受限于核心计算能力（1024 &gt; 116.8 计算密集）</p><p>所有算法都可以从这一角度，划分为上述两个类型之一。<br>计算密集的算法，可以针对硬件架构进行优化。</p><h3 id="16点卷积"><a href="#16点卷积" class="headerlink" title="16点卷积"></a>16点卷积</h3><p>一维卷积核H：16 float （直接拷贝到寄存器）<br>输入数据量X：1024000 float<br>输出数据量Y：1024000 + 16 float （最后16个Y为了方便，没有进行计算）<br>线程结构 : 32个thread一个blcok，每个thread计算16点Y，一共需要2000个block。<br>片上存储 ： 分配544 float * 4 bytes = 2176 bytes 大小的共享内存空间用于存储。</p><h4 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a><strong>存储结构</strong></h4><p><strong>Shared存储</strong><br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avomqh2tgdqqvvct61ubn3u583.png" alt=""></p><p>每个block负责32*16=512点Y计算<br>32个threads 对显存合并访问 读取X<br>每个thread进行4次LDG.128访问 + 一次LDG访问（32bit，末尾的32个X，只用到1/2的数据）<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1be05en8318a1vtp1fqd1jetng5m.png" alt=""></p><p><strong>Shared访存</strong><br>整体来看，一个线程需要访问8次共享内存（128bit形式）<br>eg：<br>第一次，所有线程访问第一行，第二次第二行，第三次第三行，第四次第四行<br>第五次时，需要向左移一列访问，接下来再访问三行</p><p>（注：图看不清的话，可以点击放大）<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avolnfikeh316dp1crrcciqgt7m.png" alt=""></p><p>以thread 0为例 ，蓝色框是其访存范围，红色线条是访存顺序。<br>黑框是 thread 1 的访存范围，前后两个线程共享16个 float 数据<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avon1gnd1c38v0j1clh15kt78d8g.png" alt=""></p><p>这样的LD/ST方法，利用了完整的共享内存带宽。</p><hr><h4 id="算法结构"><a href="#算法结构" class="headerlink" title="算法结构"></a><strong>算法结构</strong></h4><p>计算示意（结合 1、一维卷积计算过程 理解）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avonaiq71mp1i8p11od4qnfka8t.png" alt=""></p><p>完全避免寄存器冲突的办法</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avonpv7ieokp3dis41r66f289a.png" alt=""></p><h3 id="1024点卷积"><a href="#1024点卷积" class="headerlink" title="1024点卷积"></a>1024点卷积</h3><p>计算的核心循环与16点卷积一致，32点X与16点H参与计算。<br>区别在于下一次循环还要使用32个当前计算X中的的后16个；下一次计算使用的H是新的16个。</p><p>理论分析：由于一个小循环算完16点Y，数据都在片上，所以再套上大循环，对X、Y做双缓冲不会有速度提升<br>实际处理：H加双缓冲无效，寄存器双缓冲预取共享内存中的X，计算掩盖延迟。</p><p>一维卷积核H：1024 float<br>输入数据量X：2097152 float<br>输出数据量Y：2097152+1024 float （最后1024个Y为了方便，没有进行计算）<br>线程结构：128个thread一个blcok，每个thread计算16点Y，一共需要1024个block。</p><h4 id="存储结构-1"><a href="#存储结构-1" class="headerlink" title="存储结构"></a><strong>存储结构</strong></h4><p>X数据存储与16点类似，一个block存128线程<em>(4float</em>6次加载)=3072个单精度浮点。<br>还需要1024<em>4byes存放H。<br>共分配4096</em>4bytes的共享内存空间<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avoqm9741tr17n6b7jd4f1eufbb.png" alt=""></p><h4 id="计算结构"><a href="#计算结构" class="headerlink" title="计算结构"></a><strong>计算结构</strong></h4><p>block内计算示意<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avop9ch21fqrovk1nrsk9k1821a4.png" alt=""></p><p>block间计算示意<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avopgujc51q1fhf1nhf1j7iu8sau.png" alt=""></p><blockquote><p>注:Y存回片外时通过共享内存reshape，有bank conflict，只用到了共享内存1/4的完整带宽。<br>但实测对kernel执行时间基本没有影响（占比太小）。</p></blockquote><p>SASS代码：<br><a href="https://github.com/Velaciela/1D-convolution-with-SASS/blob/master/conv1024_nobuffer.sass">conv1024_nobuffer.sass</a></p><hr><h2 id="3、实验结果"><a href="#3、实验结果" class="headerlink" title="3、实验结果"></a>3、实验结果</h2><h3 id="16点卷积-1"><a href="#16点卷积-1" class="headerlink" title="16点卷积"></a>16点卷积</h3><p>每个线程使用60个寄存器（优化后），恰好达到1个SM上32个Block的占用上限。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avoeeebb182op4fusfj8s1r053u.png" alt=""></p><p>实测，只要一个SM上达到6个Block的占用，就可占满带宽。<br>6占用kernel执行时间和32个占用的时间一致。<br>占用多了，带宽跟不上，还是需要等待。</p><p>数据量(32*16*2000)*2 = 2*1024000FP32 = 2*1024000*4bytes/1024/1024 = 7.8125MB</p><p>下图可知，kernel平均计算时间为56us<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avof1ebn1grl7jo7am9qmso54o.png" alt=""></p><p>带宽占用： 7.8125MB / 0.056ms = 139.51GB/s<br>与理论分析一致</p><hr><h3 id="1024点卷积-1"><a href="#1024点卷积-1" class="headerlink" title="1024点卷积"></a>1024点卷积</h3><p>Visual Profiler （nvvp）分析：</p><p>带宽未占满<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avofvn3l1ngb1l8a10r2ulobpl5v.png" alt=""></p><p>计算指令占比接近90%<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avofsmlrdqc1bq7415114d11o65i-20210604233547601.png" alt=""></p><p>SM占用6<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avog5jnc1g2tapc1iej1f7negm6p-20210604233513105.png" alt=""><br>实测占用 6、4、2个block的情况，kernel执行时间一致。<br>由于计算能力限制，占用多了也算不过来。</p><p>kernel执行时间1.4ms<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avog45dp1nhm14qc15ho14m9e86c.png" alt=""></p><table><thead><tr><th style="text-align:center">结果</th><th style="text-align:center">达到硬件峰值计算能力的74% （未达到90%）</th></tr></thead><tbody><tr><td style="text-align:center">数据量</td><td style="text-align:center">输入X+输出Y = 2<em>2097152 float = 2</em>2097152*4bytes/1024/1024 = 16MB</td></tr><tr><td style="text-align:center">kernel时间</td><td style="text-align:center">1.4ms</td></tr><tr><td style="text-align:center">带宽占用</td><td style="text-align:center">16MB / 1.4ms = 11.42GB/s</td></tr><tr><td style="text-align:center">kernel算力</td><td style="text-align:center">2097152_X输入<em>1024_H卷核</em>2_乘加 / 1.4ms = 4.295 GigaFloatOp / 0.0014s = 3067.86 GFLOPS</td></tr><tr><td style="text-align:center">硬件算力</td><td style="text-align:center">1.25GHz <em> 1664cores </em> 2 = 4160 GFLOPS</td></tr></tbody></table><hr><p><strong>数据生成及结果验证</strong></p><p>原始数据X、卷积核H和结果Y的长度分别为N、M、P。<br>卷积核与原始数据用伪随机数填充。<br>CPU计算的卷积结果放在数据块T中。<br>GPU计算的卷积结果拷贝到数据块Y中。</p><p>打印前1024个T与Y的误差<br>Y与T差的绝对值大于1则打印出来（计算精度所在位置随数据大小变化）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avoclkfcqsai1f1uu71sua123u3h.png" alt=""></p><h2 id="4、小结"><a href="#4、小结" class="headerlink" title="4、小结"></a>4、小结</h2><p>常用的优化方法：<br>调整算法结构（匹配硬件特性），kernel间整合（减少访存），kernel优化（数值计算过程与汇编）</p><p>16点卷积和1024点卷积恰好可划分在访存密集和计算密集这两个类型中。<br>是比较好的练手项目。</p><p>访存密集型的算法不需要用汇编优化，保证基本的合并访存即可。<br>但一些算法结构复杂，需要仔细分析，利用好片上和片外内存的带宽。</p><p>1024点卷积kernel应该达到90%+的峰值算力，未找到受限原因。<br>理论上1024点卷积不需要嵌套循环，不用对X做双缓冲预取，因为当前计算的Y与下一大组X没有数据依赖关系。</p><p>按现在的理解，在SM上block占用低于一个阈值的情况下，双缓冲可以带来一定的计算能力提升。<br>在占用足够的情况下，线程级并行、线程间的切换，其实和双缓冲是等效的。<br>（2017补充）</p><p>实际工程中常用FFT在频域进行快速卷积。</p><h2 id="5、后续"><a href="#5、后续" class="headerlink" title="5、后续"></a>5、后续</h2><p>二维卷积<br>FFT</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本项目使用MaxAs提供的汇编器编写GPU一维卷积算法&lt;/p&gt;
&lt;p&gt;有16点和1024点卷积核两种实现，SASS代码在目录下，两个VS2013工程在文件夹中&lt;/p&gt;
&lt;p&gt;实验环境：WIN7 + VS2013 + CUDA 6.5 + MaxAs (SASS代码已注入cubin，运行不需要MaxAs)&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;16点卷积和1024点卷积恰好可划分在访存密集和计算密集这两个类型中，是比较好的练手项目。&lt;/p&gt;
&lt;p&gt;1024点卷积在Maxwell架构下达到硬件75%的峰值算力，还在找未达到90%以上的原因。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CUDA" scheme="http://yuanquanquan.top/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>SEGMM的CUDA Core实现</title>
    <link href="http://yuanquanquan.top/2021/20210602/"/>
    <id>http://yuanquanquan.top/2021/20210602/</id>
    <published>2021-06-02T15:20:08.000Z</published>
    <updated>2021-06-03T20:06:37.805Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>SGEMM：Single precision floatiing General Matrix Multiply</p><p>本文的SGEMM实现思路整理于MaxAs wiki和对MaxAs项目SGEMM汇编代码的理解。<br>MaxAs是一个开源的Maxwell架构GPU汇编器： <a href="https://github.com/NervanaSystems/maxas">Github链接</a><br>作者<a href="https://github.com/scott-gray">Scott Gray</a>提到两篇相关论文<a href="http://icl.cs.utk.edu/projectsfiles/magma/pubs/fermi_gemm.pdf">MAGMA paper</a>和<a href="https://hal.inria.fr/file/index/docid/789958/filename/112_Lai.pdf">Kepler sgemm paper</a></p></blockquote><span id="more"></span><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>​        $A=\left(a_{i j}\right) $ 是一个$ m \times s$ 矩阵, $B=\left(b_{i j}\right) $是一个 $s \times n $矩阵,规定矩阵A与矩阵B的乘积是一个$m \times n$矩阵$,C=\left(c_{i j}\right)$其中$c_{i j}=a_{i 1} b_{1 j}+a_{i 1} b_{1 j}+\cdots+a_{i s} b_{s j}=\sum_{k=1}^{s} a_{i k} b_{k j} \quad(i=1,2, \ldots, m ; \quad j=1,2, \ldots, n)$,把这个乘积记做$C=A B$。</p><p>​        由定义可知，计算一个元素$c_{i j}$的时间复杂度是$O(N)$，矩阵$C$中有$N^2$个数据，矩阵乘法的时间复杂度为$O(N^3)$。</p><h3 id="计算示例"><a href="#计算示例" class="headerlink" title="计算示例"></a>计算示例</h3><p>$A=\left(\begin{array}{cc}-2 &amp; 4 \ 1 &amp; -2\end{array}\right) \quad B=\left(\begin{array}{cc}2 &amp; 4 \ -3 &amp; -6\end{array}\right)$</p><p>$c_{11}=A$ 的第一行 $\cdot B$ 的第一列 $=a_{11} b_{11}+a_{12} b_{21}=-2 \times 2-3 \times 4=-16$</p><p>$c_{12}=A$ 的第一行 $\cdot B$ 的第二列 $=a_{11} b_{21}+a_{12} b_{22}=-2 \times 4-4 \times 6=-32$</p><p>$c_{21}=A$ 的第二行 $\cdot B$ 的第一列 $=a_{21} b_{11}+a_{22} b_{21}=1 \times 2+2 \times 3=8$</p><p>$c_{22}=A$ 的第二行・ $B$ 的第二列 $=a_{21} b_{21}+a_{22} b_{22}=1 \times 4+2 \times 6=16$</p><p>即 $C=\left(\begin{array}{cc}-2 &amp; 4 \ 1 &amp; -2\end{array}\right)\left(\begin{array}{cc}2 &amp; 4 \ -3 &amp; -6\end{array}\right)=\left(\begin{array}{cc}-16 &amp; -32 \ 8 &amp; 16\end{array}\right)$</p><h2 id="计算结构"><a href="#计算结构" class="headerlink" title="计算结构"></a>计算结构</h2><h3 id="朴素算法"><a href="#朴素算法" class="headerlink" title="朴素算法"></a>朴素算法</h3><p>解题时的书面计算习惯，通常是在A、B两矩阵中各取第i行、第j列，按照定义点乘累加，计算矩阵C中的一个元素$c_{i j}$<img src="http://static.zybuluo.com/Velaciela/72psazacnux5gx9lbdtnsd1h/image_1bbb66jog11h2p5d1uta9c1jh49.png" alt=""></p><p>设矩阵A、B和C都是NxN的矩阵，则有朴素算法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">    <span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(k=<span class="number">0</span>;k&lt;n;k++)</span><br><span class="line">            sum += A[i][k]*B[k][j];</span><br><span class="line">        C[i][j] += sum;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>​    由图示可知，使用朴素算法实现时，不连续地读取矩阵B的一列数据（内存空间数据跨度为N），空间局部性很差。<br>​    另外，与$c_{i j}$同行或同列的C元素，都由相同行或列的A、B数据计算得到，但朴素算法每个核心循环只计算一个元素$c_{i j}$，不能重复利用缓存中的矩阵数据（N较大的情况下，还没轮到下一个循环，cache空间已经被新数据占用了，不能命中，A的首次不命中概率为：1/cache字长，B的不命中概率为1），同行、列的数据利用率只有1/N，所以时间局部性也很差。</p><p>计算量分析：</p><p>​    设A、B和C都为N阶方阵，则计算一个元素$c_{i j}$的访存量为$2N+1$，计算量是$2N$（乘加按两次操作算）单位访存的计算量为：1</p><p>​    各类硬件的单位片外访存时间 远高于 单位片内计算操作的时间消耗，所以这个计算结构是访存密集的。算法的执行时间受访存带宽限制，大多数时间里，计算单元在等待数据加载。</p><h3 id="分块算法"><a href="#分块算法" class="headerlink" title="分块算法"></a>分块算法</h3><p>​    分块（blocking）技术可以提高内循环的局部性（locality）。分块的大致思想是将数据结构组织成的片（chunks）称为块（block）。（在这个上下文中，“块”指的是一个应用级的数据块，而不是高速缓存块。）这样构造程序，使得能够将一个片加载到高速缓存中，并在这个片中进行所需的所有读写，然后丢掉这个片，加载下一个片，依此类推。</p><p>​    不同于为提高空间局部性做简单的循环变换（朴素算法有ijk、jki、kij几类循环方式，内层循环数据加载对应AB、AC和BC，BC加载类型每次迭代的总不命中次数最少），分块虽然在一些系统上能获得很大的性能收益，但也使得代码更难阅读和理解，更适合优化编译器或者频繁执行的库函数。</p><p>​    根据硬件资源，划分A、B、C子块的大小，使它们都能放到片上存储。每取一列A数据和一行B数据，都能计算n*n次C数据，多次取数据点乘累加得到完整的$c_{i j}$数值。对于矩阵C（或矩阵C子块）中的一点，每次读取AB的2组n点数据后，核心计算为：$c_{i j}=a_{i k} b_{k j}+c_{i j}$<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbbq122dk9i1e2nn15boakgjm.png" alt=""></p><p>计算量分析：</p><p><strong>注：分析针对每个block的核心循环，故按分块，分析小矩阵。</strong></p><p>​    假设A子矩阵大小为$n \times K$，B子矩阵大小为$K \times n$，C子矩阵大小为$n \times n$，则分块矩阵乘法的访存量为$2Kn$（<strong>注：在核心循环里不访问C，计算完成后才放回，故不计入$n^2$</strong>)，计算量是$n \times n \times K \times 2=2 K n^{2}$，即K组对应的行列都有$2n^2$次计算，乘加计算视为两次操作。</p><p>​    单位访存的计算量为（即kernel的<strong>计算强度</strong>）：$\frac{2 K n^{2}}{2 K n}=n$,即n取值越大（子块越大），单位访存的计算量越大。n=128时，单位访存的计算量为128.</p><h3 id="算法与硬件性能"><a href="#算法与硬件性能" class="headerlink" title="算法与硬件性能"></a>算法与硬件性能</h3><table><thead><tr><th style="text-align:center">显卡</th><th style="text-align:center">GTX970</th></tr></thead><tbody><tr><td style="text-align:center">峰值计算能力</td><td style="text-align:center">4TFLOPS （乘加实测达到）</td></tr><tr><td style="text-align:center">访存带宽</td><td style="text-align:center">224GB/S （实测连续访存140GB/s ，即35G/s的FP32数据）</td></tr></tbody></table><p>硬件的单位访存计算能力 = 浮点计算能力/浮点带宽 = 4000GFLOPS/(35G FP32 DATA) = 114.3</p><p>1&lt;&lt;114.3&lt;128</p><p>所以，就GTX970来说，128分块矩阵的计算需求比硬件能力稍大，可以完整占用计算资源，是很好的实现方式。<br>属于计算密集算法，需要在算法结构和汇编细节上做优化，才能利用好硬件算力。</p><h2 id="MaxAs中的SGEMM实现"><a href="#MaxAs中的SGEMM实现" class="headerlink" title="MaxAs中的SGEMM实现"></a>MaxAs中的SGEMM实现</h2><p>按照上一小节的分块算法，构造计算结构，尽可能重复利用从各个存储层次获取的数据。在GPU上，也就是：</p><p><strong>显存&gt;&gt;L2缓存&gt;&gt;纹理缓存&gt;&gt;寄存器&gt;&gt;共享内存&gt;&gt;寄存器&gt;&gt;指令操作数缓存&gt;&gt;寄存器&gt;&gt;显存</strong></p><p>(其中，指令操作数缓存是Maxwell架构的新特性)<br>上述的每个数据路径都有延迟，要通过指令级并行和线程级并行（instruction and thread level parallelism ，ILP &amp; TLP）来掩盖。此外，还存在存储单元和合并访存的限制（banking and coalescing constraints）。作者的SGEMM代码可以在这些限制下，使算法的计算效率达到98%的GPU峰值计算性能。</p><p>这里，先说一下最终采用的基本计算模型，稍后会解释参数选择的原因。<br>把A、B矩阵划分为多个128x8的小块，如图所示，将矩阵A中所有横向数据块和矩阵B中所有纵向数据块带入计算，乘加，最后得到128x128的C矩阵子块的结果。一个时间段内，只需要拷贝A、B各一片对应数据块到片上共享内存，计算过程里，128x128点数据的中间结果保存在各线程的片上寄存器里，图中两组蓝色长条数据块都计算完成后，再把矩阵C的128x128子块计算结果放回显存。<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bb8hga1815hsfle1b0p1mfr1gcv9-20210604040513431.png" alt=""></p><h3 id="关键结构与参数"><a href="#关键结构与参数" class="headerlink" title="关键结构与参数"></a>关键结构与参数</h3><h4 id="双缓冲"><a href="#双缓冲" class="headerlink" title="双缓冲"></a><strong>双缓冲</strong></h4><p>使用大小为8的<strong>双缓冲寄存器块</strong>（double buffered）存放从显存加载的矩阵数据。<br>双缓冲能基本掩盖数据传输延迟：在计算一组数据的同时加载下一组数据。<br>寄存器块的寄存器数量设为8，可以配合长度为4的<strong>向量化内存加载指令</strong>（quad vector memory instructions），同时保证每个线程的寄存器使用数量在128个以下。超过128个寄存器的界限的话，SM的block占用会下降。</p><p>对共享内存也做双缓冲处理，这样可以移除一个本来需要的在主循环中使用的BAR.SYNCS：在存<br>储下一组数据时等待所有共享内存加载完成（按上下文应该是shared加载到reg）。<br>有了双缓冲以后，数据可以直接写入另一块的共享内存区域，各线程仍可以从之前的共享内存区域读取数据。这样做需要在主循环中增加三条XOR指令，但（时间）代价比BAR.SYNCS小。</p><p>每个block有256个线程，每个线程计算88=64点子矩阵C的数据，<br>每个block处理一块正方形的子矩阵C，方形的边长就是所有点数的平方根。<br>可以计算得到：$\sqrt(256*88)$ = $128 units wide$</p><h4 id="展开因子"><a href="#展开因子" class="headerlink" title="展开因子"></a><strong>展开因子</strong></h4><p>​    展开因子(unroll factor)对应每次从A和B中读入的行数（global&gt;&gt;shared），也是从共享内存中存储、加载以及计算数据的量。展开因子的选择有两个考虑：一方面希望用尽可能多的计算来掩盖纹理内存加载延迟，另一方面不希望循环大小超过指令缓冲区大小。否则，额外增加的取指延迟，又要想办法掩盖。作者测得Maxwell的<strong>指令缓冲区大小</strong>为8KB，所以不能让主循环大小超过1024个8byte指令，而且4个指令一组，其中的第4个指令是控制代码（control code），所以实际可用指令数限制为768个。另外还有指令对齐方面的考虑,所以指令数最好低于768。</p><p>​    综上考虑，把展开因子设为8，8*64=512个FFMA指令（浮点乘加）加上循环所需的访存指令和整数计算指令（大约40个），低于768。每个循环8行展开也能较好地配合纹理内存加载，512个FFMA计算足够掩盖200+时钟的纹理<strong>内存加载延迟</strong>。</p><h4 id="共享内存大小"><a href="#共享内存大小" class="headerlink" title="共享内存大小"></a><strong>共享内存大小</strong></h4><p>​    共享内存大小，由每个block的加载宽度乘以展开因子得到。需要256线程：每个循环处理8行，每个block加载宽度128，单精度浮点数据字长4bytes，矩阵A数据双缓冲，矩阵B数据双缓冲 &gt;&gt; 需要$8<em>128</em>4<em>2</em>2 = 16384$ bytes 的共享内存空间，Maxwell架构下：一个SM有65536个32bit的寄存器，98304 bytes的共享内存空间。<br>​    98304/16384 = 6<br>​    65536/(256线程*128寄存器) = 65536 / 32768 = 2,也就是说，共享内存大小可供6个block占用，但由于寄存器资源不够，只能得到2个blcok的占用。这里，共享内存的大小不是SM上block占用的主要因素，SGEMM的占用主要受寄存器资源的影响。（占用多不代表算得快，快慢主要受计算和访存能力影响。这里2个block的占用已经足够了）</p><h3 id="256线程算法实现"><a href="#256线程算法实现" class="headerlink" title="256线程算法实现"></a>256线程算法实现</h3><h4 id="加载A、B到共享内存"><a href="#加载A、B到共享内存" class="headerlink" title="加载A、B到共享内存"></a><strong>加载A、B到共享内存</strong></h4><p>​    作者把线程分为两部分，一半线程加载一个矩阵。256线程的话，就是4个warp（128线程）加载一个矩阵。条件加载在cuda中优化得不好，因为编译器不会判断加载是否以warp为单位而做优化。对于纹理内存加载来说（MaxAs中的SGEMM用了纹理，但相关的开源项目，后来不使用纹理，直接加载global），分开处理是必要的，因为<strong>每个warp的指令在一个时间只能处理一个纹理内存</strong>（分开加载更高效）。<strong>编译器会给纹理加载加入warp切换和分支</strong>（warp shuffles and branch），还有同步指令来强制执行。如果Nvidia给出以warp为单位的条件判断结构会很好（而不是仅仅有分支，ie：bra.uni）。</p><p>​    一个线程来只负责加载某一个矩阵，所以只需要一组索引寄存器（track registers）来存放纹理加载的索引。主循环里的整数加法指令因此减少一半，this is a big win 。在核心循环中，要抓住任何提升FFMA指令/非FFMA指令比例的机会。</p><p>​    另外，维持4个单独的索引变量以避免使用依赖栅栏（dependency barriers），每次纹理加载后给索引寄存器加上加一次的地址偏移。在架构上，<strong>访存指令发出后，并不会保存它调用的寄存器值</strong>，这样做可能是为了节省硬件资源。指令发出后，访存工作仍在执行(in flight)，而地址索引仍保存在一个寄存器中，这就需要用一个栅栏来保证不对相关寄存器进行写入。栅栏等待不一定就是坏事，线程级并行（TLP）可以掩盖延迟，但减少整体的延迟有助于提高性能，<strong>提高有可用warp来掩盖延迟的几率</strong>。</p><p>​    接下来就是从纹理单元加载。使用显式的纹理加载，而不用全局加载或不连续缓存<strong>（?）</strong>，有两个好处。一是代码更简单，不需要担心加载超出边界。二是，同样的kernel代码可以加载8 bit或16 bit浮点数据，大大减少带宽和存储的需求，对于一些不需要32 bit精度应用很有优势。</p><p>​    此外，对访存做4单位的<strong>向量化加载</strong>。cublas没有使用向量加载，因为输入数据有4字长对齐的限制，不能适用于普遍情况，cublas有自己固定的形式。使用4点向量化数据加载后索引偏置常数lda和ldb相应减小4倍，附带的好处是加载矩阵的索引可以增加到31bits，而普通纹理加载受限于27bits。另一个vec4加载的效果是，这一访存模式每次读取都占用整个缓冲区，缓存性能主要受L2限制（这点不太明白）。</p><h4 id="从共享内存加载数据到寄存器"><a href="#从共享内存加载数据到寄存器" class="headerlink" title="从共享内存加载数据到寄存器"></a><strong>从共享内存加载数据到寄存器</strong></h4><p>​    每个线程从共享内存的A、B区域读取数据，原则是避免bank conflict。根据文档，只要访存在32字长（128bytes）以内即可。warp内各线程进行vec4加载，某几个线程从相同地址加载，通过广播机制，可以在128 btyes的限制下读取数据。Maxwell的文档说明并不完整，<strong>在一些情况下，即使一个warp中的线程访问的数据在128 bytes以内也有触发存储冲突的情况</strong>（理论上，Maxwell架构的共享内存有32个32bit端口，同时读取的数据超过128 bytes带宽，就要排队等待，同时读取的数据占用相同端口，也要排队等待，即访存冲突。另外，实测warp内不连续或不同深度的访问，时间也会增加）。</p><p>​    128 bytes带宽可供加载8组16 bytes的4点向量化数据(vec4)。在这一限制下从A、B共享内存中加载。每次从共享内存块加载的宽度为4*64=256 bytes，所以把加载分为为两次，第二个加载指令跨度为64个数据。把这两个一维加载构造为二维，得到加载后线程寄存器空间与C子矩阵数据空间的对应关系，即C子矩阵数据在每个线程的64个寄存器中的存放位置：<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbe9jvnkbrnbfjfbp13511ik53u.png" alt=""></p><p>​    矩阵A中一行（这里作者按已经转置的A矩阵描述）的每个点与矩阵B中一行的每个点一一对应。</p><p>​    在warp内部，比较直接的方式是连续或有跨度地加载，但这样会发生不明原因的存储冲突。如果按照线程号用<strong>Z形加载就不会产生冲突</strong>（见上图warp0线程与数据关系，0、1、2、3……线程是Z字形映射）（自己没有验证过，平时只是合并访存，不知道是特定CUDA版本或架构有这个问题，还是都这样，作者没有对加载规格和模式做详尽地测试来检查哪个有效，哪个不行）。</p><h4 id="计算C子矩阵-寄存器组与重用"><a href="#计算C子矩阵-寄存器组与重用" class="headerlink" title="计算C子矩阵: 寄存器组与重用"></a><strong>计算C子矩阵: 寄存器组与重用</strong></h4><p>​    现在，每个线程里都有两个寄存器组，每组8个寄存器，保存矩阵A和B的数据，通过64次FFMA乘加计算得到C子矩阵的中间结果。为了实现全速低功耗地计算，需要考虑几个问题。最基本的就是<strong>寄存器组和操作数重用</strong>。</p><p>在Maxwell架构下，寄存器组（register banks）宽度为4（即4个32bit寄存器 ）。Kepler架构（宽度也是4）直接把序号和组关联起来。而在Maxwell架构下，关联由寄存器序号对4取余得到（即每 序号%4 相同的寄存器一组，占用同一个寄存器访问端口，类似共享内存）。在Kepler架构下，可以通过调整64个FFMA指令来消除所有存储冲突。在Maxwell架构下通过<strong>操作数重用缓存</strong>（operand reuse cache）解决这一问题，同时能减少寄存器传输次数。指令的每个<strong>源操作数槽位(source operand slot)</strong>有8 bytes的数据重用缓存。每次发射指令的时候，有一个标志位用来指定对应的操作数是否将被再次引用。设置标志位后，下一条指令在同一个操作数槽位引用同一个寄存器时，不需要再到寄存器组去取这个数据。可以利用这个特性，来避免寄存器冲突。</p><p>寄存器组概念参考下图：<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbec68ea1p26177ql57d7iq64b.png" alt=""></p><p>第一步是通过操作数重用来减少寄存器存储冲突。为实现目的，需要显式地选择要占用的寄存器。这是用MaxAs做汇编器的一个重要优点，ptxas在避免寄存器冲突方面做得还行，但不够完美，向量化存储操作做得不够好（在SGEMM实现中这点很重要）。实现如下：</p><ul><li>0-63作为矩阵C的寄存器</li><li>64-71和80-87作为矩阵A的双缓冲寄存器块</li><li>72-79和88-95作为矩阵B的双缓冲寄存器块</li></ul><p>按照下图来安排8*8矩阵的寄存器，用不同的颜色表示每个寄存器所在的组位。矩阵C中，选择颜色与对应的A、B不同的寄存器（不在同一端口）。通过这种方式可以消除C中所有的寄存器冲突。但还剩下A、B的16个寄存器冲突，这些冲突位置用黑框示意（作者图）：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbecjim8j781mdu1nppmj9gk4o.png" alt=""></p><p>如果不重用缓存，16个冲突每个都有1个时钟的延迟，理论上使得计算效率下降20%左右(在518时钟的循环上再加128个时钟)。实测时，执行没有重用标记的SGEMM汇编代码，发现性能只下降了200 Gflops左右。Nvidia的<a href="http://www.google.com/patents/US7834881">operand collectors</a>相关专利里的存储冲突（bank conflict）章节中描述了缓和存储冲突的一系列办法。不好说Maxwell架构是怎么处理的，可能利用了线程级并行（TLP）来掩盖存储冲突延迟。operand collectors能在一定程度上掩盖了存储冲突，但在大量冲突下可能不堪重负。持续地缓存能让硬件更好地避免存储冲突导致的延迟。MaxAs项目通过汇编器控制重用标记，预先判断哪些寄存器值得缓存，哪些寄存器在使用后就可以丢弃。</p><p>优化重用标记的工作已经由MaxAs实现了。我们要做的就是规划指令顺序，尽可能多地重用。最简单的排序是两个嵌套for循环一行一行地遍历矩阵，但这样只能用到8 bytes操作数缓存中的4 bytes，不能完全避免存储冲突。 通过往复遍历数据，可以避免所有的冲突并提高寄存器的重用率（39%）。不过作者最高效的实现方法是旋转遍历（重用率47%）。下面是FFMA指令按照矩阵C寄存器序号的执行顺序：</p><blockquote><p> 1, 0, 2, 3, 5, 4, 6, 7, 33, 32, 34, 35, 37, 36, 38, 39,<br>45, 44, 46, 47, 41, 40, 42, 43, 13, 12, 14, 15, 9, 8, 10, 11,<br>17, 16, 18, 19, 21, 20, 22, 23, 49, 48, 50, 51, 53, 52, 54, 55,<br>61, 60, 62, 63, 57, 56, 58, 59, 29, 28, 30, 31, 25, 24, 26, 27</p></blockquote><p>自己做的标记如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbedhbgn1b6o1dvhpc8ibp1gvc5i.png" alt=""></p><p>通过旋转，使得C寄存器组交错加载，避免冲突。此外，向量化数据加载除了高效，还可以减少访存指令数量，从而减少存储冲突的概率。</p><p>指令访问寄存器时可能发生存储冲突，导致延迟，所以应该为操作数选则不同组的寄存器。MaxAs可以控制寄存器的映射，SGEMM代码中为track0-3, tex, readAs, readBs和writeS选择了合适的寄存器组。作者提到在cublas中（当时的版本是CUDA 6.5），第一个FFMA指令选择的寄存器有存储冲突，这里冲突不能通过重用缓存避免，因为之前没有指令加载，也就没有寄存器值能放到操作数缓存。在GM204上，这个”bug”降低了cublas 28 Gflops的性能。</p><p>最后一个有关FFMA指令的的问题是，如何交错地执行FFMA和上述提到的存储操作。具体可以对照源码sgemm_pre_64.sass来看。为了掩盖延迟，共享内存的双缓冲加载越早越好，所以和第一个FFMA指令一起双发射执行。用两条FFMA指令隔开两条加载指令，因为<strong>存储单元</strong>好像在<strong>一半吞吐率</strong>的时候工作得<strong>最优</strong>（<strong>※不要连续发射存储指令※</strong>）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">01:-:-:-:0      FFMA cx02y00, j0Ax02, j0By00, cx02y00; // Wait Dep 1--:-:-:-:1      LDS.U.128 j1Ax00, [readAs + 4x&lt;1*64 + 00&gt;];--:-:-:-:1      FFMA cx02y01, j0Ax02, j0By01, cx02y01;--:-:-:-:0      FFMA cx00y01, j0Ax00, j0By01, cx00y01;--:-:-:-:1      LDS.U.128 j1By00, [readBs + 4x&lt;1*64 + 00&gt;];</span><br></pre></td></tr></table></figure><p>为了不让存储单元被指令淹没（overwhelm，没译准），纹理加载在两组共享内存加载中间执行，给纹理加载指令读取操作数的机会（in flight概念）。为下一个大循环加载的数据共享内存加载指令放在最后一个FFMA指令块中。之前还有一个加在第7和第8个FFMA指令块之间的BAR.SYNC指令（同步，作用是保证整个block tex加载的数据都通过STS保存到另一共享内存buffer后，再继续下一次加载计算）。</p><p>以上所有指令执行位置是经过大量测试取最优的，这样细粒度的指令位置控制在ptxas是中无法实现的。而且ptxas有优化掉共享内存双缓冲加载方法的倾向。在选择寄存器组、优化指令执行顺序以实现操作数重用和选择存储指令位置以后，之前只能跑到硬件计算能力70%的kernel现在可以跑到98%。</p><p>理论性能的计算，用主循环中的FFMA指令总数（518）除以所有指令需要的发射时间（双发射不算）即可得到。以下是256线程版本的具体分析：</p><table><thead><tr><th style="text-align:center">Op</th><th style="text-align:center">Count</th></tr></thead><tbody><tr><td style="text-align:center">FFMA</td><td style="text-align:center">512</td></tr><tr><td style="text-align:center">LDS</td><td style="text-align:center">32  dual issued</td></tr><tr><td style="text-align:center">STS</td><td style="text-align:center">2  dual issued</td></tr><tr><td style="text-align:center">TLD</td><td style="text-align:center">2  dual issued</td></tr><tr><td style="text-align:center">IADD</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">XOR</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">STEP</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">BAR</td><td style="text-align:center">1  dual issued</td></tr><tr><td style="text-align:center">BAR</td><td style="text-align:center">1  dual issued</td></tr></tbody></table><p>一共用了518个时钟来发送指令（双发射不计入时钟消耗），所以这个kernel的性能上限是512/518，即98.8%。大型矩阵计算的实际性能接近理论值。</p><p>根据主循环代码和指令的时钟消耗，可以粗略估计这个kernel的访存带宽上限。对于GM204架构，有：</p><ul><li>每个线程加载2组vec4的4byte数据（即32bytes）</li><li>每个loop需要518个clock（之后的计算部分有具体细节）</li><li>每个SM同一时刻时有128个线程执行</li><li>有13个时钟频率1.2GHz的SM</li><li>每GB相当于0.931GiB</li></ul><p>所以，对于GTX 970 有：<br>（这里可以和上一大节的分析对比，两种分析基本能对应上，充分利用了硬件算力）</p><hr><h4 id="线程数据交换与写回"><a href="#线程数据交换与写回" class="headerlink" title="线程数据交换与写回"></a>线程数据交换与写回</h4><p>再次用图：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bb8hga1815hsfle1b0p1mfr1gcv9.png" alt=""></p><p>在循环的末尾（end=N-8，标记判断），线程块中的C子矩阵数据已经计算完成，要把结果存回显存。由于从共享内存进行4点向量化加载，在写回全局内存的时候，对应的矩阵C的地址并没有作合并访存优化。虽然可以直接写回，但还有优化的余地。线程间通过共享内存交换矩阵C寄存器的数据，重新组织数据，构造合并访存的写入操作。注意，这里warp shuffle指令并不适用，因为需要在不同的线程之间交换不同寄存器的数据。</p><p>如下图示意，把要交换的数据分为8块，在存放C子矩阵的寄存器中按顺序沿纵向划分。这里还是以0号线程数据为例，红色线条是第1次处理的8个数据，注意相邻线程同一次加载的B方向数据，跨度为4（两蓝色线条示意）。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbgveibjnkl14n41lja133317s61t.png" alt=""></p><p>每个线程每次从8个寄存器读取数据到共享内存，借用作者配图表示：<br>此时的数据是按A方向存储的。绿色格子是0号线程的两组vec4数据。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbgvji6bnt01uf212unqsftvb2a.png" alt=""></p><p>然后马上读回数据，此时按B方向读取，由于数据不连续了，分成8次单独访问。<br>黄色格子是线程，绿色格子是0号线程读取的数据。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbgvt0u9l218ko1d3mget1r4r2n.png" alt=""></p><p>​    这里有一个细节是，<strong>上述共享内存读写不作同步</strong>，因为数据交换是在同一warp内进行的。（猜测，存储单元按指令先后处理，前面的存储工作完成后才进行后面的读取工作）最后C子矩阵计算结果从寄存器放回显存。<br>​    整体来看，256个线程分为8个warp，线程与数据空间映射关系如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbh01h71n6ldkn1m3b15bc1tvh34.png" alt=""></p><p>下图是64*64数据块的传输细节。<br>从并行的角度，各线程在A方向连续读取，因为B方向相邻4点存在同一个线程中，而且在数据空间有一行的跨度所以（为了合并访存）不能在一次传输中连续处理。因此，每次存放到共享内存空间的<strong>一行数据</strong>在B方向上有<strong>四行跨度</strong>（图中带颜色的数据方块就是1/8次传输的前一组vec4数据）。<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbh01ta5fcg1tce1bj01dprku03h.png" alt=""><br>至此，就完成了一个block的计算。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;SGEMM：Single precision floatiing General Matrix Multiply&lt;/p&gt;
&lt;p&gt;本文的SGEMM实现思路整理于MaxAs wiki和对MaxAs项目SGEMM汇编代码的理解。&lt;br&gt;MaxAs是一个开源的Maxwell架构GPU汇编器： &lt;a href=&quot;https://github.com/NervanaSystems/maxas&quot;&gt;Github链接&lt;/a&gt;&lt;br&gt;作者&lt;a href=&quot;https://github.com/scott-gray&quot;&gt;Scott Gray&lt;/a&gt;提到两篇相关论文&lt;a href=&quot;http://icl.cs.utk.edu/projectsfiles/magma/pubs/fermi_gemm.pdf&quot;&gt;MAGMA paper&lt;/a&gt;和&lt;a href=&quot;https://hal.inria.fr/file/index/docid/789958/filename/112_Lai.pdf&quot;&gt;Kepler sgemm paper&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CUDA" scheme="http://yuanquanquan.top/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>flask实现图像处理API</title>
    <link href="http://yuanquanquan.top/2021/20210515/"/>
    <id>http://yuanquanquan.top/2021/20210515/</id>
    <published>2021-05-15T06:30:36.000Z</published>
    <updated>2021-05-15T06:43:29.165Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Flask是一个轻量级的后台框架，学习成本低，维护简单，传统<a href="https://www.java.com/">java</a>、<a href="https://www.php.net/">Php</a>太过笨重。如果只是从简单这个角度出发，Flask是开发后端的佼佼者，最快只需要7行代码完成一个<a href="https://en.wikipedia.org/wiki/Web">Web</a>应用。</p><p>本文通过flask实现了一个图像处理的API接口，写了不少，hexo有字符无法转义，就转pdf放上来了</p></blockquote><span id="more"></span>  <div class="row">    <embed src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding/flask.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Flask是一个轻量级的后台框架，学习成本低，维护简单，传统&lt;a href=&quot;https://www.java.com/&quot;&gt;java&lt;/a&gt;、&lt;a href=&quot;https://www.php.net/&quot;&gt;Php&lt;/a&gt;太过笨重。如果只是从简单这个角度出发，Flask是开发后端的佼佼者，最快只需要7行代码完成一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Web&quot;&gt;Web&lt;/a&gt;应用。&lt;/p&gt;
&lt;p&gt;本文通过flask实现了一个图像处理的API接口，写了不少，hexo有字符无法转义，就转pdf放上来了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="flask" scheme="http://yuanquanquan.top/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow分布式实现-Kubernetes</title>
    <link href="http://yuanquanquan.top/2021/20210415/"/>
    <id>http://yuanquanquan.top/2021/20210415/</id>
    <published>2021-04-15T00:23:00.000Z</published>
    <updated>2021-05-17T08:15:17.592Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>随着海量数据的出现和模型参数的增多，我们必然需要更大的集群来运行模型，这样最大的好处在于把原本可能需要周级别的训练时间缩短到天级别甚至小时级别。未来的模型训练面对的都是上亿数据和上亿参数，稳定的计算能力和管理便捷的集群环境至关重要。Kubernetes 是目前应用最广泛的容器集群管理工具之一，它可以为对分布式TensorFlow 的监控、调度等生命周期管理提供所需的保障。</p></blockquote><span id="more"></span> <h2 id="分布式TensorFlow-在Kubernetes-中的运行"><a href="#分布式TensorFlow-在Kubernetes-中的运行" class="headerlink" title="分布式TensorFlow 在Kubernetes 中的运行"></a>分布式TensorFlow 在Kubernetes 中的运行</h2><p>本节介绍在Kubernetes 中运行分布式TensorFlow 的方法。首先学习如何部署Kubernetes 环境，接着在搭建好的环境中运行分布式TensorFlow，并用MNIST 来训练。</p><h3 id="部署及运行"><a href="#部署及运行" class="headerlink" title="部署及运行"></a>部署及运行</h3><p>首先需要先安装Kubernetes。</p><p>用Minikube 来创建本地Kubernetes 集群。安装Minikube 需要预先安装VirtualBox 虚拟机，可以从<a href="https://www.virtualbox.org/">官网</a>上直接下载安装，注意选择对应的操作系统版本即可。<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517145615182.png" alt=""></p><p>Minikube 用Go 语言编写，发布形式是一个独立的二进制文件，所以只需要下载下来，然后放在对应的位置即可。因此安装Minikube，只需要一条命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.14.0/</span><br><span class="line"></span><br><span class="line">minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/</span><br></pre></td></tr></table></figure><p>Kubernetes 提供了一个客户端kubectl，可直接通过kubectl 以命令行的方式与集群交互。</p><p>安装kubectl 的方法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -Lo kubectl http://storage.googleapis.com/kubernetes-release/release/v1.5.1/bin/darwin/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/</span><br></pre></td></tr></table></figure><p>下面在Minikube 中启动 Kubernetes 集群，如图所示。<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517145843210.png" alt=""><br>可以观察到VirtualBox 中也启动了相应的虚拟机，如图所示。<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517145904869.png" alt=""><br>采用<a href="https://hub.docker.com/r/tensorflow/tensorflow/">Docker Hub</a>上的最新镜像tensorflow/tensorflow（基于TensorFlow 的1.0 版本）。<br>首先，配置参数服务器的部署（deployment）文件，命名为tf-ps-deployment.json。代码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;apiVersion&quot;</span>: <span class="string">&quot;extensions/v1beta1&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;kind&quot;</span>: <span class="string">&quot;Deployment&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line">  <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-ps2&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;replicas&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="attr">&quot;template&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;labels&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line">            <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-ps2&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;role&quot;</span>: <span class="string">&quot;ps&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;containers&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;ps&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;image&quot;</span>: <span class="string">&quot;tensorflow/tensorflow&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;ports&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;containerPort&quot;</span>: <span class="number">2222</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置参数服务器的服务（Service）文件，命名为tf-ps-service.json，代码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;apiVersion&quot;</span>: <span class="string">&quot;v1&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;kind&quot;</span>: <span class="string">&quot;Service&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;ports&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;port&quot;</span>: <span class="number">2222</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;targetPort&quot;</span>: <span class="number">2222</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">],</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;selector&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-ps2&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;labels&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;role&quot;</span>: <span class="string">&quot;service&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-ps2-service&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置计算服务器的部署文件，命名为tf-worker-deployment.json，代码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;apiVersion&quot;</span>: <span class="string">&quot;extensions/v1beta1&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;kind&quot;</span>: <span class="string">&quot;Deployment&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-worker2&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;replicas&quot;</span>: <span class="number">2</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;template&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;labels&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-worker2&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;role&quot;</span>: <span class="string">&quot;worker&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;containers&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;worker&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;image&quot;</span>: <span class="string">&quot;tensorflow/tensorflow&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;ports&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;containerPort&quot;</span>: <span class="number">2222</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置计算服务器的服务文件，命名为tf-worker-service.json，代码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;apiVersion&quot;</span>: <span class="string">&quot;v1&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;kind&quot;</span>: <span class="string">&quot;Service&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;ports&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;port&quot;</span>: <span class="number">2222</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;targetPort&quot;</span>: <span class="number">2222</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">],</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;selector&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-worker2&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;labels&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-worker2&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;role&quot;</span>: <span class="string">&quot;service&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-wk2-service&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f tf-ps-deployment.json</span><br><span class="line"></span><br><span class="line">kubectl create -f tf-ps-service.json</span><br><span class="line"></span><br><span class="line">kubectl create -f tf-worker-deployment.json</span><br><span class="line"></span><br><span class="line">kubectl create -f tf-worker-service.json</span><br></pre></td></tr></table></figure><p>分别输出以下结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">deployment &quot;tensorflow-ps2&quot; created</span><br><span class="line"></span><br><span class="line">service &quot; tensorflow-ps2-service&quot; created</span><br><span class="line"></span><br><span class="line">deployment &quot;tensorflow-worker2&quot; created</span><br><span class="line"></span><br><span class="line">service &quot;tensorflow-wk2-service&quot; created</span><br></pre></td></tr></table></figure><p>稍等片刻，运行kubectl get pod，可以看到参数服务器和计算服务器全部创建完成</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517150527645.png" alt=""></p><p>下面我们进入每个服务器（Pod）中，部署好需要运行的mnist_replica.py 文件。</p><p>首先查看以下2 台ps_host 的 IP 地址</p><p>然后查看2 台worker_host 的IP 地址</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517150600269.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517150612314.png" alt=""></p><p>打开4 个终端，分别进入4 个Pod 当中，命令如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -ti tensorflow-ps2-3073558082-3b08h /bin/bash</span><br><span class="line"></span><br><span class="line">kubectl exec -ti tensorflow-ps2-3073558082-4x3j2 /bin/bash</span><br><span class="line"></span><br><span class="line">kubectl exec -ti tensorflow-worker2-3070479207-k6z8f /bin/bash</span><br><span class="line"></span><br><span class="line">kubectl exec -ti tensorflow-worker2-3070479207-6hvsk /bin/bash</span><br></pre></td></tr></table></figure><p>通过下面的方式将mnist_replica.py 分别部署到4 个Pod 中，如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl https://raw.githubusercontent.com/tensorflow/tensorflow/master/</span><br><span class="line"></span><br><span class="line">tensorflow/tools/dist_test/python/mnist_replica.py -o mnist_replica.py</span><br></pre></td></tr></table></figure><p>在参数服务器的两个容器中分别执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python mnist_replica.py --ps_hosts=172.17.0.16:2222,172.17.0.17:2222 --worker_</span><br><span class="line"></span><br><span class="line">hosts=172.17.0.3:2222,172.17.0.8:2222 --job_name=&quot;ps&quot; --task_index=0</span><br><span class="line"></span><br><span class="line">python mnist_replica.py --ps_hosts=172.17.0.16:2222,172.17.0.17:2222 --worker_</span><br><span class="line"></span><br><span class="line">hosts=172.17.0.3:2222,172.17.0.8:2222 --job_name=&quot;ps&quot; --task_index=1</span><br></pre></td></tr></table></figure><p>在计算服务器的两个容器中分别执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python mnist_replica.py --ps_hosts=172.17.0.16:2222,172.17.0.17:2222 --worker_</span><br><span class="line"></span><br><span class="line">hosts=172.17.0.3:2222,172.17.0.8:2222 --job_name=&quot;worker&quot; --task_index=0</span><br><span class="line"></span><br><span class="line">python mnist_replica.py --ps_hosts=172.17.0.16:2222,172.17.0.17:2222 --worker_</span><br><span class="line"></span><br><span class="line">hosts=172.17.0.3:2222,172.17.0.8:2222 --job_name=&quot;worker&quot; --task_index=1</span><br></pre></td></tr></table></figure><p>执行输出与14.6 节的输出类似。一共执行200 次迭代，工作节点1（172.17.0.3:2222）执行了144 次迭代，如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">job name = worker</span><br><span class="line"></span><br><span class="line">task index = 0</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize</span><br><span class="line"></span><br><span class="line">GrpcChannelCache for job ps -&gt; &#123;0 -&gt; localhost:2222&#125;</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannel</span><br><span class="line"></span><br><span class="line">Cache for job worker -&gt; &#123;0 -&gt; localhost:2223, 1 -&gt; localhost:2224&#125;</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:217] Started server</span><br><span class="line"></span><br><span class="line">with target: grpc://localhost:2223</span><br><span class="line"></span><br><span class="line">Worker 0: Initializing session...</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/master_session.cc:994] Start master session</span><br><span class="line"></span><br><span class="line">0d791a02977e5701 with config:</span><br><span class="line"></span><br><span class="line">device_filters: &quot;/job:ps&quot;</span><br><span class="line"></span><br><span class="line">device_filters: &quot;/job:worker/task:0&quot;</span><br><span class="line"></span><br><span class="line">allow_soft_placement: true</span><br><span class="line"></span><br><span class="line">Worker 0: Session initialization complete.</span><br><span class="line"></span><br><span class="line">Training begins @ 1483516057.489495</span><br><span class="line"></span><br><span class="line">1483516057.518419: Worker 0: training step 1 done (global step: 0)</span><br><span class="line"></span><br><span class="line">1483516057.541053: Worker 0: training step 2 done (global step: 1)</span><br><span class="line"></span><br><span class="line">1483516057.569677: Worker 0: training step 3 done (global step: 2)</span><br><span class="line"></span><br><span class="line">1483516057.584578: Worker 0: training step 4 done (global step: 3)</span><br><span class="line"></span><br><span class="line">1483516057.646970: Worker 0: training step 5 done (global step: 4)</span><br><span class="line"></span><br><span class="line">\# ……中间略去</span><br><span class="line"></span><br><span class="line">1483516059.286596: Worker 0: training step 141 done (global step: 197)</span><br><span class="line"></span><br><span class="line">1483516059.291600: Worker 0: training step 142 done (global step: 198)</span><br><span class="line"></span><br><span class="line">1483516059.297347: Worker 0: training step 143 done (global step: 199)</span><br><span class="line">1483516059.303738: Worker 0: training step 144 done (global step: 200)</span><br><span class="line"></span><br><span class="line">Training ends @ 1483516059.303808</span><br><span class="line"></span><br><span class="line">Training elapsed time: 1.614513 s</span><br><span class="line"></span><br><span class="line">After 200 training step(s), validation cross entropy = 1235.56</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>工作节点2（172.17.0.8:2222）执行了56 次迭代，输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">job name = worker</span><br><span class="line"></span><br><span class="line">task index = 1</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannel</span><br><span class="line"></span><br><span class="line">Cache for job ps -&gt; &#123;0 -&gt; localhost:2222&#125;</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannel</span><br><span class="line"></span><br><span class="line">Cache for job worker -&gt; &#123;0 -&gt; localhost:2223, 1 -&gt; localhost:2224&#125;</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:217] Started server</span><br><span class="line"></span><br><span class="line">with target: grpc://localhost:2224</span><br><span class="line"></span><br><span class="line">Worker 1: Waiting for session to be initialized...</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/master_session.cc:994] Start master session</span><br><span class="line"></span><br><span class="line">92e671f3dd1ffd05 with config:</span><br><span class="line"></span><br><span class="line">device_filters: &quot;/job:ps&quot;</span><br><span class="line"></span><br><span class="line">device_filters: &quot;/job:worker/task:1&quot;</span><br><span class="line"></span><br><span class="line">allow_soft_placement: true</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Worker 1: Session initialization complete.</span><br><span class="line"></span><br><span class="line">Training begins @ 1483516058.803010</span><br><span class="line"></span><br><span class="line">1483516058.832164: Worker 1: training step 1 done (global step: 121)</span><br><span class="line"></span><br><span class="line">1483516058.844464: Worker 1: training step 2 done (global step: 123)</span><br><span class="line"></span><br><span class="line">1483516058.860988: Worker 1: training step 3 done (global step: 126)</span><br><span class="line"></span><br><span class="line">1483516058.873543: Worker 1: training step 4 done (global step: 128)</span><br><span class="line"></span><br><span class="line">1483516058.884758: Worker 1: training step 5 done (global step: 130)</span><br><span class="line"></span><br><span class="line">\# ……中间略去</span><br><span class="line"></span><br><span class="line">1483516059.152332: Worker 1: training step 52 done (global step: 176)</span><br><span class="line"></span><br><span class="line">1483516059.167606: Worker 1: training step 53 done (global step: 178)</span><br><span class="line"></span><br><span class="line">1483516059.177215: Worker 1: training step 54 done (global step: 180)</span><br><span class="line"></span><br><span class="line">1483516059.301384: Worker 1: training step 55 done (global step: 182)</span><br><span class="line"></span><br><span class="line">1483516059.309557: Worker 1: training step 56 done (global step: 202)</span><br><span class="line"></span><br><span class="line">Training ends @ 1483516059.309638</span><br><span class="line"></span><br><span class="line">Training elapsed time: 0.536126 s</span><br><span class="line"></span><br><span class="line">After 200 training step(s), validation cross entropy = 1235.56</span><br></pre></td></tr></table></figure><p>在这个例子中，更好的方式是把需要执行的源代码以及训练数据和测试数据放在持久卷（persistent volume）中，在多个Pod 间实现共享，从而避免在每一个Pod 中分别部署。对应TensorFlow 的GPU 的Docker 集群部署，Nvidia 官方提供了nvidia-docker 的方式，原理主要是利用宿主机上的GPU 设备，将它映射到容器中。更多与部署相关的内容可以参考<a href="https://github.com/NVIDIA/nvidia-docker。">https://github.com/NVIDIA/nvidia-docker。</a></p><h2 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h2><p>训练好模型之后可以将它打包制作成环境独立的镜像，这样能够极大地方便测试人员部署一致的环境，也便于对不同版本的模型做标记、比较不同模型的准确率，从整体上降低测试、部署上线等的工作复杂性，具有很大的优势。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>将Kubernete 与TensorFlow 结合，借助Kubernetes 提供的稳定计算环境，对TensorFlow 集</p><p>群进行便捷的管理，降低了搭建大规模深度学习平台的难度，这也是社区非常推崇的部署方案。</p><p>本章主要讲述了用Kubernetes 管理TensorFlow 集群的方法，以及在Kubernetes 上部署分布式</p><p>TensorFlow 的方式，最后采用MNIST 的分布式例子进行了实践。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;随着海量数据的出现和模型参数的增多，我们必然需要更大的集群来运行模型，这样最大的好处在于把原本可能需要周级别的训练时间缩短到天级别甚至小时级别。未来的模型训练面对的都是上亿数据和上亿参数，稳定的计算能力和管理便捷的集群环境至关重要。Kubernetes 是目前应用最广泛的容器集群管理工具之一，它可以为对分布式TensorFlow 的监控、调度等生命周期管理提供所需的保障。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="分布式" scheme="http://yuanquanquan.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>《CUDA权威编程指南》笔记-1</title>
    <link href="http://yuanquanquan.top/2021/20210407/"/>
    <id>http://yuanquanquan.top/2021/20210407/</id>
    <published>2021-04-07T02:05:01.000Z</published>
    <updated>2021-06-04T15:44:20.681Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>CUDA是一种通用的并行计算平台和编程模型，是在C语言上扩展的。借助于CUDA，你可以像编写C语言程序一样实现并行算法。你可以在NIVDIA的GPU平台上用CUDA为多种系统编写应用程序，范围从嵌入式设备、平板电脑、笔记本电脑、台式机工作站到HPC集群。在CUDA编程平台中，GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此当我们在说GPU并行计算时，其实是指的基于CPU+GPU的异构计算架构。在异构计算架构中，GPU与CPU通过PCIe总线连接在一起来协同工作，CPU所在位置称为为主机端（host），而GPU所在位置称为设备端（device）。</p></blockquote><span id="more"></span><h2 id="第一章-基于CUDA的异构并行计算"><a href="#第一章-基于CUDA的异构并行计算" class="headerlink" title="第一章  基于CUDA的异构并行计算"></a>第一章  基于CUDA的异构并行计算</h2><h3 id="并行计算"><a href="#并行计算" class="headerlink" title="并行计算"></a>并行计算</h3><ul><li>一个大的问题可以被分解成多个小问题，然后在不同的计算资源上并行处 理这些小问题。</li><li>并行计算的软件和硬件层面是紧密联系的。</li><li>事实上，并行计算通常涉及两个不同的计算技术领域。</li><li>串行编程和并行编程</li><li>大多数现代处理器的哈佛体系结构（Harvard architecture）</li><li>计算机架构（硬件方面）</li><li>并行程序设计（软件方面）</li></ul><hr><h3 id="并行性"><a href="#并行性" class="headerlink" title="并行性"></a>并行性</h3><ul><li><p>多层次的并行性设计是架构设计的驱动力。在应用程序中有两种基本的并行类型。</p><blockquote><p>CUDA编程非常适合解决数据并行计算的问题。本书的重点便是如何使用CUDA编程 解决数据并行问题。</p></blockquote></li></ul><h4 id="数据并行程序设计"><a href="#数据并行程序设计" class="headerlink" title="数据并行程序设计"></a>数据并行程序设计</h4><ul><li><ul><li>块划分（block partitioning）</li><li>周期划分（cyclic partitioning）</li><li>一组连续的数据被分到一个块内</li><li>每个数据块以任 意次序被安排给一个线程</li><li>线程通常在同一时间只处理一个数据块</li><li>更少的数据被分到一个块内</li><li>相邻的线程处理相邻的数据块</li><li>每个线程可以处理多个数据块</li><li>为一个待处理的线程选择一个新的块，就意味着要跳过和现有线程一样多的数据块</li><li>第一步是把数据依据线程进行划分，使每个线程处理一部分数据</li><li>利用多核系统对数据进行分配</li><li>利用多核系统对任务进行分配</li><li>任务并行</li><li>数据并行</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210410133904604.png" alt=""></p><h4 id="计算机架构"><a href="#计算机架构" class="headerlink" title="计算机架构"></a>计算机架构</h4><ul><li><p>弗林分类法（Flynn’s Taxonomy）</p></li><li><ul><li>降低延迟</li><li>提高带宽</li><li>提高吞吐量</li><li>延迟是一个操作从开始到完成所需要的时间，常用微秒来表示</li><li>延迟用来衡量完成一次操作的时间</li><li>带宽是单位时间内可 处理的数据量，通常表示为MB/s或GB/s。</li><li>吞吐量是单位时间内成功处理的运算数量，通常表示为gflops（即每秒十亿次的浮点运算数量），特别是在重点使用浮点计算的科学计算领域经常用到</li><li>而吞吐量用来衡量在给定的单位时 间内处理的操作量</li><li>单指令单数据（SISD）(传统计算机，一种串行架构)</li><li>单指令多数据（SIMD） (按串行逻辑思考但对并行数据操作实现并行加速，而其他细节则由编译器来负责)</li><li>多指令单数据（MISD） (每个核心通过使用多个指令流处理同一个数据流)</li><li>多指令多数据（MIMD）(多个核心使用多个指令流来异步处理多个数据流，从而实现空间上的并行性)</li><li>根据指令和数据进入CPU的方式</li><li>实现以下目的，架构取得了许多进展</li></ul></li></ul><ul><li><p>计算机架构也能根据内存组织方式进行进一步划分</p></li><li><ul><li><p>要么是与同一个物理内存相关联</p></li><li><p>要么共用一个低延迟的链路（如PCIExpress或PCIe）</p></li><li><p>多核架构已经永久地取代了单核架构</p></li><li><p>多线程、 MIMD（多指令多数据）、SIMD（单指令多数据），以及指令级并行。</p><p>NVIDIA公司称这 种架构为SIMT（单指令多线程）</p></li><li><p>“众核”（many-core）通常是指有很多核心（几十或几百个）的多核架构</p></li><li><p>GPU代表了一种众核架构，几乎包括了前文描述的所有并行结构：</p></li><li><p>这种系统常被称作集群</p></li><li><p>分布式内存的多节点系统</p></li><li><p>共享内存的多处理器系统</p></li></ul></li><li></li><li><p>GPU核心和CPU核心</p></li><li><ul><li>较轻，用于优化具有简单控制逻辑的数据并行任务，注重并行程序的吞吐量</li><li>较重，用来处理非常复杂的控制逻辑，以优化串行程序执行</li><li>两种核心是完全不同的</li><li>CPU核心</li><li>GPU核心</li></ul></li></ul><hr><h3 id="异构计算"><a href="#异构计算" class="headerlink" title="异构计算"></a>异构计算</h3><blockquote><p>GPU指的是离散的设备从同构系统到异构系统的转变是高性能计算 史上的一个里程碑。</p></blockquote><ul><li><p>同构计算使用的是同一架构下的一个或多个处理器来执行一个应用。</p></li><li><p>而异构计算则使用一个处理器架构来执行一个应用，为任务选择适合它的架构，使其最终对性能有所改进。</p></li><li><ul><li>系统的有效利用 受限于增加应用程序设计的复杂性</li></ul></li></ul><h4 id="异构架构"><a href="#异构架构" class="headerlink" title="异构架构"></a>异构架构</h4><ul><li><p>一个典型的异构计算节点</p></li><li><ul><li>GPU不 是一个独立运行的平台而是CPU的协处理器</li><li>通过PCIe总线与基于CPU的 主机相连来进行操作</li><li>两个多核CPU插槽（主机端）</li><li>两个或更多个的众核GPU （设备端）</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210410134053193.png" alt=""></p><ul><li><p>一个异构应用包括两个部分</p></li><li><ul><li><p>在计算密集型应用中，往往有很多并行数据的程序段。</p><p>GPU就是用来提高这些并行数据的执行速度的。</p></li><li><p>在设备端加载计算密集型任务之前，CPU代码负责管理设备端的环境、代码和数据。</p></li><li><p>主机代码（CPU上运行）</p></li><li><p>设备代码（GPU上运行）</p></li></ul></li><li><p>NVIDIA公司的GPU计算平台</p></li><li><ul><li>Fermi是Tesla系列产品中的一种，用作GPU加速器，近来在高性能计算中获得了广泛应用</li><li>Fermi之后的新一代GPU 计算架构Kepler，于2012年秋季发布，其处理能力相比以往的GPU有很大提升，并且提供 了新的方法来优化和提高GPU并行工作的执行，有望将高性能计算提升到新的高度</li><li>Tegra系列产品是专为移动和嵌入式设备而设计的，如平板电脑和手机</li><li>GeForce面向图形用户</li><li>Quadro用于专业绘图设计</li><li>Tesla用于大规模的并行计算</li></ul></li><li><p>描述GPU容量的两个重要特征</p></li><li><ul><li>CUDA核心数量</li><li>内存大小</li></ul></li><li><p>两种不同的指标来评估GPU的性能</p></li><li><ul><li>内存带宽是从内存中读取或写入数据的比率。</li><li>内存带宽通常用 GB/s表示。</li><li>峰值计算性能是用来评估计算容量的一个指标，通常定义为每秒能处理的单精度或双 精度浮点运算的数量.</li><li>峰值性能通常用GFlops（每秒十亿次浮点运算）或TFlops（每秒万 亿次浮点运算）来表示。</li><li>峰值计算性能</li><li>内存带宽</li></ul></li><li><p>计算能力</p></li><li><ul><li>NVIDIA使用一个术语“计算能力”（compute capability）来描述整个Tesla系列的GPU加 速器的硬件版本。</li></ul></li></ul><h3 id="异构计算范例"><a href="#异构计算范例" class="headerlink" title="异构计算范例"></a>异构计算范例</h3><blockquote><p>GPU计算并不是要取代CPU计算</p></blockquote><blockquote><p>Tip：这一段我就不写了，本章节的主要内容用一张图即可概括</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210410134140428.png" alt=""></p><h3 id="CUDA：一种异构计算平台"><a href="#CUDA：一种异构计算平台" class="headerlink" title="CUDA：一种异构计算平台"></a>CUDA：一种异构计算平台</h3><h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><blockquote><p>CUDA提供了两层API来管理GPU设备和组织线程。</p><p>这两种API是相互排斥的，你必须使用两者之一，从两者中混合函数调用是不可能的。</p></blockquote><ul><li><p>CUDA驱动API</p></li><li><ul><li>驱动API是一种低级API，它相对来说较难编程</li><li>对于在GPU设备使用上提供了更多的控制</li></ul></li><li><p>CUDA运行时API</p></li><li><ul><li>运行时API是一个高级API</li><li>它在驱动API的上层实现</li><li>每个运行时API函数都被分解为更多传给驱动API的基本运算</li></ul></li></ul><blockquote><p>也就是说Runtime api 可以看作是由Driver api 封装而成的</p></blockquote><h4 id="CUDA程序"><a href="#CUDA程序" class="headerlink" title="CUDA程序"></a>CUDA程序</h4><ul><li><p>一个CUDA程序包含了以下两个部分的混合。</p></li><li><ul><li>在CPU上运行的主机代码</li><li>在GPU上运行的设备代码</li></ul></li></ul><ul><li><p>一个典型的CUDA编程结构包括5个主要步骤</p></li><li><ul><li>分配GPU内存</li><li>从CPU内存中拷贝数据到GPU内存</li><li>调用CUDA内核函数来完成程序指定的运算</li><li>将数据从GPU拷回CPU内存</li><li>释放GPU内存空间</li></ul></li></ul><h4 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h4><ul><li><p>数据局部性在并行编程中是一个非常重要的概念。</p></li><li><ul><li>时间局部性是指在相对较短的时间段内数据和/或资源的重用。</li><li>空间局部性是指在相对较接近的存储空间内数据元素的重用。</li><li>数据局部性指的是数据重用，以降低内存访问的延迟。</li><li>数据局部性有两种基本类型</li></ul></li><li><p>CUDA中有内存层次和线程层次的概念</p></li><li><ul><li><p>内存层次结构</p></li><li><p>线程层次结构</p></li><li><p>例如：</p><p>在CUDA编程模型中使用的共享内存（一个特殊的内存）。</p><p>共享内存可以视为 一个被软件管理的高速缓存，通过为主内存节省带宽来大幅度提高运行速度。</p><p>有了共享内存，你可以直接控制代码的数据局部性。</p></li></ul></li></ul><p><strong>编译环境</strong>：本代码将使用<code>nvcc</code>编译器来编译，你可以使用以下命令来检查CUDA是否正确安装:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ which nvcc</span><br><span class="line">/usr/local/cuda-8.0/bin/nvcc  # cuda-8.0 版本</span><br></pre></td></tr></table></figure><h3 id="用GPU输出-Hello-World"><a href="#用GPU输出-Hello-World" class="headerlink" title="用GPU输出 Hello World"></a>用GPU输出 Hello World</h3><p>不妨先写一个cuda C程序，命名为<code>helloFromGPU</code>，用它来输出字符串 “Hello World from GPU！”</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">========================== 代码清单 1-1 Hello World from GPU (hello.cu) ==========================</span><br><span class="line">// hello.cu</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">__global__ void helloFromGPU (void) </span><br><span class="line">&#123;</span><br><span class="line">    printf(&quot;Hello World from GPU!\n&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(void)</span><br><span class="line">&#123;</span><br><span class="line">    // hello from cpu</span><br><span class="line">    printf(&quot;Hello World from CPU!\n&quot;);</span><br><span class="line"></span><br><span class="line">    helloFromGPU &lt;&lt;&lt;1, 10&gt;&gt;&gt;();</span><br><span class="line">    cudaDeviceReset();</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在linux终端下使用以下命令进行编译<a href="https://github.com/YunYang1994/cuda-tutorial/blob/master/src/chapter01/hello.cu"><code>hello.cu</code></a>，然后执行程序得到</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ nvcc -arch sm_20 hello.cu -o hello</span><br><span class="line">$ ./hello</span><br><span class="line">Hello World from CPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br></pre></td></tr></table></figure><p>在上面的代码中，<code>cudaDeviceReset</code>表示重置当前线程所关联过的当前设备的所有资源；修饰符<code>__global__</code>告诉编译器这是一个内核函数，它将从CPU中调用，然后在GPU上执行，在CPU上通过下面的代码启动内核函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helloFromGPU &lt;&lt;&lt;1, 10&gt;&gt;&gt;();</span><br></pre></td></tr></table></figure><blockquote><p>三重尖号意味着从主线程到端代码的调用。1和10分别表示有1个块区域和10个线程，后续会作相关介绍。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;CUDA是一种通用的并行计算平台和编程模型，是在C语言上扩展的。借助于CUDA，你可以像编写C语言程序一样实现并行算法。你可以在NIVDIA的GPU平台上用CUDA为多种系统编写应用程序，范围从嵌入式设备、平板电脑、笔记本电脑、台式机工作站到HPC集群。在CUDA编程平台中，GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此当我们在说GPU并行计算时，其实是指的基于CPU+GPU的异构计算架构。在异构计算架构中，GPU与CPU通过PCIe总线连接在一起来协同工作，CPU所在位置称为为主机端（host），而GPU所在位置称为设备端（device）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CUDA" scheme="http://yuanquanquan.top/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>yolov5的openvino部署</title>
    <link href="http://yuanquanquan.top/2021/20210406/"/>
    <id>http://yuanquanquan.top/2021/20210406/</id>
    <published>2021-04-06T05:30:50.000Z</published>
    <updated>2021-04-06T11:43:03.005Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>OpenVINO是一个用于提升深度学习模型部署性能的跨平台的工具包，安装和使用都非常简单，下面 demo 视频是在 Mac 机器上运行的例子，安装过程在上面的文档链接中，安装主要是需要下载大量深度框架相关的库和模型，以及 Intel 已经封装好的一些脚本。</p><p>之前在<a href="https://yuanquanquan.top/2020/20201027/">神经计算棒</a>上部署过openvino来测试神经棒的速度。</p><p>Demo 大致的过程是下载一个预先训练好的模型，这个模型是用来检测图片车辆的一个模型，然后针对例子中的车的图片，并做 Inference 的过程。</p></blockquote><a id="more"></a> <h2 id="软硬件环境"><a href="#软硬件环境" class="headerlink" title="软硬件环境"></a>软硬件环境</h2><ul><li>ubuntu 18.04 64bit</li><li>openvino_2020.3.341</li><li>yolov5 4.0</li></ul><h2 id="openvino是什么"><a href="#openvino是什么" class="headerlink" title="openvino是什么"></a>openvino是什么</h2><p><code>openvino</code>是一个用于解决在<code>intel</code>硬件平台上进行深度学习部署的方案，支持<code>windows</code>、<code>linux</code>和<code>macOS</code>。</p><h2 id="openvino环境搭建"><a href="#openvino环境搭建" class="headerlink" title="openvino环境搭建"></a>openvino环境搭建</h2><p>下载地址：<a href="https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html" target="_blank" rel="noopener">https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html</a></p><p>下载后解压并进入目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar xvf l_openvino_toolkit_p_2020.3.341.tgz</span><br><span class="line">cd l_openvino_toolkit_p_2020.3.341</span><br></pre></td></tr></table></figure><p>执行脚本开始按提示安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 或者使用带GUI界面的安装脚本sudo ./install_GUI.sh</span><br><span class="line">sudo ./install.sh</span><br></pre></td></tr></table></figure><p>接下来测试下安装是否成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/intel/openvino/deployment_tools/demo</span><br><span class="line">./demo_security_barrier_camera.sh</span><br><span class="line">./demo_security_barrier_camera.sh -d GPU(测试gpu环境)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210406193810747.png" alt></p><p>这就说明，<code>openvino</code>的环境安装成功了。</p><p>这里需要注意下，由于，在安装过程中，脚本已经帮我们设置了相关的环境，所以我们去测试时无需做任何设置。但是如果下次开机，或者打开新的<code>terminal</code>，我们就需要重新来设置环境，执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /opt/intel/openvino/bin/setupvars.sh</span><br></pre></td></tr></table></figure><p>或者直接将上述命令写入<code>~/.bashrc</code>中，就不用每次手动敲了。</p><p>目录<code>/opt/intel</code>下有<code>openvino</code>和<code>openvino_2020.3.341</code>2个目录，其实它们是同一个东西，<code>openvino</code>是个软链接文件。</p><h2 id="pt转onnx"><a href="#pt转onnx" class="headerlink" title="pt转onnx"></a>pt转onnx</h2><p>首先准备依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install onnx coremltools networkx defusedxml</span><br></pre></td></tr></table></figure><p>由于目前<code>openvino</code>的版本对<code>onnx opset</code> 11 版本后的支持有问题，因此需要修改文件<code>model/export.py</code>，将原来<code>opset_version</code>由12改为10，如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.onnx.export(model, img, f, verbose=False, opset_version=10, input_names=[&apos;images&apos;],</span><br><span class="line">                    output_names=[&apos;classes&apos;, &apos;boxes&apos;] if y is None else [&apos;output&apos;])</span><br></pre></td></tr></table></figure><p>接下来就可以将<code>yolov5</code>的<code>pt</code>模型转换成<code>onnx</code>格式了，这里使用其自带的<code>yolov5s.pt</code>模型进行测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python models/export.py --weights weights/yolov5s.pt --img-size 640 --batch-size 1</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210406193513763.png" alt></p><p>转换结束后，就会在<code>weights</code>文件夹下生成<code>yolov5s.onnx</code></p><h2 id="onnx转ir"><a href="#onnx转ir" class="headerlink" title="onnx转ir"></a>onnx转ir</h2><p><code>ir</code>即<code>Intermediate Representation</code>，<code>openvino</code>的模型优化器(<code>Model Optimizer</code>)会将给定的模型转化为标准的<code>ir</code>格式，并对其进行优化。</p><p>使用<code>openvino</code>自带的脚本，就可以完成从<code>.onnx</code>到<code>.bin</code>和<code>.xml</code>的转换，命令如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mo.py --input_model weights/yolov5s.onnx</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210406193617693.png" alt></p><h2 id="openvino测试"><a href="#openvino测试" class="headerlink" title="openvino测试"></a>openvino测试</h2><p>这里使用<code>c++</code>语言编写测试程序，下载地址：<a href="https://github.com/fb029ed/yolov5_cpp_openvino，非常感谢作者`fb029ed`的分享" target="_blank" rel="noopener">https://github.com/fb029ed/yolov5_cpp_openvino，非常感谢作者`fb029ed`的分享</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd yolov5_cpp_openvino/demo</span><br><span class="line">mkdir build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">./detect_test</span><br></pre></td></tr></table></figure><p>需要将前面生成的<code>yolov5s.bin</code>、<code>yolov5s.xml</code>和一张测试图片拷贝到<code>res</code>目录下，图片重命名为<code>bus.jpg</code></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210406193652959.png" alt></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html" target="_blank" rel="noopener">https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html</a></li><li><a href="https://github.com/violet17/yolov5_demo" target="_blank" rel="noopener">https://github.com/violet17/yolov5_demo</a></li><li><a href="https://github.com/fb029ed/yolov5_cpp_openvino" target="_blank" rel="noopener">https://github.com/fb029ed/yolov5_cpp_openvino</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;OpenVINO是一个用于提升深度学习模型部署性能的跨平台的工具包，安装和使用都非常简单，下面 demo 视频是在 Mac 机器上运行的例子，安装过程在上面的文档链接中，安装主要是需要下载大量深度框架相关的库和模型，以及 Intel 已经封装好的一些脚本。&lt;/p&gt;
&lt;p&gt;之前在&lt;a href=&quot;https://yuanquanquan.top/2020/20201027/&quot;&gt;神经计算棒&lt;/a&gt;上部署过openvino来测试神经棒的速度。&lt;/p&gt;
&lt;p&gt;Demo 大致的过程是下载一个预先训练好的模型，这个模型是用来检测图片车辆的一个模型，然后针对例子中的车的图片，并做 Inference 的过程。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="openvino" scheme="http://yuanquanquan.top/tags/openvino/"/>
    
  </entry>
  
  <entry>
    <title>模型训练完成后自动发送微信、邮件</title>
    <link href="http://yuanquanquan.top/2021/20210402/"/>
    <id>http://yuanquanquan.top/2021/20210402/</id>
    <published>2021-04-01T19:05:17.000Z</published>
    <updated>2021-04-01T19:58:48.308Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这是篇关于炼丹期间如何摸鱼的文章～～</p></blockquote><a id="more"></a><h2 id="微信设置"><a href="#微信设置" class="headerlink" title="微信设置"></a>微信设置</h2><p>本来可以通过itcahr，在模型训练完成后向文件传输助手发送消息来告知模型已经训练完毕。但是现在貌似网页版微信已经不可以使用了，所以基于ServerChan发送消息，分享给大家。</p><h3 id="Server-Chan"><a href="#Server-Chan" class="headerlink" title="Server Chan"></a>Server Chan</h3><p>首先需要在server酱官网：<a href="http://sc.ftqq.com/3.version注册账号，直接绑定github账号即可，会获得一个SCKEY" target="_blank" rel="noopener">http://sc.ftqq.com/3.version注册账号，直接绑定github账号即可，会获得一个SCKEY</a></p><p>发送消息非常简单，只需要向以下URL发一个GET或者POST请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://sc.ftqq.com/SCU167709T5fac1f4393bbd77463bb8b3d13ce33836066159859760.send</span><br></pre></td></tr></table></figure><p>接受两个参数：</p><ul><li>text：消息标题，最长为256，必填。</li><li>desp：消息内容，最长64Kb，可空，支持MarkDown。</li></ul><p>最简单的消息发送方式是通过浏览器，在地址栏输入以下URL，回车后即可发送：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://sc.ftqq.com/SCU167709T5fac1f4393bbd77463bb8b3d13ce33836066159859760.send?text=主人服务器又挂掉啦~</span><br></pre></td></tr></table></figure><hr><p>在PHP中，可以直接用file_get_contents来调用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file_get_contents(&apos;https://sc.ftqq.com/SCU167709T5fac1f4393bbd77463bb8b3d13ce33836066159859760.send?text=&apos;.urlencode(&apos;主人服务器又挂掉啦~&apos;));</span><br></pre></td></tr></table></figure><p>可以把它封装成一个函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">function sc_send(  $text , $desp = &apos;&apos; , $key = &apos;SCU167709T5fac1f4393bbd77463bb8b3d13ce33836066159859760&apos;  )</span><br><span class="line">&#123;</span><br><span class="line">$postdata = http_build_query(</span><br><span class="line">    array(</span><br><span class="line">        &apos;text&apos; =&gt; $text,</span><br><span class="line">        &apos;desp&apos; =&gt; $desp</span><br><span class="line">    )</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">$opts = array(&apos;http&apos; =&gt;</span><br><span class="line">    array(</span><br><span class="line">        &apos;method&apos;  =&gt; &apos;POST&apos;,</span><br><span class="line">        &apos;header&apos;  =&gt; &apos;Content-type: application/x-www-form-urlencoded&apos;,</span><br><span class="line">        &apos;content&apos; =&gt; $postdata</span><br><span class="line">    )</span><br><span class="line">);</span><br><span class="line">$context  = stream_context_create($opts);</span><br><span class="line">return $result = file_get_contents(&apos;https://sc.ftqq.com/&apos;.$key.&apos;.send&apos;, false, $context);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>直接用pip安装dinglingling插件或者使用上文的github链接中的代码根据教程自行安装后即可。<br>直接在需要监测的函数前加上装饰器wx_reminder即可。使用demo代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dinglingling <span class="keyword">import</span> wx_reminder</span><br><span class="line"></span><br><span class="line">SCKEY = <span class="string">""</span> <span class="comment"># the sckey you get from http://sc.ftqq.com/3.version</span></span><br><span class="line">proxy = <span class="string">""</span> <span class="comment"># if you needn't proxy, ignore it.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@wx_reminder(SCKEY=SCKEY, proxy=proxy, remind_started=True)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_correct_func</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"hello world"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@wx_reminder(SCKEY=SCKEY, proxy=proxy)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_error</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">raise</span> Exception(<span class="string">"test error"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    test_correct_func()</span><br><span class="line">    <span class="comment"># test_error()</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210402035305911.png" alt></p><h3 id="邮件"><a href="#邮件" class="headerlink" title="邮件"></a>邮件</h3><ul><li>设置好邮箱的smtp服务，（和outlook客户端收发邮件一个道理）</li><li>把需要发送的信息作为邮件发送出去</li></ul><p>核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import smtplib for the actual sending function</span></span><br><span class="line"><span class="keyword">import</span> smtplib</span><br><span class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</span><br><span class="line"><span class="keyword">from</span> email.header <span class="keyword">import</span> Header</span><br><span class="line"><span class="keyword">from</span> email.mime.image <span class="keyword">import</span> MIMEImage</span><br><span class="line"><span class="keyword">from</span> email.mime.multipart <span class="keyword">import</span> MIMEMultipart</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_email</span><span class="params">(subject=<span class="string">"No subject"</span>, content=<span class="string">"I am boring"</span>)</span>:</span></span><br><span class="line">    mail_host = <span class="string">"smtp.163.com"</span></span><br><span class="line">    mail_user = <span class="string">"yuetan@163.com"</span></span><br><span class="line">    mail_pw = <span class="string">"********"</span>  <span class="comment"># 授权码</span></span><br><span class="line">    sender = <span class="string">"yuetan@163.com"</span></span><br><span class="line">    receiver = <span class="string">"yuetan@lynk.com"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the container (outer) email message.</span></span><br><span class="line">    msg = MIMEText(content, <span class="string">"plain"</span>, <span class="string">"utf-8"</span>)</span><br><span class="line">    msg[<span class="string">'Subject'</span>] = subject</span><br><span class="line">    msg[<span class="string">'From'</span>] = sender</span><br><span class="line">    msg[<span class="string">'To'</span>] = receiver</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        smtp = smtplib.SMTP_SSL(mail_host, <span class="number">994</span>)  <span class="comment"># 实例化smtp服务器</span></span><br><span class="line">        smtp.login(mail_user, mail_pw)  <span class="comment"># 登录</span></span><br><span class="line">        smtp.sendmail(sender, receiver, msg.as_string())</span><br><span class="line">        print(<span class="string">"Email send successfully"</span>)</span><br><span class="line">    <span class="keyword">except</span> smtplib.SMTPException:</span><br><span class="line">        print(<span class="string">"Error: email send failed"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    send_email(subject=<span class="string">"Training finished"</span>, content=<span class="string">"I am boring"</span>)</span><br></pre></td></tr></table></figure><h2 id="邮箱设置"><a href="#邮箱设置" class="headerlink" title="邮箱设置"></a>邮箱设置</h2><p>程序中有个mail_pw是邮箱授权码，可以通过自己的邮箱获取。登录自己常用的邮箱，以163为例。打开设置，将SMPT服务开启：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210402034224166.png" alt></p><p>开启后，通过发送短信得到授权密码。（注意保密，泄漏后就相当于邮箱密码泄漏了）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210402034141089.png" alt></p><p>将授权密码赋值给程序中的mail_pw变量即可。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li>Loss出现NAN时，自动结束训练。可通过assert或if判断，否则继续训练没有意义了</li><li>可配合nohup命令使用，即使断开服务器连接也在后台继续训练</li><li>如果想定时发送邮件，可配合crontab命令</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这是篇关于炼丹期间如何摸鱼的文章～～&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="远程炼丹" scheme="http://yuanquanquan.top/tags/%E8%BF%9C%E7%A8%8B%E7%82%BC%E4%B8%B9/"/>
    
  </entry>
  
  <entry>
    <title>When Non-Convex Modeling meets Hyperspectral Remote Sensing</title>
    <link href="http://yuanquanquan.top/2021/20201108/"/>
    <id>http://yuanquanquan.top/2021/20201108/</id>
    <published>2021-03-28T07:11:48.000Z</published>
    <updated>2021-03-29T07:35:51.015Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>第一次看高光谱相关的paper，大概？由于光谱波段之间的高度相关特性，高光谱图像受到信息冗余的影响，这可能会降低在某些极端条件的情况下对材质的判别能力。此外，高光谱纬度沿着光谱域的逐渐增加，需要大存储功能和高性能计算。因此，对高光谱图像进行降维处理，具有重要的研究和实际意义。</p><p>本文从<strong>无监督、监督和半监督模型</strong>三个方面介绍基于非凸模型的高光谱图像降维方法。</p></blockquote><a id="more"></a>  <p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328150717518.png" alt></p><h2 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1. Abstract"></a><strong>1. Abstract</strong></h2><p>由于光谱波段之间的高度相关特性，高光谱图像受到信息冗余的影响，这可能会降低在某些极端条件的情况下对材质的判别能力。此外，高光谱纬度沿着光谱域的逐渐增加，需要大存储功能和高性能计算。因此，对高光谱图像进行降维处理，具有重要的研究和实际意义。</p><div class="row">    <embed src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding/Hyperspectral/2.pdf" width="100%" height="550" type="application/pdf"></div><p>本文从<strong>无监督、监督和半监督模型</strong>三个方面介绍基于非凸模型的高光谱图像降维方法。</p><h2 id="2-无监督模型"><a href="#2-无监督模型" class="headerlink" title="2. 无监督模型"></a><strong>2. 无监督模型</strong></h2><p><strong>非负矩阵分解（NMF）</strong>是一种常用的无监督学习模型，在NMF模型的基础上，Yan等人提出了图正则化正交NMF（GONMF）的高光谱降维方法，Wen等结合多个特征，对GONMF模型进行了扩展，提高了高光谱图像降维性能。Rasti等人设计了一种正交全变分成分分析（OTVCA）的高光谱特征提取方法。下面是论文中总结的常用正则项，主要有稀疏正则化、图正则化、多图正则化、全变分和低秩图正则化。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328152012870.png" alt></p><p>另外一种无监督降维方法是<strong>图嵌入</strong>，也称之为<strong>流形学习</strong>，这类方法之间的差异主要是图的构建方式。Ma等人将KNN分类器与代表性的流形学习算法进行了融合，包括局部线性嵌入、拉普拉斯特征图等。Huang等人嵌入了稀疏图结构，通过解决L1范数优化问题，实现了高光谱图像降维。He等人在稀疏图结构的基础上，提出了加权稀疏图，Hong等人则提出了一种新的空间-光谱图结构进行高光谱图像降维，称为RLMR。下面是刚才提到的稀疏图、加权稀疏图、空间-光谱图和稀疏低秩图正则项。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328145354512.png" alt></p><h2 id="3-监督模型"><a href="#3-监督模型" class="headerlink" title="3. 监督模型"></a><strong>3. 监督模型</strong></h2><p>无监督模型主要是依赖于嵌入不同的先验信息来降低高光谱图像纬度，而监督模型则是<strong>使用标签信息学习类别分离的低维表示</strong>。监督模型可以分为两大类，如下图所示，第一类是与图嵌入和流形学习相关的判别分析，使用标签信息来构建图结构，得到更具判别性的子空间。研究人员提出了稀疏图判别分析、协同图判别分析、特征空间判别分析和空间-光谱局部判别嵌入等一系列方法。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328144250420.png" alt></p><p>第二类方法是<strong>基于回归的监督降维模型</strong>，Hong等人以拉普拉斯矩阵形式学习低维空间表示，更进一步，Hong等人使用k层线性回归将这个模型扩展为一个深度模型，称之为JPlay，该模型试图通过多层线性建模打开深度网络“黑盒子”。</p><h2 id="4-半监督模型"><a href="#4-半监督模型" class="headerlink" title="4. 半监督模型"></a><strong>4. 半监督模型</strong></h2><p>由于带标签信息的数据获取成本高，联合使用带标签和无标签信息逐渐成为热门的研究方向。半监督学习一种简单的方式是<strong>结合监督和无监督模型</strong>，比如局部判别分析和局部保持投影相结合，以及半监督判别分析（SSDA）等方法，Zhao等人使用标签传播预测的伪标签，进一步提升了SSDA性能。类似的，Wu等人使用Dirichlet过程混合模型生成伪标签，学习了低维高光谱嵌入子空间。</p><p>另外一种方式是在半监督降维任务中模拟人类大脑行为，通过在图结构上自适应学习标签传播过程，提出了迭代多任务学习框架，实现了更高效和更有效的高光谱图像降维。</p><h2 id="5-对比实验分析"><a href="#5-对比实验分析" class="headerlink" title="5. 对比实验分析"></a><strong>5. 对比实验分析</strong></h2><p>下面展示了经过不同降维方法后高光谱图像分类精度。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328144250420.png" alt></p><h2 id="6-挑战"><a href="#6-挑战" class="headerlink" title="6. 挑战"></a><strong>6. 挑战</strong></h2><p>尽管目前已经提出了许多优秀的高光谱图像降维方法，仍然存在诸多的挑战，论文总结了以下三点：</p><p><strong>Optimal Subspace Dimension：最优的子空间维度</strong>，降维后的高光谱图像维度是降维方法最关键的参数，尽管目前已经有一些子空间维度估计算法，仍然需要很多先验信息和人类的干预。</p><p><strong>Effects of Noises：噪声的影响</strong>，高光谱图像中存在复杂的噪声，在降维过程中如何避免不同类型和不同强度噪声的影响，仍然存在很大的挑战。</p><p><strong>Robustness and Generalization：鲁棒性和泛化性</strong>，复杂的噪声类型、有限的训练样本影响了降维方法的鲁棒性和泛化性，下一代降维方法应当考虑更鲁棒和智能模型。</p><p>论文地址：<a href="https://arxiv.org/abs/2103.01449" target="_blank" rel="noopener">https://arxiv.org/abs/2103.01449</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;第一次看高光谱相关的paper，大概？由于光谱波段之间的高度相关特性，高光谱图像受到信息冗余的影响，这可能会降低在某些极端条件的情况下对材质的判别能力。此外，高光谱纬度沿着光谱域的逐渐增加，需要大存储功能和高性能计算。因此，对高光谱图像进行降维处理，具有重要的研究和实际意义。&lt;/p&gt;
&lt;p&gt;本文从&lt;strong&gt;无监督、监督和半监督模型&lt;/strong&gt;三个方面介绍基于非凸模型的高光谱图像降维方法。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Hyperspectral" scheme="http://yuanquanquan.top/tags/Hyperspectral/"/>
    
  </entry>
  
  <entry>
    <title>写给憨憨姐姐的远程炼丹指南</title>
    <link href="http://yuanquanquan.top/2021/202103225/"/>
    <id>http://yuanquanquan.top/2021/202103225/</id>
    <published>2021-03-26T14:14:49.000Z</published>
    <updated>2021-03-27T14:10:44.433Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前写过<a href="https://yuanquanquan.top/2019/2019121111/">jupyter notebook如何远程炼丹</a>，今天给憨憨姐姐写一个远程炼丹指南，介绍一下PyCharm的SFTP功能</p><p>SFTP有两点要注意：</p><ul><li>配置远程连接信息。</li><li>设置本地和远程路径的映射。</li></ul><p>除了SFTP的映射外，PyCharm还支持直接使用远程的解释器，这样就多了一步：</p><ul><li>设置远程解释器。</li></ul></blockquote><a id="more"></a>  <h2 id="1-SFTP配置"><a href="#1-SFTP配置" class="headerlink" title="1. SFTP配置"></a>1. SFTP配置</h2><p>配置过程如下：</p><h3 id="1-打开配置界面"><a href="#1-打开配置界面" class="headerlink" title="1).打开配置界面"></a>1).打开配置界面</h3><ul><li>Tools-&gt;Deployment-&gt;Configuration</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327211520368.png" alt></p><h3 id="2-设置连接信息"><a href="#2-设置连接信息" class="headerlink" title="2).设置连接信息"></a>2).设置连接信息</h3><ul><li>在新建的配置界面中输入SFTP host、Port、Root path、User name、Password等。root path是可以自动检测的，在输入了其他部分后，点击test sftp connection来确认地址和用户信息是否正确，之后点击autodetect就可以自动补全root path。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327212342752.png" alt></p><h3 id="3-设置文件夹映射"><a href="#3-设置文件夹映射" class="headerlink" title="3).设置文件夹映射"></a>3).设置文件夹映射</h3><p>最后选择<code>Mappings</code>，mappings中填写你本地工程的地址和服务器那边的地址，配置好后当你在本地改程序的时候，程序就会同步到服务器上，相当于一个自动的git了~。</p><p>将本地路径与远端路径进行选择，远端路径就市部署到Linux上的路径，即本地文件上传的位置，进行保存，远端配置文件就配置完成了。</p><p>excluded path不用填</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327212450603.png" alt></p><p>到这里，SFTP就设置完毕了，接下来，再讲下如何设置远程解释器</p><h2 id="二、远程解释器配置"><a href="#二、远程解释器配置" class="headerlink" title="二、远程解释器配置"></a><strong>二、远程解释器配置</strong></h2><p>通过Files-&gt;settings路径进入配置界面，然后我们选择项目解释器<strong>Project Interpreter</strong>，选择ssh解释器已经存在的配置</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327214902257.png" alt></p><h2 id="三、将项目代码上传到远端服务器"><a href="#三、将项目代码上传到远端服务器" class="headerlink" title="三、将项目代码上传到远端服务器"></a><strong>三、将项目代码上传到远端服务器</strong></h2><p>最后还得选择上传到远端服务器的位置勾选<code>Automatically upload</code>选项就是在<code>Finish</code>之后将所有项目代码自动上传。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327215815546.png" alt></p><p>上传速度很快，耐心等待后，整个项目就完全部署到远端服务器上了，这时候在本地运行，实际上代码是跑在远端服务器上的。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前写过&lt;a href=&quot;https://yuanquanquan.top/2019/2019121111/&quot;&gt;jupyter notebook如何远程炼丹&lt;/a&gt;，今天给憨憨姐姐写一个远程炼丹指南，介绍一下PyCharm的SFTP功能&lt;/p&gt;
&lt;p&gt;SFTP有两点要注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置远程连接信息。&lt;/li&gt;
&lt;li&gt;设置本地和远程路径的映射。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了SFTP的映射外，PyCharm还支持直接使用远程的解释器，这样就多了一步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设置远程解释器。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="远程炼丹" scheme="http://yuanquanquan.top/tags/%E8%BF%9C%E7%A8%8B%E7%82%BC%E4%B8%B9/"/>
    
  </entry>
  
  <entry>
    <title>《组合优化》部分结果整理-1</title>
    <link href="http://yuanquanquan.top/2021/20210314/"/>
    <id>http://yuanquanquan.top/2021/20210314/</id>
    <published>2021-03-13T17:11:22.000Z</published>
    <updated>2021-03-27T11:09:24.120Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>​    《Combinatorial Optimization Theory and Algorithms(6th,2018)》是一本内容相当广泛而全面的教材，但因其证明的晦涩使人望而却步，本文的目的是整理本学期所学内容涉及的相关结果，仅给出概括性描述，如对相关结果感兴趣，可从此书中找到相关内容及引文。</p></blockquote><span id="more"></span>  <ul><li><p>多项式时间算法：算法运行时间为<br>$$<br>O(n^k)<br>$$</p></li><li><p>强多项式时间算法：此问题的运行时间不因输入的数字大小而变动，而是依照输入数据的结构复杂度</p></li><li><p>给定图的极小连通支撑子图是支撑树，我们的目标是找一个最小（权）的树</p></li><li><p><strong>钻孔问题：</strong>在完全图中寻找一条含有所有顶点的最短路(最短的Hamiltonian path)。</p></li><li><p><strong>最小树形图</strong>（带有方向的树）<strong>问题：</strong>在有向图中找一个带有方向的最小支撑树。</p></li><li><p>求解<strong>最小支撑树问题</strong>的“贪婪算法”：</p></li><li><p><strong>Kruskal算法</strong>(多项式时间算法):O(mn) → O(mlogn) → O(mα(m,n))(几乎是线性的)</p></li></ul><p><em>Idea：</em>从一个空图开始，每次增加原图中权最小的边，同时要避开长出圈。</p><ul><li><strong>Prim算法</strong>(多项式时间算法)：O(n2)→O(m+nlogn)（结合Fibonacci堆）→O(mlogβ(n,m))</li></ul><p><em>Idea:</em> 任选一个顶点作为初始集，每次不断选取初始集同剩余集间的最小权边，最终将一个点的初始集长成V(G)。</p><ul><li><p>(<strong>Cheriton, Tarjan 1976</strong>) 平面图的最小支撑树问题能线性时间被求解。</p></li><li><p>平面上n个点的最小支撑树问题能在O(nlogn)时间内被求解。</p></li><li><p><img src="https://i.loli.net/2021/03/14/G7AdgNIac2x1nlR.png" alt=""></p></li><li><p>(<strong>Karger,Klein,Tarjan 1976</strong>)寻找最小支撑树的一个<strong>随机算法有线性期望</strong>的运行时间。</p></li><li></li><li><p>最小树形图（带有方向的树）问题等价于<strong>最大权分支问题</strong>：</p></li><li><p><strong>Edmonds分支算法</strong> O(mn) →O(m+nlogn)（<strong>Gabow等1986</strong> 用Fibonacci得到）</p></li><li><p>寻求<strong>有向图G中边不交支撑树形图(根节点r)的最大集</strong>的多项式时间算法：目前最有效算法由<strong>Gabow(1995)</strong>给出。</p></li><li><p>处理更困难的组合最优化问题最常见的子问题：<strong>最小支撑树+最短路。</strong></p></li><li><p><strong>非负边权的最短路问题</strong>：</p></li><li><p>如果<strong>所有权值为1</strong>，则最短路问题可直接由BFS求解；</p></li><li><p>对<strong>权值都为正整数</strong>，可将边e替换成长度为c(e)的路，然后再运用BFS(可能增加指数多条边，很笨！)；</p></li><li><p>更为高明的构思是<strong>Dijkstra(1959)算法</strong>(已知最好的强多项式时间算法): O(n2)→ O(m+nlogn)（<strong>Fredman，Tarjan, 1987</strong> 使用Fibonacci堆）；</p></li><li><p>假设权是<strong>固定范围内的整数</strong>（0到C之间的整数），<strong>Dial(1969)</strong>用一个编号为0,…,|V(G)|·C的数组，按照当前的l值去储存顶点，从而得到简单的线性时间算法；</p></li><li><p>对<strong>平面有向图</strong>，<strong>Henzinger等(1997)</strong>给出了线性时间算法；</p></li><li><p>对<strong>非负整数权的无向图</strong>，<strong>Thorup(1999)</strong>找到了线性时间算法。（在无向图中寻找最短路极其困难）</p></li><li><p>一般守恒权（每条边权不需要非负，只要求不存在总权为负的圈(负圈)就好了）</p></li><li><p><strong>Moore-Bellman-Ford算法</strong>(迄今最快的强多项式时间算法)：O(nm)</p></li><li><p>边权为整数且有下界cmin时，<strong>Goldberg(1995)</strong>的缩放尺度法具有运行时间O(√nmlog|cmin|+2)</p></li><li><p>平面图，<strong>Fakcharoenphol和Rao(2006)</strong>阐述了一个O(nlog3n)算法。</p></li><li><p>若G含有负圈，则至今未有多项式时间算法（问题变为NP-hard）</p></li><li><p>对给定一个有向图G，权c:E(G)→R，可在O(nm)时间找到一个可行势，或找到负圈。</p></li></ul><ul><li><p><strong>全点对最短路问题</strong>（找一个有向图的所有顶点有序对(s,t)求出最短的s-t路）：</p></li><li><p>调用n次<strong>Moore-Bellman-Ford算法</strong>，每次选定一个s：O(n2m)→O(mn+n2logn)(<strong>P**</strong>ettie 2004**，目前已知最好的时间界)；</p></li></ul><ul><li><p>对于非负权的稠密图，<strong>Chan(2007)</strong>得到界O(n3log3logn/log2n)；</p></li><li><p>对于<strong>所有边权都是较小的正整数</strong>，<strong>Zwick(2002)</strong>运用快速矩阵乘积可以改进Pettie的时间界。</p></li><li><p><strong>Floyd-Warshall**</strong>算法**：O(n3)</p></li><li><p>在<strong>具有守恒权的无向图G</strong>中，求全部点对之间的最短路问题可以在O(n4)时间内解决。<strong>Gabow(1983)</strong>将运行时间改进到O(min{n3,nmlogn})</p></li></ul><ul><li><p>在一个具有守恒权的有向图中，我们用上述最短路算法容易求出最小总权的圈。</p></li><li><p>如何求平均权为最小的圈？<strong>最小平均圈问题</strong>。</p></li><li><p><strong>Karp(1978)**</strong>最小平均圈算法**:O(nm) 注：对任意边权的无向图，此算法不能用来求最小平均权的圈。</p></li></ul><ul><li><p><strong>最大流问题</strong></p></li><li><p><strong>Ford-Fulkerson(1957**</strong>)最大流算法**：整值容量，指数次增流，存在整值的最大流，所以算法一定在有限步结束。</p></li></ul><p><em>Idea：</em>找可扩路，再沿可扩路增流</p><p>注：如果<strong>允许容量取无理数</strong>，在选择可扩路时运气不好，算法可能不会终止。</p><p><img src="https://i.loli.net/2021/03/14/sIK5dzgX3ZGUfaj.png" alt=""></p><ul><li><strong>Edmonds-Karp(1972)</strong>算法（最大流问题的第一个多项式时间算法）O(m2n)（至多有mn/2次增流，每次增流运用BFS的时间是O(m)）</li></ul><p><em>Idea.</em> 把任选一条可扩路修改为找一条最短的可扩路。</p><ul><li>分层图容易由BFS在O(m)时间构造出来。</li><li><strong>Dinic算法</strong>（多项式时间算法）：至多进行n-1个阶段，每个阶段O(nm)→O(n2)</li></ul><p><strong>Sleator, Tarjan(1983)</strong>用一种称为动态树的数据结构使得Dinic算法成为最大流问题的O(mnlogn)算法</p><ul><li><p><strong>Fujishige(2003)算法</strong>（弱多项式时间算法，简单，缩放尺度技术）：对简单有向图G及整容量可在O(mnlogumax)时间内正确求解最大流问题。</p></li><li><p><strong>Goldberg,Tarjan(1988)**</strong>推流-重标算法<strong>：O(n2√m</strong>)<strong>→O(nmlog(n2/m))（</strong>Goldberg,Tarjan 1988<strong>）→O(nmlog2+m/(nlogn)n)(</strong>King, Rao, Tarjan 1994**)</p></li><li><p><img src="https://i.loli.net/2021/03/14/v9IJQqWtbCTg4dZ.png" alt=""></p></li><li><p><img src="https://i.loli.net/2021/03/14/ghpbx12jq6aF593.png" alt=""></p></li></ul><ul><li><p>一个具有n个顶点及m条边的<strong>最小费用流问题</strong>的实例可以变换为等价的<strong>Hitchcock运输问题</strong>的实例，其中有n+m个顶点及2m条边。</p></li><li><p>求解最小费用流的<strong>最小平均圈消去算法</strong>(强多项式时间算法)：O(m3n2logn)</p></li></ul><p><em>Idea.</em> 首先用最大流算法求出一个b-流，然后逐次沿负权的可扩圈增流，直到没有这样的圈为止，为得到多项式的运行时间，每次选择一个平均权最小的圈是有必要的。</p><ul><li><strong>容量缩放算法</strong>(最小费用流问题的第一个多项式时间算法)：O(n(m+nlogn)logbmax)</li><li><strong>Orlin算法</strong>(已知最快捷的强多项式时间算法)：只能处理无容量限制问题 O(nlogm(m+nlogn))</li><li><p>原始的<strong>网络单纯形法</strong>不是多项式时间算法，但它在实用上是十分有效的。<strong>Orlin(1997)</strong>提出一种变形，可在多项式时间运行。多项式时间的<strong>对偶网路单纯形法</strong>由<strong>Orlin, Plotkin</strong>与<strong>Tardos(1993)</strong>以及<strong>Armstrong与Jim(1997)</strong>得到。</p></li><li><p><strong>Ford与Fulkerson(1958)：</strong>动态最大流（每条边上的流量可以随时间变化，而进入一条边的流要在规定的延迟时间之后才能到达另一端）问题可以用同最小费用流问题一样的运行时间求解。</p></li><li><strong>Hoppe与Tardos(2000)</strong>运用子模函数最小化解决了<strong>最速转运问题</strong>，其中有多源多汇且有整数传输时间。</li><li><strong>Klinz, Woeginger(2004)</strong>证明了求最小费用动态流是NP-hard的。</li></ul><hr><p><strong>附录：一些有价值的结论</strong></p><ul><li><p>关于阶乘的一个不等式</p><p><img src="https://i.loli.net/2021/03/14/niTY3GwRksNFHpv.png" alt=""></p></li><li><p><img src="https://i.loli.net/2021/03/14/fjGAEcoilMu813v.png" alt=""></p></li><li><p><img src="https://i.loli.net/2021/03/14/xcrBGktMIHDUAsa.png" alt=""></p></li><li><p>按照边的权重排边可以使用合并整序算法，时间复杂度O(mlogm)。</p><p><img src="https://i.loli.net/2021/03/14/shqmzIKSuokdPiL.png" alt=""></p></li><li><p>判定一个至多有n条边的图中的圈能在O(n)内完成(用DFS\BFS来完成。</p></li><li><p>满足<br>$$<br>|E(G)|&gt;\binom{|V(G)|-1}{2}<br>$$<br>的任一无向图G是连通的。</p></li></ul><hr><p><em>就像格罗滕迪克所说：<strong>“构成一个研究者创造力和想象力的本质，是他们聆听事情内部声音的能力。</strong>”这里没有等级高下，没有阶层之分，在对未知的探索前人人平等，每个人都拥有绝对的自由。</em></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;​    《Combinatorial Optimization Theory and Algorithms(6th,2018)》是一本内容相当广泛而全面的教材，但因其证明的晦涩使人望而却步，本文的目的是整理本学期所学内容涉及的相关结果，仅给出概括性描述，如对相关结果感兴趣，可从此书中找到相关内容及引文。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="优化理论" scheme="http://yuanquanquan.top/tags/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>Jane Street日内高频交易预测</title>
    <link href="http://yuanquanquan.top/2021/20210205/"/>
    <id>http://yuanquanquan.top/2021/20210205/</id>
    <published>2021-02-05T08:28:52.000Z</published>
    <updated>2021-02-05T09:39:48.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一次kaggle比赛，这次比赛的场景主要是Jane Street日内高频交易，是短线模型，使用夏普率衡量模型效果，但是提供的数据并非原始数据，不清楚feature含义。</p></blockquote><span id="more"></span> <h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><h3 id="1-1-Description"><a href="#1-1-Description" class="headerlink" title="1.1 Description"></a><strong>1.1 Description</strong></h3><p>In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their “fair values” and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.</p><p>Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. </p><h3 id="1-2-Evaluation"><a href="#1-2-Evaluation" class="headerlink" title="1.2 Evaluation"></a><strong>1.2 Evaluation</strong></h3><ul><li><p>Utility score</p><p>Each row in the test set represents a trading opportunity for which <strong>you will be predicting an <code>action</code> value, 1 to make the trade and 0 to pass on it.</strong> Each trade <code>j</code> has an associated <code>weight</code> and <code>resp</code>, which represents a return.</p></li></ul><p>$$<br>p_i = \sum_j(weight_{ij} <em> resp_{ij} </em> action_{ij}),<br>$$</p><p>$$<br>t = \frac{\sum p_i }{\sqrt{\sum p_i^2}} * \sqrt{\frac{250}{|i|}},<br>$$<br>                where |i| is the number of unique dates in the test set. The utility is                 then defined as:<br>$$<br>u = min(max(t,0), 6)  \sum p_i.<br>$$</p><blockquote><p> <a href="https://www.kaggle.com/renataghisloti/understanding-the-utility-score-function">https://www.kaggle.com/renataghisloti/understanding-the-utility-score-function</a></p></blockquote><ul><li><p>_Pi_ </p><p>Each row or trading opportunity can be chosen (action == 1) or not (action == 0). </p><p>The variable _pi_ is a indicator for each day _i_, showing how much return we got for that day.</p><p>Since we want to maximize u, we also want to maximize _pi_. To do that, we have to select the least amount of negative <em>resp</em> values as possible (since this is the only negative value in my equation and only value that would make the total sum of p going down) and maximize the positive number of positive <em>resp</em> transactions we select.</p></li><li><p><em><code>t</code></em> </p><p><strong>_t_</strong> is <strong>larger</strong> when the return for <strong>each day is better distributed and has lower variation.</strong> It is better to have returns uniformly divided among days than have all of your returns concentrated in just one day. It reminds me a little of a <strong>_L1_</strong> over <strong>_L2_</strong> situation, where the <strong>_L2_</strong> norm penalizes outliers more than <strong>_L1_</strong>.</p></li></ul><p>  Basically, we want to select uniformly distributed distributed returns over days, maximizing our return but giving a penalty on choosing too many dates.</p><ul><li>t is simply the annualized sharpe ratio assuming that there are 250 trading days in a year, an important risk adjusted performance measure in investing. If sharpe ratio is negative, utility is zero. A sharpe ratio higher than 6 is very unlikely, so it is capped at 6. The utility function overall try to maximize the product of sharpe ratio and total return.</li></ul><hr><h2 id="2-Implementing"><a href="#2-Implementing" class="headerlink" title="2. Implementing."></a>2. Implementing.</h2><blockquote><p><a href="https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning">https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning</a></p><p><a href="https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners">https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners</a></p><p><a href="https://www.kaggle.com/smilewithme/jane-street-eda-of-day-0-and-feature-importance/edit">https://www.kaggle.com/smilewithme/jane-street-eda-of-day-0-and-feature-importance/edit</a></p></blockquote><h3 id="2-0-Preprocessing"><a href="#2-0-Preprocessing" class="headerlink" title="2.0 Preprocessing"></a>2.0 Preprocessing</h3><h3 id="2-1-EDA"><a href="#2-1-EDA" class="headerlink" title="2.1. EDA"></a>2.1. EDA</h3><p><strong>Market Basics:</strong> Financial market is a dynamic world where investors, speculators, traders, hedgers understand the market by different strategies and use the opportunities to make profit. They may use fundamental, technical analysis, sentimental analysis,etc. to place their bet. As data is growing, many professionals use data to understand and analyze previous trends and predict the future prices to book profit.</p><p><strong>Competition Description:</strong> The dataset provided contains set of features, <strong>feature_{0…129}</strong>,representing real stock market data. Each row in the dataset represents a trading opportunity, for which we will be predicting an action value: <strong>1</strong> to make the trade and <strong>0</strong> to pass on it. </p><p>Each trade has an associated weight and resp, which together represents a return on the trade. In the training set, <strong>train.csv</strong>, you are provided a <strong>resp</strong> value, as well as several other <strong>resp_{1,2,3,4}</strong> values that represent returns over different time horizons.</p><p>In <strong>Test set</strong> we don’t have <strong>resp</strong> value, and other <strong>resp_{1,2,3,4}</strong> data, so we have to use only <strong>feature_{0…129}</strong> to make prediction.</p><p>Trades with <strong>weight = 0</strong> were intentionally included in the dataset for completeness, although such trades <strong>will not</strong> contribute towards the scoring evaluation. So we will ignore it.</p><h3 id="2-2-Using-XGBoost-Algorithm"><a href="#2-2-Using-XGBoost-Algorithm" class="headerlink" title="2.2 Using XGBoost Algorithm"></a>2.2 Using XGBoost Algorithm</h3><h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost?"></a>XGBoost?</h1><p>it is contents of ensemble learning.</p><blockquote><p><a href="https://www.youtube.com/watch?v=VHky3d_qZ_E">https://www.youtube.com/watch?v=VHky3d_qZ_E</a></p><p><a href="https://www.youtube.com/watch?v=1OEeguDBsLU&amp;list=PL23__qkpFtnPHVbPnOm-9Br6119NMkEDE&amp;index=4">https://www.youtube.com/watch?v=1OEeguDBsLU&amp;list=PL23__qkpFtnPHVbPnOm-9Br6119NMkEDE&amp;index=4</a></p><p><a href="https://www.youtube.com/watch?v=4Jz4_IOgS4c">https://www.youtube.com/watch?v=4Jz4_IOgS4c</a></p><p><a href="https://www.youtube.com/watch?v=VkaZXGknN3g&amp;feature=youtu.be">https://www.youtube.com/watch?v=VkaZXGknN3g&amp;feature=youtu.be</a></p><p><a href="https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost">https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost</a></p></blockquote><h2 id="2-2-1-Ensemble-learning"><a href="#2-2-1-Ensemble-learning" class="headerlink" title="2.2.1 Ensemble learning"></a>2.2.1 Ensemble learning</h2><p>x - dataset, y - error rate, each color is algorithm.</p><ul><li>No Free Lunch Theorem.<ul><li>any classification method cannot be superior or inferior overall</li></ul></li></ul><p>Ensemble means harmony or unity.</p><p>When we predict the value of a data, we use one model. But if we learn several models in harmony and use their predictions, we’ll get a more accurate estimate.</p><p>Ensemble learning is a machine learning technique that combines multiple decision trees to perform better than a single decision tree. The key to ensemble learning is to combine several weak classifiers to create a Strong Classifier. This improves the accuracy of the model.</p><h3 id="2-2-1-1-Bagging"><a href="#2-2-1-1-Bagging" class="headerlink" title="2.2.1.1 Bagging"></a>2.2.1.1 Bagging</h3><p>Bagging is Bootstrap Aggregation. Bagging is a method of aggregating results by taking samples multiple times (Bootstrap) each model.</p><p><img src="https://i.loli.net/2021/02/05/m4nEZyg3qUCu1Da.png" alt="bagging"></p><p>First, bootstrap from the data. (Restore random sampling) Examine the bootstrap data to learn the model. It aggregates the results of the learned model to obtain the final result value.</p><p>Categorical data aggregates results in Voting, and Continuous data is averaged.</p><p>When it’s categorical data, voting means that the highest number of values predicted by the overall model is chosen as the final prediction. Let’s say there are six crystal tree models. If you predicted four as A, and two as B, four models will predict A as the final result by voting.</p><p>Aggregating by means literally means that each decision tree model averages the predicted values to determine the predicted values of the final bagging model.</p><p>Bagging is a simple yet powerful method. Random Forest is representative model of using bagging.</p><h3 id="2-2-1-2-Boosting"><a href="#2-2-1-2-Boosting" class="headerlink" title="2.2.1.2 Boosting"></a>2.2.1.2 Boosting</h3><p>Boosting is a method of making weak classifiers into strong classifiers using weights. The Bagging predicts results independently of the Deicison Tree1 and Decision Tree2. This is how multiple independent decision trees predict the values, and then aggregate the resulting values to predict the final outcome. </p><p>Boosting, however, takes place between models. When the first model predicts, the data is weighted according to its prediction results, and the weights given affect the next model. Repeat the steps for creating new classification rules by focusing on misclassified data.</p><p><img src="https://blog.kakaocdn.net/dn/kCejr/btqyghvqEZB/9o3rKTEsuSIDHEfelYFJlk/img.png" alt=""></p><h2 id="2-2-2-Different-of-Bagging-and-Boosting"><a href="#2-2-2-Different-of-Bagging-and-Boosting" class="headerlink" title="2.2.2 Different of Bagging and Boosting"></a>2.2.2 Different of Bagging and Boosting</h2><p><img src="https://i.loli.net/2021/02/05/slDmWHtPY3quxfp.png" alt=""></p><p>Bagging is learned in parallel, while boosting is learned sequentially. After learning once, weights are given according to the results. Such weights affect the prediction of the results of the following models.</p><p>High weights are given for incorrect answers and low weights for correct answers. This allows you to focus more on the wrong answers in order to get them right.</p><p>Boosting has fewer errors compared to bagging. That is, performance is good performance. However, the speed is slow and there is a possibility of over fitting. So, which one should you choose between bagging or boosting when you actually use it? It depends on the situation. If the low performance of the individual decision tree is a problem, boosting is a good idea, or over-fitting problem bagging is a good idea.</p><h1 id="2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting"><a href="#2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting" class="headerlink" title="2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)"></a>2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)</h1><p><a href="https://xgboost.readthedocs.io/en/latest/">https://xgboost.readthedocs.io/en/latest/</a></p><p><a href="https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d">https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d</a></p><p>Optimized Gradient Boosting algorithm through parallel processing, tree-pruning, handling missing values and regularization to avoid over-fitting/bias</p><p><strong>XGBoost = GBM + Regularization</strong></p><p>Red part is Regularization term.</p><p>T : weak learner(node)</p><p>w : node score</p><p>Therefore, it can be seen that the regularization term of XGboost prevents overfitting by giving penalty to loss as the tree complexity increases.</p><h2 id="2-2-1-Split-finding-Algorithm"><a href="#2-2-1-Split-finding-Algorithm" class="headerlink" title="2.2.1 Split finding Algorithm"></a>2.2.1 Split finding Algorithm</h2><ul><li><p>Basing exact greedy algorithm</p><ul><li>Pros: <strong>Always find the optimal split</strong> point because it enumerates over <em>all possible</em> splitting points greedily</li><li>Cons: <ul><li>Impossible to efficiently do so when the data does not fit entirely into memory</li><li>Cannot be done under a distributed setting</li></ul></li></ul></li><li><p>Approximate algorithm</p><ul><li>Example<ul><li>Assume that the value is sorted in an ascending order</li><li>Divide the dataset into 10 buckets</li><li>Global variant(Per tree) vs Local variant(per split)</li></ul></li></ul></li></ul><h2 id="2-2-2-Sparsity-Aware-Split-Finding"><a href="#2-2-2-Sparsity-Aware-Split-Finding" class="headerlink" title="2.2.2 Sparsity-Aware Split Finding"></a>2.2.2 Sparsity-Aware Split Finding</h2><ul><li>In many real-world Problems, it is quite common for the input x to be sparse<ul><li>presence of missing values in the data</li><li>frequent zero entries in the statistics</li><li>artifacts of feature engineering such as one-hot encoding</li></ul></li><li>Solution : Set the default direction that is learned from the data</li></ul><h2 id="2-2-3-System-Design-for-Efficient-Computing"><a href="#2-2-3-System-Design-for-Efficient-Computing" class="headerlink" title="2.2.3 System Design for Efficient Computing"></a>2.2.3 System Design for Efficient Computing</h2><ul><li>The most time-consuming part of tree learning<ul><li>to get the data into sorted order</li></ul></li><li>XGBoost propose to store the data in in-memory units called block<ul><li>Data in each block is stored in the compressed column(CSC) format, with each column sorted by the corresponding feature value</li><li>This input data layout only needs to be computed once before training and can be reused in later iterations.</li></ul></li><li>Cache-aware access<ul><li>For the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm</li><li>For approximate algorithms, we solve the problem by choosing a correct block size</li></ul></li><li>Out-of-core computing<ul><li>Besides processors and memory, it is important to utilize disk space to handle data that does not fit into main memory</li><li>To enable out-of-core computation, the data is divided into multiple blocks and store each block on disk</li><li>To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk</li><li>It is important to reduce the overhead and increase the throughout of disk IO</li></ul></li><li>Block Compression<ul><li>The block is compressed by columns and decompressed on the fly by an independent thread when loading into main memory</li><li>This helps to trade some of the computation in decompression with the disk reading cost</li></ul></li><li>Block Sharding<ul><li>A pre-fetcher thread is assigned to each disk and fetches the data into an in-memory buffer</li><li>The training thread then alternatively reads the data from each buffer.</li><li>This helps to increase the throughput of disk reading when multiple disks are available.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&#x27;/kaggle/input/jane-street-market-prediction/train.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_memory_usage</span>(<span class="params">df</span>):</span></span><br><span class="line">    </span><br><span class="line">    start_memory = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Memory usage of dataframe is <span class="subst">&#123;start_memory&#125;</span> MB&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtype</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="string">&#x27;object&#x27;</span>:</span><br><span class="line">            c_min = df[col].<span class="built_in">min</span>()</span><br><span class="line">            c_max = df[col].<span class="built_in">max</span>()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">str</span>(col_type)[:<span class="number">3</span>] == <span class="string">&#x27;int&#x27;</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).<span class="built_in">min</span> <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).<span class="built_in">max</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    end_memory = df.memory_usage().<span class="built_in">sum</span>() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Memory usage of dataframe after reduction <span class="subst">&#123;end_memory&#125;</span> MB&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Reduced by <span class="subst">&#123;<span class="number">100</span> * (start_memory - end_memory) / start_memory&#125;</span> % &quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><h3 id="Importing-dataset"><a href="#Importing-dataset" class="headerlink" title="Importing dataset"></a>Importing dataset</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = reduce_memory_usage(train)</span><br></pre></td></tr></table></figure><h2 id="1-import-library"><a href="#1-import-library" class="headerlink" title="1) import library"></a>1) import library</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># This Python 3 environment comes with many helpful analytics libraries installed</span><br><span class="line"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span><br><span class="line"># For example, here&#x27;s several helpful packages to load</span><br><span class="line"></span><br><span class="line">#import numpy as np # linear algebra</span><br><span class="line">#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import sklearn</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">import xgboost as xgb</span><br><span class="line">import optuna</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"># Input data files are available in the read-only &quot;../input/&quot; directory</span><br><span class="line"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">for dirname, _, filenames in os.walk(&#x27;/kaggle/input&#x27;):</span><br><span class="line">    for filename in filenames:</span><br><span class="line">        print(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span><br><span class="line"># You can also write temporary files to /kaggle/temp/, but they won&#x27;t be saved outside of the current session</span><br></pre></td></tr></table></figure><h2 id="2-load-and-clean-dataset"><a href="#2-load-and-clean-dataset" class="headerlink" title="2) load and clean dataset"></a>2) load and clean dataset</h2><h3 id="Cleaning-data"><a href="#Cleaning-data" class="headerlink" title="Cleaning data"></a>Cleaning data</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># This Python 3 environment comes with many helpful analytics libraries installed</span><br><span class="line"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span><br><span class="line"># For example, here&#x27;s several helpful packages to load</span><br><span class="line"></span><br><span class="line">#import numpy as np # linear algebra</span><br><span class="line">#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import sklearn</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">import xgboost as xgb</span><br><span class="line">import optuna</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"># Input data files are available in the read-only &quot;../input/&quot; directory</span><br><span class="line"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">for dirname, _, filenames in os.walk(&#x27;/kaggle/input&#x27;):</span><br><span class="line">    for filename in filenames:</span><br><span class="line">        print(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span><br><span class="line"># You can also write temporary files to /kaggle/temp/, but they won&#x27;t be saved outside of the current session</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;记录一次kaggle比赛，这次比赛的场景主要是Jane Street日内高频交易，是短线模型，使用夏普率衡量模型效果，但是提供的数据并非原始数据，不清楚feature含义。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="kaggle" scheme="http://yuanquanquan.top/tags/kaggle/"/>
    
      <category term="量化" scheme="http://yuanquanquan.top/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>ORB_SLAM2的Rviz可视化</title>
    <link href="http://yuanquanquan.top/2021/20210109/"/>
    <id>http://yuanquanquan.top/2021/20210109/</id>
    <published>2021-01-09T08:02:16.000Z</published>
    <updated>2021-01-09T08:14:27.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近我在考虑能不能使用RVIZ可视化ORB_SLAM2,突然发现一位前辈的分享,为防止以后大佬删除了,我fork到了自己的github,这里做个笔记,记录一下.</p><p>这篇博客主要分析一下相关代码</p></blockquote><a id="more"></a>  <h2 id="可视化的几个要点"><a href="#可视化的几个要点" class="headerlink" title="可视化的几个要点"></a><strong>可视化的几个要点</strong></h2><p><code>ORB-SLAM</code>中关于<code>Rviz</code>的可视化：</p><ul><li>ORB-SLAM的Rviz可视化使用单独的一个类来完成可视化信息的发布：<code>MapPublisher</code>类.</li><li>所有的可视化信息都是Rviz的Mark类型,根据发布的地图点、关键帧、<code>Covisibility Graph、Spanning Tree</code>和相机轨迹，使用了不同的Mark类型.</li><li>所有的可视化信息,包括地图、轨迹等都是从<code>ORB-SLAM</code>中的<code>Map</code>类中获取的.</li><li>每次获得一帧图像,进行<code>Track</code>后，利用<code>MapPublisher</code>类发布可视化信息.</li><li>在配置相应的<code>Rviz</code>,使其可以接收可视化信息.</li></ul><p>明白了这几点之后，在ORB-SLAM2中添加Rviz可视化模块就很简单了，主要对源代码做以下改动：</p><ul><li>添加<code>MapPublisher</code>类和配置<code>Rviz</code>,可以直接复用<code>ORB-SLAM</code>中的<code>MapPublisher</code>类和<code>Rviz</code>文件,并在每次<code>Track</code>之后(执行完<code>mpSLAM-&gt;TrackStereo()</code>), 利用<code>MapPublisher</code>类发布可视化信息.</li><li>为Map类添加返回相关信息的接口.</li></ul><p>特别要注意:</p><p>ORB-SLAM2的坐标系下，<code>z轴</code>是<strong>朝前</strong>的，而<code>Rviz</code>的坐标系下，<code>z轴</code>是<strong>朝上</strong>的，因此要做相应的转换.</p><h2 id="ROS包建立"><a href="#ROS包建立" class="headerlink" title="ROS包建立"></a><strong>ROS包建立</strong></h2><h3 id="创建相关软件包"><a href="#创建相关软件包" class="headerlink" title="创建相关软件包"></a>创建相关软件包</h3><p>在<code>catkin_ws/src</code>目录下新建软件包并编译：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">catkin_create_pkg my_image_transport image_transport cv_bridge</span><br><span class="line">cd ..</span><br><span class="line">catkin_make -DCATKIN_WHITELIST_PACKAGES=&quot;my_image_transport&quot;</span><br><span class="line">source devel/setup.bash</span><br></pre></td></tr></table></figure><h3 id="创建图像发布者程序"><a href="#创建图像发布者程序" class="headerlink" title="创建图像发布者程序"></a>创建图像发布者程序</h3><p>新建<code>my_image_transport/src/my_publisher.cpp</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;ros/ros.h&gt;</span><br><span class="line">#include &lt;image_transport/image_transport.h&gt;</span><br><span class="line">#include &lt;opencv2/highgui/highgui.hpp&gt;</span><br><span class="line">#include &lt;cv_bridge/cv_bridge.h&gt;</span><br><span class="line"></span><br><span class="line">int main(int argc, char** argv)</span><br><span class="line">&#123;</span><br><span class="line">  ros::init(argc, argv, &quot;image_publisher&quot;);</span><br><span class="line">  ros::NodeHandle nh;</span><br><span class="line">  image_transport::ImageTransport it(nh);</span><br><span class="line">  image_transport::Publisher pub = it.advertise(&quot;camera/image&quot;, 1);</span><br><span class="line">  cv::Mat image = cv::imread(argv[1], CV_LOAD_IMAGE_COLOR);</span><br><span class="line">  cv::waitKey(30);//不断刷新图像，频率时间为delay，单位为ms</span><br><span class="line">  sensor_msgs::ImagePtr msg = cv_bridge::CvImage(std_msgs::Header(), &quot;bgr8&quot;, image).toImageMsg();</span><br><span class="line"></span><br><span class="line">  ros::Rate loop_rate(5);</span><br><span class="line">  while (nh.ok()) &#123;</span><br><span class="line">    pub.publish(msg);</span><br><span class="line">    ros::spinOnce();</span><br><span class="line">    loop_rate.sleep();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码解释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">line 1-4：</span><br><span class="line">ros.h头文件是所有的ros节点中必须要包含的，下面三个分别是实现图像的发布和订阅，调用opencv库，完成opencv图像格式转化为ROS图像格式所要用到的头文件；</span><br><span class="line"></span><br><span class="line">line 11：</span><br><span class="line">告知结点管理器要在camera/image话题发布图像消息，参数1是话题名称，话题2是缓冲区大小（即消息队列的长度，在发布图像消息时消息队列的长度只能是1）；</span><br><span class="line"></span><br><span class="line">line 12：</span><br><span class="line">根据运行时给定的参数（图像文件的路径）读取图像；</span><br><span class="line"></span><br><span class="line">line 14：</span><br><span class="line">将opencv格式的图像转化为ROS所支持的消息类型，从而发布到相应的话题上；</span><br><span class="line"></span><br><span class="line">line 16-21：</span><br><span class="line">发布图片消息，使消息类型匹配的节点订阅该消息。</span><br></pre></td></tr></table></figure><h3 id="创建图像订阅者程序"><a href="#创建图像订阅者程序" class="headerlink" title="创建图像订阅者程序"></a>创建图像订阅者程序</h3><p>新建<code>my_image_transport/src/my_subscriber.cpp</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;ros/ros.h&gt;</span><br><span class="line">#include &lt;image_transport/image_transport.h&gt;</span><br><span class="line">#include &lt;opencv2/highgui/highgui.hpp&gt;</span><br><span class="line">#include &lt;cv_bridge/cv_bridge.h&gt;</span><br><span class="line"></span><br><span class="line">void imageCallback(const sensor_msgs::ImageConstPtr&amp; msg)</span><br><span class="line">&#123;</span><br><span class="line">  try</span><br><span class="line">  &#123;</span><br><span class="line">    cv::imshow(&quot;view&quot;, cv_bridge::toCvShare(msg, &quot;bgr8&quot;)-&gt;image);</span><br><span class="line">    cv::waitKey(30);</span><br><span class="line">  &#125;</span><br><span class="line">  catch (cv_bridge::Exception&amp; e)</span><br><span class="line">  &#123;</span><br><span class="line">    ROS_ERROR(&quot;Could not convert from &apos;%s&apos; to &apos;bgr8&apos;.&quot;, msg-&gt;encoding.c_str());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">  ros::init(argc, argv, &quot;image_listener&quot;);</span><br><span class="line">  ros::NodeHandle nh;</span><br><span class="line">  cv::namedWindow(&quot;view&quot;);</span><br><span class="line">  cv::startWindowThread();</span><br><span class="line">  image_transport::ImageTransport it(nh);</span><br><span class="line">  image_transport::Subscriber sub = it.subscribe(&quot;camera/image&quot;, 1, imageCallback);</span><br><span class="line">  ros::spin();</span><br><span class="line">  cv::destroyWindow(&quot;view&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码解释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">line 6：回调函数，当有新的图像消息到达camera/image时，该函数就会被调用；</span><br><span class="line">line 10：显示捕捉到的图像，其中cv_bridge::toCvShare(msg, &quot;bgr8&quot;)-&gt;image用于将ROS图像消息转化为Opencv支持的图像格式（采用BGR8编码方式）。这部分用法恰好与上一节中发布者节点中的CvImage(std_msgs::Header(), &quot;bgr8&quot;, image).toImageMsg(); 的作用相反</span><br><span class="line">line 11：刷新图像的频率，实践过程中发现如果注释这一行图像将无法在窗口的显示</span><br></pre></td></tr></table></figure><h3 id="相关配置文件"><a href="#相关配置文件" class="headerlink" title="相关配置文件"></a>相关配置文件</h3><p><code>CMakeLists.txt</code>内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8.3)</span><br><span class="line">project(my_image_transport)</span><br><span class="line"></span><br><span class="line">## Compile as C++11, supported in ROS Kinetic and newer</span><br><span class="line">add_compile_options(-std=c++11)</span><br><span class="line"></span><br><span class="line">find_package(catkin REQUIRED COMPONENTS</span><br><span class="line">  cv_bridge</span><br><span class="line">  image_transport</span><br><span class="line">)</span><br><span class="line">find_package(OpenCV REQUIRED)</span><br><span class="line"></span><br><span class="line">set(LIBS</span><br><span class="line"> $&#123;OpenCV_LIBS&#125; </span><br><span class="line"> $&#123;catkin_LIBRARIES&#125;)</span><br><span class="line"> </span><br><span class="line">catkin_package(</span><br><span class="line">#  INCLUDE_DIRS include</span><br><span class="line">#  LIBRARIES my_image_transport</span><br><span class="line">#  CATKIN_DEPENDS cv_bridge image_transport</span><br><span class="line">#  DEPENDS system_lib</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">include_directories(</span><br><span class="line"># include</span><br><span class="line">  $&#123;catkin_INCLUDE_DIRS&#125;</span><br><span class="line">  $&#123;OpenCV_INCLUDE_DIRS&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">add_executable(my_publisher src/my_publisher.cpp)</span><br><span class="line">target_link_libraries(my_publisher $&#123;LIBS&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(my_subscriber src/my_subscriber.cpp)</span><br><span class="line">target_link_libraries(my_subscriber $&#123;LIBS&#125;)</span><br></pre></td></tr></table></figure><p><code>package.xml</code>文件中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;build_depend&gt;opencv2&lt;/build_depend&gt;</span><br><span class="line">&lt;exec_depend&gt;opencv2&lt;/exec_depend&gt;</span><br></pre></td></tr></table></figure><h3 id="编译软件包"><a href="#编译软件包" class="headerlink" title="编译软件包"></a>编译软件包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/catkin_ws</span><br><span class="line">catkin_make -DCATKIN_WHITHELIST_PACKAGES=&quot;my_image_transport&quot;</span><br></pre></td></tr></table></figure><h3 id="运行节点"><a href="#运行节点" class="headerlink" title="运行节点"></a>运行节点</h3><p>单独开启一个终端执行<code>roscore</code>,启动ros节点管理器。</p><p>开启另一个终端，启动发布者节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosrun my_image_transport my_publisher /home/xxx/catkin_ws/src/my_image_transport/000.png</span><br></pre></td></tr></table></figure><p>运行订阅者节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosrun my_image_transport my_subscriber</span><br></pre></td></tr></table></figure><h2 id="ORB-SLAM2-ROS模块结点的编译"><a href="#ORB-SLAM2-ROS模块结点的编译" class="headerlink" title="ORB_SLAM2 ROS模块结点的编译"></a><strong>ORB_SLAM2 ROS模块结点的编译</strong></h2><p>在环境变量<code>ROS_PACKAGE_PATH</code>中添加<code>Examples/ROS/ORB_SLAM2</code>的路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;source ~/slam/ORB_SLAM2/Examples/ROS/ORB_SLAM2/build/devel/setup.sh&quot; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure><p>在~/slam/ORB_SLAM2目录下执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x build_ros.sh</span><br><span class="line">./build_ros.sh</span><br></pre></td></tr></table></figure><p>等待编译成功.</p><h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a><strong>运行结果</strong></h2><p>执行命令</p><p><img src="https://i.loli.net/2021/01/09/pwP2GjXoCuaWFfL.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roslaunch my_image_transport stereo_image_transport.launch</span><br></pre></td></tr></table></figure><h3 id="使用PCL库显示地图点"><a href="#使用PCL库显示地图点" class="headerlink" title="使用PCL库显示地图点"></a>使用PCL库显示地图点</h3><p>注意：<code>ORB_SLAM2</code>系统<code>Map</code>类中的地图点是世界坐标系中的所有地图点，而不是每个关键</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ROSpointcloud_to_laserscan包pointcloud_to_laserscan_node`节点将点云`PointCloud`转成`2D激光扫描</span><br></pre></td></tr></table></figure><p>订阅节点：<code>cloud_in(sensor_msgs/PointCloud2)</code></p><p>发布节点：<code>scan(sensor_msg/LaserScan)</code></p><p>配置好ROS程序包，在<code>MapPulisher</code>类中稍作修改，参照如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;ros/ros.h&gt;</span><br><span class="line">#include&lt;pcl/point_cloud.h&gt;</span><br><span class="line">#include&lt;pcl_conversions/pcl_conversions.h&gt;</span><br><span class="line">#include&lt;sensor_msgs/PointCloud2.h&gt;</span><br><span class="line"></span><br><span class="line">main (int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">  ros::init (argc, argv, &quot;pcl_create&quot;);</span><br><span class="line">  ros::NodeHandle nh;</span><br><span class="line">  ros::Publisher pcl_pub = nh.advertise&lt;sensor_msgs::PointCloud2&gt; (&quot;pcl_output&quot;, 1);</span><br><span class="line">  pcl::PointCloud&lt;pcl::PointXYZ&gt; cloud;</span><br><span class="line">  sensor_msgs::PointCloud2 output;</span><br><span class="line">  // Fill in the cloud data</span><br><span class="line">  cloud.width = 100;</span><br><span class="line">  cloud.height = 1;</span><br><span class="line">  cloud.points.resize(cloud.width * cloud.height);</span><br><span class="line">  for (size_t i = 0; i &lt; cloud.points.size(); ++i)</span><br><span class="line">  &#123;</span><br><span class="line">    cloud.points[i].x = 1024 * rand () / (RAND_MAX + 1.0f);</span><br><span class="line">    cloud.points[i].y = 1024 * rand () / (RAND_MAX + 1.0f);</span><br><span class="line">    cloud.points[i].z = 1024 * rand () / (RAND_MAX + 1.0f);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  //Convert the cloud to ROS message</span><br><span class="line">  pcl::toROSMsg(cloud, output);</span><br><span class="line">  output.header.frame_id = &quot;odom&quot;;//this has been done in order to be able to visualize our PointCloud2 message on the RViz visualizer</span><br><span class="line">  ros::Rate loop_rate(1);</span><br><span class="line">  while (ros::ok())</span><br><span class="line">  &#123;</span><br><span class="line">    pcl_pub.publish(output);</span><br><span class="line">    ros::spinOnce();</span><br><span class="line">    loop_rate.sleep();</span><br><span class="line">  &#125;</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>效果展示:</p><p><img src="https://i.loli.net/2021/01/09/TEtB9WROelcZk3z.png" alt></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] 2D Grid Mapping and Navigation with ORB SLAM. Abhineet Kumar Singh, Ali Jahani Amiri.</p><p>[2]  <a href="http://ttshun.com/2018/10/14/ORB-SLAM2%E7%B3%BB%E7%BB%9FRviz%E5%8F%AF%E8%A7%86%E5%8C%96%E6%96%B9%E6%A1%88/" target="_blank" rel="noopener">系统Rviz可视化方案</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近我在考虑能不能使用RVIZ可视化ORB_SLAM2,突然发现一位前辈的分享,为防止以后大佬删除了,我fork到了自己的github,这里做个笔记,记录一下.&lt;/p&gt;
&lt;p&gt;这篇博客主要分析一下相关代码&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="slam" scheme="http://yuanquanquan.top/tags/slam/"/>
    
  </entry>
  
  <entry>
    <title>Autoware</title>
    <link href="http://yuanquanquan.top/2020/20201221/"/>
    <id>http://yuanquanquan.top/2020/20201221/</id>
    <published>2020-12-21T08:17:06.000Z</published>
    <updated>2020-12-21T08:18:39.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Autoware最早是由名古屋大学研究小组在加藤伸平教授的领导下于2015年8月正式发布，2015年12月下旬，加藤伸平教授创立了Tier IV，以维护Autoware并将其应用于真正的自动驾驶汽车。</p><p>随着时间流逝，Autoware已成为公认的开源项目，Autoware也是世界上第一个用于自动驾驶“多合一”开源软件，在业内使用也较为广泛。</p></blockquote><a id="more"></a>  <h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>首先看看官网的匹配关系图，由于是在Ubuntu上运行的，官方给出了Autoware与UBuntu的对应关系</p><table><thead><tr><th style="text-align:center">Autoware version</th><th style="text-align:center">Ubuntu 14.04</th><th style="text-align:center">Ubuntu 16.04</th><th style="text-align:center">Ubuntu 18.04</th></tr></thead><tbody><tr><td style="text-align:center">v1.140</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">x</td></tr><tr><td style="text-align:center">v1.130</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">x</td></tr><tr><td style="text-align:center">v1.120</td><td style="text-align:center"></td><td style="text-align:center">x</td><td style="text-align:center">x</td></tr><tr><td style="text-align:center">v1.11.1</td><td style="text-align:center"></td><td style="text-align:center">x</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">v1.11.0</td><td style="text-align:center"></td><td style="text-align:center">x</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">v1.10.0</td><td style="text-align:center"></td><td style="text-align:center">x</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">v1.9.1</td><td style="text-align:center">x</td><td style="text-align:center">x</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">v1.9.0</td><td style="text-align:center">x</td><td style="text-align:center">x</td></tr></tbody></table><p>注：图标中的x号意思是支持，且还需要提前安装好ROS</p><h3 id="系统依赖"><a href="#系统依赖" class="headerlink" title="系统依赖"></a>系统依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install -y python-catkin-pkg python-rosdep ros-$ROS_DISTRO-catkin</span><br><span class="line">$ sudo apt install -y python3-pip python3-colon-common-extensions python3-setuptools python3-vcstool</span><br><span class="line">$ pip3 install -U setuptools</span><br></pre></td></tr></table></figure><p>如果需要GPU支持，那就需要安装CUDA10。CUDA10.0以上不支持。</p><h3 id="安装Eigen3-3-7"><a href="#安装Eigen3-3-7" class="headerlink" title="安装Eigen3.3.7"></a>安装Eigen3.3.7</h3><p>Ubuntu自带的Eigen要卸掉，重新安装此版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cd &amp;&amp; wget http://bitbucket.org/eigen/eigen/get/3.3.7.tar.gz</span><br><span class="line">$ mkdir eigen &amp;&amp; tar build &amp;&amp; tar --strip-compoents=1 -xzvf 3.37.tar.gz -C eigen #Decompress</span><br><span class="line">$ cd eigen &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make &amp;&amp; make install #Build and install</span><br><span class="line">$ cd &amp;&amp; rm -rf 3.37.tar.gz &amp;&amp; rm -rf eigen</span><br></pre></td></tr></table></figure><h3 id="安装Qt5"><a href="#安装Qt5" class="headerlink" title="安装Qt5"></a>安装Qt5</h3><table><thead><tr><th style="text-align:center">product</th><th style="text-align:center">ubuntu 14.04</th><th style="text-align:center">ubuntu 1604</th><th style="text-align:center">ubuntu 18.04</th></tr></thead><tbody><tr><td style="text-align:center">ROS</td><td style="text-align:center">Indigo</td><td style="text-align:center">kinetic</td><td style="text-align:center">Melodic</td></tr><tr><td style="text-align:center">Qt</td><td style="text-align:center">4.8.6 or higher</td><td style="text-align:center">5.2.1 or higher</td><td style="text-align:center">5.95 or higher</td></tr><tr><td style="text-align:center">CUDA(optional)</td><td style="text-align:center">8.0GA(?)</td><td style="text-align:center">9.0</td><td style="text-align:center">10.0</td></tr><tr><td style="text-align:center">FlyCapture2(optional)</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">Armdillo(optional)</td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table><h3 id="可选项"><a href="#可选项" class="headerlink" title="可选项"></a>可选项</h3><ul><li>FlyCapture2：相机的开发套件</li><li>Armdillo：用于线性代数和科学计算的CPP库</li></ul><h2 id="安装编译"><a href="#安装编译" class="headerlink" title="安装编译"></a>安装编译</h2><h3 id="创建工作空间"><a href="#创建工作空间" class="headerlink" title="创建工作空间"></a>创建工作空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p autoware.ai/src</span><br><span class="line">$ cd autoware.ai</span><br></pre></td></tr></table></figure><h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ wget -0 autoware.ai.repos &quot;https://raw.githubusercontent.com/Autoware-AI/autoware.ai/1.14.0/autoware.ai.repos&quot;</span><br><span class="line">$ vcs import src &lt; autoware.ai.repos</span><br></pre></td></tr></table></figure><h3 id="使用rosdep安装依赖项"><a href="#使用rosdep安装依赖项" class="headerlink" title="使用rosdep安装依赖项"></a>使用rosdep安装依赖项</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rosdep update</span><br><span class="line">$ rosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO</span><br></pre></td></tr></table></figure><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>若已经配置CUDA</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ AUTOWARE_COMPILE_WITH_CUDA=1 colon build --cmake-args -DCMAKE_BUILD_TYPE=Release</span><br></pre></td></tr></table></figure><p>若没有CUDA支持/</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ colon build --cmake-args -DCMAKE_BUILD_TYPE=Release</span><br></pre></td></tr></table></figure><h2 id="数据集测试"><a href="#数据集测试" class="headerlink" title="数据集测试"></a>数据集测试</h2><p>以官方提供的一个数据集为例</p><h3 id="数据集下载"><a href="#数据集下载" class="headerlink" title="数据集下载"></a>数据集下载</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://autoware-ai.s3.us-east-2.amazonaws.com/sample_moriyama_data.tar.gz</span><br><span class="line">wget https://autoware-ai.s3.us-east-2.amazonaws.com/sample_moriyama_150324.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir .autoware</span><br><span class="line">cp /home/xx/Downloads/my_launch.sh ./.autoware</span><br><span class="line">cp /home/xx/Downloads/sample_moriyama_data.tar.gz ./. autoware</span><br><span class="line">cp /home/xx/Downloads/sample_moriyama_150324.tar.gz ./.autoware</span><br><span class="line">cd .autoware/</span><br><span class="line">sh my_launch.sh</span><br><span class="line">tar -xzvf sample_moriyama_data.tar.gz</span><br><span class="line">tar -xzvf sample_moriyama_150324.gz</span><br></pre></td></tr></table></figure><h3 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd autoware.ai</span><br><span class="line">$ source install/seyup.bash</span><br><span class="line">$ roslaunch runitme_manager runtime_manager.launch</span><br></pre></td></tr></table></figure><p>出现如图所示界面以后，进入simulation选项卡，单机ref按钮找到下载好的bag包<img src="https://tva1.sinaimg.cn/large/0081Kckwly1glvgeh2t23j30mh0iv40u.jpg" alt></p><p>加载完毕之后设置开始时间，点击play，pause为暂停播放</p><p>再点击quick start分别点击map和location的右侧ref按钮，选择一下路径对应文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoware.ai/src/autoware/documentation/autoware_quickstart_example/launch/rosbag_demo/</span><br></pre></td></tr></table></figure><p>最后点击左侧的Map和Locations按钮。</p><p>下面就是在Rviz里面显示了，按照视频点击Rviz来启动Rviz，最后加载.rviz文件</p><p>回到simulation点击Pause案件开始播放，rviz里面就会开始运行数据集。</p><h2 id="用户手册"><a href="#用户手册" class="headerlink" title="用户手册"></a><a href="https://github.com/CPFL/Autoware-Manuals" target="_blank" rel="noopener">用户手册</a></h2><h3 id="Quick-Start选项说明"><a href="#Quick-Start选项说明" class="headerlink" title="Quick Start选项说明"></a>Quick Start选项说明</h3><ul><li>Map：地图配置文件，可以点击Ref按钮从系统中选择脚本</li><li>Sensing：传感器配置文件点击后面的Ref按钮可从系统中选择脚本</li><li>Localization：定位文件，点击后面的Ref按钮可从系统中选择脚本</li><li>Detection：目标检测，点击后面的Ref按钮可从系统中选择脚本</li><li>Mission planning：任务规划启动，可以点击后面的Ref按钮可从系统中选择脚本</li><li>Motion Planning：运动规划，可以点击后面的Ref按钮可从系统中选择脚本</li></ul><h3 id="Setup选项说明"><a href="#Setup选项说明" class="headerlink" title="Setup选项说明"></a>Setup选项说明</h3><p>在这个选项卡下面有个TF，包含了车辆基座标位置到激光雷达位置的主题发布，x，y，z，yaw，pitch，roll为激光雷达到达车辆坐标的相对位置输入。</p><p>总的来说，setup为安装设置模块，由于Autoware建图使用的是激光雷达。TF输入的是激光雷达和车辆之间的旋转平移矩阵信息。所以该模块还需要导入车辆的模型文件，配置文件也在下面的选项里有，点击Ref选择即可。</p><h3 id="Map·选项说明"><a href="#Map·选项说明" class="headerlink" title="Map·选项说明"></a>Map·选项说明</h3><ul><li>Point Cloud：点云地图</li><li>AutoUpdate：自动更新指定的场景数据</li><li>Vector Map：矢量地图</li><li>PCD Fillter：PCD滤波器</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Autoware最早是由名古屋大学研究小组在加藤伸平教授的领导下于2015年8月正式发布，2015年12月下旬，加藤伸平教授创立了Tier IV，以维护Autoware并将其应用于真正的自动驾驶汽车。&lt;/p&gt;
&lt;p&gt;随着时间流逝，Autoware已成为公认的开源项目，Autoware也是世界上第一个用于自动驾驶“多合一”开源软件，在业内使用也较为广泛。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="自动驾驶" scheme="http://yuanquanquan.top/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
      <category term="slam" scheme="http://yuanquanquan.top/tags/slam/"/>
    
  </entry>
  
  <entry>
    <title>Lyft Motion Prediction for Autonomous Vehicles</title>
    <link href="http://yuanquanquan.top/2020/20201216/"/>
    <id>http://yuanquanquan.top/2020/20201216/</id>
    <published>2020-12-16T08:04:46.000Z</published>
    <updated>2020-12-16T16:23:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>几个月前，lyft在kaggle平台上组织了一个比赛[1]：<a href="https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles" target="_blank" rel="noopener">Lyft Motion Prediction for Autonomous Vehicles</a>，用算法预测自动驾驶汽车周围的交通参与者的运动轨迹。和几个同学参加了这个比赛排名8%，拿了个铜牌，这篇博客是对比赛的总结。</p></blockquote><a id="more"></a>  <p><strong>赛题背景：</strong>The ridesharing company Lyft started Level 5 to take on the self-driving challenge and build a full self-driving system they’re hiring!). Their previous competition tasked participants with identifying 3D objects, an important step prior to detecting their movement. Now, they’re challenging you to predict the motion of these traffic agents.</p><p><strong>赛题任务：</strong>In this competition, you’ll apply your data science skills to build motion prediction models for self-driving vehicles. You’ll have access to the largest Prediction Dataset ever released to train and test your models. Your knowledge of machine learning will then be required to predict how cars, cyclists,and pedestrians move in the AV’s environment.</p><p><strong>Loss：</strong>The goal of this competition is to predict the trajectories of other traffic participants. You can employ uni-modal models yielding a single prediction per sample, or multi-modal ones generating multiple hypotheses (up to 3) - further described by a confidence vector.</p><p>Due to the high amount of multi-modality and ambiguity in traffic scenes, the used evaluation metric to score this competition is tailored to account for multiple predictions.</p><p><strong>Note:</strong> The following is a brief excerpt of our <a href="https://github.com/lyft/l5kit/blob/master/competition.md" target="_blank" rel="noopener">metrics page in the L5Kit repository</a></p><p>We calculate the negative log-likelihood of the ground truth data given the multi-modal predictions. Let us take a closer look at this. Assume, ground truth positions of a sample trajectory are<br>$$<br>x_{1}, \ldots, x_{T}, y_{1}, \ldots, y_{T}<br>$$</p><h3 id="问题定义和数据集"><a href="#问题定义和数据集" class="headerlink" title="问题定义和数据集"></a>问题定义和数据集</h3><p>自动驾驶中的运动预测或轨迹预测，在这里我们<strong>定义为</strong>：</p><blockquote><p>给定</p><ul><li>交通参与者（例如行人、骑车人、车辆）在过去一段时间的路径/轨迹</li><li>道路信息（例如道路结构、交通灯信息）</li><li>相关交通参与者过去一段时间对路径/轨迹等</li></ul><p>对</p><ul><li>未来一段时间的（一段或多段可能）路径/轨迹</li><li>或运动意图</li><li>或占用格栅图</li></ul><p>进行预测。</p></blockquote><p>上面这段文字便定义了问题的输入（“给定”部分）和输出（“对”部分），基于问题的定义，现存的研究有诸多方法，传统的方法如：</p><ul><li>基于运动学和动力学的预测（如卡尔曼滤波），缺点在于没有考虑到环境中其他交通参与者对被预测目标运动的影响；</li><li>考虑了环境因素的方法，如社会力模型、逆强化学习等。</li></ul><p><strong>基于deep learning的方法可以大致分为</strong>：</p><ul><li>循环神经网络（RNN）方法，由于轨迹具有明显的时序信息，通过RNN对时间维度建模；</li><li>卷积神经网络（CNN）方法，将轨迹和环境信息编码成图的形式（例如多通道的鸟瞰图），用CNN方法进行建模。在比赛中的baseline和大多数参赛者的方法均基于此；</li><li>其他，例如图神经网络、RNN+CNN的结合等；</li></ul><h3 id="Tips："><a href="#Tips：" class="headerlink" title="Tips："></a>Tips：</h3><p>由于这个官方baseline并不弱，并且为参赛选手提供了便利，因此大多数参赛者都是基于此进行了修改。从赛道上top解决方案[5]来看，有一些重要的修改技巧摘选如下（都是图像比赛的基本操作，但根据问题领域的不同有所调整）：</p><ol><li><h4 id="对样本的筛选："><a href="#对样本的筛选：" class="headerlink" title="对样本的筛选："></a>对样本的筛选：</h4><p>对训练样本进行筛选，使其与评测样本保持一致。例如在这个赛题中，评测样本基本在未来十帧都有数据，因此在训练样本筛选时也只保留有未来十帧数据的样本；</p></li><li><h4 id="对数据的编码："><a href="#对数据的编码：" class="headerlink" title="对数据的编码："></a>对数据的编码：</h4><p>历史帧的数目：第一名方案中history_num_frames=30，即构建了66通道的“图像”输入，包括30通道自车运动历史轨迹、30通道他车运动历史轨迹、3通道语义地图、3通道卫星地图；</p><p>其他可以实验的参数：例如“图像”的整体大小、“图像”每个像素点代表的物理范围；</p></li><li><h4 id="数据增强："><a href="#数据增强：" class="headerlink" title="数据增强："></a>数据增强：</h4><p>图像级增强：如 cutout、模糊、下采样；</p><p>栅格级增强：如 随机丢弃他车数据；</p></li><li><h4 id="对训练过程的加速："><a href="#对训练过程的加速：" class="headerlink" title="对训练过程的加速："></a>对训练过程的加速：</h4><p>lyft官方提供的上述数据栅格编码过程复杂，一次训练中的大部分时间都耗费在CPU上的数据处理中。因此可以优化编码部分代码，解决CPU运算瓶颈，提升多GPU训练效率，有助于在比赛中快速实验；</p></li><li><h4 id="单模型的选择和训练："><a href="#单模型的选择和训练：" class="headerlink" title="单模型的选择和训练："></a>单模型的选择和训练：</h4><p>第一名方案：EfficientNetB3模型，先用低分辨率图像预训练4个epoch，而后从第5个epoch开始，在原图上用余弦退火和阶梯下降学习率的方式进行训练；</p></li><li><h4 id="多模型的融合："><a href="#多模型的融合：" class="headerlink" title="多模型的融合："></a>多模型的融合：</h4><p>第一名方案：用不同的数据编码参数训练得到5个模型，用stacking方法进行融合；</p><p>第四名方案：用GMM方法将多模型的多条轨迹采样并拟合成最终的三条轨迹；</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;几个月前，lyft在kaggle平台上组织了一个比赛[1]：&lt;a href=&quot;https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Lyft Motion Prediction for Autonomous Vehicles&lt;/a&gt;，用算法预测自动驾驶汽车周围的交通参与者的运动轨迹。和几个同学参加了这个比赛排名8%，拿了个铜牌，这篇博客是对比赛的总结。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="自动驾驶" scheme="http://yuanquanquan.top/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
      <category term="kaggle" scheme="http://yuanquanquan.top/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>图像语义融合关键技术知识点</title>
    <link href="http://yuanquanquan.top/2020/20201212/"/>
    <id>http://yuanquanquan.top/2020/20201212/</id>
    <published>2020-12-10T11:27:36.000Z</published>
    <updated>2020-12-16T06:32:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>图像融合是将两个或多个具有互补信息和冗余特性的图像进行多层次、多级别、多方向的综合处理，减小图像的冗余性和模糊性，使得融合后的图像信息更丰富、更精准、更可靠，为后续的图像检测、图像识别、图像分类、图像理解等相关研究和应用提供技术基础。</p></blockquote> <a id="more"></a> <p>​     <strong>机器识别图像</strong>主要依赖图像的各种底层视觉特性信息，提取出<em>图像的形状</em>、<em>颜色直方图</em>、<em>纹理</em>、<em>轮廓</em>等底层视觉特征，分析不同的底层视觉特征之间的联系，实现图像的分类，进而达到图像识别的目的。其缺点是：纯粹的依赖图像底层特征的做法，缺乏对图像内容潜在的语义分析，难以捕捉到图像的直接视觉效果以外的人文或情感信息。<strong>图像的底层视觉特性和图像的高级语义之间存在语义鸿沟</strong>，导致图像的检索不理想。在图像检索，图像识别和分类一定程度上依赖于图像语义标注，这使得研究图像的语义标注算法成为图像理解领域的热点，也是人工智能的重要研究课题。</p><h4 id="图像标注算法和模型的涌现"><a href="#图像标注算法和模型的涌现" class="headerlink" title="图像标注算法和模型的涌现"></a>图像标注算法和模型的涌现</h4><ul><li>共现模型。利用统计学建立图像与标注词之间的映射关系。共现模型首先把图像划分成若干规则区域，然后对分割得到的区域进行分类，得到图像的区域与关键词之间的共生概率，然后选择对分割得到的区域进行分类，得到图像的区域与关键词之间的共生概率，然后选择共生概率大的关键词对图像进行标注。</li><li>机器翻译模型。利用传统的语言统计翻译模型，同样基于分割图像的做法，但是机器翻译模型的分割强调有意义的分割，分割过程能够实现对象识别，从标注图像中分割出来的图像区域与现实对象存在对应关系，以便于把分割出的区域能与具体对象建立关联，并将视觉特征转化为语义标注词。</li><li>CMRM模型（ cross modia relevance model）</li><li>CRM模型（continous-space relevance model）</li><li>图学习模型 自然语言处理中的语义上下文关系被用来图像底层视觉特征和高级语义之间建立关联。其中：</li><li>LSA模型（latent semantic analysis）</li><li>PLSA模型（probailistic latent semantic analysis） 这两种模型被用来分析图像和标注词之间的关系。</li></ul><p><strong>图像融合</strong>解释：<strong>将两个或多个具有互补信息和冗余特性的图像进行多层次、多级别、多方向的综合性处理</strong>，减小图像的冗余性和模糊性，使得融合后的图像信息更丰富、更为精准可靠、具有更佳的互补性、更有利于理解，更加适合人类接受和计算机理解，为后续图像的检测和识别、图像分类和图像理解等处理提供了技术支撑。</p><h4 id="图像分割算法现状"><a href="#图像分割算法现状" class="headerlink" title="图像分割算法现状"></a>图像分割算法现状</h4><p><strong>阈值分割算法</strong>，<strong>边缘检测分割算法</strong>，<strong>区域分割算法</strong>，以及近年流行的<strong>图论分割算法</strong>，<strong>聚类分割算法</strong></p><p><em>基于边缘分割算法</em>有：<strong>Prewitt算法</strong>、<strong>Roberts算法</strong>、<strong>Laplacian算法</strong>以及<strong>Sobel算法</strong>等。想要取得理想分割效果经常需要合理运用各种边缘检测算法。</p><p><em>基于区域的分割算法</em>，利用灰度级的不连续性来查找区域的边界。常用的操作是区域生长，以及区域的分离与合并。</p><p><em>基于图论的分割算法</em>，将图像映射成带有权重的无向图，然后对图进行划分处理，得到若干个不同的子图。<strong>归一化割</strong>的图划分方法，简称“<strong>N-cut</strong>”。<strong>最小割算法</strong>（<strong>Min-cut</strong>）是一种典型的N-cut算法。Min-cut利用图像的局部信息计算图结点之间的距离。</p><p><em>基于聚类的分割算法</em>；缺点：图像的空间信息没有得到处理。例如<strong>基于模糊C-均值聚类</strong>（<strong>FCM</strong>）对噪声比较敏感。FCM算法的改进算法如下：针对FCM算法的空间信息的改进：</p><ul><li><strong>FCM_S</strong>算法，加入了领域信息</li><li><strong>FCM_S1</strong>算法，利用预算值进行聚类。</li><li><strong>FCM_S2</strong>算法</li></ul><p>针对FCM_S是像素级的改进</p><ul><li><strong>EnFCM</strong>算法，结合图像的统计信息，把计算工作量降低到灰度级量级。</li><li><strong>FGFCM</strong>算法，建立空间相关性和灰度相关性，提高算法抗造性。</li><li><strong>NWFCM</strong>算法，利用非局部空间信息来计算领域像素到聚类中心的距离。</li></ul><p>针对FCM聚类算法和邻域加权的FCM聚类算法中的参数问题的改进：</p><ul><li><strong>FLICM</strong>算法，使用模糊因子取代参数。</li><li><strong>NDFCM</strong>算法，基于核函数的局部FCM算法。</li></ul><p>基于非局部空间信息的改进：</p><ul><li>FCM_NLS算法，基于非局部空间信息，提前对原图像的非局部空间进行滤波。</li><li>FCM_SNLS算法，基于自调节非局部空间信息的FCM聚类算法。</li></ul><p>新的图像分割理论：</p><ul><li>图像的颜色分布和纹理空间处理方面的经验模态分解EMD。</li></ul><h4 id="图像融合算法现状"><a href="#图像融合算法现状" class="headerlink" title="图像融合算法现状"></a>图像融合算法现状</h4><p>图像融合的主要流程是先进行图像的配准和特征提取，再进行决策，最后进行图像融合。依据所处上述处理流程中的不同阶段，图像融合可分为像素级融合、特征级融合和决策级融合三个层次。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;图像融合是将两个或多个具有互补信息和冗余特性的图像进行多层次、多级别、多方向的综合处理，减小图像的冗余性和模糊性，使得融合后的图像信息更丰富、更精准、更可靠，为后续的图像检测、图像识别、图像分类、图像理解等相关研究和应用提供技术基础。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="cv" scheme="http://yuanquanquan.top/tags/cv/"/>
    
  </entry>
  
</feed>
