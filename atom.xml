<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yuanquanquan的个人博客 | 我愿做你光华中淡淡的一笔</title>
  
  <subtitle>我愿做你光华中淡淡的一笔</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanquanquan.top/"/>
  <updated>2020-12-16T15:48:47.724Z</updated>
  <id>http://yuanquanquan.top/</id>
  
  <author>
    <name>理科生写给世界的情书</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Lyft Motion Prediction for Autonomous Vehicles</title>
    <link href="http://yuanquanquan.top/2020/20201216/"/>
    <id>http://yuanquanquan.top/2020/20201216/</id>
    <published>2020-12-16T08:04:46.000Z</published>
    <updated>2020-12-16T15:48:47.724Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>几个月前，lyft在kaggle平台上组织了一个比赛[1]：<a href="https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles" target="_blank" rel="noopener">Lyft Motion Prediction for Autonomous Vehicles</a>，用算法预测自动驾驶汽车周围的交通参与者的运动轨迹。和几个同学参加了这个比赛排名8%，拿了个铜牌，这篇博客是对比赛的总结。</p></blockquote><p><strong>赛题背景：</strong>The ridesharing company Lyft started Level 5 to take on the self-driving challenge and build a full self-driving system they’re hiring!). Their previous competition tasked participants with identifying 3D objects, an important step prior to detecting their movement. Now, they’re challenging you to predict the motion of these traffic agents.</p><p><strong>赛题任务：</strong>In this competition, you’ll apply your data science skills to build motion prediction models for self-driving vehicles. You’ll have access to the largest Prediction Dataset ever released to train and test your models. Your knowledge of machine learning will then be required to predict how cars, cyclists,and pedestrians move in the AV’s environment.</p><p><strong>Loss：</strong>The goal of this competition is to predict the trajectories of other traffic participants. You can employ uni-modal models yielding a single prediction per sample, or multi-modal ones generating multiple hypotheses (up to 3) - further described by a confidence vector.</p><p>Due to the high amount of multi-modality and ambiguity in traffic scenes, the used evaluation metric to score this competition is tailored to account for multiple predictions.</p><p><strong>Note:</strong> The following is a brief excerpt of our <a href="https://github.com/lyft/l5kit/blob/master/competition.md" target="_blank" rel="noopener">metrics page in the L5Kit repository</a></p><p>We calculate the negative log-likelihood of the ground truth data given the multi-modal predictions. Let us take a closer look at this. Assume, ground truth positions of a sample trajectory are<br>$$<br>x_{1}, \ldots, x_{T}, y_{1}, \ldots, y_{T}<br>$$<br>and we predict K hypotheses, represented by means<br>$$<br>\bar{x}<em>{1}^{k}, \ldots, \bar{x}</em>{T}^{k}, \bar{y}<em>{1}^{k}, \ldots, \bar{y}</em>{T}^{k}<br>$$<br>In addition, we predict confidences c of these K hypotheses. We assume the ground truth positions to be modeled by a mixture of multi-dimensional independent Normal distributions over time, yielding the likelihood<br>$$<br>p\left(x_{1, \ldots, T}, y_{1, \ldots, T} \mid c^{1, \ldots, K}, \bar{x}<em>{1, \ldots, T}^{1, \ldots, K}, \bar{y}</em>{1, \ldots, T}^{1, \ldots, K}\right)<br>=\sum_{k} c^{k} \mathcal{N}\left(x_{1, \ldots, T} \mid \bar{x}<em>{1, \ldots, T}^{k}, \Sigma=1\right) \mathcal{N}\left(y</em>{1, \ldots, T} \mid \bar{y}<em>{1, \ldots, T}^{k}, \Sigma=1\right)<br>=\sum</em>{k} c^{k} \prod_{t} \mathcal{N}\left(x_{t} \mid \bar{x}<em>{t}^{k}, \sigma=1\right) \mathcal{N}\left(y</em>{t} \mid \bar{y}_{t}^{k}, \sigma=1\right)<br>$$</p><p>which results in the loss<br>$$<br>L=-\log p\left(x_{1, \ldots, T}, y_{1, \ldots, T} \mid c^{1, \ldots, K}, \bar{x}<em>{1, \ldots, T}^{1, \ldots, K}, \bar{y}</em>{1, \ldots, T}^{1, \ldots, K}\right)<br>=-\log \sum_{k} e^{\log \left(c^{k}\right)-\frac{1}{2} \sum_{t}\left(\bar{x}<em>{t}^{k}-x</em>{t}\right)^{2}+\left(\bar{y}<em>{t}^{k}-y</em>{t}\right)^{2}}<br>$$<br>01</p><h3 id="问题定义和数据集"><a href="#问题定义和数据集" class="headerlink" title="问题定义和数据集"></a>问题定义和数据集</h3><p>自动驾驶中的运动预测或轨迹预测，在这里我们<strong>定义为</strong>：</p><blockquote><p>给定</p><ul><li>交通参与者（例如行人、骑车人、车辆）在过去一段时间的路径/轨迹</li><li>道路信息（例如道路结构、交通灯信息）</li><li>相关交通参与者过去一段时间对路径/轨迹等</li></ul><p>对</p><ul><li>未来一段时间的（一段或多段可能）路径/轨迹</li><li>或运动意图</li><li>或占用格栅图</li></ul><p>进行预测。</p></blockquote><p>上面这段文字便定义了问题的输入（“给定”部分）和输出（“对”部分），基于问题的定义，现存的研究有诸多方法，传统的方法如：</p><ul><li>基于运动学和动力学的预测（如卡尔曼滤波），缺点在于没有考虑到环境中其他交通参与者对被预测目标运动的影响；</li><li>考虑了环境因素的方法，如社会力模型、逆强化学习等。</li></ul><p><strong>基于deep learning的方法可以大致分为</strong>：</p><ul><li>循环神经网络（RNN）方法，由于轨迹具有明显的时序信息，通过RNN对时间维度建模；</li><li>卷积神经网络（CNN）方法，将轨迹和环境信息编码成图的形式（例如多通道的鸟瞰图），用CNN方法进行建模。在比赛中的baseline和大多数参赛者的方法均基于此；</li><li>其他，例如图神经网络、RNN+CNN的结合等；</li></ul><h3 id="Tips："><a href="#Tips：" class="headerlink" title="Tips："></a>Tips：</h3><p>由于这个官方baseline并不弱，并且为参赛选手提供了便利，因此大多数参赛者都是基于此进行了修改。从赛道上top解决方案[5]来看，有一些重要的修改技巧摘选如下（都是图像比赛的基本操作，但根据问题领域的不同有所调整）：</p><ol><li><h4 id="对样本的筛选："><a href="#对样本的筛选：" class="headerlink" title="对样本的筛选："></a>对样本的筛选：</h4></li><li><ol><li>对训练样本进行筛选，使其与评测样本保持一致。例如在这个赛题中，评测样本基本在未来十帧都有数据，因此在训练样本筛选时也只保留有未来十帧数据的样本；</li></ol></li><li><h4 id="对数据的编码："><a href="#对数据的编码：" class="headerlink" title="对数据的编码："></a>对数据的编码：</h4></li><li><ol><li>历史帧的数目：第一名方案中history_num_frames=30，即构建了66通道的“图像”输入，包括30通道自车运动历史轨迹、30通道他车运动历史轨迹、3通道语义地图、3通道卫星地图；</li><li>其他可以实验的参数：例如“图像”的整体大小、“图像”每个像素点代表的物理范围；</li></ol></li><li><h4 id="数据增强："><a href="#数据增强：" class="headerlink" title="数据增强："></a>数据增强：</h4></li><li><ol><li>图像级增强：如 cutout、模糊、下采样；</li><li>栅格级增强：如 随机丢弃他车数据；</li></ol></li><li><h4 id="对训练过程的加速："><a href="#对训练过程的加速：" class="headerlink" title="对训练过程的加速："></a>对训练过程的加速：</h4><p>lyft官方提供的上述数据栅格编码过程复杂，一次训练中的大部分时间都耗费在CPU上的数据处理中。因此可以优化编码部分代码，解决CPU运算瓶颈，提升多GPU训练效率，有助于在比赛中快速实验；</p></li><li><h4 id="单模型的选择和训练："><a href="#单模型的选择和训练：" class="headerlink" title="单模型的选择和训练："></a>单模型的选择和训练：</h4></li><li><ol><li>第一名方案：EfficientNetB3模型，先用低分辨率图像预训练4个epoch，而后从第5个epoch开始，在原图上用余弦退火和阶梯下降学习率的方式进行训练；</li></ol></li><li><h4 id="多模型的融合："><a href="#多模型的融合：" class="headerlink" title="多模型的融合："></a>多模型的融合：</h4></li><li><ol><li>第一名方案：用不同的数据编码参数训练得到5个模型，用stacking方法进行融合；</li><li>第四名方案：用GMM方法将多模型的多条轨迹采样并拟合成最终的三条轨迹；</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;几个月前，lyft在kaggle平台上组织了一个比赛[1]：&lt;a href=&quot;https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles&quot; target=&quot;_blank&quot; 
      
    
    </summary>
    
    
      <category term="kaggle" scheme="http://yuanquanquan.top/tags/kaggle/"/>
    
      <category term="自动驾驶" scheme="http://yuanquanquan.top/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
  </entry>
  
  <entry>
    <title>图像语义融合关键技术知识点</title>
    <link href="http://yuanquanquan.top/2020/20201212/"/>
    <id>http://yuanquanquan.top/2020/20201212/</id>
    <published>2020-12-10T11:27:36.000Z</published>
    <updated>2020-12-16T06:32:50.041Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>图像融合是将两个或多个具有互补信息和冗余特性的图像进行多层次、多级别、多方向的综合处理，减小图像的冗余性和模糊性，使得融合后的图像信息更丰富、更精准、更可靠，为后续的图像检测、图像识别、图像分类、图像理解等相关研究和应用提供技术基础。</p></blockquote> <a id="more"></a> <p>​     <strong>机器识别图像</strong>主要依赖图像的各种底层视觉特性信息，提取出<em>图像的形状</em>、<em>颜色直方图</em>、<em>纹理</em>、<em>轮廓</em>等底层视觉特征，分析不同的底层视觉特征之间的联系，实现图像的分类，进而达到图像识别的目的。其缺点是：纯粹的依赖图像底层特征的做法，缺乏对图像内容潜在的语义分析，难以捕捉到图像的直接视觉效果以外的人文或情感信息。<strong>图像的底层视觉特性和图像的高级语义之间存在语义鸿沟</strong>，导致图像的检索不理想。在图像检索，图像识别和分类一定程度上依赖于图像语义标注，这使得研究图像的语义标注算法成为图像理解领域的热点，也是人工智能的重要研究课题。</p><h4 id="图像标注算法和模型的涌现"><a href="#图像标注算法和模型的涌现" class="headerlink" title="图像标注算法和模型的涌现"></a>图像标注算法和模型的涌现</h4><ul><li>共现模型。利用统计学建立图像与标注词之间的映射关系。共现模型首先把图像划分成若干规则区域，然后对分割得到的区域进行分类，得到图像的区域与关键词之间的共生概率，然后选择对分割得到的区域进行分类，得到图像的区域与关键词之间的共生概率，然后选择共生概率大的关键词对图像进行标注。</li><li>机器翻译模型。利用传统的语言统计翻译模型，同样基于分割图像的做法，但是机器翻译模型的分割强调有意义的分割，分割过程能够实现对象识别，从标注图像中分割出来的图像区域与现实对象存在对应关系，以便于把分割出的区域能与具体对象建立关联，并将视觉特征转化为语义标注词。</li><li>CMRM模型（ cross modia relevance model）</li><li>CRM模型（continous-space relevance model）</li><li>图学习模型 自然语言处理中的语义上下文关系被用来图像底层视觉特征和高级语义之间建立关联。其中：</li><li>LSA模型（latent semantic analysis）</li><li>PLSA模型（probailistic latent semantic analysis） 这两种模型被用来分析图像和标注词之间的关系。</li></ul><p><strong>图像融合</strong>解释：<strong>将两个或多个具有互补信息和冗余特性的图像进行多层次、多级别、多方向的综合性处理</strong>，减小图像的冗余性和模糊性，使得融合后的图像信息更丰富、更为精准可靠、具有更佳的互补性、更有利于理解，更加适合人类接受和计算机理解，为后续图像的检测和识别、图像分类和图像理解等处理提供了技术支撑。</p><h4 id="图像分割算法现状"><a href="#图像分割算法现状" class="headerlink" title="图像分割算法现状"></a>图像分割算法现状</h4><p><strong>阈值分割算法</strong>，<strong>边缘检测分割算法</strong>，<strong>区域分割算法</strong>，以及近年流行的<strong>图论分割算法</strong>，<strong>聚类分割算法</strong></p><p><em>基于边缘分割算法</em>有：<strong>Prewitt算法</strong>、<strong>Roberts算法</strong>、<strong>Laplacian算法</strong>以及<strong>Sobel算法</strong>等。想要取得理想分割效果经常需要合理运用各种边缘检测算法。</p><p><em>基于区域的分割算法</em>，利用灰度级的不连续性来查找区域的边界。常用的操作是区域生长，以及区域的分离与合并。</p><p><em>基于图论的分割算法</em>，将图像映射成带有权重的无向图，然后对图进行划分处理，得到若干个不同的子图。<strong>归一化割</strong>的图划分方法，简称“<strong>N-cut</strong>”。<strong>最小割算法</strong>（<strong>Min-cut</strong>）是一种典型的N-cut算法。Min-cut利用图像的局部信息计算图结点之间的距离。</p><p><em>基于聚类的分割算法</em>；缺点：图像的空间信息没有得到处理。例如<strong>基于模糊C-均值聚类</strong>（<strong>FCM</strong>）对噪声比较敏感。FCM算法的改进算法如下：针对FCM算法的空间信息的改进：</p><ul><li><strong>FCM_S</strong>算法，加入了领域信息</li><li><strong>FCM_S1</strong>算法，利用预算值进行聚类。</li><li><strong>FCM_S2</strong>算法</li></ul><p>针对FCM_S是像素级的改进</p><ul><li><strong>EnFCM</strong>算法，结合图像的统计信息，把计算工作量降低到灰度级量级。</li><li><strong>FGFCM</strong>算法，建立空间相关性和灰度相关性，提高算法抗造性。</li><li><strong>NWFCM</strong>算法，利用非局部空间信息来计算领域像素到聚类中心的距离。</li></ul><p>针对FCM聚类算法和邻域加权的FCM聚类算法中的参数问题的改进：</p><ul><li><strong>FLICM</strong>算法，使用模糊因子取代参数。</li><li><strong>NDFCM</strong>算法，基于核函数的局部FCM算法。</li></ul><p>基于非局部空间信息的改进：</p><ul><li>FCM_NLS算法，基于非局部空间信息，提前对原图像的非局部空间进行滤波。</li><li>FCM_SNLS算法，基于自调节非局部空间信息的FCM聚类算法。</li></ul><p>新的图像分割理论：</p><ul><li>图像的颜色分布和纹理空间处理方面的经验模态分解EMD。</li></ul><h4 id="图像融合算法现状"><a href="#图像融合算法现状" class="headerlink" title="图像融合算法现状"></a>图像融合算法现状</h4><p>图像融合的主要流程是先进行图像的配准和特征提取，再进行决策，最后进行图像融合。依据所处上述处理流程中的不同阶段，图像融合可分为像素级融合、特征级融合和决策级融合三个层次。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;图像融合是将两个或多个具有互补信息和冗余特性的图像进行多层次、多级别、多方向的综合处理，减小图像的冗余性和模糊性，使得融合后的图像信息更丰富、更精准、更可靠，为后续的图像检测、图像识别、图像分类、图像理解等相关研究和应用提供技术基础。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="cv" scheme="http://yuanquanquan.top/tags/cv/"/>
    
  </entry>
  
  <entry>
    <title>关于脉冲特征提取的想法</title>
    <link href="http://yuanquanquan.top/2020/202011077/"/>
    <id>http://yuanquanquan.top/2020/202011077/</id>
    <published>2020-11-07T05:33:18.000Z</published>
    <updated>2020-11-07T06:58:46.189Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>选的毕设题目是《脉冲信号的智能提取软件设计》，虽然之前基本没处理过脉冲数据，但是数据处理的套路都差不多，无非先进行数据清洗，然后去直流、降噪，再对信号进行加窗（滤波、分析信号得到想要频段内的信号），再通过一些方法，如FFT、小波或者一些其他变换，通过这些变换得出频率成分，异常特征值。</p></blockquote><a id="more"></a>  <p>关于这个题目我大概有两种思路，一种较为传统，基本就是将之前各种传统算法整合在一起，另一种则是基于特征编码和一维CNN卷积神经网络对信号对脉冲矩阵进行信号提取。</p><h3 id="一、使用传统算法对脉冲信号进行提取："><a href="#一、使用传统算法对脉冲信号进行提取：" class="headerlink" title="一、使用传统算法对脉冲信号进行提取："></a>一、使用传统算法对脉冲信号进行提取：</h3><p><strong>1、首先，在特征提取之前需明确是怎样的信号，怎样的应用，怎样的场景，因为针对不同应用和场景选择的特征提取也不近相同</strong></p><p><strong>2、信号特征的提取往往都是用最简单有效的参数表示信号中的信息，这是根本目的。</strong></p><p><strong>3、针对不同后端模型需要确定特征维度。</strong></p><p><strong>4、开始特征提取前，信号往往需要做一些预处理，如滤波、去均值、去异常等等。</strong></p><h3 id="特征提取有哪些方法："><a href="#特征提取有哪些方法：" class="headerlink" title="特征提取有哪些方法："></a><strong>特征提取有哪些方法：</strong></h3><p><strong>1、时域一维信号简单统计和运算可以得到的特征有：均值，方差，均方根，峰值因子，峭度系数，波形因子，裕度因子、脉冲因子。</strong></p><p><strong>2、估计–分布参数一般服从某一类分布；</strong></p><p><strong>3、频域，特征频率，均方频率，重心频率，频率方差；</strong></p><p><strong>4、小波方法提取的系数，小波滤波后的特征频率等等；</strong></p><p><strong>5、信号熵，谱熵，排列熵，小波熵，EMD熵，包络谱熵等；</strong></p><p><strong>6、谱峭度，快速谱峭度、小波谱峭度等；</strong></p><p><strong>7、基于数学工具和降维的特征，如PCA，矩阵特征向量，矩阵的秩，特征根，SVD-奇异值、ICA等等；</strong></p><p><strong>8、一些基于距离的度量、范数、马氏距离、分形参数，同胚流行等等；</strong></p><p><strong>9、任何能表征信号特征的自定义参数均可以，注意有意义有时是结合实际需求的。</strong></p><h3 id="二、基于特征编码和一维CNN卷积神经网络"><a href="#二、基于特征编码和一维CNN卷积神经网络" class="headerlink" title="二、基于特征编码和一维CNN卷积神经网络"></a>二、基于特征编码和一维CNN卷积神经网络</h3><p><strong>目前已有的脉冲信号识别方法大多是基于脉间特征和使用机器学习方法来实现。整体上来说，已有的方法已经能对大多数的脉冲信号进行较为准确的识别，但是对于一些参数较为相近的信号识别效果并不好。</strong></p><p><strong>我的思路是根据脉冲信号的载频(RF)、重频(PRI)、脉宽(PW)等参数构建脉冲描述矩阵。</strong></p><p><img src="https://i.loli.net/2020/11/07/LX1V6mlEw7WhiHZ.png" alt="脉冲信号及其特征"></p><p><strong>通过传感器获取的脉冲信号，经过信号分选等处理后得到属于一组脉冲描述字(PDW)信息，组成脉冲描述矩阵（PDM）。</strong></p><p><img src="https://i.loli.net/2020/11/07/kQVEKAPt8RsLW3j.png" alt="脉冲描述矩阵"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;选的毕设题目是《脉冲信号的智能提取软件设计》，虽然之前基本没处理过脉冲数据，但是数据处理的套路都差不多，无非先进行数据清洗，然后去直流、降噪，再对信号进行加窗（滤波、分析信号得到想要频段内的信号），再通过一些方法，如FFT、小波或者一些其他变换，通过这些变换得出频率成分，异常特征值。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="毕设" scheme="http://yuanquanquan.top/tags/%E6%AF%95%E8%AE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Effective AER Object Classification Using Segmented Probability-Maximization Learning in Spiking Neural Networks</title>
    <link href="http://yuanquanquan.top/2020/20201103/"/>
    <id>http://yuanquanquan.top/2020/20201103/</id>
    <published>2020-11-02T17:30:48.000Z</published>
    <updated>2020-11-07T04:05:30.174Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前因为一些emmmm俗事，很久没看论文了，现在因为要做毕设，又久违看起了论文，这次是浙大普适与智能实验室AAAI 2020的论文《基于脉冲神经网络的事件表示的物体分类方法》</p></blockquote><a id="more"></a><p>事件相机是一种神经拟态视觉传感器，通过模拟人类的视网膜机制来记录场景。与传统相机不同，事件相机没有 “帧” 的概念。当它捕捉到视野中任意位置发生了一定程度的光线变化时，就会产生一些像素级的输出（即事件）。一个事件中包括三个信息：事件在2D空间的像素坐标，事件的时间戳，以及事件的极性（1和-1，表示是由光线变亮或变暗触发的）。由于事件相机关注的是场景中的光线变化，因此它只会记录场景中的动态信息，忽略不变的静态冗余信息，从而能显著减少内存使用和能源消耗。</p><p>这种基于事件的数据形式与脉冲神经网络（Spiking Neural Network，SNN）不谋而合。脉冲神经网络通过脉冲传递信息，而脉冲也具有事件的属性。相比传统人工神经网络，脉冲神经网络更具有生物解释性，在处理时空数据的能力上也有更大的潜力。</p><p>然而，将这种新的事件表示与SNN相结合来进行物体分类，还存在着若干挑战。首先，与传统相机的视频流数据形式相比，事件相机产生的事件流是不稳定的。由于事件相机对视觉感受域内的动态信息非常敏感，在场景记录过程中，除了与目标的相关的事件被记录下来，相机抖动和环境光的变化等因素也会产生大量的噪声事件，影响SNN网络中神经元响应的可靠性和网络学习性能。其次，事件相机输出的事件流记录了一段时间内的信息，我们希望SNN的响应能够足够快，以便在提供的测试信息还不完整的情况下，仍达到具有竞争力的准确性水平。</p><p><img src="https://i.loli.net/2020/11/07/e3xkVDZrl2w718d.png" alt></p><p>最近，我们发表在AAAI2020的论文《Effective AER Object Classification Using Segmented Probability-Maximization Learning in Spiking Neural Networks》就针对事件表示的物体提出了一个分类模型，在这个模型中，我们提出了一个新的脉冲神经网络学习算法——分段概率最大化算法（Segmented Probability-MAximization，SPA）。SPA基于脉冲神经元之间的相对响应定义了一个响应概率，利用梯度更新来迭代增加响应正确的概率，从而提高神经元响应的可靠性。同时我们根据神经元的电压将事件流的处理过程动态分段，以便能充分利用一段时长内的大量事件，增加信息的利用率，提高网络的学习的性能。</p><p> <img src="https://i.loli.net/2020/11/07/SgR67PrvBj3GLeW.png" alt="图1 峰值检测示意图"></p><p><strong>分段概率最大化算法</strong></p><p>SPA算法是一个电压驱动的脉冲神经网络监督学习算法。首先，该算法定义了一个脉冲神经元响应与其电压的关系，电压越高，其响应就越积极。在监督学习算法中，每个神经元会对应一个物体类别，并对属于其代表类的样本敏感。因此，SPA算法的训练目标是使得样本对应类的脉冲神经元响应最积极。基于此目的，我们定义了一个响应概率，即样本对应类的脉冲神经元响应占总神经元响应比例。这个比例越高，说明样本对应类的脉冲神经元的相对响应越积极，样本被分到正确类的概率就越高，结果越可靠。为了优化这个概率，SPA算法使用交叉熵定义损失函数，并使用梯度更新迭代优化网络的权重。</p><p>事件表示这种新的数据表达形式是通过一时间段内的事件流来描绘物体的，那如何充分利用期间的信息呢？在这里，我们提出了分段峰值检测（Peak Detection）机制：从tS＝0开始，首先划出一较长的固定时间段tR, 然后在此时间段中检测各神经元的电压峰值，将此峰值带入上述权重更新过程中。更新后，将起始时间tS更新为各神经元峰值中的最大值所对应的时间点，再划出时间段tR，重复上述寻找峰值和权重更新过程。神经元的电压峰值是积累的结果，是由一串激发的输入脉冲触发的，表明神经元在电压峰值时已经接收了大量的信息。因此，通过分段峰值检测算法可以将整个事件流时间窗内的峰值信息有效利用起来进行网络训练。</p><p><strong>实验结果与分析</strong></p><p><img src="https://i.loli.net/2020/11/07/4txYD2QhKeBzkf8.png" alt="表1 四个数据集分类结果的比较"></p><p>我们在若干公开发布的事件表示的物体数据集上评估模型的效果。首先，我们将本文提出的分类模型的结果与其它的事件表示的物体分类框架（特别是基于脉冲的物体分类框架）的结果比较。由表1可以看到，本文提出的分类模型在四个公开数据集都表现较好。</p><p>接下来，我们通过更细致的实验来进一步验证模型的效果。首先，我们研究了事件流时长对算法结果的影响。事件流的长度代表了信息量，事件流越长即提供的信息越丰富。我们在MNIST-DVS数据集四种不同事件流长度上进行实验，并选择了表1中在该数据集上表现较好的Zhao的模型与我们的方法进行比较。如表2所示，我们看到：1）事件流时长逐渐增加，两种方法的准确性都会提高，这也印证了较长的事件流提供了更多的信息；2）在MNIST-DVS的各个时长上，本模型的效果都优于Zhao的方法，甚至在时间长度为100ms的事件流上，本模型就超过了Zhao的方法在全长事件流上实现的精度。这一结果表明在相同甚至更少的信息下，我们的模型可以达到更好的分类效果。</p><p><img src="https://i.loli.net/2020/11/07/u16zsqjmZkWHCa9.png" alt="表2 在MNIST-DVS数据集不同时间长度下的性能"></p><p>接着我们验证模型不完整信息推理的能力，即模型在测试过程中，当描述对象的信息不完整时，是否仍然能够保持一定的识别精度。我们同样在MNIST-DVS数据集上进行验证，使用500ms的事件流进行训练，并在测试时观察前300ms三种不同分类算法的性能（本文的SPA学习算法，Zhao的方法中使用的Tempotron学习算法，和非时序分类器SVM）。如图2所示，随着事件流的流入，三种算法模型的分类精度都不断提高。SPA模型是所有方法中性能最好的，特别是在输入信息极其不完整的前100ms时。这是因为SPA在训练模型时，每个训练样本都会由于峰值检测机制基于片段中的峰值被多次训练，继而增加了训练样本的多样性。因此，我们的模型具有较好的泛化能力，即使在信息不完整的情况下，也可以在早期推断出结果。</p><p><img src="https://i.loli.net/2020/11/07/RKD8G4jzPtwyuJa.png" alt="图2 在MNIST-DVS数据集上不完全信息推理的性能"></p><p><strong>结论和讨论</strong></p><p>综上，我们提出了一种有效的事件表示的物体分类模型，该模型提出了一种新的脉冲神经网络学习算法——SPA算法。SPA学习算法通过迭代增加神经元响应正确的概率这一方式提高神经元响应的可靠性。同时，通过SPA算法中的峰值检测机制提高事件流时长内信息的学习利用率。实验结果验证了本模型的性能，以及模型所具有信息利用效率高、泛化能力强等优点。</p><p>论文链接：<a href="https://arxiv.org/abs/2002.06199" target="_blank" rel="noopener">https://arxiv.org/abs/2002.06199</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前因为一些emmmm俗事，很久没看论文了，现在因为要做毕设，又久违看起了论文，这次是浙大普适与智能实验室AAAI 2020的论文《基于脉冲神经网络的事件表示的物体分类方法》&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="SNN" scheme="http://yuanquanquan.top/tags/SNN/"/>
    
  </entry>
  
  <entry>
    <title>Some Virtual Try-on (VTON) Research</title>
    <link href="http://yuanquanquan.top/2020/20201030/"/>
    <id>http://yuanquanquan.top/2020/20201030/</id>
    <published>2020-10-30T14:51:09.000Z</published>
    <updated>2020-10-29T15:44:32.838Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近跟项目在做一些，虚拟试衣Virtual Try-on (VTON）的工作，记录一下调研的数据以及开源的论文以及模型。</p></blockquote><a id="more"></a><h2 id="Clothing-dataset"><a href="#Clothing-dataset" class="headerlink" title="Clothing dataset"></a>Clothing dataset</h2><p>Over 5,000 images of 20 different classes.</p><p>This dataset can be freely used for any purpose, including commercial:</p><p>For example:</p><ul><li>Creating a tutorial or a course (free or paid)</li><li>Writing a book</li><li>Kaggle competitions (as an external dataset)</li><li>Training an internal model at any company</li></ul><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>The <code>images.csv</code> file contains:</p><ul><li><code>image</code> - the ID of the image (use it to load the image from <code>images/&lt;ID&gt;.jpg</code>)</li><li><code>sender_id</code> - the ID of a person who contributed the image</li><li><code>label</code> - the class of the image</li><li><code>kids</code> - flag, <code>True</code> if it’s clothes for kids</li></ul><h3 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h3><ul><li>If you’re looking for a subset of the clothing dataset, check here: <a href="https://github.com/alexeygrigorev/clothing-dataset-small" target="_blank" rel="noopener">https://github.com/alexeygrigorev/clothing-dataset-small</a></li><li>You can read more about this dataset here: <a href="https://medium.com/data-science-insider/clothing-dataset-5b72cd7c3f1f" target="_blank" rel="noopener">https://medium.com/data-science-insider/clothing-dataset-5b72cd7c3f1f</a></li><li>This dataset is also awailable on Kaggle (with images in higher resolution): </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.kaggle.com/agrigorev/clothing-dataset-full/</span><br></pre></td></tr></table></figure><h3 id="Top-10-subset"><a href="#Top-10-subset" class="headerlink" title="Top-10 subset"></a>Top-10 subset</h3><p>Images of some classes don’t appear very often. Training a neural network to predict these classes is quite difficult — we need at least 100-200 images of each class to make a meaningful model.</p><p>That’s why, for educational purposes, we created a subset of the full dataset that covers only the top-10 classes.</p><p>Check it here: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/alexeygrigorev/clothing-dataset-small</span><br></pre></td></tr></table></figure><p>Examples</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.kaggle.com/agrigorev/collage</span><br></pre></td></tr></table></figure><p>Do you use this dataset somewhere? Please submit a PR with a link</p><h3 id="Acknowledgements"><a href="#Acknowledgements" class="headerlink" title="Acknowledgements"></a>Acknowledgements</h3><p>We’d like to thank</p><ul><li><p>Kenes Shangereyev and Tagias.com for helping with 3000 images</p></li><li><p>All the 32 people who contributed their images to the dataset via the forms:</p></li><li><ul><li>Patricia Goldberg</li></ul></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.linkedin.com/in/patricia-goldberg/</span><br></pre></td></tr></table></figure></li><li><p>Everyone who supported the initiative by engaging with the announcements on social media</p></li></ul><p>It wouldn’t be possible to collect this dataset without your help!</p><p>A curated list of awesome research papers, projects, code, dataset, workshops etc. related to virtual try-on (VTON).</p><ul><li><a href="#Image-based-2D-Virtual-Try-on">Image-based (2D) Virtual Try-on</a></li><li><a href="#3D-virtual-try-on">3D Virtual Try-on</a></li><li><a href="#Multi-Pose-Guided-Virtual-Try-on">Multi-Pose Guided Virtual Try-on</a></li><li><a href="#Video-Virtual-Try-on">Video Virtual Try-on</a></li><li><a href="#non-clothing-virtual-try-on">Non-clothing Virtual Try-on</a></li><li><a href="#pose-guided-human-synthesis">Pose-Guided Human Synthesis</a></li><li><a href="#Datasets-for-Virtual-Try-on">Datasets for Virtual Try-on</a></li><li><a href="#Related-Conference-Workshops">Related Conference Workshops</a></li><li><a href="#Related-Repositories">Related Repositories</a></li></ul><h2 id="Image-based-2D-Virtual-Try-on"><a href="#Image-based-2D-Virtual-Try-on" class="headerlink" title="Image-based (2D) Virtual Try-on"></a>Image-based (2D) Virtual Try-on</h2><h4 id="ACCV-2020"><a href="#ACCV-2020" class="headerlink" title="ACCV 2020"></a>ACCV 2020</h4><ul><li><p>CloTH-VTON: Clothing Three-dimensional reconstruction for Hybrid image-based Virtual Try-ON - <a href="https://minar09.github.io/clothvton/" target="_blank" rel="noopener">Project</a></p><h4 id="ECCV-2020"><a href="#ECCV-2020" class="headerlink" title="ECCV 2020"></a>ECCV 2020</h4></li><li><p>Do Not Mask What You Do Not Need to Mask: a Parser Free Virtual Try-On - <a href="https://arxiv.org/pdf/2007.02721.pdf" target="_blank" rel="noopener">Paper</a></p><h4 id="CVPR-2020"><a href="#CVPR-2020" class="headerlink" title="CVPR 2020"></a>CVPR 2020</h4></li><li><p>Towards Photo-Realistic Virtual Try-On by Adaptively Generating↔Preserving Image Content - <a href="https://github.com/switchablenorms/DeepFashion_Try_On" target="_blank" rel="noopener">Paper/Code/Data</a></p></li><li>Image Based Virtual Try-On Network From Unpaired Data - <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Neuberger_Image_Based_Virtual_Try-On_Network_From_Unpaired_Data_CVPR_2020_paper.html" target="_blank" rel="noopener">Paper</a></li><li><p>Semantically Multi-modal Image Synthesis - <a href="https://seanseattle.github.io/SMIS/" target="_blank" rel="noopener">Paper/Code/Model</a></p><h4 id="CVPRW-2020"><a href="#CVPRW-2020" class="headerlink" title="CVPRW 2020"></a>CVPRW 2020</h4></li><li><p>CP-VTON+: Clothing Shape and Texture Preserving Image-Based Virtual Try-On - <a href="https://minar09.github.io/cpvtonplus/" target="_blank" rel="noopener">Paper/Code/Data/Model</a></p></li><li><p>3D Reconstruction of Clothes using a Human Body Model and its Application to Image-based Virtual Try-On - <a href="https://minar09.github.io/c3dvton/" target="_blank" rel="noopener">Paper/Project</a></p><h4 id="ICCV-2019"><a href="#ICCV-2019" class="headerlink" title="ICCV 2019"></a>ICCV 2019</h4></li><li><p>VTNFP: An Image-Based Virtual Try-On Network With Body and Clothing Feature Preservation - <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_VTNFP_An_Image-Based_Virtual_Try-On_Network_With_Body_and_Clothing_ICCV_2019_paper.html" target="_blank" rel="noopener">Paper</a></p></li><li><p>ClothFlow: A Flow-Based Model for Clothed Person Generation - <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.html" target="_blank" rel="noopener">Paper</a></p><h4 id="ICCVW-2019"><a href="#ICCVW-2019" class="headerlink" title="ICCVW 2019"></a>ICCVW 2019</h4></li><li><p>UVTON: UV Mapping to Consider the 3D Structure of a Human in Image-Based Virtual Try-On Network, <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Kubo_UVTON_UV_Mapping_to_Consider_the_3D_Structure_of_a_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></p></li><li>LA-VITON: A Network for Looking-Attractive Virtual Try-On - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Lee_LA-VITON_A_Network_for_Looking-Attractive_Virtual_Try-On_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Robust Cloth Warping via Multi-Scale Patch Adversarial Loss for Virtual Try-On Framework - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/HBU/Ayush_Robust_Cloth_Warping_via_Multi-Scale_Patch_Adversarial_Loss_for_Virtual_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Powering Virtual Try-On via Auxiliary Human Segmentation Learning - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Ayush_Powering_Virtual_Try-On_via_Auxiliary_Human_Segmentation_Learning_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li><p>Generating High-Resolution Fashion Model Images Wearing Custom Outfits - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Yildirim_Generating_High-Resolution_Fashion_Model_Images_Wearing_Custom_Outfits_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></p><h4 id="ECCV-2018"><a href="#ECCV-2018" class="headerlink" title="ECCV 2018"></a>ECCV 2018</h4></li><li><p>Toward Characteristic-Preserving Image-based Virtual Try-On Network - <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/sergeywong/cp-vton" target="_blank" rel="noopener">Code</a></p></li><li><p>SwapNet: Garment Transfer in Single View Images - <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Amit_Raj_SwapNet_Garment_Transfer_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/andrewjong/SwapNet" target="_blank" rel="noopener">Code (unofficial)</a></p><h4 id="CVPR-2018"><a href="#CVPR-2018" class="headerlink" title="CVPR 2018"></a>CVPR 2018</h4></li><li><p>VITON: An Image-based Virtual Try-on Network - <a href="https://arxiv.org/abs/1711.08447" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/xthan/VITON" target="_blank" rel="noopener">Code/Model</a></p><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4></li><li><p>Keypoints-Based 2D Virtual Try-on Network System, JAKO 2020 - <a href="https://www.koreascience.or.kr/article/JAKO202010163508810.pdf" target="_blank" rel="noopener">Paper</a></p></li><li>Virtual Try-On With Generative Adversarial Networks: A Taxonomical Survey - <a href="https://www.igi-global.com/chapter/virtual-try-on-with-generative-adversarial-networks/260791" target="_blank" rel="noopener">Book chapter</a></li><li>LGVTON: A Landmark Guided Approach to Virtual Try-On - <a href="https://arxiv.org/abs/2004.00562v1" target="_blank" rel="noopener">Paper</a></li><li>SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On, WACV 2020 - <a href="http://openaccess.thecvf.com/content_WACV_2020/html/Jandial_SieveNet_A_Unified_Framework_for_Robust_Image-Based_Virtual_Try-On_WACV_2020_paper.html" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/levindabhi/SieveNet" target="_blank" rel="noopener">Code/Model (unofficial)</a></li><li>GarmentGAN: Photo-realistic Adversarial Fashion Transfer - <a href="https://arxiv.org/abs/2003.01894" target="_blank" rel="noopener">Paper</a></li><li>Toward Accurate and Realistic Virtual Try-on Through Shape Matching and Multiple Warps - <a href="https://arxiv.org/abs/2003.10817" target="_blank" rel="noopener">Paper</a></li><li>FashionFit Analysis of Mapping 3D Pose and neural body fit for custom virtual try-on, IEEE Access 2020 - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091008" target="_blank" rel="noopener">Paper</a></li><li>SP-VITON: shape-preserving image-based virtual try-on network, Multimedia Tools and Applications 2019 - <a href="https://link.springer.com/article/10.1007/s11042-019-08363-w" target="_blank" rel="noopener">Paper</a></li><li>VITON-GAN: Virtual Try-on Image Generator Trained with Adversarial Loss, Eurographics 2019 Posters - <a href="https://arxiv.org/abs/1911.07926v1" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/shionhonda/viton-gan" target="_blank" rel="noopener">Code/Model</a></li><li>Image-Based Virtual Try-on Network with Structural Coherence, ICIP 2019 - <a href="https://ieeexplore.ieee.org/document/8803811" target="_blank" rel="noopener">Paper</a></li><li>End-to-End Learning of Geometric Deformations of Feature Maps for Virtual Try-On - <a href="https://arxiv.org/abs/1906.01347v2" target="_blank" rel="noopener">Paper</a></li><li>M2E-Try On Net: Fashion from Model to Everyone - <a href="https://arxiv.org/abs/1811.08599v1" target="_blank" rel="noopener">Paper</a></li></ul><h2 id="3D-Virtual-Try-on"><a href="#3D-Virtual-Try-on" class="headerlink" title="3D Virtual Try-on"></a>3D Virtual Try-on</h2><h4 id="ECCV-2020-1"><a href="#ECCV-2020-1" class="headerlink" title="ECCV 2020"></a>ECCV 2020</h4><ul><li>BCNet: Learning Body and Cloth Shape from A Single Image - <a href="https://arxiv.org/pdf/2004.00214.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/jby1993/BCNet" target="_blank" rel="noopener">Code/Data</a></li><li>GAN-based Garment Generation Using Sewing Pattern Images - <a href="https://gamma.umd.edu/researchdirections/virtualtryon/garmentgeneration/" target="_blank" rel="noopener">Paper/Code/Model/Data</a></li><li>Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images - <a href="https://kv2000.github.io/2020/03/25/deepFashion3DRevisited/" target="_blank" rel="noopener">Paper/Data</a></li><li>SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing - <a href="https://virtualhumans.mpi-inf.mpg.de/sizer/" target="_blank" rel="noopener">Paper/Code/Data</a></li><li><p>CLOTH3D: Clothed 3D Humans - <a href="https://arxiv.org/pdf/1912.02792.pdf" target="_blank" rel="noopener">Paper</a></p><h4 id="CVPR-2020-1"><a href="#CVPR-2020-1" class="headerlink" title="CVPR 2020"></a>CVPR 2020</h4></li><li><p>Learning to Transfer Texture from Clothing Images to 3D Humans - <a href="http://virtualhumans.mpi-inf.mpg.de/pix2surf/" target="_blank" rel="noopener">Paper/Code</a></p></li><li>TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style - <a href="http://virtualhumans.mpi-inf.mpg.de/tailornet/" target="_blank" rel="noopener">Paper/Code/Data</a></li><li><p>Learning to Dress 3D People in Generative Clothing - <a href="https://cape.is.tue.mpg.de/" target="_blank" rel="noopener">Paper/Code/Data</a></p><h4 id="ICCV-2019-1"><a href="#ICCV-2019-1" class="headerlink" title="ICCV 2019"></a>ICCV 2019</h4></li><li><p>Multi-Garment Net: Learning to Dress 3D People from Images - <a href="http://virtualhumans.mpi-inf.mpg.de/mgn/" target="_blank" rel="noopener">Paper/Code/Data</a></p></li><li>3DPeople: Modeling the Geometry of Dressed Humans - <a href="https://arxiv.org/abs/1904.04571" target="_blank" rel="noopener">Paper</a></li><li><p>GarNet: A Two-Stream Network for Fast and Accurate 3D Cloth Draping - <a href="https://www.epfl.ch/labs/cvlab/research/garment-simulation/garnet/" target="_blank" rel="noopener">Paper/Data</a></p><h4 id="ECCV-2018-1"><a href="#ECCV-2018-1" class="headerlink" title="ECCV 2018"></a>ECCV 2018</h4></li><li><p>DeepWrinkles: Accurate and Realistic Clothing Modeling - <a href="https://arxiv.org/abs/1808.03417" target="_blank" rel="noopener">Paper</a></p><h4 id="CVPR-2018-1"><a href="#CVPR-2018-1" class="headerlink" title="CVPR 2018"></a>CVPR 2018</h4></li><li><p>Video Based Reconstruction of 3D People Models - <a href="http://gvv.mpi-inf.mpg.de/projects/wxu/VideoAvatar/" target="_blank" rel="noopener">Paper/Code/Data</a></p><h4 id="Others-1"><a href="#Others-1" class="headerlink" title="Others"></a>Others</h4></li><li><p>CloTH-VTON: Clothing Three-dimensional reconstruction for Hybrid image-based Virtual Try-ON, ACCV 2020 - <a href="https://minar09.github.io/clothvton/" target="_blank" rel="noopener">Project</a></p></li><li>Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On, ACM SCA 2020 - <a href="http://mslab.es/projects/FullyConvolutionalGraphVirtualTryOn" target="_blank" rel="noopener">Paper/Project</a></li><li>DeePSD: Automatic Deep Skinning And Pose Space Deformation For 3D Garment Animation - <a href="https://arxiv.org/pdf/2009.02715.pdf" target="_blank" rel="noopener">Paper</a></li><li>3D Reconstruction of Clothes using a Human Body Model and its Application to Image-based Virtual Try-On, CVPRW 2020 - <a href="https://minar09.github.io/c3dvton/" target="_blank" rel="noopener">Paper/Project</a></li><li>Learning-Based Animation of Clothing for Virtual Try-On, Eurographics 2019 - <a href="http://dancasas.github.io/projects/LearningBasedVirtualTryOn/index.html" target="_blank" rel="noopener">Paper/Project</a></li><li>Learning an Intrinsic Garment Space for Interactive Authoring of Garment Animation, SIGGRAPH Asia 2019 - <a href="http://geometry.cs.ucl.ac.uk/projects/2019/garment_authoring/" target="_blank" rel="noopener">Paper/Code</a></li><li>3D Virtual Garment Modeling from RGB Images, ISMAR 2019 - <a href="https://arxiv.org/abs/1908.00114" target="_blank" rel="noopener">Paper</a></li><li>Deep Garment Image Matting for a Virtual Try-on System, ICCVW 2019 - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Shin_Deep_Garment_Image_Matting_for_a_Virtual_Try-on_System_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Learning a Shared Shape Space for Multimodal Garment Design, SIGGRAPH Asia 2018 - <a href="http://geometry.cs.ucl.ac.uk/projects/2018/garment_design/" target="_blank" rel="noopener">Paper/Code/Data</a></li><li>Detailed Garment Recovery from a Single-View Image, ACM TOG 2018 - <a href="https://arxiv.org/abs/1608.01250" target="_blank" rel="noopener">Paper</a></li><li>ClothCap: Seamless 4D Clothing Capture and Retargeting, SIGGRAPH 2017 - <a href="http://clothcap.is.tue.mpg.de/" target="_blank" rel="noopener">Paper</a></li><li>Virtual Try-On through Image-Based Rendering, IEEE T-VCG 2013 - <a href="https://ieeexplore.ieee.org/document/6487501" target="_blank" rel="noopener">Paper</a></li><li>Markerless Garment Capture, ACM TOG 2008 - <a href="http://www.cs.ubc.ca/labs/imager/tr/2008/MarkerlessGarmentCapture/" target="_blank" rel="noopener">Paper/Data</a></li></ul><h2 id="Multi-Pose-Guided-Virtual-Try-on"><a href="#Multi-Pose-Guided-Virtual-Try-on" class="headerlink" title="Multi-Pose Guided Virtual Try-on"></a>Multi-Pose Guided Virtual Try-on</h2><ul><li><p>Down to the Last Detail: Virtual Try-on with Detail Carving - <a href="https://arxiv.org/abs/1912.06324v2" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/JDAI-CV/Down-to-the-Last-Detail-Virtual-Try-on-with-Detail-Carving" target="_blank" rel="noopener">Code/Model</a></p></li><li><p>Towards Multi-pose Guided Virtual Try-on Network, ICCV 2019 - <a href="https://arxiv.org/abs/1902.11026" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/thaithanhtuan/MyMGVTON" target="_blank" rel="noopener">Code</a></p></li><li><p>FIT-ME: IMAGE-BASED VIRTUAL TRY-ON WITH ARBITRARY POSES, ICIP 2019 - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803681&amp;casa_token=2CL5K9pwy1IAAAAA:OTa5P-h6RWj9BdQVvkxQURR8tDy4Eg1BZynYOizMyQACnE-zL_EHu2xRzyXBOWijP_cItaO4" target="_blank" rel="noopener">Paper</a></p></li><li><p>Virtually Trying on New Clothing with Arbitrary Poses, ACM MM 2019 - <a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3350946?casa_token=w7EzejnZIaEAAAAA:KvDBsi1xYswuQuzEdJO-rsTDvysnSLYlAYi1J2st5lf8lnyotm5-umPKQupGaMEPUGxyBzijUkA9" target="_blank" rel="noopener">Paper</a></p></li><li><p>FashionOn: Semantic-guided Image-based Virtual Try-on with Detailed Human and Clothing Information, ACM MM 2019 - <a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3351075?casa_token=7y85FCo6B-QAAAAA:diZbVYmcSK13bMQ94MzrMG_-VvVG_oNFoGpI8wCBFJ_dHEzYnLBAPn2ZwbAgj_pmOWFMD6_1hOuk" target="_blank" rel="noopener">Paper</a></p></li></ul><h2 id="Video-Virtual-Try-on"><a href="#Video-Virtual-Try-on" class="headerlink" title="Video Virtual Try-on"></a>Video Virtual Try-on</h2><ul><li>FW-GAN: Flow-Navigated Warping GAN for Video Virtual Try-On, ICCV 2019 - <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_FW-GAN_Flow-Navigated_Warping_GAN_for_Video_Virtual_Try-On_ICCV_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Unsupervised Image-to-Video Clothing Transfer, ICCVW 2019 - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Pumarola_Unsupervised_Image-to-Video_Clothing_Transfer_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li></ul><h2 id="Non-clothing-Virtual-Try-on"><a href="#Non-clothing-Virtual-Try-on" class="headerlink" title="Non-clothing Virtual Try-on"></a>Non-clothing Virtual Try-on</h2><ul><li>CA-GAN: Weakly Supervised Color Aware GAN for Controllable Makeup Transfer - <a href="https://arxiv.org/pdf/2008.10298.pdf" target="_blank" rel="noopener">Paper</a></li><li>Regularized Adversarial Training for Single-Shot Virtual Try-On, ICCVW 2019 - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Kikuchi_Regularized_Adversarial_Training_for_Single-Shot_Virtual_Try-On_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Disentangled Makeup Transfer with Generative Adversarial Network - <a href="https://arxiv.org/pdf/1907.01144v1.pdf" target="_blank" rel="noopener">Paper</a></li><li>PIVTONS: Pose Invariant Virtual Try-On Shoe with Conditional Image Completion, ACCV 2018 - <a href="https://winstonhsu.info/wp-content/uploads/2018/09/chou18PIVTONS.pdf" target="_blank" rel="noopener">Paper</a></li><li>Virtual Try-on of Eyeglasses using 3D Model of the Head - <a href="https://dl.acm.org/doi/pdf/10.1145/2087756.2087838" target="_blank" rel="noopener">Paper</a></li><li>A MIXED REALITY SYSTEM FOR VIRTUAL GLASSES TRY-ON - <a href="https://dl.acm.org/doi/pdf/10.1145/2087756.2087816" target="_blank" rel="noopener">Paper</a></li><li>A virtual try-on system in augmented reality using RGB-D cameras for footwear personalization - <a href="https://www.sciencedirect.com/science/article/abs/pii/S0278612514000594" target="_blank" rel="noopener">Paper</a></li></ul><h2 id="Pose-Guided-Human-Synthesis"><a href="#Pose-Guided-Human-Synthesis" class="headerlink" title="Pose-Guided Human Synthesis"></a>Pose-Guided Human Synthesis</h2><ul><li>PoNA: Pose-Guided Non-Local Attention for Human Pose Transfer, IEEE T-IP 2020 - <a href="https://ieeexplore.ieee.org/abstract/document/9222550" target="_blank" rel="noopener">Paper</a></li><li>Pose-Guided High-Resolution Appearance Transfer via Progressive Training - <a href="https://arxiv.org/pdf/2008.11898.pdf" target="_blank" rel="noopener">Paper</a></li><li>Recapture as You Want - <a href="https://arxiv.org/pdf/2006.01435.pdf" target="_blank" rel="noopener">Paper</a></li><li>Generating Person Images with Appearance-aware Pose Stylizer, IJCAI 2020 - <a href="https://arxiv.org/pdf/2007.09077.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/siyuhuang/PoseStylizer" target="_blank" rel="noopener">Code</a></li><li>Controllable Person Image Synthesis with Attribute-Decomposed GAN, CVPR 2020 - <a href="https://arxiv.org/pdf/2003.12267.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/menyifang/ADGAN" target="_blank" rel="noopener">Code</a></li><li>Deep Image Spatial Transformation for Person Image Generation, CVPR 2020 - <a href="https://arxiv.org/pdf/2003.00696v2.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/RenYurui/Global-Flow-Local-Attention" target="_blank" rel="noopener">Code</a></li><li>Neural Pose Transfer by Spatially Adaptive Instance Normalization, CVPR 2020 - <a href="https://arxiv.org/pdf/2003.07254v2.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/jiashunwang/Neural-Pose-Transfer" target="_blank" rel="noopener">Code</a></li><li>Guided Image-to-Image Translation with Bi-Directional Feature Transformation, ICCV 2019 - <a href="https://arxiv.org/pdf/1910.11328v1.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/vt-vl-lab/Guided-pix2pix" target="_blank" rel="noopener">Code</a></li><li>Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis, ICCV 2019 - <a href="https://arxiv.org/pdf/1909.12224.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/svip-lab/impersonator" target="_blank" rel="noopener">Code</a></li><li>ClothFlow: A Flow-Based Model for Clothed Person Generation, ICCV 2019 - <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Progressive Pose Attention for Person Image Generation, CVPR 2019 - <a href="https://arxiv.org/pdf/1904.03349.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/tengteng95/Pose-Transfer" target="_blank" rel="noopener">Code</a></li><li>Dense Intrinsic Appearance Flow for Human Pose Transfer, CVPR 2019 - <a href="https://arxiv.org/pdf/1903.11326v1.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/ly015/intrinsic_flow" target="_blank" rel="noopener">Code</a></li><li>Unsupervised Person Image Generation with Semantic Parsing Transformation, CVPR 2019, TPAMI 2020 - <a href="https://arxiv.org/pdf/1904.03379.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/SijieSong/person_generation_spt" target="_blank" rel="noopener">Code</a></li><li>Pose Guided Fashion Image Synthesis Using Deep Generative Model - <a href="https://arxiv.org/pdf/1906.07251.pdf" target="_blank" rel="noopener">Paper</a></li><li>Synthesizing Images of Humans in Unseen Poses, CVPR 2018 - <a href="https://arxiv.org/pdf/1804.07739.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/balakg/posewarp-cvpr2018" target="_blank" rel="noopener">Code</a></li><li>Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis, NeurIPS 2018 - <a href="https://arxiv.org/pdf/1810.11610.pdf" target="_blank" rel="noopener">Paper</a></li><li>Deformable GANs for Pose-based Human Image Generation, CVPR 2018 - <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Siarohin_Deformable_GANs_for_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a></li><li>Pose-Normalized Image Generation for Person Re-identification, ECCV 2018 - <a href="https://arxiv.org/pdf/1712.02225.pdf" target="_blank" rel="noopener">Paper</a></li><li>Disentangled Person Image Generation, CVPR 2018 - <a href="https://homes.esat.kuleuven.be/~liqianma/CVPR18_DPIG/index.html" target="_blank" rel="noopener">Paper/Code/Data</a></li><li>A Variational U-Net for Conditional Appearance and Shape Generation, CVPR 2018 - <a href="https://arxiv.org/pdf/1804.04694.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/CompVis/vunet" target="_blank" rel="noopener">Code</a></li><li>Human Appearance Transfer, CVPR 2018 - <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Human_Appearance_Transfer_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a></li><li>Pose guided person image generation, NeurIPS 2017 - <a href="https://arxiv.org/pdf/1705.09368.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/charliememory/Pose-Guided-Person-Image-Generation" target="_blank" rel="noopener">Code</a></li></ul><h2 id="Datasets-for-Virtual-Try-on"><a href="#Datasets-for-Virtual-Try-on" class="headerlink" title="Datasets for Virtual Try-on"></a>Datasets for Virtual Try-on</h2><ul><li>VITON - <a href="https://drive.google.com/file/d/1MxCUvKxejnwWnoZ-KoCyMCXo3TLhRuTo/view" target="_blank" rel="noopener">Download</a>, <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Han_VITON_An_Image-Based_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a></li><li>MPV - <a href="https://drive.google.com/drive/folders/1e3ThRpSj8j9PaCUw8IrqzKPDVJK_grcA" target="_blank" rel="noopener">Download</a>, <a href="https://arxiv.org/abs/1902.11026" target="_blank" rel="noopener">Paper</a></li><li>Deep Fashion3D - <a href="https://arxiv.org/abs/2003.12753" target="_blank" rel="noopener">Paper</a></li><li>Digital Wardrobe - <a href="http://virtualhumans.mpi-inf.mpg.de/mgn/" target="_blank" rel="noopener">Download/Paper/Project</a></li><li>TailorNet Dataset - <a href="https://github.com/zycliao/TailorNet_dataset" target="_blank" rel="noopener">Download</a>, <a href="http://virtualhumans.mpi-inf.mpg.de/tailornet/" target="_blank" rel="noopener">Project</a></li><li>CLOTH3D - <a href="https://arxiv.org/abs/1912.02792" target="_blank" rel="noopener">Paper</a></li><li>3DPeople - <a href="https://www.albertpumarola.com/research/3DPeople/index.html" target="_blank" rel="noopener">Project</a></li><li>THUman Dataset - <a href="http://www.liuyebin.com/deephuman/deephuman.html" target="_blank" rel="noopener">Project</a></li><li>Garment Dataset, Wang et al. 2018 - <a href="http://geometry.cs.ucl.ac.uk/projects/2018/garment_design/" target="_blank" rel="noopener">Project</a></li></ul><h2 id="Related-Conference-Workshops"><a href="#Related-Conference-Workshops" class="headerlink" title="Related Conference Workshops"></a>Related Conference Workshops</h2><ul><li>Workshop on Computer Vision for Fashion, Art and Design: <a href="https://sites.google.com/view/cvcreative2020" target="_blank" rel="noopener">CVPR 2020</a>, <a href="https://sites.google.com/view/cvcreative" target="_blank" rel="noopener">ICCV 2019</a>, <a href="https://sites.google.com/view/eccvfashion" target="_blank" rel="noopener">ECCV 2018</a></li><li>Workshop on Towards Human-Centric Image/Video Synthesis: <a href="https://vuhcs.github.io/" target="_blank" rel="noopener">CVPR 2020</a>, <a href="https://vuhcs.github.io/vuhcs-2019/index.html" target="_blank" rel="noopener">CVPR 2019</a></li></ul><h2 id="Related-Repositories"><a href="#Related-Repositories" class="headerlink" title="Related Repositories"></a>Related Repositories</h2><ul><li><a href="https://github.com/ayushidalmia/awesome-fashion-ai" target="_blank" rel="noopener">awesome-fashion-ai</a></li><li><a href="https://github.com/lzhbrian/Cool-Fashion-Papers" target="_blank" rel="noopener">Cool Fashion Papers</a></li><li><a href="https://github.com/lzhbrian/Clothes-3D" target="_blank" rel="noopener">Clothes-3D</a></li><li><a href="https://github.com/lijiaman/awesome-3d-human" target="_blank" rel="noopener">Awesome 3D Human</a></li><li><a href="https://github.com/openMVG/awesome_3DReconstruction_list" target="_blank" rel="noopener">Awesome 3D reconstruction list</a></li><li><a href="https://github.com/chenweikai/Body_Reconstruction_References" target="_blank" rel="noopener">Human Body Reconstruction</a></li></ul><h4 id="Pull-requests-are-welcome"><a href="#Pull-requests-are-welcome" class="headerlink" title="Pull requests are welcome!"></a>Pull requests are welcome!</h4>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近跟项目在做一些，虚拟试衣Virtual Try-on (VTON）的工作，记录一下调研的数据以及开源的论文以及模型。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="VTON" scheme="http://yuanquanquan.top/tags/VTON/"/>
    
  </entry>
  
  <entry>
    <title>树莓派4B+英特尔神经计算棒(Intel Movidius Neural Computing Stick)-YOLOV3目标检测(3)</title>
    <link href="http://yuanquanquan.top/2020/20201029/"/>
    <id>http://yuanquanquan.top/2020/20201029/</id>
    <published>2020-10-28T16:18:02.000Z</published>
    <updated>2020-10-29T02:06:36.550Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列的最后一篇，NCS这个东西感觉上跑demo是可用的，但是用在产品上开发工作量怕是不小，需要转换成IR文件。调用方式比较复杂，支持的东西不太全。Intel的文档已经算可以了，但是还是少。算力的话标准的边缘端算力，被别人的i7完爆，和我的i7一样，哭。</p></blockquote><a id="more"></a><h2 id="模型在树莓派上的部署过程"><a href="#模型在树莓派上的部署过程" class="headerlink" title="模型在树莓派上的部署过程"></a>模型在树莓派上的部署过程</h2><ul><li>选择预训练模型；</li><li>使用模型优化器，来转换模型；</li><li>最后在树莓派上推理模型。</li></ul><h3 id="五、如何训练自己的模型"><a href="#五、如何训练自己的模型" class="headerlink" title="五、如何训练自己的模型"></a>五、如何训练自己的模型</h3><h4 id="准备和优化训练好的模型"><a href="#准备和优化训练好的模型" class="headerlink" title="准备和优化训练好的模型"></a>准备和优化训练好的模型</h4><p>推断引擎可以<em>部署</em>经过任何受支持深度学习框架训练的网络模型：Caffe<em>、TensorFlow</em>、Kaldi<em>、MXNet</em> 或转换为 ONNX* 格式。要执行推断，推断引擎不使用原始模型操作，而是使用其中间表示文件  (IR)，后者已为在目标端点设备上执行进行过优化。要为训练好的模型生成中间表示文件，会使用模型优化器工具。</p><h4 id="模型优化器如何工作"><a href="#模型优化器如何工作" class="headerlink" title="模型优化器如何工作"></a>模型优化器如何工作</h4><p>模型优化器将模型加载到内存中、读取模型、构建模型的内部表示文件、优化模型，并生成中间表示文件。中间表示文件是推断引擎接受的唯一格式。</p><blockquote><p><strong>注意</strong>：模型优化器不推断模型。模型优化器是推断发生之前运行的一种离线工具。</p></blockquote><p>模型优化器有两个主要用途：</p><ul><li><strong>生成有效的中间表示文件</strong>。如果这个主要转换文件无效，则推断引擎无法运行。模型优化器的主要责任是生成构成中间表示文件的两个文件（<code>.xml</code>和<code>.bin</code>）。</li><li><strong>生成优化的中间表示文件</strong>。预训练的模型包含对于训练至关重要的层，如<code>Dropout</code>层。这些层在推断过程中毫无用处，反而可能增加推断时间。在许多情况下，这些操作可以自动从由此产生的中间表示文件中移除。但是，如果可以将一组操作表示为单个数学操作，然后成为模型图表中的单个操作节点，那么模型优化器会识别这一模式并用该单个操作节点代替这一组操作节点。结果就是一个运行节点数量比原始模型少的中间表示文件。这就降低了推断时间。</li></ul><p>要生成有效的中间表示文件，模型优化器必须能够读取原始模型包含的操作、处理它们的属性，并以中间表示文件的格式表示它们，同时保持所产生的中间表示文件的有效性。由此产生的模型由<a href="https://docs.openvinotoolkit.org/cn/latest/_docs_ops_opset.html" target="_blank" rel="noopener">操作规格</a>中描述的操作组成。</p><h2 id="有关模型，需要了解哪些内容"><a href="#有关模型，需要了解哪些内容" class="headerlink" title="有关模型，需要了解哪些内容"></a>有关模型，需要了解哪些内容</h2><p>在已知的框架和神经网络拓扑中存在着许多普通层。这类层的例子包括<code>Convolution</code>、<code>Pooling</code>和<code>Activation</code>。要读取原始模型并生成模型的中间表示文件，模型优化器必须能够处理这些层。</p><p>它们的完整列表取决于框架，并可以在<a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html" target="_blank" rel="noopener">受支持的框架层</a>部分中找到。如果您的拓扑仅包含来自层列表的层，像大多数用户所使用的拓扑一样，那么模型优化器便可以轻松创建中间表示文件。在这之后，您就可以继续使用推断引擎。</p><p>但是，如果您使用的拓扑含有模型优化器无法识别的层，请查看<a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html" target="_blank" rel="noopener">模型优化器中的自定义层</a>以了解如何处理自定义层。</p><h2 id="模型优化器目录结构"><a href="#模型优化器目录结构" class="headerlink" title="模型优化器目录结构"></a>模型优化器目录结构</h2><p>在使用 OpenVINO™ 工具套件或英特尔® Deep Learning Deployment Toolkit 进行安装后，模型优化器文件夹具有以下结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">|-- model_optimizer</span><br><span class="line"></span><br><span class="line">​    |-- extensions</span><br><span class="line"></span><br><span class="line">​        |-- front - Front-End framework agnostic transformations (operations output shapes are not defined yet). </span><br><span class="line"></span><br><span class="line">​            |-- caffe - Front-End Caffe-specific transformations and Caffe layers extractors</span><br><span class="line"></span><br><span class="line">​                |-- CustomLayersMapping.xml.example - example of file for registering custom Caffe layers (compatible</span><br></pre></td></tr></table></figure><p>with the 2017R3 release)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">            |-- kaldi - Front-End Kaldi-specific transformations and Kaldi operations extractors</span><br><span class="line"></span><br><span class="line">​            |-- mxnet - Front-End MxNet-specific transformations and MxNet symbols extractors</span><br><span class="line"></span><br><span class="line">​            |-- onnx - Front-End ONNX-specific transformations and ONNX operators extractors            </span><br><span class="line"></span><br><span class="line">​            |-- tf - Front-End TensorFlow-specific transformations, TensorFlow operations extractors, sub-graph</span><br></pre></td></tr></table></figure><p>replacements configuration files. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">        |-- middle - Middle-End framework agnostic transformations (layers output shapes are defined).</span><br><span class="line"></span><br><span class="line">​        |-- back - Back-End framework agnostic transformations (preparation for IR generation).        </span><br><span class="line"></span><br><span class="line">​    |-- mo</span><br><span class="line"></span><br><span class="line">​        |-- back - Back-End logic: contains IR emitting logic</span><br><span class="line"></span><br><span class="line">​        |-- front - Front-End logic: contains matching between Framework-specific layers and IR specific, calculation of output shapes for each registered layer</span><br><span class="line"></span><br><span class="line">​        |-- graph - Graph utilities to work with internal IR representation</span><br><span class="line"></span><br><span class="line">​        |-- middle - Graph transformations - optimizations of the model</span><br><span class="line"></span><br><span class="line">​        |-- pipeline - Sequence of steps required to create IR for each framework</span><br><span class="line"></span><br><span class="line">​        |-- utils - Utility functions</span><br><span class="line"></span><br><span class="line">​    |-- tf_call_ie_layer - Source code that enables TensorFlow fallback in Inference Engine during model inference</span><br><span class="line"></span><br><span class="line">​    |-- mo.py - Centralized entry point that can be used for any supported framework</span><br><span class="line"></span><br><span class="line">​    |-- mo_caffe.py - Entry point particularly for Caffe</span><br><span class="line"></span><br><span class="line">​    |-- mo_kaldi.py - Entry point particularly for Kaldi</span><br><span class="line"></span><br><span class="line">​    |-- mo_mxnet.py - Entry point particularly for MXNet</span><br><span class="line"></span><br><span class="line">​    |-- mo_onnx.py - Entry point particularly for ONNX</span><br><span class="line"></span><br><span class="line">​    |-- mo_tf.py - Entry point particularly for TensorFlow</span><br></pre></td></tr></table></figure><p>以下部分提供了有关如何使用模型优化器的信息，从配置工具、生成针对给定模型的中间表示文件，到根据您的需求定制工具：</p><ul><li><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Config_Model_Optimizer.html" target="_blank" rel="noopener">配置模型优化器</a></li><li><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model.html" target="_blank" rel="noopener">将模型转换为中间表示文件</a></li><li><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html" target="_blank" rel="noopener">模型优化器中的自定义层</a></li><li><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Model_Optimization_Techniques.html" target="_blank" rel="noopener">模型优化技术</a></li><li><p><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html" target="_blank" rel="noopener">有关模型优化器的常见问题</a></p></li><li><p>](<a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html" target="_blank" rel="noopener">https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html</a>)</p></li></ul><p><strong>第一步：</strong></p><p>找到label文件</p><p>C:\Users\（你的用户名）\Documents\Intel\OpenVINO\openvino_models\ir\public\squeezenet1.1\FP16\squeezenet1.1.labels。</p><p>找到模型文件</p><p>C:\Users\（你的用户名）\Documents\Intel\OpenVINO\openvino_models\models\public\squeezenet1.1\squeezenet1.1.caffemodel</p><p>找一个input</p><p>C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\demo\car.png</p><p><strong>第二步：</strong></p><p>转换模型，用到model_optimizer下的mo.py. 把上面的文件拷到了D:OPENVN下，用下面的代码转换。就能转换出.xml的文件。</p><p><img src="https://pic1.zhimg.com/80/v2-4d0c114cf7184e8a97488e52b54d71cc_720w.png" alt="img"></p><p><strong>第三步：</strong></p><p>跑转换好的文件。。由于这一步骤其实主要是展示如何转换模型。。。所以我们还是用demo自带的业务逻辑。在这个文件夹夹下有刚才建立好的classification_sample_async.exe文件。。。下面这个文件夹下找。</p><p>C:\Users<username>\Documents\Intel\OpenVINO\inference_engine_samples_build\intel64\Release</username></p><p>cd到上面这个文件夹。</p><p>然后运行</p><p> .\classification_sample_async.exe -i “D:\OPENVN\car.png” -m “D:\OPENVN\squeezenet1.1.xml” -d MYRIAD</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本系列的最后一篇，NCS这个东西感觉上跑demo是可用的，但是用在产品上开发工作量怕是不小，需要转换成IR文件。调用方式比较复杂，支持的东西不太全。Intel的文档已经算可以了，但是还是少。算力的话标准的边缘端算力，被别人的i7完爆，和我的i7一样，哭。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Intel-Neural-Compute-stick" scheme="http://yuanquanquan.top/tags/Intel-Neural-Compute-stick/"/>
    
  </entry>
  
  <entry>
    <title>树莓派4B+英特尔神经计算棒(Intel Movidius Neural Computing Stick)-YOLOV3目标检测(2)</title>
    <link href="http://yuanquanquan.top/2020/20201028/"/>
    <id>http://yuanquanquan.top/2020/20201028/</id>
    <published>2020-10-28T10:18:53.000Z</published>
    <updated>2020-10-28T10:45:46.599Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>接上一章，分别在树莓派和Windows上安装了NCS环境，接下来进行格式转换，在windows训练Caffe或TensorFlow模型，编译成NCS可以执行的graph；测试端则面向ncs python mvnc api编程，可以运行在树莓派上raspbian stretch版本，也可以运行在训练端这种机器上。</p></blockquote><a id="more"></a><h3 id="三、格式转换"><a href="#三、格式转换" class="headerlink" title="三、格式转换"></a>三、格式转换</h3><h4 id="1、下载darknet版官网训练模型"><a href="#1、下载darknet版官网训练模型" class="headerlink" title="1、下载darknet版官网训练模型"></a>1、下载darknet版官网训练模型</h4><p>如果没有现成的，可以从<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">pjreddie网站</a>下载</p><p><a href="https://blog.csdn.net/c20081052/article/details/90056746" target="_blank" rel="noopener">Yolo V3 COCO weights</a>（237MB），<a href="https://pjreddie.com/media/files/yolov3-tiny.weights" target="_blank" rel="noopener">Tiny Yolo V3 COCO weights</a>（34MB）， 标签文件 <a href="https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names" target="_blank" rel="noopener">coco.names</a></p><h4 id="2、将weights文件转换为pb文件"><a href="#2、将weights文件转换为pb文件" class="headerlink" title="2、将weights文件转换为pb文件"></a>2、将weights文件转换为pb文件</h4><p>OpenVINO不支持直接使用Yolo V3的.weights文件，目前仅支持ONNX、TensorFlow、Caffe和MXNet。需要先<a href="https://github.com/PINTO0309/OpenVINO-YoloV3" target="_blank" rel="noopener">把.weights文件转换成TensorFlow的.pb文件。</a><br> 自己在桌面新建一个文件夹例如raspberry，将下载的仓库放进新建的文件夹进行解压，并将前面下载的<strong>两个权重</strong>放进解压的仓库中。</p><p><strong>转换yolov3.weights的指令如下：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert_weights_pb.py --weights_file yolov3-tiny.weights --tiny True --size 416 --output_graph frozen_darknet_yolov3_tiny_model.pb --data_format NHWC</span><br></pre></td></tr></table></figure><p>上面指令是将<strong>yolov3-tiny.weights转为pb文件</strong>，传的参数：–tiny必须指定True, –data_format <strong>必须是NHWC</strong>，否则后面你拿转成功的pb，然后再去转xml和bin，然后做目标检测会遇到此类错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[ ERROR ]  Cannot infer shapes or values for node &quot;detector/yolo-v3/meshgrid_1/mul_1/YoloRegion&quot;.</span><br><span class="line">[ ERROR ]  index 2 is out of bounds for axis 0 with size 2</span><br><span class="line">[ ERROR ]</span><br><span class="line">[ ERROR ]  It can happen due to bug in custom shape infer function &lt;function RegionYoloOp.regionyolo_infer at 0x000001DE82253EA0&gt;.</span><br><span class="line">[ ERROR ]  Or because the node inputs have incorrect values/shapes.</span><br><span class="line">[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).</span><br><span class="line">[ ERROR ]  Run Model Optimizer with --log_level=DEBUG for more information.</span><br><span class="line">[ ERROR ]  Stopped shape/value propagation at &quot;detector/yolo-v3/meshgrid_1/mul_1/YoloRegion&quot; node.</span><br><span class="line"> For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html),</span><br></pre></td></tr></table></figure><p>错误解析就是数据维度格式不对！参考： <a href="https://software.intel.com/en-us/node/802233" target="_blank" rel="noopener">https://software.intel.com/en-us/node/802233</a><br> 所以必须指定数据格式为NHWC （这种格式支持神经计算棒和CPU设备）</p><p><strong>转换yolov3.weights的指令如下：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python convert_weights_pb.py --weights_file yolov3.weights --size 416 --data_format NHWC</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020010819375385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt><br> 会在OpenVINO-YoloV3-master目录下生成<strong>frozen_darknet_yolov3_tiny_model.pb</strong> 和 <strong>frozen_darknet_yolov3_model.pb</strong> ；</p><p><img src="https://img-blog.csdnimg.cn/20200110184001358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt></p><h4 id="3、将pb模型文件转为IR文件"><a href="#3、将pb模型文件转为IR文件" class="headerlink" title="3、将pb模型文件转为IR文件"></a>3、将pb模型文件转为IR文件</h4><p>将之前安装好的openvino文件夹里的deployment_tools文件夹拷贝出来。<br> <img src="https://img-blog.csdnimg.cn/20200110182837145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt><br> 将拷贝的文件夹粘贴进raspberry文件夹。<br> <img src="https://img-blog.csdnimg.cn/20200110183739695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt><br> 切到deploment_tools目录下的model_optimizer并运行以下命令：<br> <strong>转yolov3.pb的指令是：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E:\桌面\raspberry\deployment_tools\model_optimizer&gt;python mo_tf.py --input_model E:\桌面\raspberry\OpenVINO-YoloV3-master\frozen_darknet_yolov3_model.pb --tensorflow_use_custom_operations_config E:\桌面\raspberry\deployment_tools\model_optimizer\extensions\front\tf\yolo_v3.json --input_shape [1,416,416,3] --data_type=FP16</span><br></pre></td></tr></table></figure><p>相应文件的路径切换成自己的~需要说明的是树莓派据说支持数据类型是半浮点型的FP16, 官网很多是FP32的是针对PC的。</p><p><strong>转yolov3-tiny.pb的指令如下：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E:\桌面\raspberry\deployment_tools\model_optimizer&gt;python mo_tf.py --input_model E:\桌面\raspberry\OpenVINO-YoloV3-master\frozen_darknet_yolov3_tiny_model.pb --tensorflow_use_custom_operations_config E:\桌面\raspberry\deployment_tools\model_optimizer\extensions\front\tf\yolo_v3_tiny.json --input_shape [1,416,416,3] --data_type=FP16</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200110191229870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>执行完上述指令会生成以下六个文件：<br> <img src="https://img-blog.csdnimg.cn/20200110192121519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 转换过程中，需要你安装了如下依赖项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. tensorflow&gt;=1.2.0 </span><br><span class="line">2. networkx&gt;=1.11</span><br><span class="line">3. numpy&gt;=1.12.0</span><br><span class="line">4. test-generator==0.1.1</span><br><span class="line">5. defusedxml&gt;=0.5.0</span><br></pre></td></tr></table></figure><p>用pip安装就行</p><h3 id="四、在树莓派上部署"><a href="#四、在树莓派上部署" class="headerlink" title="四、在树莓派上部署"></a>四、在树莓派上部署</h3><h4 id="1、环境激活"><a href="#1、环境激活" class="headerlink" title="1、环境激活"></a>1、环境激活</h4><p>打开树莓派，终端激活OpenVINO工具包环境变量，后续操作都在这个终端窗口下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /home/pi/Downloads/l_openvino_toolkit_runtime_raspbian_p_2019.3.334/bin/setupvars.sh</span><br></pre></td></tr></table></figure><p>当完成如上操作后，会在终端显示如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[setupvars.sh] OpenVINO environment initialized</span><br></pre></td></tr></table></figure><h4 id="2、编译和运行yoloV3"><a href="#2、编译和运行yoloV3" class="headerlink" title="2、编译和运行yoloV3"></a>2、编译和运行yoloV3</h4><p>将上文转换的六个模型文件拷贝到树莓派上的/home/pi/Downloads/yolo_model目录下（可更改成你自己的目录）；并将coco.names复制两份，分别重命名为：frozen_darknet_yolov3_model.labels 和frozen_darknet_yolov3_tiny_model.labels ，所以/home/pi/Downloads/yolo_model目录下共有以下8个文件<br> <img src="https://img-blog.csdnimg.cn/20200110200435754.png" alt="在这里插入图片描述"><br> 接着</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir build &amp;&amp; cd build</span><br></pre></td></tr></table></figure><p><strong>编译</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=&quot;-march=armv7-a&quot; /home/pi/Downloads/l_openvino_toolkit_runtime_raspbian_p_2019.3.334/deployment_tools/inference_engine/samples</span><br></pre></td></tr></table></figure><p><strong>编译目标检测例子</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j2 object_detection_demo_yolov3_async</span><br></pre></td></tr></table></figure><p>编译完成后，会在/build/armv 7l/Release下生成可执行文件：object_detection_demo_yolov3_async；接下来就是给执行文件传入模型文件就可以跑深度学习了！</p><p><strong>（测摄像头）运行如下指令：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./armv7l/Release/object_detection_demo_yolov3_async -m /home/pi/Downloads/yolo_model/frozen_darknet_yolov3_model.xml -d MYRIAD -i cam</span><br></pre></td></tr></table></figure><p>此处-i cam表示读取的是camera；</p><p><strong>（测视频文件）运行如下指令：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./armv7l/Release/object_detection_demo_yolov3_async -m /home/pi/Downloads/yolo_model/frozen_darknet_yolov3_model.xml -d MYRIAD -i /path_to_video</span><br></pre></td></tr></table></figure><p>测试视频需要给-i指定视频路径；</p><h4 id="3、编译与运行yoloV3-tiny"><a href="#3、编译与运行yoloV3-tiny" class="headerlink" title="3、编译与运行yoloV3 tiny"></a>3、编译与运行yoloV3 tiny</h4><p>同样的编译和执行流程，在编译前需要你更改下源cpp文件，先找到yolo  v3的源文件，在：/home/pi/Downloads/l_openvino_toolkit_runtime_raspbian_p_2019.3.334/deployment_tools/inference_engine/samples/object_detection_demo_yolov3_async/这个目录下；你可以发现这个目录下有<br> <strong>main.cpp<br> CMakelists.txt<br> object_detection_demo_yolov3_async.hpp<br> README.md</strong><br> 我们需要更改<strong>main.cpp</strong>,更改前先备份一份main.cpp<br> <strong>更改：</strong> 第126行左右，注释掉原先yolov3的锚点框尺寸，更改成yolo v3 tiny的（共12个数：10, 14, 23, 27, 37, 58, 81, 82, 135, 169, 344, 319）<br> 更改完保存并退出。<br> 然后重新编译:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j2 object_detection_demo_yolov3_async</span><br></pre></td></tr></table></figure><p>再编译</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./armv7l/Release/object_detection_demo_yolov3_async -m /home/pi/Downloads/yolo_model/frozen_darknet_yolov3_tiny_model.xml -d MYRIAD -i cam</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;接上一章，分别在树莓派和Windows上安装了NCS环境，接下来进行格式转换，在windows训练Caffe或TensorFlow模型，编译成NCS可以执行的graph；测试端则面向ncs python mvnc api编程，可以运行在树莓派上raspbian stretch版本，也可以运行在训练端这种机器上。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Intel-Neural-Compute-stick" scheme="http://yuanquanquan.top/tags/Intel-Neural-Compute-stick/"/>
    
  </entry>
  
  <entry>
    <title>树莓派4B+英特尔神经计算棒(Intel Movidius Neural Computing Stick)-YOLOV3目标检测(1)</title>
    <link href="http://yuanquanquan.top/2020/20201027/"/>
    <id>http://yuanquanquan.top/2020/20201027/</id>
    <published>2020-10-27T02:28:17.000Z</published>
    <updated>2020-10-29T02:07:19.880Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在看各种AI的加速方案以及边缘计算，jetsonnano、英伟达TX2、华为昇腾啊什么的</p><p>jetson nano 感觉性价比不如rk3399，还有Intel的计算棒。做终端应用成本太高，但是。。。。。3399得1000+了啊</p><p>下面就是关于Intel神经棒二代的上手教程。</p></blockquote><a id="more"></a><h2 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h2><p><a href="https://link.zhihu.com/?target=https%3A//developer.movidius.com/" target="_blank" rel="noopener">Intel Movidius Neural Computing Stick</a> </p><p><img src="https://pic2.zhimg.com/80/v2-68d4491ad612eb93863b60750f56819d_720w.jpg" alt></p><p>NCS是一个专用计算芯片，但能起到类似GPU对神经网络运算的加速作用。</p><p>SDK是开源的：<a href="https://link.zhihu.com/?target=https%3A//github.com/movidius/ncsdk" target="_blank" rel="noopener">https://github.com/movidius/ncsdk</a></p><p>提问不在GitHub issue里，而是在一个专门的论坛：<a href="https://link.zhihu.com/?target=https%3A//ncsforum.movidius.com/" target="_blank" rel="noopener">https://ncsforum.movidius.com/</a></p><p>虽然目前NCSDK支持的框架包含Tensorflow和Caffe，但并不是支持所有的模型，目前已支持的模型列表可以在这里查到：<a href="https://link.zhihu.com/?target=https%3A//github.com/movidius/ncsdk/releases" target="_blank" rel="noopener">https://github.com/movidius/ncsdk/releases</a></p><h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p>ncsdk的环境分为两部分，训练端和测试端。</p><ol><li>训练端通常是一个Ubuntu 带GPU主机，训练Caffe或TensorFlow模型，编译成NCS可以执行的graph；</li><li>测试端则面向ncs python mvnc api编程，可以运行在树莓派上raspbian stretch版本，也可以运行在训练端这种机器上。</li></ol><h2 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h2><h4 id="一、在树莓派上安装NCS环境"><a href="#一、在树莓派上安装NCS环境" class="headerlink" title="一、在树莓派上安装NCS环境"></a>一、在树莓派上安装NCS环境</h4><h4 id="二、在windows上安装NCS环境（格式转换用到）"><a href="#二、在windows上安装NCS环境（格式转换用到）" class="headerlink" title="二、在windows上安装NCS环境（格式转换用到）"></a>二、在windows上安装NCS环境（格式转换用到）</h4><h4 id="三、格式转换"><a href="#三、格式转换" class="headerlink" title="三、格式转换"></a>三、格式转换</h4><h4 id="四、在树莓派上部署"><a href="#四、在树莓派上部署" class="headerlink" title="四、在树莓派上部署"></a>四、在树莓派上部署</h4><h2 id="Step1-在树莓派上安装NCS环境"><a href="#Step1-在树莓派上安装NCS环境" class="headerlink" title="Step1 在树莓派上安装NCS环境"></a>Step1 在树莓派上安装NCS环境</h2><p><strong>Operating Systems</strong></p><ul><li>Raspbian* Buster, 32-bit</li><li>Raspbian* Stretch, 32-bit</li></ul><p><strong>Software</strong></p><ul><li>CMake* 3.7.2 or higher</li><li>Python* 3.5, 32-bit</li></ul><h4 id="1、下载OpenVINO-toolkit-for-Raspbian"><a href="#1、下载OpenVINO-toolkit-for-Raspbian" class="headerlink" title="1、下载OpenVINO toolkit for Raspbian"></a>1、下载<a href="https://download.01.org/opencv/2020/openvinotoolkit/" target="_blank" rel="noopener">OpenVINO toolkit for Raspbian</a></h4><h4 id="2、树莓派上安装OpenVINO工具包"><a href="#2、树莓派上安装OpenVINO工具包" class="headerlink" title="2、树莓派上安装OpenVINO工具包"></a>2、树莓派上安装OpenVINO工具包</h4><p>可以参考官网教程：<a href="https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_raspbian.html" target="_blank" rel="noopener">Install OpenVINO™ toolkit for Raspbian* OS</a><br> 下载完后工具包位于/home/pi/Downloads目录下，如果不是，可以创建一个Downloads目录并把工具包放在此目录下<br> <strong>切换至Downloads目录下:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd  ~/Downloads</span><br></pre></td></tr></table></figure><p><strong>配置路径与环境：</strong><br> 执行以下命令，会自动对setupvars.sh文件做修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s|&lt;INSTALLDIR&gt;|$(pwd)/l_openvino_toolkit_runtime_raspbian_p_2019.3.334|&quot; l_openvino_toolkit_runtime_raspbian_p_2019.3.334/bin/setupvars.sh</span><br></pre></td></tr></table></figure><p>配置虚拟环境：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">suorce l_openvino_toolkit_runtime_raspbian_p_2019.3.334/bin/setupvars.sh</span><br></pre></td></tr></table></figure><p><strong>添加USB规则：</strong><br> 将当前Linux用户添加到users组：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -a -G users &quot;$(whoami)&quot;</span><br></pre></td></tr></table></figure><p>注：1、”$(whoami)”是用户名，2、这里要说的是我们现在是root用户，如果打开新窗口的话起始用户是pi，所以出现[  setupvars.sh] OpenVINO environment  initialized，是对于pi用户来说的。如果在新窗口中用root执行程序，其实并没有成功加载[ setupvars.sh]  OpenVINO environment initialized，需要自己再执行一遍<br>  source/home/pi/Downloads/l_openvino_toolkit_runtime_raspbian_p_2019.3.334/bin/setupvars.sh，才能给root用户配置好OpenVINO environment initialized。<br> 接下来配置USB规则，执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh l_openvino_toolkit_runtime_raspbian_p_2019.3.334/install_dependencies/install_NCS_udev_rules.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20191230164606485.png" alt></p><p><strong>demo测试验证安装是否成功</strong><br> 运行人脸检测的实例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd l_openvino_toolkit_runtime_raspbian_p_2019.3.334/deployment_tools/inference_engine/samples</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=&quot;-march=armv7-a&quot;</span><br><span class="line">make -j2 object_detection_sample_ssd</span><br></pre></td></tr></table></figure><p>编译完成后，下载网络和权重文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget --no-check-certificate https://download.01.org/opencv/2019/open_model_zoo/R1/models_bin/face-detection-adas-0001/FP16/face-detection-adas-0001.bin</span><br><span class="line">wget --no-check-certificate https://download.01.org/opencv/2019/open_model_zoo/R1/models_bin/face-detection-adas-0001/FP16/face-detection-adas-0001.xml</span><br></pre></td></tr></table></figure><p>然后自己在网上找一张人脸的图片，执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./armv7l/Release/object_detection_sample_ssd -m face-detection-adas-0001.xml -d MYRIAD -i &lt;path_to_image&gt;图片的绝对路径</span><br></pre></td></tr></table></figure><p>如果运行成功，会在build文件夹下输出一种观念out_0.bmp图片，即表示计算棒运行成功。</p><p><strong>Opencv+python api 调用方法</strong><br> 新建一个文件夹，先建立一个face_detection.py文件，写入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="comment"># Load the model </span></span><br><span class="line">net = cv.dnn.readNet(<span class="string">'face-detection-adas-0001.xml'</span>, <span class="string">'face-detection-adas-0001.bin'</span>) </span><br><span class="line"><span class="comment"># Specify target device </span></span><br><span class="line">net.setPreferableTarget(cv.dnn.DNN_TARGET_MYRIAD)</span><br><span class="line"><span class="comment"># Read an image </span></span><br><span class="line">frame = cv.imread(<span class="string">'/path/to/image'</span>)</span><br><span class="line"><span class="comment"># Prepare input blob and perform an inference </span></span><br><span class="line">blob = cv.dnn.blobFromImage(frame, size=(<span class="number">672</span>, <span class="number">384</span>), ddepth=cv.CV_8U) net.setInput(blob) </span><br><span class="line">out = net.forward()</span><br><span class="line"><span class="comment"># Draw detected faces on the frame </span></span><br><span class="line"><span class="keyword">for</span> detection <span class="keyword">in</span> out.reshape(<span class="number">-1</span>, <span class="number">7</span>): </span><br><span class="line">    confidence = float(detection[<span class="number">2</span>]) </span><br><span class="line">    xmin = int(detection[<span class="number">3</span>] * frame.shape[<span class="number">1</span>]) </span><br><span class="line">    ymin = int(detection[<span class="number">4</span>] * frame.shape[<span class="number">0</span>]) </span><br><span class="line">    xmax = int(detection[<span class="number">5</span>] * frame.shape[<span class="number">1</span>]) </span><br><span class="line">    ymax = int(detection[<span class="number">6</span>] * frame.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> confidence &gt; <span class="number">0.5</span>:</span><br><span class="line">        cv.rectangle(frame, (xmin, ymin), (xmax, ymax), color=(<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>))</span><br><span class="line"><span class="comment"># Save the frame to an image file </span></span><br><span class="line">cv.imwrite(<span class="string">'out.png'</span>, frame)</span><br></pre></td></tr></table></figure><p>在文件夹中放入刚刚下载的那两个文件：face-detection-adas-0001.bin和face-detection-adas-0001.xml还有用于检测用的脸的图片，执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 face_detection.py</span><br></pre></td></tr></table></figure><h3 id="Workflow-for-Raspberry-Pi"><a href="#Workflow-for-Raspberry-Pi" class="headerlink" title="Workflow for Raspberry Pi"></a>Workflow for Raspberry Pi</h3><p>If you want to use your model for inference, the model must be  converted to the .bin and .xml Intermediate Representation (IR) files  that are used as input by Inference Engine. OpenVINO™ toolkit support on Raspberry Pi only includes the Inference Engine module of the Intel®  Distribution of OpenVINO™ toolkit. The Model Optimizer is not supported  on this platform. To get the optimized models you can use one of the  following options:</p><ul><li><p>Download a set of ready-to-use pre-trained models for the appropriate version of OpenVINO from the Intel® Open Source  Technology Center:</p><ul><li>Models for the 2020.1 release of OpenVINO are available at <a href="https://download.01.org/opencv/2020/openvinotoolkit/2020.1/open_model_zoo/" target="_blank" rel="noopener">https://download.01.org/opencv/2020/openvinotoolkit/2020.1/open_model_zoo/</a>.</li><li>Models for the 2019 R1 release of OpenVINO are available at <a href="https://download.01.org/opencv/2019/open_model_zoo/R1/" target="_blank" rel="noopener">https://download.01.org/opencv/2019/open_model_zoo/R1/</a>.</li><li>Models for the 2018 R5 release of OpenVINO are available at <a href="https://download.01.org/openvinotoolkit/2018_R5/open_model_zoo/" target="_blank" rel="noopener">https://download.01.org/openvinotoolkit/2018_R5/open_model_zoo/</a>.</li></ul><p>For more information on pre-trained models, see <a href="https://docs.openvinotoolkit.org/latest/omz_models_intel_index.html" target="_blank" rel="noopener">Pre-Trained Models Documentation</a></p></li><li><p>Convert the model using the Model Optimizer from a full installation of Intel® Distribution of OpenVINO™ toolkit on one of the supported platforms. Installation instructions are available:</p><ul><li><a href="https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_macos.html" target="_blank" rel="noopener">Installation Guide for macOS*</a></li><li><a href="https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_windows.html" target="_blank" rel="noopener">Installation Guide for Windows*</a></li><li><a href="https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_linux.html" target="_blank" rel="noopener">Installation Guide for Linux*</a></li></ul></li></ul><h2 id="Step2-在Windows上安装NCS环境"><a href="#Step2-在Windows上安装NCS环境" class="headerlink" title="Step2 在Windows上安装NCS环境"></a>Step2 在Windows上安装NCS环境</h2><p><strong>Operating System</strong></p><ul><li>Microsoft Windows* 10 64-bit</li></ul><p><strong>Software</strong></p><ul><li><p><a href="http://visualstudio.microsoft.com/downloads/" target="_blank" rel="noopener">Microsoft Visual Studio* with C++ <strong>2019 or 2017</strong> with MSBuild</a></p></li><li><p>CMake <strong>3.10 or higher</strong> 64-bit</p><blockquote><p><strong>NOTE</strong>: If you want to use Microsoft Visual Studio 2019, you are required to install CMake 3.14. </p></blockquote></li><li><p><a href="https://www.python.org/downloads/windows/" target="_blank" rel="noopener">Python <strong>3.6</strong> - <strong>3.8</strong> 64-bit</a></p></li></ul><h4 id="1、安装英特尔®分布式OpenVINO™工具包核心组件"><a href="#1、安装英特尔®分布式OpenVINO™工具包核心组件" class="headerlink" title="1、安装英特尔®分布式OpenVINO™工具包核心组件"></a>1、安装英特尔®分布式OpenVINO™工具包核心组件</h4><p>可以进入<a href="https://software.intel.com/zh-cn/openvino-toolkit" target="_blank" rel="noopener">OpenVINO官网</a>参考官方安装步骤，也可以参考我的安装步骤。</p><p><strong>英特尔®分布式OpenVINO™工具包核心组件安装步骤：</strong></p><ol><li>如果您尚未下载英特尔®分布式OpenVINO™工具包，请下载最新版本。默认情况下，该文件将保存到Downloads目录w_openvino_toolkit_p_2019.3.379.exe。</li><li>转到该Downloads文件夹。</li><li>双击w_openvino_toolkit_p_2019.3.379.exe。将打开一个窗口，您可以选择安装目录和组件。默认安装目录是C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379 .为了简便，还会创建 C:\Program  Files (x86)\IntelSWTools\openvino安装目录快捷方式。如果选择其他安装目录，安装程序将为您创建目录：<br> <img src="https://img-blog.csdnimg.cn/20191230211524163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li></ol><p>4.单击下一步。</p><p>5.系统会询问您是否同意收集信息。选择您选择的选项。单击下一步。</p><p>6.如果您缺少外部依赖项，则会看到警告屏幕。记下您缺少的依赖项。<strong>此时您不需要采取任何行动。</strong>安装英特尔®分布式OpenVINO™工具包核心组件后，将向您提供安装缺少的依赖项说明。下面的屏幕表示您缺少两个依赖项：</p><p>7.单击下一步。</p><p>8.安装的第一部分完成后，最终屏幕会通知您已安装核心组件并仍需执行自他步骤：</p><h4 id="2、设置环境变量"><a href="#2、设置环境变量" class="headerlink" title="2、设置环境变量"></a>2、设置环境变量</h4><p>在编译和运行OpenVINO™应用程序之前，必须更新多个环境变量。打开命令提示符并运行以下批处理文件以临时设置环境变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files(x86)\IntelSWTools\openvino\bin\setupvars.bat</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p><strong>（可选）</strong>：关闭“命令提示符”窗口时，将删除OpenVINO工具箱环境变量。作为选项，您可以手动永久设置环境变量。</p><h4 id="下列步骤进一步确认使用OpenVINO"><a href="#下列步骤进一步确认使用OpenVINO" class="headerlink" title="下列步骤进一步确认使用OpenVINO"></a>下列步骤进一步确认使用OpenVINO</h4><h5 id="1、配置模型优化程序"><a href="#1、配置模型优化程序" class="headerlink" title="1、配置模型优化程序"></a>1、配置模型优化程序</h5><p><strong>重要信息</strong>：这些步骤是必需的。您必须为至少一个框架配置Model Optimizer。如果您未完成本节中的步骤，模型优化程序将失败。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer</span><br></pre></td></tr></table></figure><h5 id="2、模型优化器配置步骤"><a href="#2、模型优化器配置步骤" class="headerlink" title="2、模型优化器配置步骤"></a>2、模型优化器配置步骤</h5><p>您可以一次为所有受支持的框架配置模型优化程序，也可以一次为一个框架配置模型优化程序。选择最适合您需求的选项。如果看到错误消息，请确保已安装所有依赖项。<br> <strong>选项1：同时为所有支持的框架配置Model Optimizer</strong><br> 打开命令提示符，转到Model Optimizer条件目录，执行下列命令以配置Caffe <em>，TensorFlow </em>，MXNet <em>，Kaldi </em>和ONNX *的模型优化器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install_prerequisites.bat</span><br></pre></td></tr></table></figure><p>也可以安装你需要的。比如本次demo用的是caffe.所以装个caffe的。</p><p>把后面那个文件换成install_prerequistites_caffe.bat即可。</p><h5 id="3、测试"><a href="#3、测试" class="headerlink" title="3、测试"></a>3、测试</h5><p>先瞎跑个测试脚本，证明我们把一套openvino装成功了。。。。</p><p>在下面这个地址找到demo_squeezenet_download_convert_run.bat，跑起来。。。</p><p><img src="https://pic3.zhimg.com/80/v2-a28ce9b7f0b3ef6718a5373db803cf62_720w.png" alt></p><p><img src="https://img-blog.csdnimg.cn/20200108191904455.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近在看各种AI的加速方案以及边缘计算，jetsonnano、英伟达TX2、华为昇腾啊什么的&lt;/p&gt;
&lt;p&gt;jetson nano 感觉性价比不如rk3399，还有Intel的计算棒。做终端应用成本太高，但是。。。。。3399得1000+了啊&lt;/p&gt;
&lt;p&gt;下面就是关于Intel神经棒二代的上手教程。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Intel-Neural-Compute-stick" scheme="http://yuanquanquan.top/tags/Intel-Neural-Compute-stick/"/>
    
  </entry>
  
  <entry>
    <title>APDS-9960 RGB and Gesture Sensor</title>
    <link href="http://yuanquanquan.top/2020/20200720/"/>
    <id>http://yuanquanquan.top/2020/20200720/</id>
    <published>2020-07-20T00:19:47.000Z</published>
    <updated>2020-07-21T06:48:43.161Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>通过APDS-9960手势识别芯片在Arduino上实现手势识别。</p></blockquote><a id="more"></a><p> <img src="https://i.loli.net/2020/07/20/V7QfZtiyl1X3kOj.png" alt></p><h1 id="1-HARDWARE"><a href="#1-HARDWARE" class="headerlink" title="1. HARDWARE"></a>1. HARDWARE</h1><p>· Osoyoo UNO Board (Fully compatible with Arduino UNO rev.3) x 1</p><p>· APDS-9960 RGB and Gesture Sensor x 1</p><p>· Jumpers</p><p>· USB Cable x 1</p><p>· PC x 1</p><h1 id="2-SOFTWARE"><a href="#2-SOFTWARE" class="headerlink" title="2. SOFTWARE"></a>2. SOFTWARE</h1><p>· Arduino IDE (version 1.6.4+)</p><p>· Arduino library: <a href="https://codeload.github.com/adafruit/Adafruit_APDS9960/zip/master" target="_blank" rel="noopener">APDS9960.h</a></p><h1 id="3-About-APDS-9960-RGB-and-Gesture-Sensor"><a href="#3-About-APDS-9960-RGB-and-Gesture-Sensor" class="headerlink" title="3. About APDS-9960 RGB and Gesture Sensor"></a>3. About APDS-9960 RGB and Gesture Sensor</h1><p><img src="https://i.loli.net/2020/07/20/6fex8zmILMlo5Ua.png" alt> </p><p>This is the  RGB and Gesture Sensor, a small breakout board with a built in APDS-9960 sensor that offers ambient light and color measuring, proximity detection, and touchless gesture sensing. With this RGB and Gesture Sensor you will be able to control a computer, microcontroller, robot, and more with a simple swipe of your hand! This is, in fact, the same sensor that the Samsung Galaxy S5 uses and is probably one of the best gesture sensors on the market for the price.</p><p> <img src="https://i.loli.net/2020/07/20/Qt2qSDv4sdORk7o.png" alt></p><p>The APDS-9960 is a serious little piece of hardware with built in UV and IR blocking filters, four separate diodes sensitive to different directions, and an I2C compatible interface. For your convenience we have broken out the following pins: VL (optional power to IR LED), GND (Ground), VCC (power to APDS-9960 sensor), SDA (I2C data), SCL (I2C clock), and INT (interrupt). Each APDS-9960 also has a detection range of 4 to 8 inches (10 to 20 cm).</p><p><img src="https://i.loli.net/2020/07/20/Rtq621F7UxkYDlH.png" alt> </p><h2 id="3-1-PIN-DESCRIPTIONS"><a href="#3-1-PIN-DESCRIPTIONS" class="headerlink" title="3.1 PIN DESCRIPTIONS"></a>3.1 PIN DESCRIPTIONS</h2><table><thead><tr><th><strong><em>\</em>Pin Label**</strong></th><th><strong><em>\</em>Description**</strong></th></tr></thead><tbody><tr><td>VL</td><td>Optional power to the IR LED if PS jumper is disconnected. Must be 3.0 - 4.5V</td></tr><tr><td>GND</td><td>Connect to ground.</td></tr><tr><td>VCC</td><td>Used to power the APDS-9960 sensor. Must be 2.4 - 3.6V</td></tr><tr><td>SDA</td><td>I2C data</td></tr><tr><td>SCL</td><td>I2C clock</td></tr><tr><td>INT</td><td>External interrupt pin. Active LOW on interrupt event</td></tr></tbody></table><h2 id="3-2-FEATURES"><a href="#3-2-FEATURES" class="headerlink" title="3.2 FEATURES:"></a>3.2 FEATURES:</h2><p>· Model: GY-APDS 9960-3.3</p><p>· Using chip: APDS-9960</p><p>· Operational Voltage: 3.3V</p><p>· Ambient Light &amp; RGB Color Sensing</p><p>· Proximity Sensing</p><p>· Gesture Detection</p><p>· Operating Range: 4-8in (10-20cm)</p><p>· I2C Interface (I2C Address: 0x39)</p><p>· Size: 20mm * 15.3mm</p><p><strong>Recommended Reading</strong></p><p>Before getting started with the APDS-9960, there are a few concepts that you should be familiar with. Consider <a href="http://osoyoo.com/2017/09/21/osoyoo-advanced-kit-for-arduino/" target="_blank" rel="noopener">reading some of these tutorials before continuing.</a></p><h2 id="3-3-SETTING-THE-JUMPERS"><a href="#3-3-SETTING-THE-JUMPERS" class="headerlink" title="3.3 SETTING THE JUMPERS"></a>3.3 SETTING THE JUMPERS</h2><p><img src="https://i.loli.net/2020/07/20/Qlw6Am5xdPqIeiW.jpg" alt> </p><p>On the front of the breakout board are 2 solder jumpers:</p><p><strong>·</strong> <strong><em>\</em>PS**</strong> – This jumper connects the power supplies of the sensor and IR LED (also located on the APDS-9960) together. When the jumper is closed (i.e. connected), you only need to supply power to the VCC pin to power both the sensor and the IR LED. If the jumper is open, you need to provide power to both the VCC (2.4 - 3.6V) and VL (3.0 - 4.5V) pins separately. This jumper is <strong><em>\</em>closed**</strong> by default.</p><p><strong>·</strong> <strong><em>\</em>I2C PU**</strong> – This is a 3-way solder jumper that is used to connect and disconnect the I2C pullup resistors. By default, this jumper is <strong><em>\</em>closed**</strong>, which means that both SDA and SCL lines have connected pullup resistors on the breakout board. Use some solder wick to open the jumper if you do not need the pullup resistors (e.g. you have pullup resistors that are located on the I2C bus somewhere else).</p><h2 id="3-4-HARDWARE-HOOKUP"><a href="#3-4-HARDWARE-HOOKUP" class="headerlink" title="3.4 HARDWARE HOOKUP"></a>3.4 HARDWARE HOOKUP</h2><h3 id="3-4-1-Add-Headers"><a href="#3-4-1-Add-Headers" class="headerlink" title="3.4.1 Add Headers"></a>3.4.1 Add Headers</h3><p>Solder a row of break away male headers to the 6 headers holes on the board.</p><p><img src="https://i.loli.net/2020/07/20/eVvx14DpdJnFG8M.jpg" alt> </p><h3 id="3-4-2-Connect-the-Breakout-Board"><a href="#3-4-2-Connect-the-Breakout-Board" class="headerlink" title="3.4.2 Connect the Breakout Board"></a>3.4.2 Connect the Breakout Board</h3><p>We will be using the Arduino Pro’s regulated 3.3V power and I2C bus with the APDS-9960. Note that we are leaving VL on the breakout board unconnected. IMPORTANT: You must use 3.3V! If you try to use a 5V power supply  you risk damaging the APDS-9960. Connect the breakout board to the following pins on the Arduino:</p><table><thead><tr><th><strong><em>\</em>APDS-9960 Breakout Board**</strong></th><th><strong><em>\</em>OSOYOO UNO**</strong></th></tr></thead><tbody><tr><td>GND</td><td>GND</td></tr><tr><td>VCC</td><td>3.3V</td></tr><tr><td>SDA</td><td>A4</td></tr><tr><td>SCL</td><td>A5</td></tr></tbody></table><p><strong><em>\</em>NOTE:**</strong></p><p>· Connect the <strong><em>\</em>SCL**</strong> pin to the I2C clock <strong><em>\</em>SCL**</strong> pin on your Arduino. On an UNO &amp; ‘328 based Arduino, this is also known as <strong><em>\</em>A5**</strong>, on a Mega it is also known as <strong><em>\</em>digital 21**</strong> and on a Leonardo/Micro, <strong><em>\</em>digital 3**</strong></p><p>· Connect the <strong><em>\</em>SDA**</strong> pin to the I2C data <strong><em>\</em>SDA**</strong> pin on your Arduino. On an UNO &amp; ‘328 based Arduino, this is also known as <strong><em>\</em>A4**</strong>, on a Mega it is also known as <strong><em>\</em>digital 20**</strong> and on a Leonardo/Micro, <strong><em>\</em>digital 2**</strong></p><p><img src="https://i.loli.net/2020/07/20/Gutm3UWAv95jQgN.png" alt></p><h2 id="3-5-ARDUINO-LIBRARY-INSTALLATION"><a href="#3-5-ARDUINO-LIBRARY-INSTALLATION" class="headerlink" title="3.5 ARDUINO LIBRARY INSTALLATION"></a>3.5 ARDUINO LIBRARY INSTALLATION</h2><p>To use the APDS-9960, you will need some supporting software. If you are using an Arduino, then you are in luck! We created an Arduino library that makes the APDS-9960 easy to use. Click the button below to download the latest version of the APDS-9960 breakout board project, which includes the Arduino library. <a href="https://codeload.github.com/adafruit/Adafruit_APDS9960/zip/master" target="_blank" rel="noopener">DOWNLOAD THE PROJECT FILES!</a> Follow <a href="http://osoyoo.com/2017/05/08/how-to-install-additional-arduino-libraries/" target="_blank" rel="noopener">this guide on installing Arduino libraries</a> to install the files within the APDS9960 directory as an Arduino library. </p><h1 id="Gesture-Sensing-Example"><a href="#Gesture-Sensing-Example" class="headerlink" title="\Gesture Sensing Example**"></a><strong><em>\</em>Gesture Sensing Example**</strong></h1><h1 id="4-UPLOAD-SKETCH"><a href="#4-UPLOAD-SKETCH" class="headerlink" title="4. UPLOAD SKETCH"></a>4. UPLOAD SKETCH</h1><p>After above operations are completed, connect the Arduino board to your computer using the USB cable. The green power LED (labelled <strong><em>\</em>PWR**</strong>) should go on. </p><h1 id="5-CODE-PROGRAM"><a href="#5-CODE-PROGRAM" class="headerlink" title="5.CODE PROGRAM"></a>5.CODE PROGRAM</h1><p>You can copy below code to your Arduino IDE window, then select corresponding board type and port type for your Arduino board. </p><p><img src="https://i.loli.net/2020/07/20/FEjCHOW8l5XB4Nb.png" alt></p><h1 id="6-Running-Result"><a href="#6-Running-Result" class="headerlink" title="6.Running Result"></a>6.Running Result</h1><p>Click the Upload button and wait for the program to finish uploading to the Arduino. Once uploaded to your Adruino, open up the serial monitor at 115200 baud speed.More info on the Serial Terminal can be found <a href="http://osoyoo.com/2017/07/06/arduino-lesson-the-serial-monitor/" target="_blank" rel="noopener">here</a>. You should see a messages noting that “Device initialized! ” Hover your hand 4 to 8 inches (10 to 20 cm) above the sensor but off to one side (i.e. not directly above the sensor). While maintaining the same height, swipe your hand over the sensor (into and then immediately out of range of the sensor). If you move too fast, the sensor will not recognize the gesture.</p><p><img src="https://i.loli.net/2020/07/20/hHpdYazO9xjXJ7w.jpg" alt> </p><p>#</p><h1 id="附：Adafruit-Community-Code-of-Conduct"><a href="#附：Adafruit-Community-Code-of-Conduct" class="headerlink" title="附：Adafruit Community Code of Conduct"></a>附：Adafruit Community Code of Conduct</h1><h2 id="Our-Pledge"><a href="#Our-Pledge" class="headerlink" title="Our Pledge"></a>Our Pledge</h2><p>In the interest of fostering an open and welcoming environment, we as<br>contributors and leaders pledge to making participation in our project and<br>our community a harassment-free experience for everyone, regardless of age, body<br>size, disability, ethnicity, gender identity and expression, level or type of<br>experience, education, socio-economic status, nationality, personal appearance,<br>race, religion, or sexual identity and orientation.</p><h2 id="Our-Standards"><a href="#Our-Standards" class="headerlink" title="Our Standards"></a>Our Standards</h2><p>We are committed to providing a friendly, safe and welcoming environment for<br>all.</p><p>Examples of behavior that contributes to creating a positive environment<br>include:</p><ul><li>Be kind and courteous to others</li><li>Using welcoming and inclusive language</li><li>Being respectful of differing viewpoints and experiences</li><li>Collaborating with other community members</li><li>Gracefully accepting constructive criticism</li><li>Focusing on what is best for the community</li><li>Showing empathy towards other community members</li></ul><p>Examples of unacceptable behavior by participants include:</p><ul><li>The use of sexualized language or imagery and sexual attention or advances</li><li>The use of inappropriate images, including in a community member’s avatar</li><li>The use of inappropriate language, including in a community member’s nickname</li><li>Any spamming, flaming, baiting or other attention-stealing behavior</li><li>Excessive or unwelcome helping; answering outside the scope of the question<br>asked</li><li>Trolling, insulting/derogatory comments, and personal or political attacks</li><li>Public or private harassment</li><li>Publishing others’ private information, such as a physical or electronic<br>address, without explicit permission</li><li>Other conduct which could reasonably be considered inappropriate</li></ul><p>The goal of the standards and moderation guidelines outlined here is to build<br>and maintain a respectful community. We ask that you don’t just aim to be<br>“technically unimpeachable”, but rather try to be your best self. </p><p>We value many things beyond technical expertise, including collaboration and<br>supporting others within our community. Providing a positive experience for<br>other community members can have a much more significant impact than simply<br>providing the correct answer.</p><h2 id="Our-Responsibilities"><a href="#Our-Responsibilities" class="headerlink" title="Our Responsibilities"></a>Our Responsibilities</h2><p>Project leaders are responsible for clarifying the standards of acceptable<br>behavior and are expected to take appropriate and fair corrective action in<br>response to any instances of unacceptable behavior.</p><p>Project leaders have the right and responsibility to remove, edit, or<br>reject messages, comments, commits, code, issues, and other contributions<br>that are not aligned to this Code of Conduct, or to ban temporarily or<br>permanently any community member for other behaviors that they deem<br>inappropriate, threatening, offensive, or harmful.</p><h2 id="Moderation"><a href="#Moderation" class="headerlink" title="Moderation"></a>Moderation</h2><p>Instances of behaviors that violate the Adafruit Community Code of Conduct<br>may be reported by any member of the community. Community members are<br>encouraged to report these situations, including situations they witness<br>involving other community members.</p><p>You may report in the following ways:</p><p>In any situation, you may send an email to <a href="mailto:&#x73;&#x75;&#x70;&#x70;&#111;&#114;&#116;&#x40;&#x61;&#x64;&#x61;&#x66;&#114;&#117;&#x69;&#x74;&#x2e;&#x63;&#x6f;&#x6d;" target="_blank" rel="noopener">&#x73;&#x75;&#x70;&#x70;&#111;&#114;&#116;&#x40;&#x61;&#x64;&#x61;&#x66;&#114;&#117;&#x69;&#x74;&#x2e;&#x63;&#x6f;&#x6d;</a>.</p><p>On the Adafruit Discord, you may send an open message from any channel<br>to all Community Helpers by tagging @community helpers. You may also send an<br>open message from any channel, or a direct message to @kattni#1507,<br>@tannewt#4653, @Dan Halbert#1614, @cater#2442, @sommersoft#0222, or<br>@Andon#8175.</p><p>Email and direct message reports will be kept confidential.</p><p>In situations on Discord where the issue is particularly egregious, possibly<br>illegal, requires immediate action, or violates the Discord terms of service,<br>you should also report the message directly to Discord.</p><p>These are the steps for upholding our community’s standards of conduct.</p><ol><li>Any member of the community may report any situation that violates the<br>Adafruit Community Code of Conduct. All reports will be reviewed and<br>investigated.</li><li>If the behavior is an egregious violation, the community member who<br>committed the violation may be banned immediately, without warning.</li><li>Otherwise, moderators will first respond to such behavior with a warning.</li><li>Moderators follow a soft “three strikes” policy - the community member may<br>be given another chance, if they are receptive to the warning and change their<br>behavior.</li><li>If the community member is unreceptive or unreasonable when warned by a<br>moderator, or the warning goes unheeded, they may be banned for a first or<br>second offense. Repeated offenses will result in the community member being<br>banned.</li></ol><h2 id="Scope"><a href="#Scope" class="headerlink" title="Scope"></a>Scope</h2><p>This Code of Conduct and the enforcement policies listed above apply to all<br>Adafruit Community venues. This includes but is not limited to any community<br>spaces (both public and private), the entire Adafruit Discord server, and<br>Adafruit GitHub repositories. Examples of Adafruit Community spaces include<br>but are not limited to meet-ups, audio chats on the Adafruit Discord, or<br>interaction at a conference.</p><p>This Code of Conduct applies both within project spaces and in public spaces<br>when an individual is representing the project or its community. As a community<br>member, you are representing our community, and are expected to behave<br>accordingly.</p><h2 id="Attribution"><a href="#Attribution" class="headerlink" title="Attribution"></a>Attribution</h2><p>This Code of Conduct is adapted from the [Contributor Covenant][homepage],<br>version 1.4, available at<br><a href="https://www.contributor-covenant.org/version/1/4/code-of-conduct.html" target="_blank" rel="noopener">https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</a>,<br>and the <a href="https://www.rust-lang.org/en-US/conduct.html" target="_blank" rel="noopener">Rust Code of Conduct</a>.</p><p>For other projects adopting the Adafruit Community Code of<br>Conduct, please contact the maintainers of those projects for enforcement.<br>If you wish to use this code of conduct for your own project, consider<br>explicitly mentioning your moderation policy or making a copy with your<br>own moderation policy so as to avoid confusion.</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;通过APDS-9960手势识别芯片在Arduino上实现手势识别。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="-手势识别" scheme="http://yuanquanquan.top/tags/%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Arcore-Android</title>
    <link href="http://yuanquanquan.top/2020/20200706/"/>
    <id>http://yuanquanquan.top/2020/20200706/</id>
    <published>2020-07-06T07:56:07.000Z</published>
    <updated>2020-07-06T08:50:03.794Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ARCore 是 Google 为开发者构建的增强现实平台，如何让虚拟物体和真实世界完美融合，这一直是 Google ARCore 技术所探讨的问题。众所周知，当虚拟物体附近有现实物体时，有可能会出现互相交融、重叠等效果，大大地影响了用户体验。这一直是 AR 技术的难点，也是 Google 不懈努力的方向。</p></blockquote><a id="more"></a>  <h1 id="1-Quickstart-for-Android"><a href="#1-Quickstart-for-Android" class="headerlink" title="1. Quickstart for Android"></a>1. Quickstart for Android</h1><p><img src="https://developers.google.cn/ar/develop/java/images/android-studio.png" alt></p><h2 id="1-2-Set-up-your-development-environment"><a href="#1-2-Set-up-your-development-environment" class="headerlink" title="1.2 Set up your development environment"></a>1.2 Set up your development environment</h2><ul><li>Install <a href="https://developer.android.google.cn/studio/index.html" target="_blank" rel="noopener">Android Studio</a> version 3.1 or higher with Android SDK Platform version 7.0 (API level 24) or higher.</li><li>You will need a basic understanding of Android development. If you are new to Android, see <a href="https://developer.android.google.cn/training/basics/firstapp/index.html" target="_blank" rel="noopener">Building your first Android app for beginners</a>.</li></ul><h2 id="1-3-Open-the-sample-project"><a href="#1-3-Open-the-sample-project" class="headerlink" title="1.3 Open the sample project"></a>1.3 Open the sample project</h2><p>This quickstart uses <a href="https://en.wikipedia.org/wiki/OpenGL" target="_blank" rel="noopener">OpenGL</a>, a programming interface for rendering 2D and 3D vector graphics. Review the <a href="https://developers.google.cn/ar/develop/java/enable-arcore" target="_blank" rel="noopener">Enable ARCore</a> documentation before getting started with the steps below.</p><p>Get the sample project by cloning the repository with the following command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/google-ar/arcore-android-sdk.git</span><br></pre></td></tr></table></figure><p>In Android Studio, open the <strong>HelloAR</strong> sample project, located in the <strong>samples</strong> subdirectory within the <code>arcore-android-sdk</code> directory.</p><h2 id="1-4-Prepare-your-device-or-emulator"><a href="#1-4-Prepare-your-device-or-emulator" class="headerlink" title="1.4 Prepare your device or emulator"></a>1.4 Prepare your device or emulator</h2><p>You can run AR apps on a <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">supported device</a> or in the Android Emulator:</p><ul><li>In the emulator, you must sign into the Google Play Store or <a href="https://developers.google.cn/ar/develop/java/emulator#update-arcore" target="_blank" rel="noopener">update Google Play Services for AR</a> manually.</li></ul><h2 id="1-5-Run-the-sample"><a href="#1-5-Run-the-sample" class="headerlink" title="1.5 Run the sample"></a>1.5 Run the sample</h2><p>Make sure your Android device is connected to the development machine and click <strong>Run</strong> <img src="https://developers.google.cn/ar/develop/java/images/toolbar-run.png" alt="img"> in Android Studio. Then, choose your device as the deployment target and click <strong>OK</strong>.</p><p><img src="https://developers.google.cn/ar/develop/java/images/deployment-target.png" alt></p><p>Android Studio builds your project into a debuggable APK, installs the APK, and then runs the app on your device. For more information, see <a href="https://developer.android.google.cn/studio/run/index.html" target="_blank" rel="noopener">Build and Run Your App</a>.</p><p>You may be prompted to install or update <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> if it is missing or out of date. Select <strong>CONTINUE</strong> to install it from Google Play Store.</p><p>The <strong>HelloAR</strong> app lets you place and manipulate Android figurines on detected AR plane surfaces. It is implemented with <a href="https://developer.android.google.cn/reference/android/opengl/GLSurfaceView" target="_blank" rel="noopener">Android GL SurfaceView</a>, which is used to render the camera preview and basic AR objects such as Planes and Anchors. <strong>HelloAR</strong>‘s render can be found <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples/hello_ar_java/app/src/main/java/com/google/ar/core/examples/java/common/rendering" target="_blank" rel="noopener">here</a>.</p><p><img src="https://developers.google.cn/ar/develop/java/images/helloar-demo.jpg" alt></p><p><strong>Note:</strong> The lifecycle methods in <strong>HelloAR</strong> are different than those normally found in OpenGL applications. To ensure the correct AR setup for your own applications, follow the lifecycle management logic in <strong>HelloAR</strong>.</p><h2 id="1-6-Next-steps"><a href="#1-6-Next-steps" class="headerlink" title="1.6 Next steps"></a>1.6 Next steps</h2><ul><li>Try building and running other <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples" target="_blank" rel="noopener">sample projects</a> in the ARCore SDK.</li><li>Learn how to <a href="https://developers.google.cn/ar/develop/java/enable-arcore" target="_blank" rel="noopener">Enable ARCore</a> in your app.</li><li>Use <a href="https://developers.google.cn/ar/develop/java/augmented-images" target="_blank" rel="noopener">Augmented Images</a> to build apps that can respond to 2D images, such as posters or logos, in the user’s environment.</li><li>Use <a href="https://developers.google.cn/ar/develop/java/cloud-anchors/cloud-anchors-overview-android" target="_blank" rel="noopener">Cloud Anchors</a> to create shared AR experiences across iOS and Android users.</li><li>Review the <a href="https://developers.google.cn/ar/develop/developer-guides/runtime-considerations" target="_blank" rel="noopener">Runtime Considerations</a>.</li><li>Review the <a href="https://developers.google.cn/ar/develop/developer-guides/design-guidelines" target="_blank" rel="noopener">Design Guidelines</a>.</li></ul><h1 id="2-Enable-ARCore"><a href="#2-Enable-ARCore" class="headerlink" title="2. Enable ARCore"></a>2. Enable ARCore</h1><p>This page describes how to enable ARCore functionality in your Android Studio projects. To do this, you need to:</p><ol><li><a href="https://developers.google.cn/ar/develop/java/enable-arcore#manifest" target="_blank" rel="noopener">Add AR Required or AR Optional entries to the manifest</a></li><li><a href="https://developers.google.cn/ar/develop/java/enable-arcore#dependencies" target="_blank" rel="noopener">Add build dependencies</a> to your project</li><li><a href="https://developers.google.cn/ar/develop/java/enable-arcore#runtime" target="_blank" rel="noopener">Perform runtime checks</a> to ensure the device is <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore-supported</a>, that Google Play Services for AR is installed on it, and that camera permission has been granted</li><li>Make sure your app complies with ARCore’s <a href="https://developers.google.cn/ar/distribute/privacy-requirements" target="_blank" rel="noopener">User Privacy Requirements</a></li></ol><h2 id="2-1-Using-Google-Play-Services-for-AR-to-enable-ARCore-functionality"><a href="#2-1-Using-Google-Play-Services-for-AR-to-enable-ARCore-functionality" class="headerlink" title="2.1 Using Google Play Services for AR to enable ARCore functionality"></a>2.1 Using Google Play Services for AR to enable ARCore functionality</h2><p>ARCore SDKs make AR features available on <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore supported devices</a> that have <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> installed. Users can install and update Google Play Services for AR from the Google Play Store.</p><h2 id="2-2-Add-AR-Required-or-AR-Optional-entries-to-the-manifest"><a href="#2-2-Add-AR-Required-or-AR-Optional-entries-to-the-manifest" class="headerlink" title="2.2 Add AR Required or AR Optional entries to the manifest"></a>2.2 Add AR Required or AR Optional entries to the manifest</h2><p>An app that supports AR features can be configured in two ways: <strong>AR Required</strong> and <strong>AR Optional</strong>.</p><h3 id="2-2-1-AR-Required-apps"><a href="#2-2-1-AR-Required-apps" class="headerlink" title="2.2.1 AR Required apps"></a>2.2.1 AR Required apps</h3><p>To be usable, an <em>AR Required</em> app requires an <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore Supported Device</a> that has <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> installed on it.</p><ul><li>The Google Play Store makes AR Required apps available only on <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">devices that support ARCore</a>.</li><li>When users install an AR Required app, the Google Play Store automatically installs <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a>. However, your app must still perform additional <a href="https://developers.google.cn/ar/develop/java/enable-arcore#runtime" target="_blank" rel="noopener">runtime checks</a> in case Google Play Services for AR must be updated or has been manually uninstalled.</li></ul><p>For more information, see <a href="https://developers.google.cn/ar/distribute" target="_blank" rel="noopener">Publishing AR Apps in the Google Play Store</a>.</p><p>To declare your app to be <em>AR Required</em>, modify your <code>AndroidManifest.xml</code> to include the following entries:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;uses-permission android:name=&quot;android.permission.CAMERA&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Limits app visibility in the Google Play Store to ARCore supported devices</span><br><span class="line">     (https://developers.google.com/ar/discover/supported-devices). --&gt;</span><br><span class="line">&lt;uses-feature android:name=&quot;android.hardware.camera.ar&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;application …&gt;</span><br><span class="line">    …</span><br><span class="line"></span><br><span class="line">  &lt;!-- &quot;AR Required&quot; app, requires &quot;Google Play Services for AR&quot; (ARCore)</span><br><span class="line">       to be installed, as the app does not include any non-AR features. --&gt;</span><br><span class="line">    &lt;meta-data android:name=&quot;com.google.ar.core&quot; android:value=&quot;required&quot; /&gt;</span><br><span class="line">&lt;/application&gt;</span><br></pre></td></tr></table></figure><p>Then, modify your app’s <code>build.gradle</code> to specify a <code>minSdkVersion</code> of at least 24:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">android &#123;  defaultConfig &#123;    …    minSdkVersion 24  &#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-2-AR-Optional-apps"><a href="#2-2-2-AR-Optional-apps" class="headerlink" title="2.2.2 AR Optional apps"></a>2.2.2 AR Optional apps</h3><p>An <em>AR Optional</em> app has optional AR features, which are activated only on <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore supported devices</a> that have <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> installed.</p><ul><li>AR Optional apps can be installed and run on devices that don’t support ARCore.</li><li>When users install an AR Optional app, the Google Play Store will <em>not</em> automatically install <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> with the app.</li></ul><p>To declare your app to be <em>AR Optional</em>, modify your <code>AndroidManifest.xml</code> to include the following entries:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;uses-permission android:name=&quot;android.permission.CAMERA&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;application …&gt;</span><br><span class="line">    …</span><br><span class="line"></span><br><span class="line">    &lt;!-- &quot;AR Optional&quot; app, contains non-AR features that can be used when</span><br><span class="line">         &quot;Google Play Services for AR&quot; (ARCore) is not available. --&gt;</span><br><span class="line">    &lt;meta-data android:name=&quot;com.google.ar.core&quot; android:value=&quot;optional&quot; /&gt;</span><br><span class="line">&lt;/application&gt;</span><br></pre></td></tr></table></figure><p>Then, modify your app’s <code>build.gradle</code> to specify a <code>minSdkVersion</code> of at least 14:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">android &#123;</span><br><span class="line">    defaultConfig &#123;</span><br><span class="line">        …</span><br><span class="line">        minSdkVersion 14</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-3-Add-build-dependencies"><a href="#2-3-Add-build-dependencies" class="headerlink" title="2.3 Add build dependencies"></a>2.3 Add build dependencies</h2><p>To add ARCore to your Android Studio project, perform these steps:</p><ul><li><p>Make sure your <strong>project’s</strong> <code>build.gradle</code> file includes Google’s Maven repository:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">allprojects &#123;</span><br><span class="line">    repositories &#123;</span><br><span class="line">        google()</span><br><span class="line">        …</span><br></pre></td></tr></table></figure></li><li><p>Add the latest ARCore library as a dependency in your <strong>app’s</strong> <code>build.gradle</code> file:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dependencies &#123;</span><br><span class="line">    …</span><br><span class="line">    implementation &apos;com.google.ar:core:1.18.0&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-4-Perform-runtime-checks"><a href="#2-4-Perform-runtime-checks" class="headerlink" title="2.4 Perform runtime checks"></a>2.4 Perform runtime checks</h2><h3 id="2-4-1-Check-whether-ARCore-is-supported-AR-Optional-apps-only"><a href="#2-4-1-Check-whether-ARCore-is-supported-AR-Optional-apps-only" class="headerlink" title="2.4.1 Check whether ARCore is supported (AR Optional apps only)"></a>2.4.1 Check whether ARCore is supported (<em>AR Optional</em> apps only)</h3><p><em>AR Optional</em> apps can use <code>ArCoreApk.checkAvailability()</code> to determine if the current device supports ARCore. On devices that do not support ARCore, AR Optional apps should disable AR related functionality and hide associated UI elements:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">protected void onCreate(Bundle savedInstanceState) &#123;</span><br><span class="line">  super.onCreate(savedInstanceState);</span><br><span class="line"></span><br><span class="line">  // Enable AR related functionality on ARCore supported devices only.</span><br><span class="line">  maybeEnableArButton();</span><br><span class="line">  …</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void maybeEnableArButton() &#123;</span><br><span class="line">  ArCoreApk.Availability availability = ArCoreApk.getInstance().checkAvailability(this);</span><br><span class="line">  if (availability.isTransient()) &#123;</span><br><span class="line">    // Re-query at 5Hz while compatibility is checked in the background.</span><br><span class="line">    new Handler().postDelayed(new Runnable() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      public void run() &#123;</span><br><span class="line">        maybeEnableArButton();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, 200);</span><br><span class="line">  &#125;</span><br><span class="line">  if (availability.isSupported()) &#123;</span><br><span class="line">    mArButton.setVisibility(View.VISIBLE);</span><br><span class="line">    mArButton.setEnabled(true);</span><br><span class="line">    // indicator on the button.</span><br><span class="line">  &#125; else &#123; // Unsupported or unknown.</span><br><span class="line">    mArButton.setVisibility(View.INVISIBLE);</span><br><span class="line">    mArButton.setEnabled(false);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Note, <code>checkAvailability()</code> may need to query network resources to determine whether the device supports ARCore. During this time, it will return <code>UNKNOWN_CHECKING</code>. To reduce the perceived latency and pop-in, apps should call <code>checkAvailability()</code> once early in it’s life cycle to initiate the query, ignoring the returned value. This way a cached result will be available immediately when <code>maybeEnableArButton()</code> is called.</p><p>This flowchart illustrates the logic in the preceding code sample:</p><p><img src="https://developers.google.cn/ar/images/check-availability-flowchart.png" alt></p><h3 id="2-4-2-Request-camera-permission-AR-Optional-and-AR-Required-apps"><a href="#2-4-2-Request-camera-permission-AR-Optional-and-AR-Required-apps" class="headerlink" title="2.4.2 Request camera permission (AR Optional and AR Required apps)"></a>2.4.2 Request camera permission (<em>AR Optional</em> and <em>AR Required</em> apps)</h3><p>Both <em>AR Optional</em> and <em>AR Required</em> apps must ensure that the camera permission has been granted before creating an AR Session. The <strong>hello_ar_java</strong> sample includes a <a href="https://github.com/google-ar/arcore-android-sdk/search?q=CameraPermissionHelper.java" target="_blank" rel="noopener"><code>CameraPermissionHelper</code></a> class which can be copied into your project and should be called from your AR activity’s <code>onResume()</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">protected void onResume() &#123;</span><br><span class="line">  super.onResume();</span><br><span class="line"></span><br><span class="line">  // ARCore requires camera permission to operate.</span><br><span class="line">  if (!CameraPermissionHelper.hasCameraPermission(this)) &#123;</span><br><span class="line">    CameraPermissionHelper.requestCameraPermission(this);</span><br><span class="line">    return;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Your AR activity must also implement <code>onRequestPermissionsResult(…)</code>, as seen in <a href="https://github.com/google-ar/arcore-android-sdk/search?q=HelloArActivity.java" target="_blank" rel="noopener"><code>HelloArActivity</code></a>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void onRequestPermissionsResult(int requestCode, String[] permissions, int[] results) &#123;</span><br><span class="line">  if (!CameraPermissionHelper.hasCameraPermission(this)) &#123;</span><br><span class="line">    Toast.makeText(this, &quot;Camera permission is needed to run this application&quot;, Toast.LENGTH_LONG)</span><br><span class="line">        .show();</span><br><span class="line">    if (!CameraPermissionHelper.shouldShowRequestPermissionRationale(this)) &#123;</span><br><span class="line">      // Permission denied with checking &quot;Do not ask again&quot;.</span><br><span class="line">      CameraPermissionHelper.launchPermissionSettings(this);</span><br><span class="line">    &#125;</span><br><span class="line">    finish();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-3-Check-whether-Google-Play-Services-for-AR-is-installed-AR-Optional-and-AR-Required-apps"><a href="#2-4-3-Check-whether-Google-Play-Services-for-AR-is-installed-AR-Optional-and-AR-Required-apps" class="headerlink" title="2.4.3 Check whether Google Play Services for AR is installed (AR Optional and AR Required apps)"></a>2.4.3 Check whether Google Play Services for AR is installed (<em>AR Optional</em> and <em>AR Required</em> apps)</h3><p>To check whether a compatible version of Google Play Services for AR is installed, apps must also call <code>ArCoreApk.requestInstall()</code> before creating an ARCore session. This prompts the user to install or update the service if needed.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">// Set to true ensures requestInstall() triggers installation if necessary.</span><br><span class="line">private boolean mUserRequestedInstall = true;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">protected void onResume() &#123;</span><br><span class="line">  super.onResume();</span><br><span class="line"></span><br><span class="line">  // Check camera permission.</span><br><span class="line">  …</span><br><span class="line"></span><br><span class="line">  // Make sure Google Play Services for AR is installed and up to date.</span><br><span class="line">  try &#123;</span><br><span class="line">    if (mSession == null) &#123;</span><br><span class="line">      switch (ArCoreApk.getInstance().requestInstall(this, mUserRequestedInstall)) &#123;</span><br><span class="line">        case INSTALLED:</span><br><span class="line">          // Success, create the AR session.</span><br><span class="line">          mSession = new Session(this);</span><br><span class="line">          break;</span><br><span class="line">        case INSTALL_REQUESTED:</span><br><span class="line">          // Ensures next invocation of requestInstall() will either return</span><br><span class="line">          // INSTALLED or throw an exception.</span><br><span class="line">          mUserRequestedInstall = false;</span><br><span class="line">          return;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch (UnavailableUserDeclinedInstallationException e) &#123;</span><br><span class="line">    // Display an appropriate message to the user and return gracefully.</span><br><span class="line">    Toast.makeText(this, &quot;TODO: handle exception &quot; + e, Toast.LENGTH_LONG)</span><br><span class="line">        .show();</span><br><span class="line">    return;</span><br><span class="line">  &#125; catch (…) &#123;  // Current catch statements.</span><br><span class="line">    …</span><br><span class="line">    return;  // mSession is still null.</span><br><span class="line">  &#125;</span><br><span class="line">  …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This flowchart illustrates the logic in the preceding code sample:</p><p><img src="upload/image-20200706161631230.png" alt></p><p>If <code>requestInstall()</code> returns <code>INSTALL_REQUESTED</code>, the current activity pauses and the user is prompted to install or update Google Play Services for AR:</p><p><img src="https://developers.google.cn/ar/images/request-install-prompt.png" alt></p><p>The activity’s <code>onResume()</code> executes again once the user returns to the activity.</p><h2 id="2-5-Compliance-with-User-Privacy-Requirements"><a href="#2-5-Compliance-with-User-Privacy-Requirements" class="headerlink" title="2.5 Compliance with User Privacy Requirements"></a>2.5 Compliance with User Privacy Requirements</h2><p>Make sure your app complies with ARCore’s <a href="https://developers.google.cn/ar/distribute/privacy-requirements" target="_blank" rel="noopener">User Privacy Requirements</a>.</p><h2 id="2-6-Next-steps"><a href="#2-6-Next-steps" class="headerlink" title="2.6 Next steps"></a>2.6 Next steps</h2><ul><li>Read the code and comments in the <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples/hello_ar_java" target="_blank" rel="noopener">hello_ar_java</a> sample</li><li>Review the <a href="https://developers.google.cn/ar/reference/java" target="_blank" rel="noopener">Java API Reference</a></li></ul><h1 id="3-Run-AR-Apps-in-Android-Emulator"><a href="#3-Run-AR-Apps-in-Android-Emulator" class="headerlink" title="3. Run AR Apps in Android Emulator"></a>3. Run AR Apps in Android Emulator</h1><p>Use the <a href="https://developer.android.google.cn/studio/run/emulator.html" target="_blank" rel="noopener">Android Emulator</a> to test AR scenarios without a physical device. The Android Emulator lets you run ARCore apps in a virtual environment with an emulated device that you control.</p><p><strong>Warning:</strong> The Android Emulator does not support ARCore APIs for Depth, Augmented Faces, or Augmented Images. When any of these features are enabled, the camera preview image does not render correctly: the GPU camera texture is entirely black, although UI elements drawn on top of the preview image still render correctly.</p><h2 id="3-1-Set-up-your-development-environment"><a href="#3-1-Set-up-your-development-environment" class="headerlink" title="3.1 Set up your development environment"></a>3.1 Set up your development environment</h2><p>Software requirements:</p><ul><li><a href="https://developer.android.google.cn/studio/" target="_blank" rel="noopener">Android Studio</a> <strong>3.1</strong> or later.</li><li><a href="https://developer.android.google.cn/studio/run/emulator.html#Requirements" target="_blank" rel="noopener">Android Emulator</a> <strong>27.2.9</strong> or later.</li></ul><h2 id="3-2-Get-Android-Studio-and-SDK-tools-for-ARCore"><a href="#3-2-Get-Android-Studio-and-SDK-tools-for-ARCore" class="headerlink" title="3.2 Get Android Studio and SDK tools for ARCore"></a>3.2 Get Android Studio and SDK tools for ARCore</h2><ol><li><p>Install <a href="https://developer.android.google.cn/studio/" target="_blank" rel="noopener">Android Studio</a> 3.1 or later.</p></li><li><p>In Android Studio, go to <strong>Preferences &gt; Appearance and Behavior &gt; System Settings &gt; Android SDK</strong>.</p></li><li><p>Select the <strong>SDK Platforms</strong> tab and check <strong>Show Package Details</strong>.</p><p>Under <strong>Android 8.1 (Oreo)</strong>, select:<br><strong>Google APIs Intel x86 Atom System Image</strong> API Level 27, version 4 or later.</p></li><li><p>Select the <strong>SDK Tools</strong> tab and add <strong>Android Emulator</strong> 27.2.9 or later.</p></li><li><p>Click <strong>OK</strong> to install the selected packages and tools.</p></li><li><p>Click <strong>OK</strong> again to confirm changes.</p></li><li><p>Accept the license agreement for the Component Installer.</p></li><li><p>Click <strong>Finish</strong>.</p></li></ol><h2 id="3-3-Create-a-virtual-device-with-AR-support"><a href="#3-3-Create-a-virtual-device-with-AR-support" class="headerlink" title="3.3 Create a virtual device with AR support"></a>3.3 Create a virtual device with AR support</h2><p>For more information, see the Android Studio instructions to <a href="https://developer.android.google.cn/studio/run/managing-avds.html#createavd" target="_blank" rel="noopener">Create a Virtual Device</a>.</p><h3 id="3-3-1-Create-a-new-Android-Virtual-Device-AVD"><a href="#3-3-1-Create-a-new-Android-Virtual-Device-AVD" class="headerlink" title="3.3.1 Create a new Android Virtual Device (AVD)"></a>3.3.1 Create a new Android Virtual Device (AVD)</h3><ol><li>In Android Studio open the <em>AVD Manager</em> by clicking <strong>Tools &gt; AVD Manager</strong>.</li><li>Click <strong>Create Virtual Device</strong>, at the bottom of the <em>AVD Manager</em> dialog.</li><li>Select or create your desired <em>Phone</em> hardware profile and select <strong>Next</strong>.</li><li>Select an <code>x86</code> or <code>x86_64</code> system image running <strong>API Level 27 or later</strong> and select <strong>Next</strong>.<ul><li>While physical ARCore devices are supported on API Level 24 or later, Android Emulator support requires API Level 27 or later.</li><li>Only x86-based Android Emulator architectures are supported. Other architectures such as <code>arm64-v8a</code>, <code>armeabi-v7</code>, are not currently supported.</li><li><strong>macOS only with ARCore SDK 1.16.0 or later:</strong> Due to a <a href="https://issuetracker.google.com/141500087" target="_blank" rel="noopener">known issue</a>, Android Emulator <code>x86_64</code> system images are not supported on macOS with ARCore SDK 1.16.0 or later. As a workaround, use an <code>x86</code> system image.</li></ul></li><li>Verify that your virtual device is configured correctly:<ul><li>Click <strong>Show Advanced Settings</strong>.</li><li>Make sure that <strong>Camera Back</strong> is set to <strong>VirtualScene</strong>.</li></ul></li><li>Click <strong>Finish</strong> to create your AVD.</li></ol><h2 id="3-4-Run-your-app"><a href="#3-4-Run-your-app" class="headerlink" title="3.4 Run your app"></a>3.4 Run your app</h2><p>Test an ARCore app on an AR-supported virtual device in the emulator. To do this, you can follow the Android Studio instructions to <a href="https://developer.android.google.cn/studio/run/emulator.html#runningapp" target="_blank" rel="noopener">Run an app in the Android Emulator</a>.</p><p><strong>Note:</strong> To run apps with NDK components in the Android Emulator, your app must be built with <a href="https://developer.android.google.cn/ndk/guides/abis.html" target="_blank" rel="noopener"><strong>x86 ABIs</strong></a>. For an example, see the <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples/hello_ar_c" target="_blank" rel="noopener"><strong>ARCore HelloAR C sample app</strong></a>.</p><h3 id="3-4-1-Update-Google-Play-Services-for-AR"><a href="#3-4-1-Update-Google-Play-Services-for-AR" class="headerlink" title="3.4.1 Update Google Play Services for AR"></a>3.4.1 Update Google Play Services for AR</h3><p>The version of Google Play Services for AR on the emulator is likely out of date. Follow these instructions to update it:</p><ol><li><p>Download the latest <strong>Google_Play_Services_for_AR_1.18.0_x86_for_emulator.apk</strong> from the GitHub <a href="https://github.com/google-ar/arcore-android-sdk/releases" target="_blank" rel="noopener">releases</a> page.</p></li><li><p>Install the downloaded APK into each AVD you’d like to use:</p><p>Start the desired AVD, then drag the downloaded APK onto the running emulator, or install it using <code>adb</code> while the virtual device is running:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adb install -r Google_Play_Services_for_AR_1.18.0_x86_for_emulator.apk</span><br></pre></td></tr></table></figure></li></ol><p>Repeat these steps process for any additional AVDs you’d like to use.</p><h3 id="3-4-2-Control-the-virtual-scene"><a href="#3-4-2-Control-the-virtual-scene" class="headerlink" title="3.4.2 Control the virtual scene"></a>3.4.2 Control the virtual scene</h3><p>When your app connects to ARCore, you’ll see an overlay describing how to control the camera and a status bar below the emulator window.</p><p><img src="https://developers.google.cn/ar/images/ar_emulator_overlay.png" alt></p><h4 id="Move-the-virtual-camera"><a href="#Move-the-virtual-camera" class="headerlink" title="Move the virtual camera"></a>Move the virtual camera</h4><p>Press and hold <strong>Option</strong> (macOS) or <strong>Alt</strong> (Linux or Windows) to access camera movement controls. Use the following controls to move the camera:</p><table><thead><tr><th style="text-align:left">Platform</th><th style="text-align:left">Action</th><th style="text-align:left">What to do</th></tr></thead><tbody><tr><td style="text-align:left"><strong>macOS</strong></td><td style="text-align:left">Move left or right</td><td style="text-align:left">Hold <strong>Option</strong> + press <strong>A</strong> or <strong>D</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move down or up</td><td style="text-align:left">Hold <strong>Option</strong> + press <strong>Q</strong> or <strong>E</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move forward or back</td><td style="text-align:left">Hold <strong>Option</strong> + press <strong>W</strong> or <strong>S</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Change device orientation</td><td style="text-align:left">Hold <strong>Option</strong> + move mouse</td></tr><tr><td style="text-align:left"><strong>Linux</strong> or <strong>Windows</strong></td><td style="text-align:left">Move left or right</td><td style="text-align:left">Hold <strong>Alt</strong> + press <strong>A</strong> or <strong>D</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move down or up</td><td style="text-align:left">Hold <strong>Alt</strong> + press <strong>Q</strong> or <strong>E</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move forward or back</td><td style="text-align:left">Hold <strong>Alt</strong> + press <strong>W</strong> or <strong>S</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Change device orientation</td><td style="text-align:left">Hold <strong>Alt</strong> + move mouse</td></tr></tbody></table><p>Release <strong>Option</strong> or <strong>Alt</strong> to return to interactive mode in the emulator.</p><p>Use the <strong>Virtual Sensors</strong> tab in <a href="https://developer.android.google.cn/studio/run/emulator.html#extended" target="_blank" rel="noopener">Extended controls</a> for more precise device positioning.</p><h3 id="3-4-3-Add-Augmented-Images-to-the-scene"><a href="#3-4-3-Add-Augmented-Images-to-the-scene" class="headerlink" title="3.4.3 Add Augmented Images to the scene"></a>3.4.3 Add Augmented Images to the scene</h3><p>Load images into the emulator’s simulated environment to test <a href="https://developers.google.cn/ar/develop/java/augmented-images" target="_blank" rel="noopener">Augmented Images</a>.</p><p><img src="https://developers.google.cn/ar/images/augmented-images-emulator.png" alt>Use the <strong>Camera</strong> tab in Extended controls to add or modify <strong>Scene images</strong>. There are two image locations, one on the wall and one on the table.</p><p><img src="https://developers.google.cn/ar/images/augmented-images-emulator-settings.png" alt></p><p>To view these image locations in the scene, launch your emulator, then move the camera to the dining room area through the door behind the camera’s starting position.</p><h3 id="3-4-4-Troubleshooting-tips"><a href="#3-4-4-Troubleshooting-tips" class="headerlink" title="3.4.4 Troubleshooting tips"></a>3.4.4 Troubleshooting tips</h3><ul><li>If your ARCore app launches and you see an “AR Core not supported” message, check the revision on your system image. Make sure you are using <strong>API Level 27 Revision 4</strong>.</li><li>If your ARCore app fails to open the camera when it launches, make sure that <strong>Camera Back</strong> is set to <strong>VirtualScene</strong>, as described in the <a href="https://developers.google.cn/ar/develop/java/emulator#configure_the_virtual_device" target="_blank" rel="noopener">configuration steps above</a>.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ARCore 是 Google 为开发者构建的增强现实平台，如何让虚拟物体和真实世界完美融合，这一直是 Google ARCore 技术所探讨的问题。众所周知，当虚拟物体附近有现实物体时，有可能会出现互相交融、重叠等效果，大大地影响了用户体验。这一直是 AR 技术的难点，也是 Google 不懈努力的方向。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="AR" scheme="http://yuanquanquan.top/tags/AR/"/>
    
  </entry>
  
  <entry>
    <title>PC-DARTS:Partial Channel Connections for Memory-Efficient DifferentiableArchitecture Search</title>
    <link href="http://yuanquanquan.top/2020/20200529/"/>
    <id>http://yuanquanquan.top/2020/20200529/</id>
    <published>2020-05-29T14:34:41.000Z</published>
    <updated>2020-06-09T21:01:40.285Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>DARTS是可微分网络架构搜索，而本文将要解读的PC-DARTS是DARTS的扩展。DARTS方法速度快，但是由于它需要训练一个超网来寻找最优结构，需要消耗大量的内存和计算资源。因此论文作者提出了Partially-ConnectedDARTS，即部分通道连接的DARTS方法，通过对super-net进行一小部分的采样，能够减少网络搜索过程中计算的内存占用。但是由于通过在通道子集上执行运算搜索，而其他部分不变，这可能导致挑选超大网络边时出现不一致，为了解决这个问题，作者提出了边正则化，在搜索中添加边级别的超参数集合，来减少搜索的不确定性。</p></blockquote><a id="more"></a><p><img src="https://i.loli.net/2020/05/30/aWSh36meoy2DkIL.png" alt="PC-DARTS"></p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>DARTS把运算操作进行连续松弛，可以让网络的超参数搜索可微，进而达到端对端的网络搜索。但是由于在一个很大的超网上进行搜索，导致它的搜索空间有大量的冗余，使得计算量和内存占用很大。作者为了减少内存和计算量，采用这样的思路：不用把全部通道都送入运算选择中，而是对通道子集进行随机采样进行运算，其他的通道直接通过。但是这种思路带来了一种问题：由于采样的随机性，网络连接的选择可能是不稳定的。由此作者又引入了边正则化（edgenormalization）进行稳定，添加一个额外的边选择超参数集合。同时得益于部分通道连接的策略，选择1/K的通道可以减少K倍内存，那么就可以增大K倍的batchsize，不仅可以加速K倍，还可以稳定搜索。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>DARTS把网络的搜索拆分为L个cell，每个cell为N个节点的有向无环边，每个节点是一个网络层。Cell的类型有两种：ReductionCell和NormalCell，在整个超网中共享。</p><p>一般网络的每一层代表着一种操作，这个操作可能是卷积、池化、激活等函数，但在超网络SuperNet中，每一层网络是由多种运算组合起来的，每一种运算对应一个系数$\alpha$,，混合运算的加权公式如下：$f_{i, j}\left(\mathbf{x}<em>{i}\right)=\sum</em>{o \in \mathcal{O}} \frac{\exp \left{\alpha_{i, j}^{o}\right}}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left{\alpha_{i, j}^{o^{\prime}}\right}} \cdot o\left(\mathbf{x}_{i}\right)$</p><h2 id="Partial-Channel-Connections"><a href="#Partial-Channel-Connections" class="headerlink" title="Partial Channel Connections"></a><strong>Partial Channel Connections</strong></h2><p>DARTS的问题是需要大量内存，为了调节$|\theta|$个运算，需要把每个运算的结果存储起来，需要使用$|\theta|$倍的内存，为了能存下必须降低batchsize大小，这就降低了速度。而本文提出了partially-connectedDARTS方法，简称为PC-DARTS，方法如图1所示。该方法将网络提取的特征在通道维度上进行了1/K采样，只对采样后的通道进行处理，然后将这些特征与剩余的特征进行拼接(concat)。为了减少由采样带来的不确定性，作者又提出了边正则化，添加了边级别的超参数$\beta$</p><p><img src="https://i.loli.net/2020/05/29/3RxMkinqbHE7IAs.png" alt="PC-DARTS方法图示"></p><p>PC-DARTS的运算加权公式为:$f_{i, j}^{\mathrm{PC}}\left(\mathbf{x}<em>{i} ; \mathbf{S}</em>{i, j}\right)=\sum_{o \in \mathcal{O}} \frac{\exp \left{\alpha_{i, j}^{o}\right}}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left{\alpha_{i, j}^{o^{\prime}}\right}} \cdot o\left(\mathbf{S}<em>{i, j} * \mathbf{x}</em>{i}\right)+\left(1-\mathbf{S}<em>{i, j}\right) * \mathbf{x}</em>{i}$</p><p>其中$S_{i,j}$为通道采样mask，标为1的通道直接作为输出。我们随机采样1/K个通道，其中这个K我们作为超参数用来平衡速度和准确率。挑选出通道的1/K可以减少K倍的计算量，还可以有更多的样本来采样，这对于网络搜索尤为重要。</p><p><strong>Edge Normalization</strong></p><p>上一节中对通道进行随机采样的好处是能减少所选操作的偏置，即对于边(i,j)，给定$x_i$,使用两组超参数${\alpha^{o}<em>{i,j}}$和$${\alpha^{o’}</em>{i,j}}$$的差距就减小了。但是它削弱了无权重运算（如跳层连接，最大池化）的优势。在早期，搜索算法更喜欢无权重的运算，因为这些运算没有参数，能够得到输出一致的结果。但是对于有权重的运算，优化过程中会出现不一致的情况。这样无权重的运算会占据很大的比重，后续即使有权重的运算优化的很好，也无法超过它们。这种现象在代理输入比较困难的时候尤其严重，这也导致DARTS在ImageNet上效果不好。</p><p>为此提出了边正则化来抑制该现象。边正则化为基本单元中的第i层网络的每个输入分配了一个$\beta$参数，公式表示如下：$\mathbf{x}<em>{j}^{\mathrm{PC}}=\sum</em>{i&lt;j} \frac{\exp \left{\beta_{i, j}\right}}{\sum_{i^{\prime}&lt;j} \exp \left{\beta_{i^{\prime}, j}\right}} \cdot f_{i, j}\left(\mathbf{x}_{i}\right)$</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>作者在两个常用的公共数据集CIFAR-10和ImageNet进行了实验。</p><p><strong>CIFAR-10实验结果</strong></p><p>CIFAR-10上的实验结果如图2所示。在搜索过程中，网络由8个cell堆叠组成（包含6个normalcells和2个reductioncells），并且每个cell由6个节点构成，normalcell和reductioncell结果如图2所示。</p><p>CIFAR-10上，选用K=4，即只有1/4的通道被采样，因此搜索期间的batchsize增加到256。训练时，SuperNet首先预热15个epochs（即固定架构超参数，只更新网络参数），使用带动量的SGD进行网络参数的优化，使用Adam优化器来对超参数${\alpha^{o}<em>{i,j}}$和$\beta</em>{i,j}$进行更新。</p><p>从表中可以看到，使用PC-DARTS方法，仅需0.1GPU天，错误率就可以达到2.57%，搜索时间和准确率都超过了baseline方法DARTS。在比较的方法中，PC-DARTS是错误率小于3%的方法中速度最快的。</p><p><img src="https://i.loli.net/2020/05/29/z71ISxRg5D2Tic4.png" alt="表1：在CIFAR-10上的实验结果"><img src="https://i.loli.net/2020/05/29/ogBx9ckQMwhSpPJ.png" alt="图2：在CIFAR-10上搜索出的cell结构"></p><h2 id="ImgaeNet上的实验结果"><a href="#ImgaeNet上的实验结果" class="headerlink" title="ImgaeNet上的实验结果"></a><strong>ImgaeNet上的实验结果</strong></h2><p>在ImageNet上的实验结果以及和SOTA方法的比较如表2所示。作者对用于CIFAR-10上的网络结构进行了小修改以适用于ImageNet。为了减小搜索时间，作者分别从ImageNet上随机采样了两个子集，采样率分别为10%和2.5%，前者用于训练网络参数权重，后者用于更新架构超参数。</p><p>由于在ImageNet进行搜索比CIFAR-10更难，为了保留更多的信息，选取K=2，即通道采样率为1/2，是CIFAR-10的两倍。仍然训练50个epochs，但是前35个epochs固定架构超参数，其他训练设置基本和CIFAR-10上的一致。</p><p>在ImageNet上的结果如表2所示，在ImageNet上的实验结果，Top-1和Top-5准确率可以达到24.2%和7.3%，是比较的方法中效果最好的，也证明了本方法在减少内存消耗上是有效的。</p><p><img src="https://i.loli.net/2020/05/29/mylQnNPtcx75V86.png" alt="表2：在ImageNet上的实验结果"></p><p>在ImageNet上搜索得到的normalcell和reductioncell如下图所示。</p><p><img src="https://i.loli.net/2020/05/30/Gr3N6YATl8Ev9XC.png" alt="图3：在ImageNet上搜索得到的cell结果"></p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a><strong>消融实验</strong></h2><h2 id="1-不同采样率的结果"><a href="#1-不同采样率的结果" class="headerlink" title="1. 不同采样率的结果"></a><strong>1. 不同采样率的结果</strong></h2><p>K是用来控制通道的采样率的一个超参数，在讨论K对实验结果的影响之前，要先明确这么一个信息：增加采样率（即使用一个更小的K值）能使得更精确的信息被传播；而对通道的更小一部分进行采样，会造成更大的正则化，可能会引起过拟合。为了研究K的影响，作者在CIFAR-10上实验了4种K值对性能的影响，分别为1/1,1/2,1/4和1/8，实验结果如图4所示。从图中可以看出来采样率在1/4时的效果最好，准确率最高，搜索速度最快。使用1/8的采样率，尽管会进一步减少搜索时间，但是会产生严重的性能下降。<img src="https://i.loli.net/2020/05/30/QTBl1tOjRS2g9i5.png" alt="图4：不同K值（即采样率）对实验结果的影响"></p><h2 id="2-PC-DARTS不同组件的作用"><a href="#2-PC-DARTS不同组件的作用" class="headerlink" title="2. PC-DARTS不同组件的作用"></a><strong>2. PC-DARTS不同组件的作用</strong></h2><p>作者又探讨PC-DARTS中的partialchannel connection（表中简称为PC）和edgenormalization（表中简称为EN）的作用，结果如表3所示。从表中可以很明显看到，EN及时在通道是全部连接的情况下，也能带来正则化的效果。同时，edgenormalization和partialchannel connection一起使用，可以提供更进一步的改进效果。而不使用edgenormalization，则网络参数的数量和精确度都会受到影响。</p><p><img src="https://i.loli.net/2020/05/30/3tqhK8Vr7bjQpXP.png" alt="表3：在CIFAR-10和ImageNet上的消融实验"></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>本篇论文提出了一个简单但却有效的PC-DARTS(partially-connecteddifferentiable architecturesearch)方法，<strong>它的核心思想是随机采样一部分通道用于运算搜索，这样能更有效的利用内存，可以使用更大的batchsize获得更高的稳定性。另一个贡献是提出了边标准化(edgenormalization)来稳定搜索的过程，这是个轻量化的模块，基本不需要太多的计算量。此方法在CIFAR-10上完整搜索只需要0.1GPU天，在Imagenet上搜索需要3.8GPU天。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;DARTS是可微分网络架构搜索，而本文将要解读的PC-DARTS是DARTS的扩展。DARTS方法速度快，但是由于它需要训练一个超网来寻找最优结构，需要消耗大量的内存和计算资源。因此论文作者提出了Partially-ConnectedDARTS，即部分通道连接的DARTS方法，通过对super-net进行一小部分的采样，能够减少网络搜索过程中计算的内存占用。但是由于通过在通道子集上执行运算搜索，而其他部分不变，这可能导致挑选超大网络边时出现不一致，为了解决这个问题，作者提出了边正则化，在搜索中添加边级别的超参数集合，来减少搜索的不确定性。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="AutoML" scheme="http://yuanquanquan.top/tags/AutoML/"/>
    
  </entry>
  
  <entry>
    <title>Typecho</title>
    <link href="http://yuanquanquan.top/2020/202005251/"/>
    <id>http://yuanquanquan.top/2020/202005251/</id>
    <published>2020-05-25T06:17:17.000Z</published>
    <updated>2020-12-16T08:10:48.972Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近因为疫情<a href="https://developer.aliyun.com/adc/student/?pid=mm_25282911_3455987_122436732" target="_blank" rel="noopener">阿里云可以白嫖半年的服务器</a>,刚好搭一个博客</p></blockquote><a id="more"></a>  <p><strong>一、安装宝塔面板</strong></p><ol><li><p>打开控制命令行:win+r，输入cmd</p></li><li><p>输入命令：ssh root@服务器IP地址，输入密码就能进入服务器.登陆成功的话如图所示!<img src="https://i.loli.net/2020/06/10/T9G27VM5Kl8e6EN.png" alt></p></li><li><p>在shell环境下输入安装宝塔的命令，安装命令宝塔官方上有，直接按照自己的系统版本选择，我这里选择ubuntn的安装命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y wget &amp;&amp; wget -O install.sh http://download.bt.cn/install/install_6.0.sh &amp;&amp; sh install.sh</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li>最后得到一个宝塔面板的链接网址和账户和登录密码。<img src="https://i.loli.net/2020/05/25/rh4Ucl7QoJmA1gs.png" alt></li></ol><p><strong>二、在面板上搭建typecho博客</strong></p><p>用刚才的账号和密码登录到宝塔面板，新建一个站点，然后输入域名（没有的话可以从阿里云买一个），勾选mysql数据库，注意记住创建之后的数据库名和密码，后面用的到。<img src="https://i.loli.net/2020/05/25/9fWHMQyv3AixCEd.png" alt></p><p>注意这里的端口号，默认是80，如果80端口号被占用，可以改成81或者82等。记住这里的端口号，之后将需要这个端口号设置typecho博客。<strong>注意阿里云要开放80端口<img src="https://i.loli.net/2020/05/25/uWCEPyclmRx2pFo.png" alt></strong></p><ul><li>从typecho官方网站下载1.1正式版：[typecho官方下载<img src="https://i.loli.net/2020/05/25/69BTLoYXgMeku5Z.png" alt></li></ul><ul><li><p>将下载之后的文件解压放到创建的网站文件夹下<br><img src="https://i.loli.net/2020/05/25/g9YlzInd3Q65LXF.png" alt></p></li><li><p>访问刚刚设置的网站域名端口号，一般80是被占用的，需要自己改一个端口号，我这里改成了81，但是此时直接访问81的端口号会404 NOT Found,这个时候需要去宝塔面板里放行端口号<img src="C:%5CUsers%5CLenovo%5CDesktop%5Cyolov1%5Ctypecho-7.png" alt></p></li></ul><ul><li><p>接下来就循规蹈矩的进行安装了,输入刚刚的数据库名、用户名和密码，密码忘记了可以去宝塔面板里自己改一个简单点的。<img src="https://i.loli.net/2020/05/25/YP2in6kyGRJhvXf.png" alt></p></li><li><p>这里会提示没有config.inc.php配置文件，这个时候需要自己去创建一个文件，将其中的代码复制进去，放到blog文件夹下</p></li></ul><p><strong>三、选择typecho主题，并可以随时切换</strong><br>typecho模板主题</p><p>将下载的主题，放到当前目录下，默认会有一个系统主题</p><p>登录到自己的博客地址，然后进行博客的相关设置。主题修改</p><p><strong>设置外观</strong></p><p>控制台 -&gt; 外观，“设置外观” 选项。里面是和主题相对应的定制选项，比如 网站 logo、功能开关、站点描述、缩略图设置等，主要是针对该主题的一些个性化、功能性设置。</p><p><img src="https://i.loli.net/2020/05/25/ZwTid3hyXcoY2b7.jpg" alt></p><p><strong>自定义修改</strong></p><p>“编辑当前外观” 自定义修改主题样式。这里列出的模板文件和主题有关，如果没有需要修改的那个文件，可以去主题文件夹里查找直接修改。</p><p><img src="https://i.loli.net/2020/05/25/vnzSfArp7Msgty1.jpg" alt></p><p><strong>Tips</strong></p><ul><li>有的主题需要更改主题文件夹为指定名称才有效，注意查看主题说明；</li><li>自定义修改主题样式后，如果刷新网站没有变化，尝试刷新 CDN 缓存或者浏览器本地缓存。</li></ul><hr><h2 id="主题推荐"><a href="#主题推荐" class="headerlink" title="主题推荐"></a>主题推荐</h2><p>这里分享的全部是免费主题。</p><p><strong>Pinghsu</strong></p><p>简介：卡片式设计，简洁美观。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//www.linpx.com/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/chakhsu/pinghsu" target="_blank" rel="noopener">https://github.com/chakhsu/pinghsu</a></li></ul><p><img src="https://i.loli.net/2020/05/25/jFxhHZwvpJztmGC.jpg" alt></p><p><strong>Material</strong></p><p>简介：Material Design theme for typecho. 扁平化设计主题。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//blog.lim-light.com/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/idawnlight/typecho-theme-material" target="_blank" rel="noopener">https://github.com/idawnlight/typecho-theme-material</a></li></ul><p><img src="https://i.loli.net/2020/05/25/Lak4Az31VHusSUt.jpg" alt></p><p><strong>NexT.Pisces</strong></p><p>简介：Hexo 主题 NexT.Pisces 的 Typecho 移植版，基于 zgq354 的 NexT.Mist 修改制作。</p><ul><li><a href="https://link.zhihu.com/?target=http%3A//notes.iissnan.com/" target="_blank" rel="noopener">Demo</a>（这个是 Hexo 的，效果差不多）</li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/newraina/typecho-theme-NexTPisces" target="_blank" rel="noopener">https://github.com/newraina/typecho-theme-NexTPisces</a></li></ul><p><img src="https://i.loli.net/2020/05/25/qNb1SeIEDVps8Td.jpg" alt></p><p><strong>Maupassant</strong></p><p>简介：极简响应式主题。</p><ul><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/pagecho/maupassant" target="_blank" rel="noopener">https://github.com/pagecho/maupassant</a></li></ul><p><img src="https://i.loli.net/2020/05/25/aXY7gzDoeGEvPBl.jpg" alt></p><p><strong>Optica</strong></p><p>简介：单栏小清新主题 Optica。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//1000yun.cn/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/iduzui/optica" target="_blank" rel="noopener">https://github.com/iduzui/optica</a></li></ul><p><img src="https://pic1.zhimg.com/80/v2-1deecd43d3f4c790780feb33462c88a4_720w.jpg" alt></p><p><strong>ArmX</strong></p><p>简介：响应式纯净前端结构，不依赖第三方前端框架；自带音乐播放器，全站pjax；支持第三方登录；支持cdn加速、生成缩略图等，功能完善。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//codeup.me/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/natcube/ArmX" target="_blank" rel="noopener">https://github.com/natcube/ArmX</a></li></ul><p><img src="https://i.loli.net/2020/05/25/kbZ2VihLnOjB8SI.jpg" alt></p><p><strong>Affinity</strong></p><p>简介：三列卡片式布局。移植自 ghost，原作地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/Showfom/Affinity" target="_blank" rel="noopener">https://github.com/Showfom/Affinity</a>。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//affinity.419.at/" target="_blank" rel="noopener">Demo</a></li><li>地址： <a href="https://link.zhihu.com/?target=https%3A//affinity.419.at/2017/07/28/affinity.html" target="_blank" rel="noopener">https://affinity.419.at/2017/07/28/affinity.html</a></li></ul><p><img src="https://i.loli.net/2020/05/25/7UpfkVOj3raQ1K8.jpg" alt></p><p><strong>Junichi</strong></p><p>简介：轻量级，无前端框架；响应式设计。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//uefeng.com/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/siseboy/junichi" target="_blank" rel="noopener">https://github.com/siseboy/junichi</a></li></ul><p><img src="https://i.loli.net/2020/05/25/jBua1S2DPHIpyiW.jpg" alt></p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Typecho 主题还是很丰富的，而且很多是博主们写着自己用，然后分享给网友的。如果喜欢可以选择购买付费主题或者赞助支持主题作者。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近因为疫情&lt;a href=&quot;https://developer.aliyun.com/adc/student/?pid=mm_25282911_3455987_122436732&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿里云可以白嫖半年的服务器&lt;/a&gt;,刚好搭一个博客&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="博客搭建" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="blog" scheme="http://yuanquanquan.top/tags/blog/"/>
    
      <category term="教程" scheme="http://yuanquanquan.top/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="typecho" scheme="http://yuanquanquan.top/tags/typecho/"/>
    
  </entry>
  
  <entry>
    <title>胃与真相</title>
    <link href="http://yuanquanquan.top/2020/20200524/"/>
    <id>http://yuanquanquan.top/2020/20200524/</id>
    <published>2020-05-24T15:25:45.000Z</published>
    <updated>2020-06-09T21:03:48.540Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>疫情期间常常听Ludovico的作品，蛰居陋室，埋头科研，不问世事，倒也颇映衬了他作品中空旷辽远的寂静和哲学思索。</p><p>在经过了盲目自信、怀疑论、疲乏应对、指责中国之后，新冠疫情在美国的爆发，终究还是成为了社会的主要议题。搬空的超市只是一个缩影，标志着不管你是否愿意，都必须接受这样一个事实 - 生活不会再像以前那样安宁祥和。而现在，被疫情考验之后的美国社会，更多的人开始意识到，再也回不到疫情之前的样子。纽约州长Cuomo在一次例行的发布会上说：人们都想要回到生活常态（back to normal），但事实是，回不去了，我们只会进入到一个新的常态（a new normal）。</p></blockquote><a id="more"></a>  <p>之前在Twitter上看到一个老美说，新冠疫情就是美国政府的一个谎言，目的是用来操纵民意，为大选服务。那时大多数美国人对于新冠是比较乐观的，而当时我只是觉得这人真傻，她不知道中国人民抗疫付出的代价有多大，得到的教训有多深刻，她只要稍微看看国际新闻，就会知道这个疾病的严重性，怎会是捏造之实。半个月之后，奥斯汀市政府宣布城市限行，民众居家禁止外出。一时间，所有人都被拽进一个陌生的现实，它就在那儿，至今仍然在那儿。</p><p>但我后来意识到，她的那种不屑，或者是无可奈何，大概是对的。那不代表她不看国际新闻，事实是，获得真相对改变现状的帮助太小，而听信谎言，或者是厌倦与逃离，也不一定是愚蠢的表现。不管形势如何发展，普通民众，除了恐慌性地抢购生活物资，在这种宏大层面的公共安全事件面前，又能做些什么呢。今天可以是新冠疫情，明天可以是任何突发事件。但生活总要继续，总要果腹，病死和饿死，又有什么区别。就算病死和饿死有区别，在最终报给总统的伤亡人数里，也都只是数字，背后的故事谁又会去讲述。</p><p>纪录片《华氏911》里说，他们把恐怖指数调到高，民众开始疯狂抢购，囤积自卫；然后又调到低，民众又像往常一样推婴儿车在公园散步和遛狗。不断地往复着模棱两可的语境，正如整个疫情当中，川普政府对于病毒危险性的评估不断反复闪烁其词，模棱两可，而真正应该领导疫情防控的Fauci博士却被拒绝参与众议院的新冠听证会。事实上，模糊的语境只会让民众不知所措，当民众足够恐慌，他们便像绵羊一样温顺，可以随意地剪他们的毛。就像训练一条狗，你叫它坐下，然后叫它打滚，刚开始要滚你又叫它站起来，狗不知道要怎么做才好。</p><p>人们希望真相，但谎言和真相，没有本质的区别。它们相互转化，而又被权威所利用，唯一的区别是，做谎言的帮凶，浑水摸鱼；还是做真相的寻找者，逆流而上。不幸的是，人类所建立的这个系统的复杂度和信息的非对称决定了，没有绝对的真相，也没有绝对的谎言。大概更永恒的，只有我们的这个胃了罢：平民的胃是早中晚三餐，是早九晚五有班可以上；既得利益者的胃是道琼斯指数，是一年四季有资本可以攫取。毕竟，胃才是任何文明，任何语境的最终动力，因为那些没法填饱自己胃的物种都灭绝了；毕竟，既得利益者们所做的一切，大概也是为了他们的后代的胃可以不用空着。</p><p>对饥饿的恐惧，是刻在人类骨子里基因里的。而疾病，同样如此。所有人都在玩着同样的生存游戏。</p><p>1976年版的爱因斯坦文集中译本第一卷开篇中“自述”的第二段写到：</p><p>“当我还是一个相对早熟的少年的时候，我就已经深切地意识到，大多数人终生无休止地追逐的那些希望和努力是毫无价值的。而且，我不久就发现了这种追逐的残酷，这在当年较之今天是更加精心地用伪善和漂亮的字句掩饰着的。每个人只是因为有个胃，就注定要参与这种追逐。而且，由于参与这种追逐，他的胃是有可能得到满足的；但是，一个有思想，有感情的人却不能由此而得到满足。”</p><p>我大概是非常同意这段话的，也惊叹于少年时期的爱因斯坦便已有这样深刻的见解。而我也对自己写下这些文字感到一些隐藏的担忧，我们这些研究物理学的人，本是没有足够的经验和资历来对社会现象做过多的评述。但我们仍然发现，社会和自然至少在基本的逻辑上，是按照相似的法则来运转的。比如说生态系统中的物质能量流动的结构就和社会中的物资供应链很相似。著名的马太效应也能找到自然中的起源，比如热力学中的Ostwald Ripening 效应就说，在同一个压力环境里面，大气泡的增长都是以小气泡的消亡为代价；又比如食物链顶端的物种数量可能只占一个生态系统中物种数量的不到百分之一，但却统治着系统中大部分的资源。所以大概自然界中的“穷者愈穷，富者愈富”比人类社会更严重，毕竟作为人类的我们还发明了公平和道德来对冲自然法则。</p><p>爱因斯坦的思路其实还是物理的思路，或许过于简化，但他揭示了一个模型，那就是社会就像自然一样，构建于一些简单的法则，比如胃的满足；但社会的复杂度，则来自于法则衍生出来的结构，比如掩饰，比如修辞，比如文明。</p><p>真相，或者谎言，都是修辞。而胃，才是真正在运作的东西。</p><p>前段时间Twitter上有一则赞数很高的帖子如是说：不是新冠疫情导致了社会的撕裂（divide），而是新冠疫情揭示（reveal）了一个撕裂着的社会。邻居们开始不赞同彼此的观点；民粹开始系统性地对抗精英，对抗建制（establishment），对抗生产力的进步；大量的人开始失业，并将失业的原因归咎于从墨西哥偷渡过来的移民；每个人都和他人物理隔离，却在互联网的世界里继续争吵。而在这种争吵中，谁也不会被谁说服。</p><p>这一切，其实早在新冠以前就已经开始。</p><p>在新冠之前，社会不就已经在往着嘈杂的网络，疏离的物理联系，越来越多的人类丢掉工作的方向发展了么？新冠只是加剧了这种转型的阵痛。人类面临的，是一个生产力模式的变革，我们将被迫更深刻地去审视人类在自然中的位置，审视个体在社会中的位置，正如人类积累的知识中所探讨的那样。人类的发展模式将越来越不依赖于人际联系，而是依赖于一个个共同的理性的大脑们以及它们所构建的生产力爆炸式增长。讽刺的是，如果说工业革命以前或者说早期的工业革命中，大部分人口为生产力与资本贡献了宝贵的劳动力，那么正在进行的智能化革命则剥夺了越来越多人口的工作。这部分人，这一大部分人，除了加入民粹和反智的潮流，还有别的选择么？这种转型是一种系统性的对低效率人类劳动的价值否定，它不仅是经济上的，也是文化上的对大众的边缘化。如果大众越来越无法参与到生产力的进步中和基于生产力的文化构建中，那社会的撕裂是必然的。也有很多人在说，智能化革命虽然替代了很多人类劳动力，但它也带来了新的工作机会，比如算法工程师，物联网开发者等等。这是一种乐观的估计，事实是，逐利的资本会愿意帮助大众具备获得这些工作机会的技能吗？如果20%的失业率是因为工作效率是普通人类10倍的机器替代了他们，那么你可以构建一种模式使得这20%的人以更高的效率去替代这些机器吗？要知道，这些机器的效率也会越来越高，而创造这些机器，可能都不需要哪怕是2%的人。</p><p>人类从蛮荒走向文明，是因为胃的驱动，我们打败了野兽，获得了食物，是因为人类可以思考，可以想象，可以定义真相，抑或是谎言。而文明也在塑造着我们的胃。这种不可思议的能力塑造着人类和自己的关系，和自然的关系。我们真正应该思考的是，胃将把人类从文明带向何方，文明会是我们存在的最终形式吗？也许会，也许不会。但确凿的理解是，文明正在以一种我们不充分理解的方式改变着修辞的版图，改变着胃的属性，而于此间，真正危险的事情是，除了接受这种改变，我们似乎没有第二种选择，因为每个人只有一个胃。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;疫情期间常常听Ludovico的作品，蛰居陋室，埋头科研，不问世事，倒也颇映衬了他作品中空旷辽远的寂静和哲学思索。&lt;/p&gt;
&lt;p&gt;在经过了盲目自信、怀疑论、疲乏应对、指责中国之后，新冠疫情在美国的爆发，终究还是成为了社会的主要议题。搬空的超市只是一个缩影，标志着不管你是否愿意，都必须接受这样一个事实 - 生活不会再像以前那样安宁祥和。而现在，被疫情考验之后的美国社会，更多的人开始意识到，再也回不到疫情之前的样子。纽约州长Cuomo在一次例行的发布会上说：人们都想要回到生活常态（back to normal），但事实是，回不去了，我们只会进入到一个新的常态（a new normal）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Conditional Random Fields</title>
    <link href="http://yuanquanquan.top/2020/20200511/"/>
    <id>http://yuanquanquan.top/2020/20200511/</id>
    <published>2020-05-11T05:30:00.000Z</published>
    <updated>2020-06-14T08:01:05.507Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>条件随机场(conditional random fields，简称 $CRF$，或$CRFs$)，是一种判别式概率模型，常用于标注或分析序列资料，如自然语言文字或是生物序列。</p><p>条件随机场是条件概率分布模型P(Y|X)，表示的是给定一组输入随机变量X的条件下另一组输出随机变量Y的马尔可夫随机场，也就是说$CRF$的特点是假设输出随机变量构成马尔可夫随机场。</p></blockquote><a id="more"></a>  <h2 id="知识框架"><a href="#知识框架" class="headerlink" title="知识框架"></a>知识框架<img src="https://i.loli.net/2020/05/14/Xodiy9fMFzDgebp.png" alt></h2><h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><p>定义：假设一个随机过程中， $t_n$时刻的状态$x_n$的条件发布，只与其前一状态$x_{n-1}$相关，即：<br>$$<br>P\left(x_{n} | x_{1}, x_{2}, \ldots, x_{n-1}\right)=P\left(x_{n} | x_{n-1}\right)<br>$$<br>则将其称为马尔可夫过程。</p><p><img src="https://i.loli.net/2020/05/14/h3UIwjL2z8pMOma.png" alt></p><h2 id="隐马尔可夫算法-HMM"><a href="#隐马尔可夫算法-HMM" class="headerlink" title="隐马尔可夫算法(HMM)"></a>隐马尔可夫算法(HMM)</h2><p><strong>1、定义</strong></p><p>隐马尔可夫算法是对含有未知参数（隐状态）的马尔可夫链进行建模的生成模型，如下图所示：</p><p><img src="https://i.loli.net/2020/05/14/EMGRTL3sxB7l25m.png" alt="CRF-3"></p><p>在隐马尔科夫模型中，包含隐状态和观察状态，隐状态$x_i$对于观察者而言是不可见的，而观察状态$y_i$对于观察者而言是可见的。隐状态间存在转移概率，隐状态$x_i$到对应的观察状态$y_i$间存在输出概率。</p><p><strong>2、假设</strong></p><p>假设隐状态$x_i$的状态满足马尔可夫过程，$i$时刻的状态$x_i$的条件分布，仅与其前一个状态$x_{i-1}$相关，即：<br>$$<br>P\left(x_{i} | x_{1}, x_{2}, \ldots, x_{i-1}\right)=P\left(x_{i} | x_{i-1}\right)<br>$$<br>假设观测序列中各个状态仅取决于它所对应的隐状态，即：<br>$$<br>P\left(y_{i} | x_{1}, x_{2}, \ldots, x_{i-1}, y_{1}, y_{2}, \ldots, y_{i-1}, y_{i+1}, \ldots\right)=P\left(y_{i} | x_{i}\right)<br>$$<br><strong>3、存在问题</strong></p><p>在序列标注问题中，隐状态（标注）不仅和单个观测状态相关，还和观察序列的长度、上下文等信息相关。例如词性标注问题中，一个词被标注为动词还是名词，不仅与它本身以及它前一个词的标注有关，还依赖于上下文中的其他词</p><h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><p>以线性链条件随机场为例</p><p><strong>1、定义</strong></p><p>给定$X=(x_1,x_2····,x_n)$,$Y=(y_1,y_2····,y_n)$均为线性链表示的随机变量序列，若在给随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔可夫性：<br>$$<br>P\left(y_{i} | x_{1}, x_{2}, \ldots, x_{i-1}, y_{1}, y_{2}, \ldots, y_{i-1}, y_{i+1}\right)=P\left(y_{i} | x, y_{i-1}, y_{i+1}\right)<br>$$<br>则称$P(Y|X)$为线性链条件随机场。</p><p>通过去除了隐马尔科夫算法中的观测状态相互独立假设，使算法在计算当前隐状态$x_i$时，会考虑整个观测序列，从而获得更高的表达能力，并进行全局归一化解决标注偏置问题。</p><p><strong>1）参数化形式</strong><br>$$<br>p(y | x)=\frac{1}{Z(x)} \prod_{i=1}^{n} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)<br>$$<br>其中：$Z(x)$为归一化因子，是在全局范围进行归一化，枚举了整个隐状态序列$x_{1…n}$的全部可能，从而解决了局部归一化带来的标注偏置问题。</p><p>$t_k$为定义在边上的特征函数，转移特征，依赖于前一个和当前位置$s_1$为定义在节点上的特征函数，状态特征，依赖于当前位置</p><p><strong>2）简化形式</strong></p><p>因为条件随机场中同一特征在各个位置都有定义，所以可以对同一个特征在各个位置求和，将局部特征函数转化为一个全局特征函数，这样就可以将条件随机场写成权值向量和特征向量的内积形式，即条件随机场的简化形式。</p><ul><li><strong>step 1</strong> 将转移特征和状态特征及其权值用统一的符号表示，设有$k_1$个转移特征，$k_2$个状态特征，,记</li></ul><p>$$<br>f_{k}\left(y_{i-1}, y_{i}, x, i\right)=\left{\begin{array}{l}<br>t_{k}\left(y_{i-1}, y_{i}, x, i\right), \quad k=1,2,3, \ldots, K_{1} \<br>s_{l}\left(y_{i}, x, i\right), \quad k=k_{1}+l ; l=1,2, \ldots, K_{2}<br>\end{array}\right.<br>$$</p><ul><li><strong>step 2</strong> 对转移与状态特征在各个位置求$i$和，记作</li></ul><p>$$<br>f_{k}(y, x)=\sum_{i=1}^{n} f_{k}\left(y_{i-1}, y_{i}, x, i\right), k=1,2, \ldots, K<br>$$</p><ul><li><strong>step 3</strong> 将 和 用统一的权重表示，记作</li></ul><p>$$<br>w_{k}=\left{\begin{array}{ll}<br>\lambda_{k}, &amp; k=1,2, \ldots, K_{1} \<br>\mu_{l}, &amp; k=K_{1}+l ; l=1,2, \ldots, K_{2}<br>\end{array}\right.<br>$$</p><ul><li><p><strong>step 4</strong> 转化后的条件随机场可表示为：<br>$$<br>\begin{aligned}<br>P(y | x) &amp;=\frac{1}{Z(x)} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \<br>Z(x) &amp;=\sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x)<br>\end{aligned}<br>$$</p><ul><li><strong>step 5</strong> 若 表示权重向量：$w=(w_1,w_2,…,w_k)^T$以$F(y,x)$表示特征向量，即<br>$$<br>F(y, x)=\left(f_{1}(y | x), f_{2}(y | x), \ldots, f_{K}(y | x)\right)^{T}<br>$$<br>   则，条件随机场写成内积形式为：<br>$$<br>\begin{array}{l}<br>P_{w}(y | x)=\frac{\exp (w \cdot F(y, x)}{Z_{w}(x)} \<br>Z_{w}(x)=\sum_{y} \exp (w \cdot F(y, x))<br>\end{array}<br>$$</li></ul></li></ul><p><strong>3）矩阵形式</strong><br>$$<br>P_{w}(y | x)=\frac{1}{Z_{w}(x)} \prod_{i=1}^{n+1} M_{i}\left(y_{i-1}, y_{i} | x\right)<br>$$<br><strong>2、基本问题</strong></p><p>条件随机场包含概率计算问题、学习问题和预测问题三个问题。</p><ul><li>概率计算问题：已知模型的所有参数，计算观测序列Y出现的概率，常用方法：前向和后向算法；</li><li>学习问题：已知观测序列Y,求解使得该观测序列概率最大的模型参数，包括隐状态序列、隐状态间的转移概率分布和从隐状态到观测状态的概率分布，常用方法：Baum-Wehch算法；</li><li>预测问题：一直模型所有参数和观测序列Y，计算最可能的隐状态序列X,常用算法：维特比算法。</li></ul><p><strong>案例：利用维特比算法计算给定输入序列$x$对应的最优输出序列$y^*$：</strong><br>$$<br>\max \sum_{i=1}^{3} w \cdot F_{i}\left(y_{i-1}, y_{i}, x\right)<br>$$<br>1.初始化<br>$$<br>\delta_{1}(j)=w \cdot F_{1}\left(y_{0}=\operatorname{start}, y_{1}=j, x\right), j=1,2 i=1, \delta_{1}(1)=1, \delta_{1}(2)=0.5<br>$$<br>2.递推，对$i=2,3,…,n$<br>$$<br>\begin{array}{c}<br>i=2, \delta_{2}(l)=\max <em>{j}\left{\delta</em>{1}(j)+w \cdot F_{2}(j, l, x)\right} \<br>\delta_{2}(1)=\max \left{1+\lambda_{2} t_{2}, 0.5+\lambda_{4} t_{4}\right}=1.6, \Psi_{2}(1)=1 \<br>\delta_{2}(2)=\max \left{1+\lambda_{1} t_{1}+\mu_{2} s_{2}, 0.5+\mu_{2} s_{2}\right}=2.5, \Psi_{2}(2)=1 \<br>i=3, \delta_{3}(l)=\max <em>{j}\left{\delta</em>{2}(j)+w \cdot F_{3}(j, l, x)\right} \<br>\delta_{3}(1)=\max \left{1.6+\mu_{5} s_{5}, 2.5+\lambda_{3} t_{3}+\mu_{3} s_{3}\right}=4.3, \Psi_{3}(1)=2 \<br>\delta_{3}(2)=\max \left{1.6+\lambda_{1} t_{1}+\mu_{4} s_{4}, 2.5+\lambda_{5} t_{5}+\mu_{4} s_{4}\right}=4.3, \Psi_{3}(2)=1<br>\end{array}<br>$$<br>3.终止<br>$$<br>\max <em>{y}(w \cdot F(y, x))=\max \delta</em>{3}(l)=\delta_{3}(1)=4.3 y_{3}^{<em>}=\operatorname{argmax}<em>{1} \delta</em>{3}(l)=1<br>$$<br>4.返回路径<br>$$<br>\begin{aligned}<br>y_{2}^{</em>}=&amp; \Psi_{3}\left(y_{3}^{<em>}=\Psi_{3}(1)=2 y_{1}^{</em>}=\Psi_{2}\left(y_{2}^{<em>}\right)=\Psi_{2}(2)=1\right.\<br>求得最优路径y^{</em>}=\left(y_{1}^{<em>}, y_{2}^{</em>}, \ldots, y_{n}^{*}\right)=(1,2,1)<br>\end{aligned}<br>$$<br>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''实现条件随机场预测问题的维特比算法</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, V, VW, E, EW)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param V:是定义在节点上的特征函数，称为状态特征</span></span><br><span class="line"><span class="string">        :param VW:是V对应的权值</span></span><br><span class="line"><span class="string">        :param E:是定义在边上的特征函数，称为转移特征</span></span><br><span class="line"><span class="string">        :param EW:是E对应的权值</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.V  = V  <span class="comment">#点分布表</span></span><br><span class="line">        self.VW = VW <span class="comment">#点权值表</span></span><br><span class="line">        self.E  = E  <span class="comment">#边分布表</span></span><br><span class="line">        self.EW = EW <span class="comment">#边权值表</span></span><br><span class="line">        self.D  = [] <span class="comment">#Delta表，最大非规范化概率的局部状态路径概率</span></span><br><span class="line">        self.P  = [] <span class="comment">#Psi表，当前状态和最优前导状态的索引表s</span></span><br><span class="line">        self.BP = [] <span class="comment">#BestPath，最优路径</span></span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Viterbi</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        条件随机场预测问题的维特比算法，此算法一定要结合CRF参数化形式对应的状态路径图来理解，更容易理解.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.D = np.full(shape=(np.shape(self.V)), fill_value=<span class="number">.0</span>)</span><br><span class="line">        self.P = np.full(shape=(np.shape(self.V)), fill_value=<span class="number">.0</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment">#初始化</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> == i:</span><br><span class="line">                self.D[i] = np.multiply(self.V[i], self.VW[i])</span><br><span class="line">                self.P[i] = np.array([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">                print(<span class="string">'self.V[%d]='</span>%i, self.V[i], <span class="string">'self.VW[%d]='</span>%i, self.VW[i], <span class="string">'self.D[%d]='</span>%i, self.D[i])</span><br><span class="line">                print(<span class="string">'self.P:'</span>, self.P)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="comment">#递推求解布局最优状态路径</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> y <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">1</span>]): <span class="comment">#delta[i][y=1,2...]</span></span><br><span class="line">                    <span class="keyword">for</span> l <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">1</span>]): <span class="comment">#V[i-1][l=1,2...]</span></span><br><span class="line">                        delta = <span class="number">0.0</span></span><br><span class="line">                        delta += self.D[i<span class="number">-1</span>, l]                      <span class="comment">#前导状态的最优状态路径的概率</span></span><br><span class="line">                        delta += self.E[i<span class="number">-1</span>][l,y]*self.EW[i<span class="number">-1</span>][l,y]  <span class="comment">#前导状态到当前状体的转移概率</span></span><br><span class="line">                        delta += self.V[i,y]*self.VW[i,y]            <span class="comment">#当前状态的概率</span></span><br><span class="line">                        print(<span class="string">'(x%d,y=%d)--&gt;(x%d,y=%d):%.2f + %.2f + %.2f='</span>%(i<span class="number">-1</span>, l, i, y, \</span><br><span class="line">                              self.D[i<span class="number">-1</span>, l], \</span><br><span class="line">                              self.E[i<span class="number">-1</span>][l,y]*self.EW[i<span class="number">-1</span>][l,y], \</span><br><span class="line">                              self.V[i,y]*self.VW[i,y]), delta)</span><br><span class="line">                        <span class="keyword">if</span> <span class="number">0</span> == l <span class="keyword">or</span> delta &gt; self.D[i, y]:</span><br><span class="line">                            self.D[i, y] = delta</span><br><span class="line">                            self.P[i, y] = l</span><br><span class="line">                    print(<span class="string">'self.D[x%d,y=%d]=%.2f\n'</span>%(i, y, self.D[i,y]))</span><br><span class="line">        print(<span class="string">'self.Delta:\n'</span>, self.D)</span><br><span class="line">        print(<span class="string">'self.Psi:\n'</span>, self.P)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#返回，得到所有的最优前导状态</span></span><br><span class="line">        N = np.shape(self.V)[<span class="number">0</span>]</span><br><span class="line">        self.BP = np.full(shape=(N,), fill_value=<span class="number">0.0</span>)</span><br><span class="line">        t_range = <span class="number">-1</span> * np.array(sorted(<span class="number">-1</span>*np.arange(N)))</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> t_range:</span><br><span class="line">            <span class="keyword">if</span> N<span class="number">-1</span> == t:<span class="comment">#得到最优状态</span></span><br><span class="line">                self.BP[t] = np.argmax(self.D[<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment">#得到最优前导状态</span></span><br><span class="line">                self.BP[t] = self.P[t+<span class="number">1</span>, int(self.BP[t+<span class="number">1</span>])]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#最优状态路径表现在存储的是状态的下标，我们执行存储值+1转换成示例中的状态值</span></span><br><span class="line">        <span class="comment">#也可以不用转换，只要你能理解，self.BP中存储的0是状态1就可以~~~~</span></span><br><span class="line">        self.BP += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'最优状态路径为：'</span>, self.BP)</span><br><span class="line">        <span class="keyword">return</span> self.BP</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CRF_manual</span><span class="params">()</span>:</span>   </span><br><span class="line">    S = np.array([[<span class="number">1</span>,<span class="number">1</span>],   <span class="comment">#X1:S(Y1=1), S(Y1=2)</span></span><br><span class="line">                  [<span class="number">1</span>,<span class="number">1</span>],   <span class="comment">#X2:S(Y2=1), S(Y2=2)</span></span><br><span class="line">                  [<span class="number">1</span>,<span class="number">1</span>]])  <span class="comment">#X3:S(Y3=1), S(Y3=1)</span></span><br><span class="line">    SW = np.array([[<span class="number">1.0</span>, <span class="number">0.5</span>], <span class="comment">#X1:SW(Y1=1), SW(Y1=2)</span></span><br><span class="line">                   [<span class="number">0.8</span>, <span class="number">0.5</span>], <span class="comment">#X2:SW(Y2=1), SW(Y2=2)</span></span><br><span class="line">                   [<span class="number">0.8</span>, <span class="number">0.5</span>]])<span class="comment">#X3:SW(Y3=1), SW(Y3=1)</span></span><br><span class="line">    E = np.array([[[<span class="number">1</span>, <span class="number">1</span>],  <span class="comment">#Edge:Y1=1---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>]], <span class="comment">#Edge:Y1=2---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                  [[<span class="number">0</span>, <span class="number">1</span>],  <span class="comment">#Edge:Y2=1---&gt;(Y3=1, Y3=2) </span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">1</span>]]])<span class="comment">#Edge:Y2=2---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">    EW= np.array([[[<span class="number">0.6</span>, <span class="number">1</span>],  <span class="comment">#EdgeW:Y1=1---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0.0</span>]], <span class="comment">#EdgeW:Y1=2---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                  [[<span class="number">0.0</span>, <span class="number">1</span>],  <span class="comment">#EdgeW:Y2=1---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0.2</span>]]])<span class="comment">#EdgeW:Y2=2---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">    </span><br><span class="line">    crf = CRF(S, SW, E, EW)</span><br><span class="line">    ret = crf.Viterbi()</span><br><span class="line">    print(<span class="string">'最优状态路径为:'</span>, ret)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    CRF_manual()</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><p><img src="https://i.loli.net/2020/05/14/4ToXzPEuKs5vqCf.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;条件随机场(conditional random fields，简称 $CRF$，或$CRFs$)，是一种判别式概率模型，常用于标注或分析序列资料，如自然语言文字或是生物序列。&lt;/p&gt;
&lt;p&gt;条件随机场是条件概率分布模型P(Y|X)，表示的是给定一组输入随机变量X的条件下另一组输出随机变量Y的马尔可夫随机场，也就是说$CRF$的特点是假设输出随机变量构成马尔可夫随机场。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="http://yuanquanquan.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Neural Architecture Search</title>
    <link href="http://yuanquanquan.top/2020/20200425/"/>
    <id>http://yuanquanquan.top/2020/20200425/</id>
    <published>2020-04-25T02:57:05.000Z</published>
    <updated>2020-06-09T21:05:39.128Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>整理了一些最近的object detection算法<br>忽然发现。。。。<br>这是要开启NAS时代的神仙打架了嘛/facepalm</p><p>不过讲真…Google这些nas论文，计算量贼大，要卡要数据的啊……复现起来不是一点两点困难</p><p>“我是FAIR时代的残党！新时代没有能载我的模型！”（说白了就是没卡。。。。）</p></blockquote><a id="more"></a>  <p><img src="https://i.loli.net/2020/06/10/CLdEjz2VFSqgoXN.png" alt></p><p>为了紧跟时代潮流，了解一下AutoML去看了一下<strong>《Neural Architecture Search: A Survey》</strong>这篇综述</p><h1 id="序"><a href="#序" class="headerlink" title="序"></a><em>序</em></h1><p>​      深度学习模型在很多任务上都取得了不错的效果，但调参对于深度模型来说是一项非常苦难的事情，众多的超参数和网络结构参数会产生爆炸性的组合，常规的 random search 和 grid search 效率非常低，为此人们想出了自动搜索神经网络架构（Neural Architecture Search）。一方面，自动神经网络架构搜索可以遍历架构找到性能最优的架构，另一方面自动神经网络架构搜索还可以打破人类思维的局限性找到人类所想不到的架构组织方式。</p><p><img src="https://i.loli.net/2020/05/03/ogn1dueI9DtS6PL.png" alt></p><p>本文从网络架构搜索的三个方面进行了分类综述，包括：</p><ul><li><p><strong>搜索空间</strong></p></li><li><p><strong>搜索策略</strong></p></li><li><p><strong>评价预估</strong></p></li></ul><h3 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h3><p>搜索空间定义了NAS方法原则上可能发现的神经体系结构即优化问题的变量。深度学习模型的性能是由参数来控制和决定的，所以只需要对复杂模型的架构参数和对应的超参数进行优化即可。</p><p><img src="https://i.loli.net/2020/05/03/Dd9WIgzb4SacLw2.png" alt></p><p>上图是两种不同的架构空间，图片中每个节点表示神经网络中的一层，例如卷积层，池化层，不同的层由不同的颜色标注。箭头描述了数据的流向。</p><p>左侧的图像是一个链式结构空间的组块，这种结构相当于一个 N 层的序列，每一层有几种可选的算子，比如卷积、池化等，每种算子包括一些超参数，比如卷积尺寸、卷积步长等。</p><p>右侧的图像是一个拥有多分支和跳远链接的搜索空间的组块。</p><p>链式结构神经网络通过层数N，层间操作，卷积核大小步幅等超参对搜索空间进行参数化，注意：搜索空间的参数不是固定长度的，而是条件空间。</p><p>分支结构用$g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)$来表示第$i$层的输入，则分支结构的特殊情况分为：</p><ul><li>链结构网络</li></ul><p>$$<br>g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)=L_{i-1}^{o u t}<br>$$</p><ul><li>残差网络</li></ul><p>$$<br>\left(g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)=L_{i-1}^{o u t}+L_{j}^{o u t}, j&lt;i-1\right.<br>$$</p><ul><li>Densenet<br>$$<br>g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)=\operatorname{concat}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)<br>$$<br>通过以上方式分别搜索这样的组块，这样的组块又被优化为保留输入维度的normal cells和减小空间维度的reduction cell（通过使stride=2）。最后通过预定义的方式堆叠这些组块构建最终的体系结构。</li></ul><h3 id="搜索策略"><a href="#搜索策略" class="headerlink" title="搜索策略"></a><strong>搜索策略</strong></h3><p>搜索策略详细说明了如何探索搜索空间，它一方面希望快速找到性能良好的架构，另一方面，也要避免过早收敛到次优架构的区域。搜索策略包括随机搜索（RS），贝叶斯优化（BO），进化方法，强化学习（RL）和基于梯度的方法。</p><p>强化学习方法，将神经体系结构的产生视为代理动作，将搜索空间视为动作空间，将体系结构的性能评估视为奖励，不同的RL方法在表示代理策略和如何优化策略方面有所不同。Neural architecture search with reinforcement learning使用递归神经网络（RNN）策略顺序采样字符串，进而对神经体系架构进行编码，他们最初使用REINFORCE policy gradient 算法训练了该网络，但是后来的工作中（Learning transferable architecturs for scalable image recognition / Proximal policy optimization algorithms）使用了Proximal Policy Optimization；Designing neural network architectures using reinforcement learning中使用Q-learning来训练一种可依次选择层的类型和相应超参的策略。</p><p>这些方法的一个替代观点是顺序决策过程，策略采样动作顺序生成架构，环境的状态包括目前采样的动作，并且只有在最后一个动作完成后后才能获得奖励。但是，过程中没有与环境发生互动（没有观察到外部状态，也没有中间奖励），因此将体系结构采样过程解释为单个动作的顺序生成更为直观，Efficient architecture search by network transformation提出了一种相关的方法，在他们的方法中，状态是当前（经过部分训练的）架构，奖励是对该架构性能的评估，而动作对应于保留功能的突变应用，随后是网络的训练阶段，为了处理可变长度的网络体系结构，他们使用双向LSTM将体系结构编码为固定长度的表示形式，基于此编码标识，动作网络决定采样的动作，这两个组成部分的组合构成了策略，该策略使用REINFORCE policy gradient 算法进行了端到端的训练。</p><p>进化方法用进化算法优化神经架构，第一个用此方法的可以追溯到30年前用遗传算法提出架构，用反向传播优化权重。自那以后，很多神经进化算法用遗传算法同时优化神经架构和和权重，然而，当扩展到具有数百万权重的当代神经体系结构以进行监督学习任务时，基于SGD的权重优化方法胜过进化的方法。所以后来人们使用基于梯度的方法优化权重，仅仅使用进化算法优化神经架构。进化算法进化出大量模型，即一组（可能训练有素的）网络；在每个进化步骤中，至少要采样种群中的一个模型，并作为父代通过对其应用突变来生成后代。在NAS的上下文中，变异是本地操作，例如添加或删除层，更改层的超参数，添加跳远连接以及更改训练超参数。在训练后代之后，评估它们的适应度（例如，在验证集中的表现）并将其添加到种群中。</p><p>神经进化方法在采样父母，更新种群和产生后代的方式上有所不同。采样父母：锦标赛选择、使用反密度从多目标Pareto前沿对父母进行采样。更新种群：去除最差的个人、去除最老的个体、不移除个人。产生后代：随机初始化子网络、Lamarckian inheritance：知识（以学习的权重的形式）通过使用网络态射从父网络传递到子网络、让后代继承不受其突变影响的父代所有参数。</p><p>Aging Evolution for Image Classifier Architecture Search比较了RL,evolution和random search方法，RL和进化在最终测试准确性方面表现均相当好，进化在任何时候都有更好的性能，并且找到更小的模型。在他们的实验中，这两种方法始终比RS表现更好，但幅度很小。</p><p>贝叶斯优化方法是最流行的超参数优化方法之一，但由于典型的BO工具箱基于高斯过程和专注于低维连续优化问题。Kernels for bayesian optimization in conditional parameter space和Neural architecture search with bayesian optimisation and optimal transport派生了用于架构搜索空间的内核函数，以便使用基于GP的经典BO方法。另外、一些作品使用基于树的模型，以达到在各种问题上有效地搜索高维条件空间并实现最先进的性能，共同优化神经体系结构及其超参数。尽管缺乏全面的比较，但初步证据表明这些方法也可以胜过进化算法。</p><p>Automatically Designing and Training Deep Architectures和Finding Competitive Network Architectures Within a Day Using UCT利用其搜索空间的树结构并使用了蒙特卡洛树搜索。Simple And Efficient Architecture Search for Convolutional Neural Networks提出了一种简单但性能良好的爬山算法，该算法通过贪婪地朝性能更好的架构的方向移动而发现高质量的架构，而无需更复杂的探索机制。上述方法采用离散搜索空间。DARTS提出了continuous relaxation来直接基于梯度优化operation的权重。SNAS,ProxylessNAS没有优化可能操作的权重α,而是建议在可能的操作上优化参数化分布。Differentiable neural network architecture search以及Ahmed和Maskconnect: Connextivity learning by gradient descent还采用了基于梯度的神经体系结构优化，但是分别专注于优化层超参数或连接模式。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;整理了一些最近的object detection算法&lt;br&gt;忽然发现。。。。&lt;br&gt;这是要开启NAS时代的神仙打架了嘛/facepalm&lt;/p&gt;
&lt;p&gt;不过讲真…Google这些nas论文，计算量贼大，要卡要数据的啊……复现起来不是一点两点困难&lt;/p&gt;
&lt;p&gt;“我是FAIR时代的残党！新时代没有能载我的模型！”（说白了就是没卡。。。。）&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>逆向工程：从Gerber到PCB</title>
    <link href="http://yuanquanquan.top/2020/20200417/"/>
    <id>http://yuanquanquan.top/2020/20200417/</id>
    <published>2020-04-17T06:40:40.000Z</published>
    <updated>2020-06-09T21:06:07.358Z</updated>
    
    <content type="html"><![CDATA[<h1 id="序"><a href="#序" class="headerlink" title="序"></a><em>序</em></h1><blockquote><p>​       最近将PCB图纸送去工厂打样，需要将Gerber转换成PCB文件，如何将Gerber转换成PCB的文章在网上可以找到很多，但大部分文章只是简单罗列了其中的几个步骤。对于初学者来说，只是”知其然而不知其所以然”，如果转换不成功、甚至于转换完成之后的PCB的逻辑连接发生了错误，也只能是一脸懵逼，不知道错在哪里。</p><p>​        所以这篇博客记录一下Gerber转换PCB过程中的每个步骤以及执行该步骤的原因、目的，能对这一”逆向工程“的过程有一个清晰的了解。</p></blockquote><a id="more"></a>  <p><strong>Gerber文件是什么？</strong></p><p><strong>Gerber</strong>，又称光绘文件，是一种标准的文件格式。PCB板厂可以通过Gerber直接生产出最终的PCB。目前常用的格式为RS-274X，大部分板厂也支持最新Gerber X2格式(多了板的层叠信息与属性)。</p><p>国内很多工程师喜欢把PCB设计文件直接发给板厂，其实这一做法是有风险的。因为板厂最终还是需要把PCB文件转换成Gerber才能进行生产，而从PCB到Gerber的这一过程存在很多潜在的问题：</p><ul><li>由于Gerber精度设置的问题，PCB上连通的网络未必在Gerber上连通</li><li>由于EDA工具版本的问题，可能板厂打开的PCB文件已经发生了变化</li><li>由于缺乏沟通，PCB上一些不需要显示的参数也遗留在成品上</li></ul><p>总而言之，PCB转成Gerber的这一过程未必是copy不走样，设计师应对这一过程负责。所以一般有经验的老工程师都会亲手把PCB转成Gerber，然后通过CAM350之类的软件，检查一下Gerber是否有问题，然后再送板厂加工。</p><p>另外，无论您使用Altium、Allegro或者是Expedition，只要转成了Gerber，板厂都可以正确识别。<img src="https://i.loli.net/2020/05/14/fWR5BlijzE73xvT.png" alt></p><p>还有一点，从Gerber反向生成PCB是比较困难的，出于安全性的考虑，企业也不愿意把设计源文件直接发给板厂。</p><p><strong>Gerber的组成</strong></p><p>Gerber由两个重要的组成部分，缺一不可：</p><ul><li>Gerber光绘文件：PCB的每个层都对应一个独立的Gerber文件，用于描述该层中的内容(有点类似于胶片)</li><li>NC Drill钻孔文件：垂直方向上的钻孔，决定了不同层中导线的连接</li></ul><p>从3D的角度，完整的Gerber文件包含了每个层的信息以及各个层中用于信号连接的钻孔(过孔)：</p><p>以AD为例，生成Gerber文件时，不是一个文件，而是一组文件，包含了PCB中每个层中的信息。下表是AD生成的Gerber文件后缀极其代表的意义。</p><table><thead><tr><th><strong>GERBER后缀</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>G1，G2，etc</td><td>Mid-layer 1, 2(中间信号层)</td></tr><tr><td>GBL</td><td>Bottom Layer(底层)</td></tr><tr><td>GBO</td><td>Bottom Overlay(底层丝印)</td></tr><tr><td>GBP</td><td>Bottom Paste Mask(底层助焊)</td></tr><tr><td>GBS</td><td>Bottom Solder Mask(底层阻焊)</td></tr><tr><td>GD1, GD2, etc.</td><td>Drill Drawing(钻孔图)</td></tr><tr><td>GG1, GG2, etc.</td><td>Drill Guide(钻孔向导)</td></tr><tr><td>GKO</td><td>Keep Out Layer</td></tr><tr><td>GM1, GM2, etc.</td><td>Mechanical Layer 1, 2(机械层)</td></tr><tr><td>GP1, GP2, etc.</td><td>Internal Plane Layer 1, 2(内电层)</td></tr><tr><td>GPB</td><td>Pad Master Bottom(底层焊盘)</td></tr><tr><td>GPT</td><td>Pad Master Top(顶层焊盘)</td></tr><tr><td>GTL</td><td>Top Layer(顶层)</td></tr><tr><td>GTO</td><td>Top Overlay(顶层丝印)</td></tr><tr><td>GTP</td><td>Top Paste Mask(底层助焊)</td></tr><tr><td>GTS</td><td>Top Solder Mask(顶层阻焊)</td></tr></tbody></table><p>如果是Allegro，那就是一堆的*.art文件(Artwork)。</p><p>当然，只有Gerber文件是不够的，因为Gerber并没有层与层之间导线连接的关系。如果一个信号流过了多个层，肯定需要由过孔进行过渡。NC Drill文件记录了PCB上所有需要钻孔的信息(包括位置、孔的大小等)，也是制造PCB不可或缺的关键信息。</p><p>以下是AD生成的NC Drill钻孔文件：</p><table><thead><tr><th>文件名</th><th>描述</th></tr></thead><tbody><tr><td>FileName.DRL(可选)</td><td>二进制格式的钻孔文件。对于一个存在盲、埋孔的多层PCB，每个Layer Pair(层对)都会生成一个单独后缀的TX*</td></tr><tr><td>FileName.DRR</td><td>钻孔报告 - 包括tool的详细信息，孔的尺寸、数量、Tool travel等</td></tr><tr><td>FileName.TXT</td><td>ASCII格式的钻孔文件。对于一个存在盲、埋孔的多层PCB，每个Layer Pair(层对)都会生成一个单独后缀的TX*</td></tr><tr><td>FileName-Plated.TXT(可选)</td><td>ASCII格式的钻孔文件. 选择后只文件中只记录PTH的钻孔。对于不通类型的钻孔(比如Slot、方形)，会生成单独的文件</td></tr><tr><td>FileName-NonPlated.TXT(可选)</td><td>ASCII格式的钻孔文件. 选择后只文件中只记录NPTH的钻孔。对于不通类型的钻孔(比如Slot、方形)，会生成单独的文件</td></tr><tr><td>FileName-BoardEdgeRout.TXT(可选)</td><td>ASCII格式的rout文件。记录了板形以及板子的cutout</td></tr><tr><td>FileName.LDP</td><td>ASCII格式的钻孔对报告。被CAM编辑器用来检测盲、埋孔</td></tr></tbody></table><p>下图是一个默认状态下AD生成的NC Drill示例：</p><p><img src="http://q8pl344z8.bkt.clouddn.com/Geber-2.webp" alt></p><p>示例中含有盲、埋孔，生成NC Drill时每种类型的盲、埋孔都会单独生成一个*.TX1/2/3的文件。不同类型的孔(Slot、方形)也会单独生成一个TXT文件。</p><p>说了那么多无非是要强调一点，无论正向还是逆向，这两个类型的文件是绝对不可以少的：</p><ol><li>一个叫Gerber</li><li>一个叫NC Drill</li></ol><p><strong>如何从Gerber生成PCB</strong></p><p>Altium Designer自带了一个类似CAM350的工具，叫Camtastic。虽然用起来不如CAM350那么顺手，但基本的功能都是没有问题的。从Gerber到PCB的逆向工程，就由Camtastic工具来完成。</p><p><strong>第一步：新建一个Cam文档</strong></p><p><img src="https://i.loli.net/2020/05/03/GSeLkUrlho5YvTu.png" alt></p><p><strong>第二步：导入Gerber及NC Drill</strong></p><p><img src="https://i.loli.net/2020/05/03/CzNMwb9ZYPWjQf1.jpg" alt></p><p>这是比较关键的一步，如果只导入Gerber，不导入NC Drill，会导致后续无法解压网表。如果有IPC Netlist，也可以一起导入，这样在解压网表时，网表的名称就会与源设计相同，而默认情况下系统会自定义网表的名称。</p><p>在Preference中，可以定义CAM识别的文件后缀。如果要导入Allegro或者Expedition的Gerber，可以在这里添加相应的后缀以便于快速识别。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/Gerber-5.webp" alt></p><p>导入完成后，就可以在AD中查看或编辑Gerber文件了：<img src="https://i.loli.net/2020/05/03/3oBV6FxwCaLASzq.png" alt></p><p><strong>第三步：层的映射</strong></p><p>这也是比较关键的一步，点击<strong>Tables » Layers</strong>打开映射界面：</p><p><img src="https://i.loli.net/2020/05/03/SewmgryXJZt4vOh.jpg" alt></p><p>左侧的Gerber层必须和Type列中的PCB层正确映射。其中信号层的映射尤其重要：顶层/底层分别映射为Top/Bottom；中间信号层映射为Internal；内电层映射为Pos Plane/Neg Plane。</p><p>除此之外，NC Drill的TXT文件也需要正确映射到Drill Top/Bottom/Int。</p><p><strong>第四步：层的顺序(Layer Order)</strong></p><p>层映射完毕之后，会弹出Create / Update Layer Order对话框(也可以通过<strong>Layer » Order</strong>菜单访问)：</p><p><img src="https://i.loli.net/2020/05/03/KJmFWs8dljNb1R4.png" alt></p><p>在这里，需要定义所有的信号层(包括Top、Bottom、中间信号层及内电层)的物理顺序。这一步也相当关键，如果存在盲、埋孔，这里定义错误的话会直接导致PCB导出的错误。记住，Top层的Physical  Order始终是1，其它层的顺序按实际的板层结构依次类推。Physical Order中的数字是不可以重复的。</p><p><strong>第五步：解压网表(Extract Net)</strong></p><p>如果以上步骤都没有问题的话，就可以通过Gerber和NC Drill反推出PCB中的网表信息。点击<strong>Tools</strong> <strong>»</strong> <strong>Netlist</strong> <strong>»</strong> <strong>Extract</strong>菜单得到PCB的网表：</p><p><img src="https://i.loli.net/2020/05/03/XrwDmWQ2xqKpoLl.png" alt></p><p><strong>第六步：导出PCB</strong></p><p>一切就绪，最后一步是导出PCB。如果缺少Net List或其它信息，Export to PCB的菜单是灰色不能点击的。</p><p><img src="https://i.loli.net/2020/05/03/TLkXbyhQFxs81eo.jpg" alt></p><p><strong>第七步：清理PCB文件</strong></p><p>​      逆向工程导出的PCB只是Track和Pad的组合体，压根就没有器件(Component)的概念。您还需要通过无数次的Ctrl+C、Ctrl+V才能重新把这些零零碎碎的对象组合成正常的PCBLib。向工程只是复原PCB的样式和逻辑连接而已，并非是恢复到原始的PCB设计文件。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;&lt;em&gt;序&lt;/em&gt;&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;​       最近将PCB图纸送去工厂打样，需要将Gerber转换成PCB文件，如何将Gerber转换成PCB的文章在网上可以找到很多，但大部分文章只是简单罗列了其中的几个步骤。对于初学者来说，只是”知其然而不知其所以然”，如果转换不成功、甚至于转换完成之后的PCB的逻辑连接发生了错误，也只能是一脸懵逼，不知道错在哪里。&lt;/p&gt;
&lt;p&gt;​        所以这篇博客记录一下Gerber转换PCB过程中的每个步骤以及执行该步骤的原因、目的，能对这一”逆向工程“的过程有一个清晰的了解。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Altium" scheme="http://yuanquanquan.top/tags/Altium/"/>
    
  </entry>
  
  <entry>
    <title>EfficientDet:Scalable and Efficient Object Detection</title>
    <link href="http://yuanquanquan.top/2020/20200323/"/>
    <id>http://yuanquanquan.top/2020/20200323/</id>
    <published>2020-03-23T09:54:14.000Z</published>
    <updated>2020-06-09T21:06:53.226Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>  EfficientDet是Google Brain于去年11月份公布的目标检测算法族，涵盖轻量级到高精度的多个模型，提出了7种不同的网络结构</p></blockquote><a id="more"></a>  <p><img src="https://i.loli.net/2020/05/03/dnoJUfuKp3lTQOR.jpg" alt>根据其复杂度不同，可以适应不同计算能力的平台，COCO数据集上达到 50.9 mAP，在coco榜单上算前无古人了，可以说是直接把YOLOV3，Mask-R CNN按在地上摩擦……..</p><p>​      前两天，Google Brain终于官方开源了，开源地址：(<a href="https://github.com/google/automl/tree/master/efficientdet)，位于Google新开的automl项目内，看样子以后这个项目还会有其他自动机器学习的算法开源。EfficientDet原出于论文" target="_blank" rel="noopener">https://github.com/google/automl/tree/master/efficientdet)，位于Google新开的automl项目内，看样子以后这个项目还会有其他自动机器学习的算法开源。EfficientDet原出于论文</a> EfficientDet: Scalable and Efficient Object Detection，开源页面显示，这篇论文已经被CVPR 2020接收</p><p>EfficientDet与EfficientNet的第一作者是同一人，可以说Google Brain在EfficientNet的基础上提出了针对于物体检测的可扩展模型架构EfficientDet。EfficientDet主要包括两方面贡献：</p><ol><li><p>双向FPN <strong><em>BiFPN（Bi-directional feature pyramid network</em></strong>，在simplified PANet上引入了lateral shortcut）和weighted-BiFPN（在不同scale的特征进行融合时引入注意力机制对不同来源的feature进行权重调整（per-feature / per-channel / pei-pixel），由实验来看带来的性能提升相比BiFPN较小，看论文总结BiFPN提高了4个百分点）</p></li><li><p>仿照EfficientNet中的Compound Scaling方法，对检测网络中的各个部分进行Compound Scaling（输入图像大小，backbone的深度、宽度，BiFPN的深度（侧向级联层数），cls/reg head的深度）。个人觉得这是最大的亮点，提出了目标检测网络联合调整复杂度的策略。</p></li></ol><p>除此之外值得一提的是EfficienDet中使用的模型缩放，作者结合BiFPN和特征融合策略设计了与YOLOv3精度相仿的EfficientDet-D0，按照一定的优化规则，在网络的深度、宽度、输入图像的分辨率上进行模型缩放，可在统一架构下得到适合移动端和追求高精度的多个模型，根据其复杂度不同，可以适应不同计算能力的平台。</p><h1 id="EfficientDet结构"><a href="#EfficientDet结构" class="headerlink" title="EfficientDet结构"></a><strong><em>EfficientDet结构</em></strong></h1><p><img src="https://i.loli.net/2020/05/03/DeR8pidSbA14kPa.jpg" alt><br>EfficientDet组合了backbone（使用了EfficientNet）和BiFPN（特征网络）和Box prediction net，上图整个框架就是EfficientDet的基本模型</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;  EfficientDet是Google Brain于去年11月份公布的目标检测算法族，涵盖轻量级到高精度的多个模型，提出了7种不同的网络结构&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CV" scheme="http://yuanquanquan.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>红外循迹传感器</title>
    <link href="http://yuanquanquan.top/2020/20200321/"/>
    <id>http://yuanquanquan.top/2020/20200321/</id>
    <published>2020-03-21T12:37:40.000Z</published>
    <updated>2020-06-09T21:07:36.606Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>​      这次尝试制作了一个简单的红外循迹传感器，红外循迹传感器是专为轮式机器人设计的一款距离可调式避障传感器。其具有一对红外线发射与接收管，发射管发射出一定频率的红外线，当检测方向遇到障碍物（反射面）时，红外线反射回来被接收管接收，经过比较器电路处理之后，开关指示灯会亮起，同时信号输出接口输出数字信号（一个低电平信号），可通过电位器旋钮调节检测距离，有效距离范围2～30cm，工作电压为3.3V-5V，由于工作电压范围宽泛，在电源电压波动比较大的情况下仍能稳定工作，该传感器的探测距离可以通过电位器调节、具有干扰小、使用方便等特点，可以广泛应用于机器人避障、避障小车和黑白线循迹等众多场合。适合多种单片机、Arduino控制器、树莓派使用，安装到机器人上即可感测周围环境的变化。</p></blockquote><a id="more"></a>  <p><img src="https://i.loli.net/2020/05/03/yRGVOFHch7bxqdS.jpg" alt></p><p>电路工作说明</p><p>1、 当模块检测到前方障碍物信号时，开关指示灯点亮电平，同时OUT端口持续输出低电平信号,该模块检测距离2～30cm，检测距离可以通过电位器进行调节。</p><p>2、传感器模块输出端口OUT可直接与单片机IO口连接即可，也可以直接驱动一个5V继电器。</p><p>3、比较器采用LM393。</p><p>4、可采用3-5V直流电源对模块进行供电。当电源接通时，电源指示灯点亮。</p><p>接口说明</p><p>1 、VCC 外接3.3V-5V电压（可以直接与5v单片机和3.3v单片机相连）；</p><p>2 、GND 外接电源负极；</p><p>3 、OUT 为数字量输出接口（输出0或1高低电平）</p><p>Altium Designer画的原理图和PCB图如下：<img src="https://i.loli.net/2020/05/03/t1X73vC2gzdZwME.png" alt></p><p><img src="https://i.loli.net/2020/05/03/PYmXjkhwLyDSEZu.png" alt="红外-2"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;​      这次尝试制作了一个简单的红外循迹传感器，红外循迹传感器是专为轮式机器人设计的一款距离可调式避障传感器。其具有一对红外线发射与接收管，发射管发射出一定频率的红外线，当检测方向遇到障碍物（反射面）时，红外线反射回来被接收管接收，经过比较器电路处理之后，开关指示灯会亮起，同时信号输出接口输出数字信号（一个低电平信号），可通过电位器旋钮调节检测距离，有效距离范围2～30cm，工作电压为3.3V-5V，由于工作电压范围宽泛，在电源电压波动比较大的情况下仍能稳定工作，该传感器的探测距离可以通过电位器调节、具有干扰小、使用方便等特点，可以广泛应用于机器人避障、避障小车和黑白线循迹等众多场合。适合多种单片机、Arduino控制器、树莓派使用，安装到机器人上即可感测周围环境的变化。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="单片机" scheme="http://yuanquanquan.top/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>SqueezeNAS:Fast neural architecture search for faster semantic segmentation</title>
    <link href="http://yuanquanquan.top/2020/20200313/"/>
    <id>http://yuanquanquan.top/2020/20200313/</id>
    <published>2020-03-13T03:32:41.000Z</published>
    <updated>2020-06-09T21:08:33.318Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>用NAS做语义分割，1.不使用代理，MobileNetV3先搜索分类任务作为代理，SqueezeNAS直接搜索语义分割。 2.使用资源感知损失，直接优化目标平台上的延迟，而不是优化MAC指标，它对于运行速度没有必然关系</p></blockquote><a id="more"></a>  <h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>真实场景下，需要DNN在目标任务上准确率高，在目标计算平台上推断延迟低。NAS已经被用于低延迟的图像分类，但是其他任务还很少。这是第一篇用于密集语义分割的无代理硬件感知的搜索。在Cityscapes数据集上达到最好的性能。我们证明了利用NAS对任务和推断同时优化可以获得极大地性能提升。</p><h2 id="1-Introduction-and-Motivation"><a href="#1-Introduction-and-Motivation" class="headerlink" title="1. Introduction and Motivation"></a>1. Introduction and Motivation</h2><p>在做非图像分类任务（如语义分割或目标检测）时，流行的方法是结构迁移：从一个图像分类网络开始，在网络末尾添加一些针对特定任务的层。</p><p>这种结构迁移的主流主要是一些传统假设，我们罗列一些并证明为什么他们已经过时了。</p><ul><li><strong>假设1： 在ImageNet上准确率最高的网络也应该是目标任务上准确率最好的骨干网络</strong></li></ul><p>实际上，ImageNet的准确率与目标任务准确率关系不太大。SqueezeNet在ImageNet上准确率低于VGG，但是对上定位图像相似部分的任务上更好。正确的网络设计取决于目标任务。</p><ul><li><strong>假设2： NAS成本过高</strong></li></ul><p>实际上，有些方法的确需要数千个GPU天，但是最近的“supernetwork”方法例如DARTS和FBNet可以在10个GPU天上取得最优结果。</p><ul><li><strong>假设3： 低MAC（Fewer multiply-accumulate）运算在目标平台上有较低的延迟</strong></li></ul><p>实际上，有工作证明了相同的平台上，同样的MAC可以有10x的延迟差异。取决于处理器和内核实现，即使是相同的MAC也有不同的速度。</p><p>为了在目标计算平台和任务上获得更低的延迟，更高的准确率：</p><ol><li>直接对目标任务运行NAS，例如目标检测、语义分割，不要去优化代理任务，如图像分类。</li><li>使用现代的基于supernetwork的NAS，相信搜索可以快速收敛。</li><li>让NAS同时优化准确率和延迟。</li></ol><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h2><p>【略】</p><h2 id="3-Architecture-Search-Space"><a href="#3-Architecture-Search-Space" class="headerlink" title="3. Architecture Search Space"></a>3. Architecture Search Space</h2><p>我们探索了顺序反向残差模块的语义分割网络编码器的空间。这些块参数化如图2。每次结构搜索，我们约束宏观结构，对每个块寻找最优参数。搜索空间和FBNet，MobileNetV2，MobileNetV3相似，这样可以直接比较对分割优化的网络和他们对分类优化的网络。</p><p><img src="https://i.loli.net/2020/05/03/LgdW9ArGtIoKjh2.png" alt></p><p>我们网络的基本结构如图1，解码器使用编码器的输出和低层特征。</p><p><img src="https://i.loli.net/2020/05/03/Bs4LQoznhZ5drAg.png" alt></p><h3 id="3-1-Constrained-Macro-Architecture"><a href="#3-1-Constrained-Macro-Architecture" class="headerlink" title="3.1. Constrained Macro-Architecture"></a>3.1. Constrained Macro-Architecture</h3><p>搜索三个空间，Small, Large, XLarge。为了定义结构空间，先约束编码网络的宏观结构。宏观结构描述了编码器的模块数量N，解码器也是这么多。对于每个模块，固定输入和输出通道，$c_{in} $和$c_{out}$每个块的深度卷积层步长使用1或2。由于运行跳跃连接，所以最终的层数可能小于N。</p><p>在Small和Large的搜索空间，使用LR-ASPP解码器。在XLarge搜索空间，我们使用完全深度卷积的ASPP变种。</p><h3 id="3-2-Block-Search-Space"><a href="#3-2-Block-Search-Space" class="headerlink" title="3.2. Block Search Space"></a>3.2. Block Search Space</h3><p>在每个宏观搜索空间，NAS挑选每个块最优的超参数，或者替换为无运算的跳跃连接。如图2，超参数定义了 1*1 卷积是否分组，深度绝技是否膨胀2倍，深度卷积的核大小k，膨胀率e。如图7，我们挑选12个可能的配置。</p><h2 id="4-Neural-Architecture-Search-Algorithm"><a href="#4-Neural-Architecture-Search-Algorithm" class="headerlink" title="4. Neural Architecture Search Algorithm"></a>4. Neural Architecture Search Algorithm</h2><p>把结构搜索看做是复杂supernetwork的路径选择问题，这样确定的结构可以看做是supernetwork的某些路径。如图3，我们定义supernetwork为superblocks的序列，图4是一个例子。</p><p><img src="https://i.loli.net/2020/05/03/Buy2SrwDzPehUv3.png" alt></p><p>同时优化卷积权重<em>w</em>和结构参数<em>θ</em>, 优化损失函数为L<em>(</em>θ<em>,</em>w<em>)=</em>L<strong>P<em>(</em>θ<em>,</em>w<em>)+</em>α<em>∗</em>L</strong>E<em>(</em>θ*)</p><p>$L_p$表示特定问题的损失，$L_E$是资源感知损失项，超参数<em>α</em>控制两个平衡。由于本工作关注与语义分割，是$L_p$像素级别的交叉熵。对于，$L_E$我们同时实验了目标平台的推断延迟和Multiply-Accumulates的估计值。</p><p><code>表示特定问题的损失，</code><em>α</em><code>LP</code>是像素级别的交叉熵。对于</p><p><code></code>我们同时实验了目标平台的推断延迟和Multiply-Accumulates的估计值。</p><h3 id="4-1-Gumbel-Softmax"><a href="#4-1-Gumbel-Softmax" class="headerlink" title="4.1. Gumbel-Softmax"></a>4.1. Gumbel-Softmax</h3><p>为了让supernetwork的优化和计算可跟踪，每个superblock独立挑选一个候选块。这样，可以把这个选择看成是对独立类别分布的采样，把superblock i的候选块j概率记作<em>p</em>(<em>i</em>,<em>j</em>) , 用softmax定义：$p ( i , j | \theta ) = \frac { e ^ { \theta _ { i , j } } } { \sum _ { j } ^ { 13 } e ^ { \theta _ { i , j } } }$</p><p>类别分布很难高效优化，所以我们使用Gumbel-Softmax松弛。Gumbel-Softmax分布由稳定参数t控制，t趋近于0，Gumbel-Softmax分布等价于类别分布，稳定参数从5.0到1.0退火。</p><h2 id="4-2-Early-Stopping"><a href="#4-2-Early-Stopping" class="headerlink" title="4.2. Early Stopping"></a>4.2. Early Stopping</h2><p>我们的supernetwork方法中，优化过程需要计算每一个候选的block，无论学到的结构分布是什么。当最优的网络结构收敛的时候，性能差的候选块虽然选择的概率低，但是依然需要继续计算。所以当候选概率小于0.5%的时候，直接移除。虽然有可能低概率的候选块可能后面是最优的，但是实践中没有发现这种情况。该优化可以减少一半的搜索时间。</p><h2 id="4-3-Resource-Aware-Architecture-Search"><a href="#4-3-Resource-Aware-Architecture-Search" class="headerlink" title="4.3. Resource-Aware Architecture Search"></a>4.3. Resource-Aware Architecture Search</h2><p>定义资源感知损失为：</p><p>$L _ { E } ( \theta ) = \sum _ { j } ^ { N } \sum _ { i } ^ { 13 } p \left( i , j | \theta _ { i } \right) C ( i , j )$</p><p><em>C</em>(<em>i</em>, <em>j</em>)表示网络块i选择候选j的资源成本。对每个块独立建模资源成本。</p><h2 id="5-Experiments-and-Results"><a href="#5-Experiments-and-Results" class="headerlink" title="5. Experiments and Results"></a>5. Experiments and Results</h2><p>证明两个关键点：第一，NAS是可以产生搞准确率低延迟网络的工具。第二，优化与硬件无直接联系的指标如MAC不是个合适的代理，可能导致局部最优。</p><p>沿用Small, Large, XLarge三种搜索空间。先用NAS在每个空间上对MAC进行搜索，然后在嵌入式设备上查看这些低MAC的网络的延迟作为基线。最后再搜索优化硬件感知的延迟找到3个新的网络。</p><h3 id="5-1-Hardware-Agnostic-Search"><a href="#5-1-Hardware-Agnostic-Search" class="headerlink" title="5.1. Hardware-Agnostic Search"></a>5.1. Hardware-Agnostic Search</h3><p>对于与硬件无关的结构搜索，使用NAS对MAC进行优化，在MAC和mIOU上找到pareto最优，根据查找表计算每个候选块j的MAC，使得<em>C</em>(<em>i</em>,<em>j</em>)=<em>M<strong>A</strong>C<strong>S</strong>i</em>,<em>j</em></p><p>。对每个搜索空间搜索找到最优的SqueezeNAS-MAC网络，结果如下表。</p><p><img src="https://i.loli.net/2020/05/03/KARr3uocmWqndG4.png" alt></p><h3 id="5-2-Hardware-Aware-Search"><a href="#5-2-Hardware-Aware-Search" class="headerlink" title="5.2. Hardware-Aware Search"></a>5.2. Hardware-Aware Search</h3><p>使用相同的NAS算法和搜索空间，但是使用延迟优化目标：<em>C</em>(<em>i</em>,<em>j</em>)=<em>L<strong>a</strong>t<strong>e</strong>n<strong>c</strong>y**i</em>,<em>j</em></p><p>。为了计算每个模块j在候选j上的延迟，我们测量了目标平台上所有候选的推断时间，最后得到三个SqueezeNAS-LAT网络。结果见表1。</p><h3 id="5-3-Implementation"><a href="#5-3-Implementation" class="headerlink" title="5.3. Implementation"></a>5.3. Implementation</h3><h3 id="5-4-Results"><a href="#5-4-Results" class="headerlink" title="5.4. Results"></a>5.4. Results</h3><h2 id="6-Network-Analysis"><a href="#6-Network-Analysis" class="headerlink" title="6. Network Analysis"></a>6. Network Analysis</h2><p>比较三者的模块选择：优化延迟的网络（Latency-aware），优化MAC的网络（MAC-aware），MobileNetV3。由于这三个都使用反向残差块，可以把MobileNetV3的块放入13个候选中，如图7。这里没有考虑SE块。</p><p><img src="https://i.loli.net/2020/05/03/Cby6WpkJZ7RnAe9.png" alt></p><p>可视化网络如图8，【略】<img src="https://i.loli.net/2020/05/03/rY8g2HIBMncEmGk.png" alt></p><p> <img src="https://i.loli.net/2020/05/03/zZOSA7D9TFeaXWH.png" alt></p><p><img src="https://i.loli.net/2020/05/03/u6zvOZDUQ2RxGWb.png" alt></p><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>第一、做无代理的语义分割搜索，我们的NAS生成的SqueezeNAS系列模型，相比MobileNet V3达到优越的延迟-准确率平衡。这种优越（至少部分）来自于MobileNetV3是用NAS优化图像分类作为代理实现语义分割任务的。</p><p>第二、虽然MobileNetV3作者使用了几千个GPU天，但是我们的方法每次搜索只需要7-15个GPU天。也就是说，基于supernetwork的NAS在8个GPU上不需要一个周末就可以跑出来最好的结果。</p><p>第三、我们做了两类NAS实验，一个搜索低MAC模型，一个搜索目标平台上低延迟模型。第二个我们获得了非常快同时准确率高的模型。最后，考虑到芯片的速度提升和计算平台的省级，NAS可以继续获得更低的延迟<img src="https://i.loli.net/2020/05/14/KEUDQtho26YVjx9.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;用NAS做语义分割，1.不使用代理，MobileNetV3先搜索分类任务作为代理，SqueezeNAS直接搜索语义分割。 2.使用资源感知损失，直接优化目标平台上的延迟，而不是优化MAC指标，它对于运行速度没有必然关系&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CV" scheme="http://yuanquanquan.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>【Object Detection】-YOLOV1</title>
    <link href="http://yuanquanquan.top/2020/20200312/"/>
    <id>http://yuanquanquan.top/2020/20200312/</id>
    <published>2020-03-12T02:36:03.000Z</published>
    <updated>2020-06-09T21:09:03.648Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>YOLO算法简介</strong></p><p>YOLO十分简单，一个网络同时对多个物体进行分类和定位，没有proposal的概念，是<strong><em>one-stage</em></strong>实时检测网络的里程碑，标准版在TitanX达到45 fps，快速版达到150fps，但精度不及当时的SOTA网络</p><p>YOLO算法使用深度神经网络进行对象的位置检测以及分类，主要的特点是速度够快，而且准确率也很高，采用直接预测目标对象的边界框的方法，将候选区和对象识别这两个阶段合二为一。</p></blockquote><a id="more"></a>  <p>Yolo算法不再是窗口滑动了，而是直接将原始图片分割成互不重合的小方块，然后通过卷积最后生产这样大小的特征图，基于上面的分析，可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标，这就是Yolo算法的朴素思想，而YOLOv3算法再以往的结构上做出了改进，增加了多尺度检测，以及更深的网络结构darknet53,这是比较主要的改进，还有某些细节上的变动。</p><p><em>Unified Detection</em></p><p><img src="https://i.loli.net/2020/05/03/2RriysU9MONjk3x.png" alt></p><p>将输入分为S*S的格子，如果GT的中心点在格子中，则格子负责该GT的预测：</p><p>对于PASCAL VOC数据集来说，设定s=7如图所示，分为7*7个小格子，每个格子预测两个bounding box。</p><p>如果一个目标的中心落入一个网格单元中，该网格单元负责检测该目标。</p><p>对每一个切割的小单元格预测（置信度，边界框的位置），每个bounding  box需要4个数值来表示其位置，(Center_x,Center_y,width,height)，即bounding  box的中心点的x坐标，y坐标，bounding box的宽度，高度)</p><p>置信度定义为<strong>是否存在目标</strong>与<strong>iou值</strong>的乘积，置信度可以反应格子是否包含物体以及包含物体的概率，无物体则为0，有则为IOU</p><p>$\text { Confidence } = \operatorname { Pr } ( \text { Object } ) * \text { IOU } _ { \text {pred } } ^ { \text {truth } }$</p><p>还要得到分类的概率结果；20个分类每个类别的概率。在测试时，将单独的bbox概率乘以类的条件概率得到最终类别的概率，综合了类别和位置的准确率</p><p> <strong>Network Design</strong></p><p>YOLO采用单个的卷积神经网络进行预测，YOLO的整个结构就是输入图片经过神经网络的变换得到一个输出的张量 。   步骤如下：</p><p>（1）骨干网络前20层接average-pooling层和全连接层进行ImageNet预训练，检测网络训练将输入从224×224增加到448×448</p><p>（2）在图像 上运行单个卷积网络</p><p>（3）由模型的置信度对所得到的检测进行阈值处理</p><p>首先，YOLO速度非常快。由于我们将检测视为回归问题，所以不需要复杂的流程。测试时在一张新图像 上简单的运行我们的神经网络来预测检测</p><p>其次，YOLO在进行预测时，会对图像进行全面地推理。与基于滑动窗口和区域提出的技术不同，YOLO在训练期间和测试时会看到整个图像，所以它隐式地编码了</p><p>关于类的上下文信息以及它们的外观。快速R-CNN是一种顶级的检测方法，但是它看不到更大的上下文信息，所以在图像中会将背景块误检为目标。与快速R-CNN相比，YOLO的背景误检数量少了一半</p><p>然后，由于YOLO具有高度泛化能力，因此在应用于新领域或碰到意外的输入时不太可能出故障。</p><p> <img src="https://i.loli.net/2020/05/03/dof2WHi4aJyIDBt.png" alt></p><p>所使用的卷积结构如图所示：受到GoogLeNet图像分类模型的启发。网络有24个卷积层，后面是2个全连接层，最后输出层用线性函数做激活函数，其它层激活函数都是Leaky ReLU。</p><p>我们 只使用1×1降维层，后面是3×3卷积层，</p><p><strong>Training</strong></p><p>最后一层使用ReLU，其它层使用leaky ReLU</p><p>YOLO的损失函数定义如下：</p><p>$\begin{array} { c } \lambda _ { \text {coord } } \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } \mathbb { 1 } _ { i j } ^ { \text {obj } } \left[ \left( x _ { i } - \hat { x } _ { i } \right) ^ { 2 } + \left( y _ { i } - \hat { y } _ { i } \right) ^ { 2 } \right] \ \quad + \lambda _ { \text {coord } } \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } \mathbb { 1 } _ { i j } ^ { \text {obj } } \left[ ( \sqrt { w _ { i } } - \sqrt { \hat { w } _ { i } } ) ^ { 2 } + ( \sqrt { h _ { i } } - \sqrt { \hat { h } _ { i } } ) ^ { 2 } \right] \end{array}$</p><p>$\begin{array} { l } \quad + \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } 1 _ { i j } ^ { \mathrm { obj } } \left( C _ { i } - \hat { C } _ { i } \right) ^ { 2 } \ + \lambda _ { \mathrm { nobbj } } \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } 1 _ { i j } ^ { \mathrm { noobj } } \left( C _ { i } - \hat { C } _ { i } \right) ^ { 2 } \end{array}$</p><p>$+ \sum _ { i = 0 } ^ { S ^ { 2 } } \mathbb { 1 } _ { i } ^ { \mathrm { obj } } \sum _ { c \in \text { classes } } \left( p _ { i } ( c ) - \hat { p } _ { i } ( c ) \right) ^ { 2 }$</p><p> 损失函数如上图，一个GT只对应一个bounding box。由于训练时非目标很多，定位的训练样本较少，所以使用权重$\begin{array} { c } \lambda _ { \text {coord } }\end{array} $和$\begin{array} { c } \lambda _ { \text {nobjd} }\end{array} $来加大定位的训练粒度，包含3个部分：</p><p><strong>第一部分</strong>为坐标回归，使用平方差损失，为了使得模型更关注小目标的小误差，而不是大目标的小误差，对宽高使用了平方根损失进行变相加权。这里$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$指代当前b box是否负责GT的预测，需要满足2个条件，首先GT的中心点在该b box对应的格子中，其次该b box要是对应的格子的个box中与GT的IOU最大</p><p><strong>第二部分</strong>为b box置信度的回归，$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$跟上述一样，$\mathbb { 1 } _ { i j } ^ { \mathrm { nooobj } }$为$$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$$的b box，由于负样本数量较多，所以给了个低权重。若有目标，$\begin{array} hat { C }\end{array}$实际为IOU，虽然很多实现直接取1.</p><p><strong>第三部</strong>分为分类置信度，相对于格子而言，$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$指代GT中心是否在格子中</p><p><img src="https://i.loli.net/2020/05/03/Bf5LEXw6rZ4xakY.png" alt="如图所示"></p><p>YOLO在ImageNet分类任务上以一半的分辨率（224*224的输入图像）预训练卷积层，然后将分辨 率加倍来进行检测。</p><p>训练中采用了drop out和数据增强（data augmentation）来防止过拟合.</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>  开创性的one-stage detector，在卷积网络后面接两个全连接层进行定位和置信度的预测，并设计了一个新的轻量级主干网络，虽然准确率与SOTA有一定距离，但是模型的速度真的很快<br>  作者提到了YOLO的几点局限性：</p><ul><li>每个格子仅预测一个类别，两个框，对密集场景预测不好</li><li>对数据依赖强，不能泛化到不常见的宽高比物体中，下采样过多，导致特征过于粗糙</li><li>损失函数没有完成对大小物体进行区别对待，应该更关注小物体的误差，因为对IOU影响较大，定位错误是模型错误的主要来源</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;YOLO算法简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;YOLO十分简单，一个网络同时对多个物体进行分类和定位，没有proposal的概念，是&lt;strong&gt;&lt;em&gt;one-stage&lt;/em&gt;&lt;/strong&gt;实时检测网络的里程碑，标准版在TitanX达到45 fps，快速版达到150fps，但精度不及当时的SOTA网络&lt;/p&gt;
&lt;p&gt;YOLO算法使用深度神经网络进行对象的位置检测以及分类，主要的特点是速度够快，而且准确率也很高，采用直接预测目标对象的边界框的方法，将候选区和对象识别这两个阶段合二为一。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="复健计划" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%A4%8D%E5%81%A5%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="Object Detection" scheme="http://yuanquanquan.top/tags/Object-Detection/"/>
    
  </entry>
  
</feed>
