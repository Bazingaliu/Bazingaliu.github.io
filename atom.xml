<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yuanquanquan的个人博客 | 我愿做你光华中淡淡的一笔</title>
  
  <subtitle>我愿做你光华中淡淡的一笔</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://fangzh.top/"/>
  <updated>2019-11-17T14:53:51.545Z</updated>
  <id>http://fangzh.top/</id>
  
  <author>
    <name>理科生写给世界的情书</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kick Start 2019-Practice Round</title>
    <link href="http://fangzh.top/2019/20190911174/"/>
    <id>http://fangzh.top/2019/20190911174/</id>
    <published>2019-11-17T14:18:32.000Z</published>
    <updated>2019-11-17T14:53:51.545Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img3.imgtn.bdimg.com/it/u=1916139364,2178340577&amp;fm=15&amp;gp=0.jpg" alt="img">Google的<a href="https://codingcompetitions.withgoogle.com/" target="_blank" rel="noopener">Competitions with Google</a>上有三个类型的练习：</p><ul><li><strong>Hash Code</strong>: 应该是一个组队编程训练，看起来比较偏工程向，介绍中说可以选择队伍与编程语言来完成一个engineering problem，还问了一句Are u up for the challenge? <del>Of course I am not.</del></li><li><strong>Code Jam</strong>: <del>编程果酱？？</del> 反正就是用多轮的算法问题来进行比赛，最后获胜者还有奖金，又问了一句Do u have what it takes? <del>emmmm..</del></li><li><strong>Kick Start</strong>: 今天的重头戏，介绍中说主要是磨炼自己的coding水平，受众是学生 <del>我</del> 和新接触编程的人 <del>我</del> ，也是按轮次进行的，可以只参加一轮也可以全部参加，最后的问题是What are u waiting for? 这看起来就正常多了，不像是向一个算法渣挑衅的言论，给好评。</li></ul><p>Kick Start的轮次是一个Practice Round和A~H八轮，其中除了Practice Round给了24h解题外，其它的八个轮次都是只给3h解题时间，所以介绍中所说的’可以只参加一轮’，实则是指哪一轮呢？</p><p>别问，问就是自闭。</p><h1 id="Quick-Start-Guide"><a href="#Quick-Start-Guide" class="headerlink" title="Quick-Start Guide"></a>Quick-Start Guide</h1><p>在开始解题之前，还要学习怎么使用Kick Start平台。本来不想写这部分的，后来粗粗看了看，发现解题形式和我之前常用的POJ或Leetcode <del>其实我不常用，但是常用一词会显得我在这方面非常qualified</del> 差别还是不小的，所以来这里整理一下：</p><ol><li>如果想参加的是比赛，要在contest’s home page（当然还要在比赛举行时间）进行解题，如果是练习就滚去<a href="https://code.google.com/codejam/past-contests" target="_blank" rel="noopener">Past Problems</a>，选择自己想自闭的轮次</li><li>要注意limits模块中的内容，因为很多时候不是WA，而是TLE，就像最难过的事情不是我不行，而是我本可以。</li><li>下载问题文件之后，需要生成result文件并提交。在practice中只需要提交result文件，在competition中还需要提交code文件。提交结果有三种：<br>A. Correct<br>B. Rejected：就是提交被并非对错的原因被否决了，比如传错了文件<br>C. Incorrect：回答错误</li><li>比较有意思的是，Kick Start中的设定是，先写自己的程序，写完觉得没问题了点击下载数据集，一开始只能下载Small规模的数据集，从下载后开始计时，小问题是4分钟，解答正确后才能下载大数据集，大问题是8分钟。需要在指定时间内提交答案，如果提交答案正确，结束计时；如果提交Rejected，计时会继续走，还能继续提交；如果提交答案错误，会被计一次罚时，<strong>值得注意的是，如果计时结束也没有提交或者还是Rejected，也会自动判为Incorrect，此时已经没有提交机会了</strong>。</li><li>大数据集评分只有在比赛结束后才能看到，也就是说，提交之后是看不到自己的结果的。对于大数据集，只会judge最后一次上传的结果，可以在8分钟限时内反复提交。</li><li><strong>当然，以上所有的限制在Practice mode下不会生效，这是给自闭儿童的一大福利。</strong></li></ol><hr><p>关于读取input.file并输出output.file</p><p><strong>对于C++</strong></p><p>在solution.cpp中编写解决问题的代码，输入输出使用cin与cout即可，在编译时使用以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ solution.cpp -o solution./solution &lt; input_file.txt &gt; output_file.txt</span><br></pre></td></tr></table></figure><p>第一步是为了生成名为solution的可执行文件，当然如果嫌麻烦直接省去-o参数，直接生成a.out也行。</p><p>第二步是运行这个可执行文件，并指定输入文件名为input_file.txt，输出到output_file.txt中。</p><p><strong>对于Python</strong></p><p>使用raw_input()函数读取，使用print输出以及format方法控制格式即可。在执行时输入以下指令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python2 solution.py input_file.txt output_file.txt</span><br></pre></td></tr></table></figure><p>Python的执行语句就非常的简单粗暴了。通过以上的例子可以发现，实际编写代码时和平时其它平台上的输入输出相比几乎没有区别，只是在执行代码时添加了条件。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://img3.imgtn.bdimg.com/it/u=1916139364,2178340577&amp;amp;fm=15&amp;amp;gp=0.jpg&quot; alt=&quot;img&quot;&gt;Google的&lt;a href=&quot;https://codingcompetit
      
    
    </summary>
    
      <category term="算法" scheme="http://fangzh.top/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="kick start" scheme="http://fangzh.top/categories/%E7%AE%97%E6%B3%95/kick-start/"/>
    
    
      <category term="算法" scheme="http://fangzh.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>芝诺之圆 | 方向、迷失与北京三年</title>
    <link href="http://fangzh.top/2019/2019091117/"/>
    <id>http://fangzh.top/2019/2019091117/</id>
    <published>2019-11-17T03:34:04.000Z</published>
    <updated>2019-11-17T03:43:39.373Z</updated>
    
    <content type="html"><![CDATA[<p>​        <img src="https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=3545615097,2982278649&amp;fm=26&amp;gp=0.jpg" alt="img"></p><p>​        物理学家喜欢模型思维，并且笃信奥卡姆剃刀原则，也就是说，在一个好的模型下，我们可以简洁、优雅地，更重要的是，用最少的知识与经验来作出正确的，符合自然行为的预测。</p><p>​        历史上这样成功的模型不多，牛顿的机械世界观，爱因斯坦的时空论，而大多数其他的工作都有赖于足够多的知识，也就是经验来作出预测，理解世界。近年来火的一塌糊涂的人工智能就是在尝试更高效地利用人类社会大量积累的经验。但我想在爱因斯坦的眼里，他或许不太会喜欢机器学习这类东西。</p><p>​        <img src="data:image/jpeg;base64,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" alt="img"></p><p>​        1925年，爱因斯坦与学生一同散步时，表达了他的核心观点：“我想知道上帝是如何创造这个世界的。我对这个或那个现象，这个或那个元素的能谱不感兴趣。我要知道的是他的思想；其余的都是细节。”  在他看来，那些需要额外的人为添加的信息都是不自然的东西，因而不是上帝的思想。</p><p>​     我举这个例子不是要说爱因斯坦一定不会为今天人工智能所取得的成绩骄傲，也不是要指责人工智能和经验打交道，我没有充分的理由做这样的论断，事实上人工智能技术底层的物理学与数学是非常深刻的。我真正想讨论的是，我们必然要面对思想模型和经验知识之间的鸿沟。或者至少说，为数不多的基本法则和真实的运转细节之间是有些距离的。爱因斯坦之所以嗤之以鼻那些“细节”，是因为这些细节完全毁坏了定律的简洁美与秩序，繁杂的计算步骤与操作算法只会让我们感到混乱而不是秩序。但这些细节似乎是追求简谐与统一的我们永远也无法消除的东西，因为科学最本质的核心任务是写实并作出预测，而实际情况总是蕴含着复杂的关系与大量的信息，它们淹没了一切的简单形式。</p><p>​      只能说上帝的思想太过稀疏，大多数的复杂性都没有落入基本原理的限制中，它们当然不会违背基本法则，但仅通过基本法则无法得出它们的具体行为，它们是那样的难以捉摸，就像遍地游走的哥布林。只能说上帝很懒，他只告诉我们朝北穿过森林，注意峡谷和荆棘，我们却对即将遇到的一切一无所知，必须如算法一般，一步一步小心翼翼地前进。</p><p>​    正如拧一个魔方，它的构造与对称性其实很简洁，但从任意一个位置要返回原始状态必须要经历一步一步的变换，变换或许不止一种，但它的物理构造无法限制哪一种应该被采用。</p><p>​      换句话说，复杂性是涌现的，它也似乎是非本质的，但它惊奇地决定着我们所能测量的。在不同的维度与层次上，所见绝非一种图景。被限制在地球二维表面的生物要穿过山脉，则所见群山巍峨、荆棘丛生、山溪涓流，身处其中，方向尽失。而飞翔在天际的雄鹰是三维的视角，千山万壑不过是平面上的褶皱，何去何从，清晰无比。一旦雄鹰落地，仍然需要处理天空中没有的杂乱与复杂度。</p><p>​       一个有趣的例子是分子系统。我们知道每个分子的运动在埃米到纳米尺度都可以用牛顿定律来描述。对每个分子而言，描述它只需要速度和位置；但当大量分子聚在一起，就会涌现出压强、温度、相态这些性质，要基于分子性质去了解这些信息非常困难。有意思的是，热力学可以在完全不知道分子层次信息的情形下，建立正确的宏观量之间的关系，这些涌现出来的性质之间也会出现秩序。同时，一个比较大的系统可以根据环境的不同自发地形成特定的功能，就像有很多人组成的社群会自发的形成社会分工与阶层一样。如果我们进一步增大空间尺度，上述可以产生相态的分子集团继续聚集达到宏观尺度，则我们来到流体力学的范畴，这时又会涌现出湍流等等新的复杂度。你无法通过分子层次这个看起来更基本的层次去完全解释湍流，你总是需要一些只属于这个层次的概念和物理量。就像一个个俄罗斯套娃，又或者说，继续之前的类比，虽然已经建立了国家内部的秩序，但一旦这个国家和其他的国家开始交流，则又会涌现出新的复杂度，它们可能会贸易，文化交流，甚至战争。你无法仅用国民的行为来解释外交现象，有一些解释权是只属于外交层面的。</p><p>​    所以，问题的核心在于，More is different. 这句话本是凝聚态物理的座右铭，意思是一个系统的性质不等于各个部分的和，因为它们内部有相互作用。</p><p>​       这里我想从另一个角度来解读这句话。那就是芝诺之圆。古希腊哲学家芝诺说，假设我们能用一个圆来包络我们所有已知的知识与经验，那么在这个圆之外的一切就都是未知的；这个圆的周长则是已知与未知的边界。设想我们通过学习与探索，我们的经验与知识越来越多，圆的面积越来越大，同样的，我们面临的未知也越多，因为周长变长了，新的复杂性不断涌现出来。换句话说，你知道的越多，不知道的也越多。认知半径太小，甚至都不知道自己不知道。所以说多而不同，是多了之后涌现出新的复杂度与未知了。</p><p>​      让我们从反方向去回溯前面的过程。按照早期的伯努利、欧拉等人发展的连续介质流体观点，人们已经可以预测流体内的压力分布、流动能量损耗等信息，可以设计飞行器的形状与考虑湍流的影响。直到有一天有人发现流体是由一个个微小的分子构成的，这显然是个认识上的巨大进步，但却打开了新的未知，如何描述这些分子的集体行为？这些离散的分子怎么会导致连续的流动？到了有人发现分子的行为可以解释之后，我们又发现有些现象是分子理论解释不清的，这引导人们发现了更小尺度上的量子力学。但这个过程会一直持续下去么？会存在一个最小的俄罗斯套娃吗？没有人知道。</p><p>​      我们会发现在每个层次上都会有秩序与法则，从量子力学到热力学，再到流体力学，更甚至于到人体，到社会，再到精神领域。当然，每个层次也有它特定的复杂度，流体力学中我们怎么也找不到纳维叶·斯托克斯方程的通解，热力学中我们也没有充分理解时间可逆的微观力学怎么就过渡到了时间不可逆的热力学，精神领域我们也不清楚意识的起源。但一旦我们要跨越不同的层次，就会涌现出新的复杂度与概念。这意味着一个悲观的事实，根本就不存在所谓的“万物理论”。即使你已经完备描述了所有基本粒子的行为，从轻子到强子到介子等等，描述了所有的基本作用力，从电磁力到强核力到弱核力再到引力，但这仍不足以解释这些基本粒子和基本力是如何共同组建了一个丰富多彩的世界，包含生命的世界。</p><p>​    这很恼人，不是么。你会问，无穷无尽的问题需要解决，需要回答，没有终点。那我们的探索还有意义么？</p><p>​      我们有方向，那是不断的去剥开自然秘密的洋葱，就像理查德·费曼所设想的那样，就像前文我们所回溯的那样，我们笃信真理就在前方；但迷失总是与方向共生，或者说如芝诺之圆，没有方向，都无法意识到什么是迷失。我们迷失这样的探索究竟将我们带向何方。自然的洋葱永远都剥不完，解决了问题只会涌现出更多的问题，并且更难，更深刻。是什么样的自然结构才能容纳如此的属性啊？无穷嵌套着无穷！</p><p>​      对于每一个略微挑剔的人来说，这种境况都像一个白色盘子上的黑点，你总想擦掉它，但你发现擦掉它之后盘子又被你抹布上的污渍弄脏了。不管怎样，都得不到一个完全白净的盘子。哦，也许你会说，问题就出在你用的抹布上，你为何不用一张干净的抹布呢？令人悲伤的是，不存在绝对干净的抹布。所以当一切都结束的时候，盘子还是脏的，那有什么实质性的改变呢？</p><p>​      没错，你学会了怎么去更好的擦干净一个盘子。这，也许就够了。</p><p>​     计算机科学先驱阿兰·图灵说，“We can only see a short distance ahead, but we can see  plenty there that needs to be done.”-  我们只能向前看到一小段未来，但我们可以在那里看到足够多可以做的事。很难说当代一个受过良好教育的人，一个理解银河系、理解宇宙演化史、理解现代社会法则的人和16世纪一个仍然相信地球是平的的人有什么本质的区别。看起来16世纪的你一定会比21世纪的你更愚蠢，但从未知的方面看，两者其实同等幼稚。今天宏大的宇宙观可能就是另一个版本的平坦地球，如果被25世纪的人观察的话。正因为如此，图灵的话才会有意义。正是因为所有人都没有大自然聪明，才会有文明的发展，才会有人秉持真理去挑战所谓的人类权威；正是因为问题永远都无法完善，才会有生机与希望，才会有足够多的工作可以做  -  一个有趣的例子是，当年的流体力学课程上，亚利桑那州立大学的Chen教授对我们说，如果纳维叶·斯托克斯方程真的被解决了，那我们这些人就都失业了。我想我应该不会喜欢失业，但我同时可以确定的是，如果真的存在一个单一的解释一切的理论，那它一定是没有活力的，因为在那里一切都是确定的，因此也不可能会有概率产生任何新的东西。</p><p>​      所以事实是，越探索，越迷失；越迷失，越探索。这就像一个引擎，驱动着理性精神不断前进，芝诺之圆不断扩张。这正是生机所在，而图灵所说的足够多可以做的事，就是这个生机的具体源泉。爱因斯坦穷其一生追求的“上帝的思想”，在某种意义上也只是短暂的未来，虽然它已经足够深刻。在我们有限的生命中，在文明有限的生命中，将永远面临芝诺之圆的困境，方向，迷失，方向，迷失…   我们不过是密闭飞船中的旅客，对飞船外的一切毫无知觉，只知道它在前进，但对它为何存在，从何来，在哪里，去向何方一无所知。人类文明尚且如此，更何况微渺的个体。</p><p>​      而我，就是那个微渺的个体，那粒飘扬的星尘。</p><p>​    所以我想实际上图灵是在说，既然都离不开这艘飞船，那就让我们在这里做一些可以做的事吧，或许会有些惊喜也说不定呢。</p><p>​      回顾在北京的三年，学习，思考，努力将生活的白草根嚼出百味，也寄托了我的青葱岁月。我从不畏惧成为一个loser，我也从不想和任何人竞争。因为我们的命运最终是一体的。只有当我周围的人们都幸福时，我也才会幸福；只有当我周围的人们都获得尊严时，我也才会挺胸抬头。特别欣赏著名理论物理学家李·斯莫林所说：当今世界没有多少人比爱德华·威滕（超弦理论大牛）更加聪明，按理说我每天都应该生活在他的阴影之下，但要知道，大自然比我们所有人都聪明，因此我总是有机会作出我独特的贡献。</p><p>​      所以勇敢的迈出步伐吧，纵使漂泊浮荡，奔波求索。莫须问生命的意义为何，因为本来就没有意义。只要你想到，组成你的身体的那些原子，诞生于宇宙之初，穿越140亿年的时空和无数的偶然事件，最终到了这里组成了无与伦比的你，这本身就是一个奇迹，一种神圣，一种宇宙演化史的绝美升华。而当我们笃定了探索的决心，不断地去拓展芝诺之圆，便是和我们的起源对话，便是完满的意义。</p><p>​    芝诺之圆是每个人的圆，你可以是物理学家，数学家，也可以是生物学家，或者是社会学家，农民，建筑工人，舞蹈演员，我们都需要去找到一种对话方式，来拓展这个圆。这就是我们的宿命。我们虚怀若谷，是因为我们深知知也无涯吾生有涯。</p><p>​             </p><p>​           </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​        &lt;img src=&quot;https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=3545615097,2982278649&amp;amp;fm=26&amp;amp;gp=0.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
      
    
    </summary>
    
      <category term="生活" scheme="http://fangzh.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="随笔" scheme="http://fangzh.top/categories/%E7%94%9F%E6%B4%BB/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活" scheme="http://fangzh.top/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>MIXED PRECISION TRAINING</title>
    <link href="http://fangzh.top/2019/20190911172/"/>
    <id>http://fangzh.top/2019/20190911172/</id>
    <published>2019-11-17T03:02:10.000Z</published>
    <updated>2019-11-17T14:01:22.926Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-fig1.png?raw=true" alt="mixed-precision-fig1.png">ABSTRACT</h2><p>增大网络通常会提升准确性，但是也增加了内存和计算量。我们的方法可以使用半精度浮点数训练网络，同时不损失准确性，也不需要修改超参数。这近乎减少了一般的内存，在最新的GPU上还能加速运算。权重，激活和梯度都使用IEEE半精度格式。由于这个格式范围比单精度小，我们提出了三种方法来避免重要信息的损失。首先，建议保留权重的单精度拷贝，每次优化器执行后累积梯度（前向和反向传播时，拷贝归整到单精度）。其次，我们提出了损失放大用于保留小的梯度值。第三，我们使用半精度累积到单精度输出，在存到内存前转为半精度。</p><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><p>神经网络两个趋势：更大的数据和更大的模型。大的模型需要更多的计算和内存，可以通过降低精度来减小这些需求。性能（速度）包括了网络的训练和推理，受三个因素先知：算术带宽，内存带宽，或者延迟。减低精度解决了其中的两个限制。内存带宽的压力可以通过更少的比特位来降低。算术时间可以通过更大的低精度吞吐量来降低。例如，在最近的GPU上半精度的吞吐量是单精度的2到8倍。除了速度提升，降低精度也减少了训练的内存。</p><p>现代的深度学习系统是使用单精度（FP32）格式。我们使用更低的精度来训练，同时保持准确率，用IEEE半精度格式FP16来训练网络。由于FP16比FP32动态范围小，我们引入了3个技术来保持准确率：保留权重的FP32主拷贝，梯度变为0时损失放大，用FP16计算累积到FP32。我们证明了这些方法可以让大部分网络匹敌FP32的准确率。而且这些方法不需要修改模型或调整超参数。</p><h2 id="2-RELATED-WORK"><a href="#2-RELATED-WORK" class="headerlink" title="2 RELATED WORK"></a>2 RELATED WORK</h2><p>【略】</p><h2 id="3-IMPLEMENTATION"><a href="#3-IMPLEMENTATION" class="headerlink" title="3 IMPLEMENTATION"></a>3 IMPLEMENTATION</h2><p>single-precision master weights and updates, loss-scaling, and accumulating FP16 products into FP32。</p><h3 id="3-1-FP32-MASTER-COPY-OF-WEIGHTS"><a href="#3-1-FP32-MASTER-COPY-OF-WEIGHTS" class="headerlink" title="3.1 FP32 MASTER COPY OF WEIGHTS"></a>3.1 FP32 MASTER COPY OF WEIGHTS</h3><p>在混合精度训练中，权重，激活和梯度用FP16存储。为了匹敌FP32的准确率，需要保留FP32的权重主拷贝，在优化的时候随梯度更新。每次迭代时，权重的FP16拷贝用于前向和反向传播，减少了FP32训练的存储和带宽。图1是混合精度的训练过程。</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-fig1.png?raw=true" alt="mixed-precision-fig1.png"></p><p>并不是所有步骤都需要FP32主权重，两个可能原因。一个解释是更新（权重梯度乘以学习率）太小，FP16无法表示，任何梯度小于$2^{-24}$在FP16中都是0。图2b中可以看到5%左右的权重梯度指数小于-24。</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-fig2.png?raw=true" alt="mixed-precision-fig2.png"></p><p>另一个解释是，权重的值小相比于更新的值很大。这样，即使权重的更新可以用FP16表示，在右移对齐二进制点的时候会变成0.当权重值量至少大于权重更新的2048倍时就会发生。由于FP16有10 bits的小数，可能在右移11或更多位置的时候变成0。这个影响可以通过计算FP32更新来解决。</p><p>为了展示FP32的必要性，我们使用了普通话语言模型进行了实验，如图2a，更新FP16权重导致了80%的准确率损失。即使保留额外的权重拷贝增加了内存，但是对于整体的内存占用还是很小的。训练内存的消耗主要是激活，这是因为每一层的批量或激活会保存下来用于重复使用。激活也使用半精度存储，整体的内存基本减半。</p><h3 id="3-2-LOSS-SCALING"><a href="#3-2-LOSS-SCALING" class="headerlink" title="3.2 LOSS SCALING"></a>3.2 LOSS SCALING</h3><p>FP16的指数偏置（exponent bias） 把标准化值的指数中心化为$[-14,15]$，而梯度值主要由很小的值（负指数）决定。例如图3为激活梯度直方图，来自SSD检测网络。注意到大部分FP16的表示范围并未使用，大量小于表示范围的值变为0。放大梯度可以让他们移到更好的表示区间内，不至于让值变为0。如果不放大网络很容易发散，放大因子设为8足够达到FP32的精度。这表明低于$2^{-27}$的值就与训练无关了，但是$\left[2^{-27}, 2^{-24}\right)$之间的值值得保留。</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-fig3.png?raw=true" alt="mixed-precision-fig3.png"></p><p>一个有效的把梯度值移动到FP16表示范围内的方法是在前向传播计算损失的时候放大损失值。通过链式法则反向传播保证了所有的梯度值可以放大的相同的范围。并不需要反向传播额外的运算，也不会让有用的梯度值变成0。权重梯度在更新的时候必须缩小回来，这样才能保证正确的FP32训练精度。</p><p>有几种方法来选择损失放大因子。最简单的是挑选一个常量。我们在8到32k的范围内训练大量的网络，可以通过经验选取一个值，或直接选择一个因子让最大值低于65504（FP16的最大值）即可。【略】</p><h3 id="3-3-ARITHMETIC-PRECISION"><a href="#3-3-ARITHMETIC-PRECISION" class="headerlink" title="3.3 ARITHMETIC PRECISION"></a>3.3 ARITHMETIC PRECISION</h3><p>网络的运算有三类：向量点乘，reduction，和逐点操作。不同的类别有不同的方式处理。为了保持模型的准确率，我们发现一些网络需要把FP16点乘部分乘积累积到FP32精度，在写入内存的时候转成FP16。如果没有这个步骤，一些FP16模型无法达到相似的准确率。</p><p>大的reduction操作（向量元素相加）可以用FP32执行。这样的操作大多来自于BN层和softmax层。我们实现的这些层都是从内存中读写FP16张量，执行FP32运算。这些操作并没有降低训练速度，因为他们是受限于内存带宽的，对运算速度不敏感。</p><p>逐点操作，例如非线性或逐点矩阵成绩，是受限于内存带宽的。运算精度不影响速度，因此使用FP16和FP32都可以。</p><h2 id="4-RESULTS"><a href="#4-RESULTS" class="headerlink" title="4 RESULTS"></a>4 RESULTS</h2><ul><li><strong>Baseline (FP32)</strong> 所有的运算FP32</li><li><strong>Mixed Precision (MP)</strong> FP16用于运算和存储。权重，激活和梯度使用FP16存储，权重的FP32主拷贝用于更新。某些应用中使用损失放大。对于卷积，全连接和循环层的矩阵相乘通过FP16运算累积到FP32精度。</li></ul><p>【略】</p><h3 id="4-1-CNNS-FOR-ILSVRC-CLASSIFICATION"><a href="#4-1-CNNS-FOR-ILSVRC-CLASSIFICATION" class="headerlink" title="4.1 CNNS FOR ILSVRC CLASSIFICATION"></a>4.1 CNNS FOR ILSVRC CLASSIFICATION</h3><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-tab1.png?raw=true" alt="mixed-precision-tab1.png"></p><h3 id="4-2-DETECTION-CNNS"><a href="#4-2-DETECTION-CNNS" class="headerlink" title="4.2 DETECTION CNNS"></a>4.2 DETECTION CNNS</h3><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-tab2.png?raw=true" alt="mixed-precision-tab2.png"></p><h3 id="4-3-SPEECH-RECOGNITION"><a href="#4-3-SPEECH-RECOGNITION" class="headerlink" title="4.3 SPEECH RECOGNITION"></a>4.3 SPEECH RECOGNITION</h3><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-tab3.png?raw=true" alt="mixed-precision-tab3.png"></p><h3 id="4-4-MACHINE-TRANSLATION"><a href="#4-4-MACHINE-TRANSLATION" class="headerlink" title="4.4 MACHINE TRANSLATION"></a>4.4 MACHINE TRANSLATION</h3><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-fig4.png?raw=true" alt="mixed-precision-fig4.png"></p><h3 id="4-5-LANGUAGE-MODELING"><a href="#4-5-LANGUAGE-MODELING" class="headerlink" title="4.5 LANGUAGE MODELING"></a>4.5 LANGUAGE MODELING</h3><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-fig5.png?raw=true" alt="mixed-precision-fig5.png"></p><h3 id="4-6-DCGAN-RESULTS"><a href="#4-6-DCGAN-RESULTS" class="headerlink" title="4.6 DCGAN RESULTS"></a>4.6 DCGAN RESULTS</h3><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/mixed-precision-fig6.png?raw=true" alt="mixed-precision-fig6.png"></p><h2 id="5-CONCLUSIONS-AND-FUTURE-WORK"><a href="#5-CONCLUSIONS-AND-FUTURE-WORK" class="headerlink" title="5 CONCLUSIONS AND FUTURE WORK"></a>5 CONCLUSIONS AND FUTURE WORK</h2><p>混合精度训练是减少内存占用的重要方法，还有运算时间和运算量。我们证明了很多不同的深度模型都可以用这个方法训练，没有准确率损失，也不需要修改超参数。对于有大量小梯度值的模型，我们引入梯度放大方法让它们可以收敛到与FP32同样的准确率。</p><p>【略】</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ABSTRACT&quot;&gt;&lt;a href=&quot;#ABSTRACT&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT&quot;&gt;&lt;/a&gt;&lt;img src=&quot;https://github.com/Bazingaliu/Bazingaliu.github.ios/
      
    
    </summary>
    
      <category term="笔记" scheme="http://fangzh.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://fangzh.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CVPR" scheme="http://fangzh.top/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science</title>
    <link href="http://fangzh.top/2019/20190911162/"/>
    <id>http://fangzh.top/2019/20190911162/</id>
    <published>2019-11-16T06:03:45.000Z</published>
    <updated>2019-11-17T14:00:53.110Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/TPOT-fig1.png?raw=true" alt="TPOT-fig1.png"></p><p><strong>ABSTRACT</strong></p><p>让机器学习去专家化有很大的需求。我们提出了基于树的管道优化（tree-based pipeline optimization）方法，可以自动化最枯燥的管道设计工作。我们实现了开源的Tree-based<br>Pipeline Optimization Tool (TPOT)，并在真实的数据集上证明了它的效果。尤其是，我们展示了TPOT设计的机器学习管道可以获得巨大的提升，同时又不需要用户的输入和先验信息。我们通过Pareto优化解决了TPOT使用过于复杂管道的倾向，同时准确率也能保持。</p><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1. INTRODUCTION"></a>1. INTRODUCTION</h2><p>现在的机器学习工具需要很多的先验知识，也需要消耗大量的时间和计算量。【略】例如同城数据科学家会以图1的方法来解决机器学习问题。每一步都有大量的选择：如何预处理数据，使用什么模型，参数是什么。有经验的数据科学家有一定的直觉，可以从比较好的起点开始，但是缺乏经验会在花费大量的时间。</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/TPOT-fig1.png?raw=true" alt="TPOT-fig1.png"></p><p>遗传算法可以自动化机器学习管道的设计吗？我们提出的TPOT使用了遗传算法自动设计和优化一系列数据转化运算和机器学习模型，尝试最大化分类准确率。我们也提出了TPOT-Pareto，它结合了Pareto，可以控制管道的复杂度。</p><h2 id="3-METHODS"><a href="#3-METHODS" class="headerlink" title="3. METHODS"></a>3. METHODS</h2><h3 id="3-1-Pipeline-Operators"><a href="#3-1-Pipeline-Operators" class="headerlink" title="3.1 Pipeline Operators"></a>3.1 Pipeline Operators</h3><p>我们列出了TPOT的4个主要运算。所有的管道运算都利用了scikit-learn现有的实现。</p><p><strong>Preprocessors</strong> StandardScaler，RobustScaler， PolynomialFeatures</p><p><strong>Decomposition</strong> RandomizedPCA，SVD</p><p><strong>Feature Selection</strong> RFE， SelectKBest， SelectPercentile， VarianceThreshold</p><p><strong>Models</strong> DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier，SVM， LogisticRegression， KNeighborsClassifier</p><h3 id="3-2-Assembling-Tree-based-Pipelines"><a href="#3-2-Assembling-Tree-based-Pipelines" class="headerlink" title="3.2 Assembling Tree-based Pipelines"></a>3.2 Assembling Tree-based Pipelines</h3><p>为了把这些结合成一个灵活的管道结构，我们如图2以树的形式实现了管道。每个基于树的管道开始于一个或多个输入数据的拷贝，把他们作为树的节点，然后输入4类管道运算之一。在多个数据拷贝处理之后，可以把他们结合成一个数据集。</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/TPOT-fig2.png?raw=true" alt="TPOT-fig2.png"></p><p>每次一个数据集传入模型运算后，分类结果被存下来，最近的分类器就会覆盖以前的预测，先前的预测作为新特征存储下来。一旦数据被整个管道处理后（例如图2中数据通过随机森林），最后的预测用于衡量整个管道的分类性能。所有的情况下，我们把数据分层划分为75%的训练集和测试集，整个管道只使用训练集训练，使用测试集测试。基于树的管道结构可以使用任意的管道表示，例如管道可以在单个数据上线性执行一系列运算，或者在多个数据拷贝上运算，最后分类的时候再结合起来。</p><h3 id="3-3-Evolving-Tree-based-Pipelines"><a href="#3-3-Evolving-Tree-based-Pipelines" class="headerlink" title="3.3 Evolving Tree-based Pipelines"></a>3.3 Evolving Tree-based Pipelines</h3><p>为了自动化生成和优化这些管道，我们使用了演化计算方法，称为遗传编程（GP）。传统的GP构建数学函数树来优化给定的指标。在TPOT中，我们使用GP来调整参数和运算，最大化分类准确率。过程如表1。</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/TPOT-tab1.png?raw=true" alt="TPOT-tab1.png"></p><p>本文中，TPOT的优化目标是测试集的分类准确率。我们也提出了TPOT的扩展，TPOT-Pareto，使用Pareto优化来优化两个目标：最大化最后的分类准确率，同时最小化管道的复杂度。</p><h3 id="3-4-GAMETES-Simulated-Data-Sets"><a href="#3-4-GAMETES-Simulated-Data-Sets" class="headerlink" title="3.4 GAMETES Simulated Data Sets"></a>3.4 GAMETES Simulated Data Sets</h3><p>【略】</p><h3 id="3-5-UCI-Benchmark-Data-Sets"><a href="#3-5-UCI-Benchmark-Data-Sets" class="headerlink" title="3.5 UCI Benchmark Data Sets"></a>3.5 UCI Benchmark Data Sets</h3><p>【略】</p><h2 id="4-RESULTS"><a href="#4-RESULTS" class="headerlink" title="4. RESULTS"></a>4. RESULTS</h2><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/TPOT-fig3.png?raw=true" alt="TPOT-fig3.png"><br><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/TPOT-fig4.png?raw=true" alt="TPOT-fig4.png"><br><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/TPOT-fig5.png?raw=true" alt="TPOT-fig5.png"></p><h2 id="5-DISCUSSION"><a href="#5-DISCUSSION" class="headerlink" title="5. DISCUSSION"></a>5. DISCUSSION</h2><p>【略】</p><h2 id="6-CONCLUSIONS"><a href="#6-CONCLUSIONS" class="headerlink" title="6. CONCLUSIONS"></a>6. CONCLUSIONS</h2><p>基于树的管道优化是一个新的技术，可以极大地：1）让非专家人员可以更好的利用机器学习工具，2）自动化机器学习中枯燥的部分，节省大量时间。我们证明了TPOT在没有先验知识的情况下可以达到一定的性能。另外很多情况下，TPOT可以自动化发掘预处理和建模的结合，极大地超过基本数据分析的性能。通过结合Pareto优化，我们证明了TPOT可以设计出紧凑，易于解析的管道，同时准确率得以保持。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/TPOT-fig1.png?raw=true&quot; alt=&quot;TPOT-fig1.png&quot;&gt;
      
    
    </summary>
    
      <category term="笔记" scheme="http://fangzh.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://fangzh.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="AutoML" scheme="http://fangzh.top/tags/AutoML/"/>
    
  </entry>
  
  <entry>
    <title>Deep-Learning-Approach-for-Surface-Defect-Detection</title>
    <link href="http://fangzh.top/2019/20190911161/"/>
    <id>http://fangzh.top/2019/20190911161/</id>
    <published>2019-11-16T05:12:08.000Z</published>
    <updated>2019-11-17T14:00:12.842Z</updated>
    
    <content type="html"><![CDATA[<p><strong><img src="https://img-blog.csdnimg.cn/2019090516131491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Zpc2hfbGlrZV9hcHBsZQ==,size_16,color_FFFFFF,t_70" alt="img"></strong></p><pre><code>表面缺陷检测是工业视觉的热点应用之一，自动的表面缺陷检测技术越来越受到重视，其中以深度学习相关技术应用为代表，它通过大量图像对检测系统进行训练学习得到一个自动的视觉检测系统。这个方面基于深度学习的检测方法基本上可以分为两个大类。</code></pre><p> 1-基于目标检测网络实现的缺陷检测系统<br> 2-基于图像分割网络实现的缺陷检测系统</p><p><strong>网络设计</strong></p><p>​        作者采用后者·实现了一个缺陷检测与缺陷大小分割的网络，实现了一个更加高效与准确的缺陷检测系统。该网络只需要20~30个样本训练量就可以得到很好的效果，避免网络训练需要成千上万的样本收集成本。</p><p>作者在论文中提到，经典的机器视觉方法已经不能满足工业4.0的技术要求，基于深度学习的方法显示出高的灵活性与准确性，传统的手工标准特征提取+SVM/KNN的视觉检测方法不如深度学习的相关方法有效。该网络模型架构策略如下：</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/20191023132239.png?raw=true" alt="20191023132239.png"></p><p>网络在最初设计时候就充分考虑了下面两个关键问题</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1-需要的标注数据多少</span><br><span class="line">2-网络参数与浮点数计算量</span><br></pre></td></tr></table></figure><p>​        通过语义分割+决策两阶段网络基于KoletorSDD（缺陷检测公开数据库）达到最好效果，实现了少量样本训练与高精度检测。整个网络架构如下：</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/20191023132307.png?raw=true" alt="20191023132307.png"></p><p>​        在网络分割部分，作者认为表面缺陷检测可以被解释成一个图像二值分割问题，所以作者首先通过一个语义分割网络实现像素级的语义分割。然后把分割结果作为输入特征构建决策部分，第一部分称为<strong>分割网络</strong>，第二部分自然就被认为是<strong>决策网络</strong></p><p><strong>分割网络详解</strong></p><p>​        分割网络包括了11个卷积层与3个池化层，在每个卷积层后面跟上一个BN层与ReLU激活层（conv+BN+ReLU），用来优化学习加速收敛。除了最后一层卷积核大小为15 x 15，所有的卷积层都采用5 x 5大小的卷积核。最后使用1 x 1的卷积得到的图像大小是一个单通道的是原图八分之一大小的mask图像，dropout正则化则被完全抛弃，作者认为这样的网络已经足够正则化（事实后面的实验数据证明确实如此），这样的网络架构有能力在高分辨率图像实现小的缺陷检测，网络具备比较大的感受野（5 x 5），同时可以实现比较小的特征捕获（像素级分割），作者还解释了网络的下采样与高层通过大的卷积核（15 x 15）的目的是为了放大感受野大小，以及底层采用多个卷积核与下采样max-pooling层的作用</p><p><strong>决策网络详解</strong></p><p>​        决策网络用分割网络的输出作为输入，使用分割网络的最后一个卷积层加上mask通道得到1025个通道数据作为输入特征，即分隔网络的倒数第二层和最后输出的集联。</p><p>采用max-pooling+conv（5 x 5的卷积核）的方式，最后网络通过全局最大池化与均值池化输出得到66个输出向</p><p>量</p><p>作者在设计决策网络的时候考虑了以下两个重要原则：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1-网络有能力应对大而复杂的形状，所以采用三次max-pooling</span><br><span class="line">2-对输入不仅采用最后一层卷积层特征同时把mask数据当成输入，最终输出66个特征向量。有效的解决了过拟合与全卷积特征参数过多的问题。</span><br></pre></td></tr></table></figure><p>同时作者提出了一个快捷路径的概念，通过全局最大池化与卷积层实现了不同的快捷路径，有效地阻止了网络的复杂性。</p><p><strong>训练</strong></p><p>分割网络的学习目标是产生一个二分类分割网络，是基于像素级别的分类操作，作者采用了两种训练方式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1-基于回归的平方错误（MSE） 损失函数：逐像素均方差</span><br><span class="line">2-基于交叉熵的二分类 损失函数：逐像素交叉熵</span><br></pre></td></tr></table></figure><p>​        值得一提的是，segmentation和decision是分开训练的，先训练segmentation部分再训练decision部分，作者说这种方式应对标准化问题，我对标准化的理解是，损失包括两部分，分别是分割损失和分类部分的损失，这两个损失需要均衡一下（标准化）才能加到一起（不过两者比例很难确定</p><p>优化器是没有momenum的<strong>随机梯度下降（SGD）</strong>batch大小为1，使用逐方差训练时损失为0,005，使用交叉熵训练时损失为0.1，一共训练了100个Epochs，值得一提的是并不是每个epoch包括固定次数的迭代，而是一正常一随机的交替选择样本，33个缺陷样本全部过一遍才算一个epoch，论文中提到也可以同时训练，不过损失函数应都是用交叉熵函数，据作者说效果更佳。</p><p><strong>数据集</strong></p><p>作者使用的数据集是KolertorSDD，这是作者自己搜集的数据集，标签有5种，图片尺寸有两种。数据扩增包括90度旋转和标签膨胀如下图<img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/papers/2019/R/20191023132357.png?raw=true" alt="20191023132357.png"></p><p>数据如何标注：需要两个数据集，一个是拍摄的图像+黑白像素的图像，一个是图像+有无瑕疵，较难的是前一个，建立一个大小一样的1矩阵，然后将瑕疵部分的像素值置为0 。</p><p><strong>个人总结</strong></p><ol><li>本篇论文最核心的思想，把缺陷检测当成是一个二值化图像分割问题，采用基于像素级别分割网络成功地减少了网络深度与参数总数，实现了少量样本训练就可以达到极高准确率的缺陷表面检测网络。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2019090516131491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_
      
    
    </summary>
    
      <category term="笔记" scheme="http://fangzh.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://fangzh.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CVPR" scheme="http://fangzh.top/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>THE CONTEST</title>
    <link href="http://fangzh.top/2019/20190911165/"/>
    <id>http://fangzh.top/2019/20190911165/</id>
    <published>2019-11-15T18:05:14.000Z</published>
    <updated>2019-11-17T14:05:25.370Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://sta.codeforces.com/s/91655/images/codeforces-logo-with-telegram.png" alt="img"></p><p><strong>题意</strong><br>三个人，每个人有一些数字，组合起来是$1$~$n$，每个人可以给另一个人一个拥有的数字，问最小操作数，使得第一个人拥有$1$~$i$的数，第二个人拥有$i+1$~$j$的数，第三个人拥有$j+1$~$n$的数，即第一个人为前缀，第二个人为中间部分，第三个人为后缀。<br>注意：可以有一个或两个人最后不拥有数字。</p><p><strong>分析</strong><br>看到三个人操作，我们先看两个人操作时的情况：<br>假设到最后，第一个人拥有$1$~$i$，第二个人拥有$i+1$~$n$，那么最小操作数为第二个人$1$~$i$中中拥有的数字加上第一个人$i+1$~$n$中拥有的数字。我们可以采用前缀和，$cnt1[k]$表示第一个人前$k$个数中拥有的个数，$cnt2[k]$表示第二个人前$k$个数中拥有的个数,则表达式为：$$cnt2[i]+cnt1[n]-cnt1[i]$$受到启发我们看三个人操作时的情况：<br>假设到最后，第一个人拥有$1$~$i$，第二个人拥有$i+1$~$j$，第三个人拥有$j+1$~$n$，那么最小操作数为第二个人和第三个人$1$~$i$中拥有的个数加上第一个人和第三个人$i+1$~$j$中拥有的个数加上第一个人和第二个人$j+1$~$n$中拥有的个数。我们可以采用前缀和，$cnt1[k]$表示第一个人前$k$个数中拥有的个数，$cnt2[k]$表示第二个人前$k$个数中拥有的个数，$cnt3[k]$表示第三个人前$k$个数中拥有的个数则表达式为：$$cnt2[i]+cnt3[i]+cnt1[j]-cnt1[i]+cnt3[j]-cnt3[i]+cnt1[n]-cnt1[j]+cnt2[n]-cnt2[j]$$化简得到:$$cnt2[i]-cnt1[i]+cnt3[j]-cnt2[j]+cnt1[n]+cnt2[n]$$我们从$0$~$n$枚举$i$，接下来我们考虑$j$的取值，我们可以看到对于固定的$i$，只需要找到一个$j$使得该式子最小即可，那么我们可以设置一个后缀$minn[]$数组，$minn[i]$表示当$i\leq j\leq n$时，$cnt3[j]-cnt2[j]$最小的值，那么答案即为：$$cnt2[i]-cnt1[i]+minn[i]+cnt1[n]+cnt2[n]$$<br><strong>代码</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> GCC optimize(3, <span class="meta-string">"Ofast"</span>, <span class="meta-string">"inline"</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> start ios::sync_with_stdio(false);cin.tie(0);cout.tie(0);</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ll long long</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LL long long</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = (ll) <span class="number">2e5</span> + <span class="number">5</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> mod = <span class="number">1000000007</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> inf = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"><span class="keyword">int</span> cnt1[maxn], cnt2[maxn], cnt3[maxn];</span><br><span class="line"><span class="keyword">int</span> minn[maxn];</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v1, v2, v3;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    start;</span><br><span class="line">    <span class="keyword">int</span> k1, k2, k3;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; k1 &gt;&gt; k2 &gt;&gt; k3;</span><br><span class="line">    v1.resize(k1 + <span class="number">5</span>);</span><br><span class="line">    v2.resize(k2 + <span class="number">5</span>);</span><br><span class="line">    v3.resize(k3 + <span class="number">5</span>);</span><br><span class="line">    <span class="comment">/*输入并标记*/</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= k1; ++i) &#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; v1[i];</span><br><span class="line">        ++cnt1[v1[i]];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= k2; ++i) &#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; v2[i];</span><br><span class="line">        ++cnt2[v2[i]];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= k3; ++i) &#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; v3[i];</span><br><span class="line">        ++cnt3[v3[i]];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> n = k1 + k2 + k3;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;<span class="comment">//前缀和</span></span><br><span class="line">        cnt1[i] = cnt1[i - <span class="number">1</span>] + cnt1[i];</span><br><span class="line">        cnt2[i] = cnt2[i - <span class="number">1</span>] + cnt2[i];</span><br><span class="line">        cnt3[i] = cnt3[i - <span class="number">1</span>] + cnt3[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*如分析*/</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= n; ++i)</span><br><span class="line">        minn[i] = cnt3[i] - cnt2[i];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span>; --i)</span><br><span class="line">        minn[i] = min(minn[i + <span class="number">1</span>], minn[i]);</span><br><span class="line">    <span class="keyword">int</span> ans = inf;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">        <span class="keyword">int</span> t = cnt2[i] - cnt1[i] + minn[i] + cnt1[n] + cnt2[n];</span><br><span class="line">        ans = min(ans, t);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; ans;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>本场比赛$D$和$E$惨痛教训：玩后缀一定要注意边界！！！<br>若有问题可在评论区提出，谢谢。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://sta.codeforces.com/s/91655/images/codeforces-logo-with-telegram.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题意&lt;/strong&gt;&lt;br&gt;三个人，每个人有一些
      
    
    </summary>
    
      <category term="算法" scheme="http://fangzh.top/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="codeforces" scheme="http://fangzh.top/categories/%E7%AE%97%E6%B3%95/codeforces/"/>
    
    
      <category term="算法" scheme="http://fangzh.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Yet Another Monster Killing Problem</title>
    <link href="http://fangzh.top/2019/20190911164/"/>
    <id>http://fangzh.top/2019/20190911164/</id>
    <published>2019-11-15T17:15:43.000Z</published>
    <updated>2019-11-17T14:04:47.786Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://sta.codeforces.com/s/91655/images/codeforces-logo-with-telegram.png" alt="img"></p><p><strong>题意</strong>：</p><p>有$n$个怪物，每个怪物有攻击力$a_i$点；有$m$个英雄，每个英雄有攻击力$p_i$点，耐力$s_{i}$点。</p><p>怪物需要被依次杀死(按输入顺序)。</p><p>每一天可以挑选一个英雄去杀怪物，他可以杀死的怪物攻击力小于等于他本身(即$a\leq p$)，每天最多可以杀死$s$个怪物。(每个英雄可以使用任意次)</p><p>问最少需要多少天可以杀死所有怪物(不能则输出$-1$)。</p><p><strong>分析</strong>：</p><p>$(1)$我们找到怪物的最大攻击力和英雄的最大攻击力，判断是否要输出$-1$。</p><p>$(2)$将英雄按攻击力$p$值排序，我们可以发现对于英雄$b[i]$而言，如果对于$i&lt;j\leq m$，且有$b[i].s&lt;b[j].s$，我们可以选择英雄$j$，而不是英雄$i$，那么我们可以把$b[i].s$替换为$b[j].s$（意思为选择英雄$i$时选择英雄$j$）。</p><p>$(3)$因此我们进行后缀操作将$b[i].s$改为英雄$i$~$n$中最大的耐力值，以便进行下一步。</p><p>$(4)$对于某个怪物而言，我们可以找到一个英雄，他的攻击力刚好大于等于该怪物（二分）。我们上一步将该英雄的耐力改为了后缀最大值，那么我们便选择这个英雄。</p><p>$(5)$我们从第一天开始，枚举每一个怪物，找到当前天我们可以杀死最多怪物的英雄，如果对于某个怪物而言，杀死他的人的耐力（我们进行了后缀操作）不足以支撑该天，我们将该怪物放到下一天，并重复操作，直至杀死所有怪物。因此我们需要保存的量有：当前的天数$k$，昨天杀死的最后一只怪物的编号$last$，今天所能杀死的最多怪物数（表现为所需要的最小耐力）$minn$。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> GCC optimize(3, <span class="meta-string">"Ofast"</span>, <span class="meta-string">"inline"</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> start ios::sync_with_stdio(false);cin.tie(0);cout.tie(0);</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ll long long</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LL long long</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = (ll) <span class="number">2e5</span> + <span class="number">5</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> mod = <span class="number">1000000007</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> inf = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> p, s;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span>&lt;(<span class="keyword">const</span> node &amp;b) &#123;<span class="comment">//用做排序</span></span><br><span class="line">        <span class="keyword">return</span> p &lt; b.p;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; b[maxn];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(<span class="keyword">const</span> node &amp;x, <span class="keyword">int</span> y)</span> </span>&#123;<span class="comment">//用做二分</span></span><br><span class="line">    <span class="keyword">return</span> x.p &lt; y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> a[maxn];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    start;</span><br><span class="line">    <span class="keyword">int</span> T;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; T;</span><br><span class="line">    <span class="keyword">while</span> (T--) &#123;</span><br><span class="line">        <span class="keyword">int</span> n;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">        <span class="keyword">int</span> maxa = <span class="number">0</span>, maxs = <span class="number">0</span>;<span class="comment">//用做判-1</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; a[i];</span><br><span class="line">            maxa = max(maxa, a[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> m;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; m;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= m; ++i) &#123;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; b[i].p &gt;&gt; b[i].s;</span><br><span class="line">            maxs = max(maxs, b[i].p);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (maxa &gt; maxs) &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="number">-1</span> &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        sort(b + <span class="number">1</span>, b + m + <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = m - <span class="number">1</span>; i &gt;= <span class="number">1</span>; --i)<span class="comment">//后缀操作</span></span><br><span class="line">            b[i].s = max(b[i].s, b[i + <span class="number">1</span>].s);</span><br><span class="line">        <span class="keyword">int</span> k = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> last = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> minn = inf;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">            <span class="keyword">int</span> t = lower_bound(b + <span class="number">1</span>, b + m + <span class="number">1</span>, a[i], cmp) - b;<span class="comment">//刚好能杀死该怪物的英雄编号</span></span><br><span class="line">            minn = min(b[t].s, minn);<span class="comment">//今天所需要的最小耐力</span></span><br><span class="line">            <span class="keyword">if</span> (minn + last &lt; i) &#123;<span class="comment">//将这只怪物放到明天杀</span></span><br><span class="line">                minn = b[t].s;</span><br><span class="line">                ++k;</span><br><span class="line">                last = i - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; k &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 本场比赛$D$和$E$惨痛教训：玩后缀一定要注意边界！！！ </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://sta.codeforces.com/s/91655/images/codeforces-logo-with-telegram.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题意&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;有$n$个
      
    
    </summary>
    
      <category term="算法" scheme="http://fangzh.top/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="codeforces" scheme="http://fangzh.top/categories/%E7%AE%97%E6%B3%95/codeforces/"/>
    
    
      <category term="算法" scheme="http://fangzh.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>cs231n作业：assignment2 - Convolutional Networks</title>
    <link href="http://fangzh.top/2018/cs231n-2-3/"/>
    <id>http://fangzh.top/2018/cs231n-2-3/</id>
    <published>2018-10-22T05:49:41.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg" alt></p><p>作业做到这里才真正进入了cnn的范畴。</p><a id="more"></a><p>先用最基本的循环来写forward</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwgyxae6rgj20xq0f475n.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward_naive</span><span class="params">(x, w, b, conv_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input consists of N data points, each with C channels, height H and</span></span><br><span class="line"><span class="string">    width W. We convolve each input with F different filters, where each filter</span></span><br><span class="line"><span class="string">    spans all C channels and has height HH and width HH.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - w: Filter weights of shape (F, C, HH, WW)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (F,)</span></span><br><span class="line"><span class="string">    - conv_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - 'stride': The number of pixels between adjacent receptive fields in the</span></span><br><span class="line"><span class="string">        horizontal and vertical directions.</span></span><br><span class="line"><span class="string">      - 'pad': The number of pixels that will be used to zero-pad the input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, F, H', W') where H' and W' are given by</span></span><br><span class="line"><span class="string">      H' = 1 + (H + 2 * pad - HH) / stride</span></span><br><span class="line"><span class="string">      W' = 1 + (W + 2 * pad - WW) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, w, b, conv_param)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the convolutional forward pass.                         #</span></span><br><span class="line">    <span class="comment"># Hint: you can use the function np.pad for padding.                      #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#N个样本，C个通道，H高度，W宽度</span></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    <span class="comment">#F个filter，C个通道，HH个核高度，WW核宽度</span></span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    <span class="comment">#步长</span></span><br><span class="line">    stride = conv_param[<span class="string">'stride'</span>]</span><br><span class="line">    <span class="comment">#padding 的像素个数</span></span><br><span class="line">    pad = conv_param[<span class="string">'pad'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#经过卷积核之后的图片大小</span></span><br><span class="line">    new_H = <span class="number">1</span> + int((H + <span class="number">2</span> * pad - HH)/stride)</span><br><span class="line">    new_W = <span class="number">1</span> + int((W + <span class="number">2</span> * pad - WW)/stride)</span><br><span class="line">    out = np.zeros([N, F, new_H, new_W])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#遍历N个样本卷积</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(F):</span><br><span class="line">            <span class="comment">#需要加上bias</span></span><br><span class="line">            conv_newH_new_W = np.ones([new_H, new_W]) * b[f]</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(C):</span><br><span class="line">                <span class="comment">#填充原图片x</span></span><br><span class="line">                padded_x = np.lib.pad(x[n, c], pad_width = pad, mode=<span class="string">'constant'</span>,constant_values=<span class="number">0</span>)</span><br><span class="line">                <span class="comment">#开始计算卷积后的图中的每一个像素，每一个像素就是对应一个卷积核乘上原来的图片的位置</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(new_H):</span><br><span class="line">                    <span class="keyword">for</span> j <span class="keyword">in</span> range(new_W):</span><br><span class="line">                        conv_newH_new_W[i, j] += np.sum(padded_x[i * stride:i * stride+HH, j * stride: j*stride+WW]* w[f, c, :, :])</span><br><span class="line">            <span class="comment">#把C个通道中的那些对应像素加在一起，得到了单张图片单个核数的out</span></span><br><span class="line">            out[n, f] = conv_newH_new_W</span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = (x, w, b, conv_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p>backward如图：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwgyxio6swj20xj0uegqr.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward_naive</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives.</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dw, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the convolutional backward pass.                        #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># 数据准备</span></span><br><span class="line">    x, w, b, conv_param = cache</span><br><span class="line">    pad = conv_param[<span class="string">'pad'</span>]</span><br><span class="line">    stride = conv_param[<span class="string">'stride'</span>]</span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    N, F, new_H, new_W = dout.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 下面，我们模拟卷积，首先填充x。</span></span><br><span class="line">    padded_x = np.lib.pad(x,</span><br><span class="line">                          ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)),</span><br><span class="line">                          mode=<span class="string">'constant'</span>,</span><br><span class="line">                          constant_values=<span class="number">0</span>)</span><br><span class="line">    padded_dx = np.zeros_like(padded_x)  <span class="comment"># 填充了的dx，后面去填充即可得到dx</span></span><br><span class="line">    dw = np.zeros_like(w)</span><br><span class="line">    db = np.zeros_like(b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):  <span class="comment"># 第n个图像</span></span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(F):  <span class="comment"># 第f个过滤器</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(new_H):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(new_W):</span><br><span class="line">                    <span class="comment">#dw 等于所有out的每一个像素求导之和，因为out每个像素都共享参数</span></span><br><span class="line">                    db[f] += dout[n, f, i, j] <span class="comment"># dg对db求导为1*dout</span></span><br><span class="line">                    dw[f] += padded_x[n, :, i*stride : HH + i*stride, j*stride : WW + j*stride] * dout[n, f, i, j]</span><br><span class="line">                    padded_dx[n, :, i*stride : HH + i*stride, j*stride : WW + j*stride] += w[f] * dout[n, f, i, j]</span><br><span class="line">    <span class="comment"># 去掉填充部分</span></span><br><span class="line">    dx = padded_dx[:, :, pad:pad + H, pad:pad + W]</span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure><p>然后是max pool 层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_forward_naive</span><span class="params">(x, pool_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a max pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - pool_param: dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - 'pool_height': The height of each pooling region</span></span><br><span class="line"><span class="string">      - 'pool_width': The width of each pooling region</span></span><br><span class="line"><span class="string">      - 'stride': The distance between adjacent pooling regions</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data</span></span><br><span class="line"><span class="string">    - cache: (x, pool_param)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the max pooling forward pass                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height = pool_param[<span class="string">'pool_height'</span>] <span class="comment"># 池化过滤器高度</span></span><br><span class="line">    pool_width  = pool_param[<span class="string">'pool_width'</span>]  <span class="comment"># 池化过滤器宽度</span></span><br><span class="line">    pool_stride = pool_param[<span class="string">'stride'</span>]      <span class="comment"># 移动步长</span></span><br><span class="line">    new_H = <span class="number">1</span> + int((H - pool_height) / pool_stride)    <span class="comment"># 池化结果矩阵高度</span></span><br><span class="line">    new_W = <span class="number">1</span> + int((W - pool_width) / pool_stride)     <span class="comment"># 池化结果矩阵宽度</span></span><br><span class="line">    out = np.zeros([N, C, new_H, new_W])</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(C):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(new_H):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(new_W):</span><br><span class="line">                    out[n,c,i,j] = np.max(x[n, c, i*pool_stride : i*pool_stride+pool_height, j*pool_stride : j*pool_stride+pool_width])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = (x, pool_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_backward_naive</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a max pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, pool_param) as in the forward pass.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx = <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the max pooling backward pass                           #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#太难</span></span><br><span class="line">    x, pool_param = cache</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height = pool_param[<span class="string">'pool_height'</span>]</span><br><span class="line">    pool_width  = pool_param[<span class="string">'pool_width'</span>]</span><br><span class="line">    pool_stride = pool_param[<span class="string">'stride'</span>]</span><br><span class="line">    new_H = <span class="number">1</span> + int((H - pool_height) / pool_stride)</span><br><span class="line">    new_W = <span class="number">1</span> + int((W - pool_width) / pool_stride)</span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(C):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(new_H):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(new_W):</span><br><span class="line">                    window = x[n, c, i * pool_stride: i * pool_stride + pool_height,j * pool_stride: j * pool_stride + pool_width]</span><br><span class="line">                    dx[n, c, i * pool_stride: i * pool_stride + pool_height, j * pool_stride: j * pool_stride + pool_width] = (window == np.max(window))*dout[n,c,i,j]</span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><p>以上只是尝试最基本的CNN和max pool结构。实际使用不用这个，因为有更高效的版本。</p><p>然后用高效的版本定义了三明治层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu_forward</span><span class="params">(x, w, b, conv_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A convenience layer that performs a convolution followed by a ReLU.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input to the convolutional layer</span></span><br><span class="line"><span class="string">    - w, b, conv_param: Weights and parameters for the convolutional layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output from the ReLU</span></span><br><span class="line"><span class="string">    - cache: Object to give to the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    a, conv_cache = conv_forward_fast(x, w, b, conv_param)</span><br><span class="line">    out, relu_cache = relu_forward(a)</span><br><span class="line">    cache = (conv_cache, relu_cache)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for the conv-relu convenience layer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    conv_cache, relu_cache = cache</span><br><span class="line">    da = relu_backward(dout, relu_cache)</span><br><span class="line">    dx, dw, db = conv_backward_fast(da, conv_cache)</span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure><p>在<code>cnn.py</code>中完成了三层的ConvNet</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThreeLayerConvNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A three-layer convolutional network with the following architecture:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    conv - relu - 2x2 max pool - affine - relu - affine - softmax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The network operates on minibatches of data that have shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    consisting of N images, each with height H and width W and with C input</span></span><br><span class="line"><span class="string">    channels.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="params">(<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span>, num_filters=<span class="number">32</span>, filter_size=<span class="number">7</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_dim=<span class="number">100</span>, num_classes=<span class="number">10</span>, weight_scale=<span class="number">1e-3</span>, reg=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dtype=np.float32)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize a new network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - input_dim: Tuple (C, H, W) giving size of input data</span></span><br><span class="line"><span class="string">        - num_filters: Number of filters to use in the convolutional layer</span></span><br><span class="line"><span class="string">        - filter_size: Size of filters to use in the convolutional layer</span></span><br><span class="line"><span class="string">        - hidden_dim: Number of units to use in the fully-connected hidden layer</span></span><br><span class="line"><span class="string">        - num_classes: Number of scores to produce from the final affine layer.</span></span><br><span class="line"><span class="string">        - weight_scale: Scalar giving standard deviation for random initialization</span></span><br><span class="line"><span class="string">          of weights.</span></span><br><span class="line"><span class="string">        - reg: Scalar giving L2 regularization strength</span></span><br><span class="line"><span class="string">        - dtype: numpy datatype to use for computation.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.reg = reg</span><br><span class="line">        self.dtype = dtype</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Initialize weights and biases for the three-layer convolutional    #</span></span><br><span class="line">        <span class="comment"># network. Weights should be initialized from a Gaussian with standard     #</span></span><br><span class="line">        <span class="comment"># deviation equal to weight_scale; biases should be initialized to zero.   #</span></span><br><span class="line">        <span class="comment"># All weights and biases should be stored in the dictionary self.params.   #</span></span><br><span class="line">        <span class="comment"># Store weights and biases for the convolutional layer using the keys 'W1' #</span></span><br><span class="line">        <span class="comment"># and 'b1'; use keys 'W2' and 'b2' for the weights and biases of the       #</span></span><br><span class="line">        <span class="comment"># hidden affine layer, and keys 'W3' and 'b3' for the weights and biases   #</span></span><br><span class="line">        <span class="comment"># of the output affine layer.                                              #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        C, H, W = input_dim</span><br><span class="line">        <span class="comment">#W1为第一层conv参数</span></span><br><span class="line">        self.params[<span class="string">'W1'</span>] = weight_scale * np.random.randn(num_filters, C, filter_size, filter_size)</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(num_filters)</span><br><span class="line">        <span class="comment">#W2为maxpool - hiddenlayer</span></span><br><span class="line">        self.params[<span class="string">'W2'</span>] = weight_scale * np.random.randn(int(H / <span class="number">2</span>) * int(W / <span class="number">2</span>)*num_filters, hidden_dim)</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(hidden_dim)</span><br><span class="line">        <span class="comment">#W3 hidden - output</span></span><br><span class="line">        self.params[<span class="string">'W3'</span>] = weight_scale * np.random.randn(hidden_dim, num_classes)</span><br><span class="line">        self.params[<span class="string">'b3'</span>] = np.zeros(num_classes)</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> self.params.items():</span><br><span class="line">            self.params[k] = v.astype(dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Evaluate loss and gradient for the three-layer convolutional network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Input / output: Same API as TwoLayerNet in fc_net.py.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</span><br><span class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">        W3, b3 = self.params[<span class="string">'W3'</span>], self.params[<span class="string">'b3'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass conv_param to the forward pass for the convolutional layer</span></span><br><span class="line">        filter_size = W1.shape[<span class="number">2</span>]</span><br><span class="line">        conv_param = &#123;<span class="string">'stride'</span>: <span class="number">1</span>, <span class="string">'pad'</span>: (filter_size - <span class="number">1</span>) // <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass pool_param to the forward pass for the max-pooling layer</span></span><br><span class="line">        pool_param = &#123;<span class="string">'pool_height'</span>: <span class="number">2</span>, <span class="string">'pool_width'</span>: <span class="number">2</span>, <span class="string">'stride'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">        scores = <span class="keyword">None</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for the three-layer convolutional net,  #</span></span><br><span class="line">        <span class="comment"># computing the class scores for X and storing them in the scores          #</span></span><br><span class="line">        <span class="comment"># variable.                                                                #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        conv_forward_out_1, cache_forward_1 = conv_relu_pool_forward(X, W1, b1, conv_param, pool_param)</span><br><span class="line">        affine_out_2, cache_forward_2 = affine_relu_forward(conv_forward_out_1, W2, b2)</span><br><span class="line">        scores, cache_forward_3 = affine_forward(affine_out_2, W3, b3)</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">        loss, grads = <span class="number">0</span>, &#123;&#125;</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for the three-layer convolutional net, #</span></span><br><span class="line">        <span class="comment"># storing the loss and gradients in the loss and grads variables. Compute  #</span></span><br><span class="line">        <span class="comment"># data loss using softmax, and make sure that grads[k] holds the gradients #</span></span><br><span class="line">        <span class="comment"># for self.params[k]. Don't forget to add L2 regularization!               #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        loss, dscore = softmax_loss(scores, y)</span><br><span class="line">        <span class="comment">#da2 即affine_out_2的d</span></span><br><span class="line">        da2, grads[<span class="string">'W3'</span>], grads[<span class="string">'b3'</span>] = affine_backward(dscore, cache_forward_3)</span><br><span class="line">        <span class="comment">#da1,即第一层经过conv pool之后的d</span></span><br><span class="line">        da1, grads[<span class="string">'W2'</span>], grads[<span class="string">'b2'</span>] = affine_relu_backward(da2, cache_forward_2)</span><br><span class="line">        _, grads[<span class="string">'W1'</span>], grads[<span class="string">'b1'</span>] = conv_relu_pool_backward(da1, cache_forward_1)</span><br><span class="line"></span><br><span class="line">        loss += <span class="number">0.5</span> * self.reg * (np.sum(W1 ** <span class="number">2</span>) + np.sum(W2 **<span class="number">2</span>) + np.sum(W3 ** <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        grads[<span class="string">'W1'</span>] += self.reg * W1</span><br><span class="line">        grads[<span class="string">'W2'</span>] += self.reg * W2</span><br><span class="line">        grads[<span class="string">'W3'</span>] += self.reg * W3</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;作业做到这里才真正进入了cnn的范畴。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="cs231n" scheme="http://fangzh.top/tags/cs231n/"/>
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>cs231n作业：assignment2 - Batch Normalization and Dropout</title>
    <link href="http://fangzh.top/2018/cs231n-2-2/"/>
    <id>http://fangzh.top/2018/cs231n-2-2/</id>
    <published>2018-10-22T05:07:45.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg" alt></p><h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><p>批量归一化相当于在每一层神经网络的激活函数前进行归一化预处理。</p><a id="more"></a><p>先写<code>batchnorm_forward</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mode = bn_param[<span class="string">'mode'</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the training-time forward pass for batch norm.      #</span></span><br><span class="line">        <span class="comment"># Use minibatch statistics to compute the mean and variance, use      #</span></span><br><span class="line">        <span class="comment"># these statistics to normalize the incoming data, and scale and      #</span></span><br><span class="line">        <span class="comment"># shift the normalized data using gamma and beta.                     #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should store the output in the variable out. Any intermediates  #</span></span><br><span class="line">        <span class="comment"># that you need for the backward pass should be stored in the cache   #</span></span><br><span class="line">        <span class="comment"># variable.                                                           #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should also use your computed sample mean and variance together #</span></span><br><span class="line">        <span class="comment"># with the momentum variable to update the running mean and running   #</span></span><br><span class="line">        <span class="comment"># variance, storing your result in the running_mean and running_var   #</span></span><br><span class="line">        <span class="comment"># variables.                                                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        </span><br><span class="line">        sample_mean = np.mean(x, axis=<span class="number">0</span>)  <span class="comment">#每一列均值</span></span><br><span class="line">        sample_var = np.var(x, axis=<span class="number">0</span>)    <span class="comment">#每一列方差</span></span><br><span class="line">        x_hat = (x - sample_mean) / (np.sqrt(sample_var + eps)) <span class="comment">#归一化后</span></span><br><span class="line">        out = gamma * x_hat + beta   <span class="comment">#变成新的均值和方差</span></span><br><span class="line">        cache = (gamma, x, sample_mean, sample_var, eps, x_hat)</span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean <span class="comment">#然后把均值和方差在每一步都进行指数加权平均</span></span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test-time forward pass for batch normalization. #</span></span><br><span class="line">        <span class="comment"># Use the running mean and variance to normalize the incoming data,   #</span></span><br><span class="line">        <span class="comment"># then scale and shift the normalized data using gamma and beta.      #</span></span><br><span class="line">        <span class="comment"># Store the result in the out variable.                               #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        scale = gamma / np.sqrt(running_var + eps)</span><br><span class="line">        out = x * scale + (beta - running_mean * scale)</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><p><code>backword</code>很难，公式看图：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwgywyfgs8j20jb0ae3zv.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></span><br><span class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></span><br><span class="line"><span class="string">    intermediate nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></span><br><span class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    gamma, x, mean, var, eps, x_hat = cache</span><br><span class="line">    N =x.shape[<span class="number">0</span>]</span><br><span class="line">    dgamma = np.sum(dout * x_hat, axis=<span class="number">0</span>)   <span class="comment"># 第5行公式</span></span><br><span class="line">    dbeta = np.sum(dout * <span class="number">1.0</span>, axis=<span class="number">0</span>)      <span class="comment"># 第6行公式</span></span><br><span class="line">    dx_hat = dout * gamma                   <span class="comment"># 第1行公式</span></span><br><span class="line">    dx_hat_numerator = dx_hat / np.sqrt(var + eps)      <span class="comment"># 第3行第1项(未负求和)</span></span><br><span class="line">    dx_hat_denominator = np.sum(dx_hat * (x - mean), axis=<span class="number">0</span>)    <span class="comment"># 第2行前半部分</span></span><br><span class="line">    dx_1 = dx_hat_numerator                 <span class="comment"># 第4行第1项</span></span><br><span class="line">    dvar = <span class="number">-0.5</span> * ((var + eps) ** (<span class="number">-1.5</span>)) * dx_hat_denominator  <span class="comment"># 第2行公式</span></span><br><span class="line">    <span class="comment"># Note var is also a function of mean</span></span><br><span class="line">    dmean = <span class="number">-1.0</span> * np.sum(dx_hat_numerator, axis=<span class="number">0</span>) + \</span><br><span class="line">              dvar * np.mean(<span class="number">-2.0</span> * (x - mean), axis=<span class="number">0</span>)  <span class="comment"># 第3行公式(部分)</span></span><br><span class="line">    dx_var = dvar * <span class="number">2.0</span> / N * (x - mean)    <span class="comment"># 第4行第2项</span></span><br><span class="line">    dx_mean = dmean * <span class="number">1.0</span> / N               <span class="comment"># 第4行第3项</span></span><br><span class="line">    <span class="comment"># with shape (D,), no trouble with broadcast</span></span><br><span class="line">    dx = dx_1 + dx_var + dx_mean            <span class="comment"># 第4行公式</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><p>另一种backword</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward_alt</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Alternative backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation you should work out the derivatives for the batch</span></span><br><span class="line"><span class="string">    normalizaton backward pass on paper and simplify as much as possible. You</span></span><br><span class="line"><span class="string">    should be able to derive a simple expression for the backward pass.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: This implementation should expect to receive the same cache variable</span></span><br><span class="line"><span class="string">    as batchnorm_backward, but might not use all of the values in the cache.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs / outputs: Same as batchnorm_backward</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></span><br><span class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># After computing the gradient with respect to the centered inputs, you   #</span></span><br><span class="line">    <span class="comment"># should be able to compute gradients with respect to the inputs in a     #</span></span><br><span class="line">    <span class="comment"># single statement; our implementation fits on a single 80-character line.#</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    gamma, x, sample_mean, sample_var, eps, x_hat = cache</span><br><span class="line">    N = x.shape[<span class="number">0</span>]</span><br><span class="line">    dx_hat = dout * gamma</span><br><span class="line">    dvar = np.sum(dx_hat* (x - sample_mean) * <span class="number">-0.5</span> * np.power(sample_var + eps, <span class="number">-1.5</span>), axis = <span class="number">0</span>)</span><br><span class="line">    dmean = np.sum(dx_hat * <span class="number">-1</span> / np.sqrt(sample_var +eps), axis = <span class="number">0</span>) + dvar * np.mean(<span class="number">-2</span> * (x - sample_mean), axis =<span class="number">0</span>)</span><br><span class="line">    dx = <span class="number">1</span> / np.sqrt(sample_var + eps) * dx_hat + dvar * <span class="number">2.0</span> / N * (x-sample_mean) + <span class="number">1.0</span> / N * dmean</span><br><span class="line">    dgamma = np.sum(x_hat * dout, axis = <span class="number">0</span>)</span><br><span class="line">    dbeta = np.sum(dout , axis = <span class="number">0</span>)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><p>然后把之前的FullyConnectedNet的use_batchnorm补上，之前已经写好了，不再赘述。</p><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>定义一个mask，用来生成0-1随机数，然后转化为大于某个数的布尔值，再把输入值乘上这个mask就可以得到一部分失活，一部分没有失活的神经元</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_forward</span><span class="params">(x, dropout_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the forward pass for (inverted) dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of any shape</span></span><br><span class="line"><span class="string">    - dropout_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - p: Dropout parameter. We drop each neuron output with probability p.</span></span><br><span class="line"><span class="string">      - mode: 'test' or 'train'. If the mode is train, then perform dropout;</span></span><br><span class="line"><span class="string">        if the mode is test, then just return the input.</span></span><br><span class="line"><span class="string">      - seed: Seed for the random number generator. Passing seed makes this</span></span><br><span class="line"><span class="string">        function deterministic, which is needed for gradient checking but not</span></span><br><span class="line"><span class="string">        in real networks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">    - out: Array of the same shape as x.</span></span><br><span class="line"><span class="string">    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout</span></span><br><span class="line"><span class="string">      mask that was used to multiply the input; in test mode, mask is None.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    p, mode = dropout_param[<span class="string">'p'</span>], dropout_param[<span class="string">'mode'</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'seed'</span> <span class="keyword">in</span> dropout_param:</span><br><span class="line">        np.random.seed(dropout_param[<span class="string">'seed'</span>])</span><br><span class="line"></span><br><span class="line">    mask = <span class="keyword">None</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement training phase forward pass for inverted dropout.   #</span></span><br><span class="line">        <span class="comment"># Store the dropout mask in the mask variable.                        #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        mask = np.random.rand(*x.shape) &gt;= p</span><br><span class="line">        mask = mask / (<span class="number">1</span> - p)</span><br><span class="line">        out = mask * x</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test phase forward pass for inverted dropout.   #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        out = x</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                            END OF YOUR CODE                         #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line"></span><br><span class="line">    cache = (dropout_param, mask)</span><br><span class="line">    out = out.astype(x.dtype, copy=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Perform the backward pass for (inverted) dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of any shape</span></span><br><span class="line"><span class="string">    - cache: (dropout_param, mask) from dropout_forward.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dropout_param, mask = cache</span><br><span class="line">    mode = dropout_param[<span class="string">'mode'</span>]</span><br><span class="line"></span><br><span class="line">    dx = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement training phase backward pass for inverted dropout   #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        dx = dout * mask</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        dx = dout</span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrlvz19onj20yh0k1dms.jpg&quot; alt&gt;&lt;/p&gt;
&lt;h1 id=&quot;Batch-Normalization&quot;&gt;&lt;a href=&quot;#Batch-Normalization&quot; class=&quot;headerlink&quot; title=&quot;Batch Normalization&quot;&gt;&lt;/a&gt;Batch Normalization&lt;/h1&gt;&lt;p&gt;批量归一化相当于在每一层神经网络的激活函数前进行归一化预处理。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="cs231n" scheme="http://fangzh.top/tags/cs231n/"/>
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达Coursera(DeepLearning.ai)笔记和作业汇总帖</title>
    <link href="http://fangzh.top/2018/dl-ai-summary/"/>
    <id>http://fangzh.top/2018/dl-ai-summary/</id>
    <published>2018-10-18T12:01:05.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>吴恩达Coursera(DeepLearning.ai)笔记和作业汇总。</p><a id="more"></a><p>历时一个多月终于把NG的五门课全部学完并且做了作业和笔记了。这里汇总一下：</p><h1 id="第一门课：神经网络和深度学习"><a href="#第一门课：神经网络和深度学习" class="headerlink" title="第一门课：神经网络和深度学习"></a>第一门课：神经网络和深度学习</h1><p>主要讲了神经网络的基本概念，以及机器学习的梯度下降法，向量化，而后进入了浅层和深层神经网络的实现。</p><ul><li>前两周太简单了，在之前的机器学习课上NG全部都讲过了，这里就不做了。</li><li><p>第三周：主要是浅层神经网络的实现</p><ul><li>笔记：<a href="http://fangzh.top/2018/2018091215/">浅层神经网络</a></li><li>作业：<a href="http://fangzh.top/2018/2018091216/">浅层神经网络</a></li></ul></li><li><p>第四周：深层神经网络的实现</p><ul><li>笔记：<a href="http://fangzh.top/2018/2018091316/">深层神经网络</a></li><li>作业：<a href="http://fangzh.top/2018/2018091318/">深层神经网络</a></li></ul></li></ul><h1 id="第二门课：改善神经网络"><a href="#第二门课：改善神经网络" class="headerlink" title="第二门课：改善神经网络"></a>第二门课：改善神经网络</h1><p>介绍了改善神经网络的方法，如正则化，超参数调节，优化算法等。</p><ul><li><p>第一周：训练集的划分、正则化、dropout</p><ul><li>笔记：<a href="http://fangzh.top/2018/20180901513/">深度学习的实践层面</a></li><li>作业：<a href="http://fangzh.top/2018/2018091515/">深度学习的实践层面</a></li></ul></li><li><p>第二周：Mini-batch、Momentum、RMS、Adam、学习率衰减</p><ul><li>笔记：<a href="http://fangzh.top/2018/2018091621/">优化算法</a></li><li>作业：<a href="http://fangzh.top/2018/2018091711/">优化算法</a></li></ul></li><li><p>第三周：超参数的调试、BatchNorm、softmax</p><ul><li>笔记：<a href="http://fangzh.top/2018/2018091720/">超参数调试</a></li><li>作业：<a href="http://fangzh.top/2018/2018091810/">超参数调试</a></li></ul></li></ul><h1 id="第三门课：结构化机器学习项目"><a href="#第三门课：结构化机器学习项目" class="headerlink" title="第三门课：结构化机器学习项目"></a>第三门课：结构化机器学习项目</h1><p>主要讲了机器学习中的一些策略。</p><ul><li>第一周：ML策略、正交化、优化指标、数据集的划分、偏差<ul><li>笔记：<a href="http://fangzh.top/2018/2018092016/">机器学习策略(1)</a></li></ul></li><li>第二周：误差分析、数据不同分布、迁移学习、多任务、端到端<ul><li>笔记：<a href="http://fangzh.top/2018/2018092017/">机器学习策略(2)</a></li></ul></li></ul><h1 id="第四门课：卷积神经网络"><a href="#第四门课：卷积神经网络" class="headerlink" title="第四门课：卷积神经网络"></a>第四门课：卷积神经网络</h1><p>主要讲了神经网络的在图像上的非常重要的应用，卷积神经网络。</p><ul><li><p>第一周：padding、步长、池化、卷积</p><ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-4-1/">卷积神经网络</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-4-1h/">卷积神经网络</a></li></ul></li><li><p>第二周：一些重要的神经网络结构，VGG、ResNet、Inception等</p><ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-4-2/">深度卷积网络实例探究</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-4-2h/">深度卷积网络实例探究</a></li></ul></li><li><p>第三周：目标检测、Bounding Box、IOU、NMS</p><ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-4-3/">目标检测</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-4-3h/">目标检测</a></li></ul></li><li><p>第四周：人脸识别和神经风格转换</p><ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-4-4/">人脸识别和神经风格转换</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-4-4h/">人脸识别和神经风格转换</a></li></ul></li></ul><h1 id="第五门课：序列模型"><a href="#第五门课：序列模型" class="headerlink" title="第五门课：序列模型"></a>第五门课：序列模型</h1><p>主要讲了神经网络在语言领域的应用，用RNN模型</p><ul><li>第一周：介绍了基本的RNN、GRU、LSTM<ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-5-1/">循环神经网络</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-5-1h1/">构建RNN</a>、<a href="http://fangzh.top/2018/dl-ai-5-1h2/">字符级生成恐龙名字</a>、<a href="http://fangzh.top/2018/dl-ai-5-1h3/">LSTM生成爵士乐</a></li></ul></li><li>第二周：自然语言处理与词嵌入<ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-5-2/">自然语言处理与词嵌入</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-5-2h/">词向量运算和emoji表情包</a></li></ul></li><li>第三周：序列模型和注意力机制<ul><li>笔记：<a href="http://fangzh.top/2018/dl-ai-5-3/">序列模型和注意力机制</a></li><li>作业：<a href="http://fangzh.top/2018/dl-ai-5-3h/">机器翻译和触发关键字</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;吴恩达Coursera(DeepLearning.ai)笔记和作业汇总。&lt;/p&gt;
    
    </summary>
    
      <category term="汇总帖" scheme="http://fangzh.top/categories/%E6%B1%87%E6%80%BB%E5%B8%96/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-3) -- 序列模型和注意力机制</title>
    <link href="http://fangzh.top/2018/dl-ai-5-3h/"/>
    <id>http://fangzh.top/2018/dl-ai-5-3h/</id>
    <published>2018-10-18T10:39:15.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>这周作业分为了两部分：</p><ul><li>机器翻译</li><li>触发关键字</li></ul><h1 id="Part1：机器翻译"><a href="#Part1：机器翻译" class="headerlink" title="Part1：机器翻译"></a>Part1：机器翻译</h1><p>你将建立一个将人类可读日期（“2009年6月25日”）转换为机器可读日期（“2009-06-25”）的神经机器翻译（NMT）模型。 你将使用注意力机制来执行此操作，这是模型序列中最尖端的一个序列。</p><p>你将创建的模型可用于从一种语言翻译为另一种语言，如从英语翻译为印地安语。 但是，语言翻译需要大量的数据集，并且通常需要几天的GPU训练。 在不使用海量数据的情况下，为了让你有机会尝试使用这些模型，我们使用更简单的“日期转换”任务。</p><p>网络以各种可能格式（例如“1958年8月29日”，“03/30/1968”，“1987年6月24日”）写成的日期作为输入，并将它们转换成标准化的机器可读的日期（例如“1958 -08-29“，”1968-03-30“，”1987-06-24“），让网络学习以通用机器可读格式YYYY-MM-DD输出日期。</p><ul><li>X: 经过处理的训练集中人类可读日期，其中每个字符都替换为其在human_vocab中映射到的索引。 每个日期用特殊字符进一步填充为Tx长度。 X.shape =（m，Tx）</li><li>Y: 经过处理的训练集中机器可读日期，其中每个字符都替换为其在machine_vocab中映射到的索引。 你应该有Y.shape =（m，Ty）。</li><li>Xoh：X的one-hot向量，Xoh.shape = (m，Tx，len(human_vocab))</li><li>Yoh：Y的one-hot向量，Yoh.shape = (m，Tx，len(machine_vocab))</li></ul><h2 id="采用注意力机制的机器翻译"><a href="#采用注意力机制的机器翻译" class="headerlink" title="采用注意力机制的机器翻译"></a>采用注意力机制的机器翻译</h2><p>定义一些layers</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defined shared layers as global variables</span></span><br><span class="line">repeator = RepeatVector(Tx)</span><br><span class="line">concatenator = Concatenate(axis=<span class="number">-1</span>)</span><br><span class="line">densor1 = Dense(<span class="number">10</span>, activation = <span class="string">"tanh"</span>)</span><br><span class="line">densor2 = Dense(<span class="number">1</span>, activation = <span class="string">"relu"</span>)</span><br><span class="line">activator = Activation(softmax, name=<span class="string">'attention_weights'</span>) <span class="comment"># We are using a custom softmax(axis = 1) loaded in this notebook</span></span><br><span class="line">dotor = Dot(axes = <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>然后根据a 和 s 得到context</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm6agwq9j20yk0vwaeg.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: one_step_attention</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_step_attention</span><span class="params">(a, s_prev)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights</span></span><br><span class="line"><span class="string">    "alphas" and the hidden states "a" of the Bi-LSTM.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)</span></span><br><span class="line"><span class="string">    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    context -- context vector, input of the next (post-attetion) LSTM cell</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states "a" (≈ 1 line)</span></span><br><span class="line">    s_prev = repeator(s_prev)</span><br><span class="line">    <span class="comment"># Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)</span></span><br><span class="line">    concat = concatenator([a, s_prev])</span><br><span class="line">    <span class="comment"># Use densor1 to propagate concat through a small fully-connected neural network to compute the "intermediate energies" variable e. (≈1 lines)</span></span><br><span class="line">    e = densor1(concat)</span><br><span class="line">    <span class="comment"># Use densor2 to propagate e through a small fully-connected neural network to compute the "energies" variable energies. (≈1 lines)</span></span><br><span class="line">    energies = densor2(e)</span><br><span class="line">    <span class="comment"># Use "activator" on "energies" to compute the attention weights "alphas" (≈ 1 line)</span></span><br><span class="line">    alphas = activator(energies)</span><br><span class="line">    <span class="comment"># Use dotor together with "alphas" and "a" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)</span></span><br><span class="line">    context = dotor([alphas, a])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure><p>实现model()</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm6apu4fj21d417g0zz.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_a = <span class="number">32</span></span><br><span class="line">n_s = <span class="number">64</span></span><br><span class="line">post_activation_LSTM_cell = LSTM(n_s, return_state = <span class="keyword">True</span>)</span><br><span class="line">output_layer = Dense(len(machine_vocab), activation=softmax)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Tx -- length of the input sequence</span></span><br><span class="line"><span class="string">    Ty -- length of the output sequence</span></span><br><span class="line"><span class="string">    n_a -- hidden state size of the Bi-LSTM</span></span><br><span class="line"><span class="string">    n_s -- hidden state size of the post-attention LSTM</span></span><br><span class="line"><span class="string">    human_vocab_size -- size of the python dictionary "human_vocab"</span></span><br><span class="line"><span class="string">    machine_vocab_size -- size of the python dictionary "machine_vocab"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- Keras model instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the inputs of your model with a shape (Tx,)</span></span><br><span class="line">    <span class="comment"># Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)</span></span><br><span class="line">    X = Input(shape=(Tx, human_vocab_size))</span><br><span class="line">    s0 = Input(shape=(n_s,), name=<span class="string">'s0'</span>)</span><br><span class="line">    c0 = Input(shape=(n_s,), name=<span class="string">'c0'</span>)</span><br><span class="line">    s = s0</span><br><span class="line">    c = c0</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize empty list of outputs</span></span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)</span></span><br><span class="line">    a = Bidirectional(LSTM(n_a, return_sequences=<span class="keyword">True</span>))(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Iterate for Ty steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Ty):</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)</span></span><br><span class="line">        context = one_step_attention(a ,s)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.B: Apply the post-attention LSTM cell to the "context" vector.</span></span><br><span class="line">        <span class="comment"># Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)</span></span><br><span class="line">        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)</span></span><br><span class="line">        out = output_layer(s)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.D: Append "out" to the "outputs" list (≈ 1 line)</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)</span></span><br><span class="line">    model = Model(inputs=[X,s0,c0], outputs=outputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h1 id="Part2-Trigger-Word-Detection"><a href="#Part2-Trigger-Word-Detection" class="headerlink" title="Part2:Trigger Word Detection"></a>Part2:Trigger Word Detection</h1><p>做触发关键字的检测。</p><p>X: 这里把每一段音频分为了10s，而10s内细分为了5511个小的片段，也就是Tx = 5511</p><p>Y: Ty = 1375，每个y都是一个布尔值，用来记录有没有收到触发关键字。</p><h2 id="生成一个训练示例"><a href="#生成一个训练示例" class="headerlink" title="生成一个训练示例"></a>生成一个训练示例</h2><p>这里把样本分为了三种，背景音乐，正向的音频，反向的音频，合成训练示例：</p><ul><li>随机选择一个10秒的背景音频剪辑</li><li>随机将0-4个正向音频片段插入此10秒剪辑中</li><li>随机将0-2个反向音频片段插入此10秒剪辑中</li></ul><p>合成后类似这样：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69v62hj20jo08sac1.jpg" alt></p><p>定义一个随机插入片段起始和终点位置的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_time_segment</span><span class="params">(segment_ms)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    segment_ms -- the duration of the audio clip in ms ("ms" stands for "milliseconds")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    segment_time -- a tuple of (segment_start, segment_end) in ms</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    segment_start = np.random.randint(low=<span class="number">0</span>, high=<span class="number">10000</span>-segment_ms)   <span class="comment"># Make sure segment doesn't run past the 10sec background </span></span><br><span class="line">    segment_end = segment_start + segment_ms - <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (segment_start, segment_end)</span><br></pre></td></tr></table></figure><p>然后需要判断在别的片段插入的时候，有没有被占用:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: is_overlapping</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_overlapping</span><span class="params">(segment_time, previous_segments)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Checks if the time of a segment overlaps with the times of existing segments.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    segment_time -- a tuple of (segment_start, segment_end) for the new segment</span></span><br><span class="line"><span class="string">    previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    True if the time segment overlaps with any of the existing segments, False otherwise</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    segment_start, segment_end = segment_time</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 line)</span></span><br><span class="line">    <span class="comment"># Step 1: Initialize overlap as a "False" flag. (≈ 1 line)</span></span><br><span class="line">    overlap = <span class="keyword">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: loop over the previous_segments start and end times.</span></span><br><span class="line">    <span class="comment"># Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines)</span></span><br><span class="line">    <span class="keyword">for</span> previous_start, previous_end <span class="keyword">in</span> previous_segments:</span><br><span class="line">        <span class="keyword">if</span> segment_start &lt;= previous_end <span class="keyword">and</span> segment_end &gt;= previous_start:</span><br><span class="line">            overlap = <span class="keyword">True</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> overlap</span><br></pre></td></tr></table></figure><p>生成input音频片段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: insert_audio_clip</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_audio_clip</span><span class="params">(background, audio_clip, previous_segments)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Insert a new audio segment over the background noise at a random time step, ensuring that the </span></span><br><span class="line"><span class="string">    audio segment does not overlap with existing segments.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    background -- a 10 second background audio recording.  </span></span><br><span class="line"><span class="string">    audio_clip -- the audio clip to be inserted/overlaid. </span></span><br><span class="line"><span class="string">    previous_segments -- times where audio segments have already been placed</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    new_background -- the updated background audio</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the duration of the audio clip in ms</span></span><br><span class="line">    segment_ms = len(audio_clip)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    <span class="comment"># Step 1: Use one of the helper functions to pick a random time segment onto which to insert </span></span><br><span class="line">    <span class="comment"># the new audio clip. (≈ 1 line)</span></span><br><span class="line">    segment_time = get_random_time_segment(segment_ms)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep </span></span><br><span class="line">    <span class="comment"># picking new segment_time at random until it doesn't overlap. (≈ 2 lines)</span></span><br><span class="line">    <span class="keyword">while</span> is_overlapping(segment_time,previous_segments):</span><br><span class="line">        segment_time = get_random_time_segment(segment_ms)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line)</span></span><br><span class="line">    previous_segments.append(segment_time)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 4: Superpose audio segment and background</span></span><br><span class="line">    new_background = background.overlay(audio_clip, position = segment_time[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> new_background, segment_time</span><br></pre></td></tr></table></figure><p>生成y标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: insert_ones</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_ones</span><span class="params">(y, segment_end_ms)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment </span></span><br><span class="line"><span class="string">    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the</span></span><br><span class="line"><span class="string">    50 followinf labels should be ones.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    y -- numpy array of shape (1, Ty), the labels of the training example</span></span><br><span class="line"><span class="string">    segment_end_ms -- the end time of the segment in ms</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    y -- updated labels</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># duration of the background (in terms of spectrogram time-steps)</span></span><br><span class="line">    segment_end_y = int(segment_end_ms * Ty / <span class="number">10000.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add 1 to the correct index in the background label (y)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(segment_end_y+<span class="number">1</span>, segment_end_y+<span class="number">51</span>):</span><br><span class="line">        <span class="keyword">if</span> i &lt; Ty:</span><br><span class="line">            y[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: create_training_example</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_example</span><span class="params">(background, activates, negatives)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a training example with a given background, activates, and negatives.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    background -- a 10 second background audio recording</span></span><br><span class="line"><span class="string">    activates -- a list of audio segments of the word "activate"</span></span><br><span class="line"><span class="string">    negatives -- a list of audio segments of random words that are not "activate"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    x -- the spectrogram of the training example</span></span><br><span class="line"><span class="string">    y -- the label at each time step of the spectrogram</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the random seed</span></span><br><span class="line">    np.random.seed(<span class="number">18</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make background quieter</span></span><br><span class="line">    background = background - <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Initialize y (label vector) of zeros (≈ 1 line)</span></span><br><span class="line">    y = np.zeros((<span class="number">1</span>, Ty))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Initialize segment times as empty list (≈ 1 line)</span></span><br><span class="line">    previous_segments = []</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Select 0-4 random "activate" audio clips from the entire list of "activates" recordings</span></span><br><span class="line">    number_of_activates = np.random.randint(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">    random_indices = np.random.randint(len(activates), size=number_of_activates)</span><br><span class="line">    random_activates = [activates[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indices]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines)</span></span><br><span class="line">    <span class="comment"># Step 3: Loop over randomly selected "activate" clips and insert in background</span></span><br><span class="line">    <span class="keyword">for</span> random_activate <span class="keyword">in</span> random_activates:</span><br><span class="line">        <span class="comment"># Insert the audio clip on the background</span></span><br><span class="line">        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)</span><br><span class="line">        <span class="comment"># Retrieve segment_start and segment_end from segment_time</span></span><br><span class="line">        segment_start, segment_end = segment_time</span><br><span class="line">        <span class="comment"># Insert labels in "y"</span></span><br><span class="line">        y = insert_ones(y, segment_end)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select 0-2 random negatives audio recordings from the entire list of "negatives" recordings</span></span><br><span class="line">    number_of_negatives = np.random.randint(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">    random_indices = np.random.randint(len(negatives), size=number_of_negatives)</span><br><span class="line">    random_negatives = [negatives[i] <span class="keyword">for</span> i <span class="keyword">in</span> random_indices]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines)</span></span><br><span class="line">    <span class="comment"># Step 4: Loop over randomly selected negative clips and insert in background</span></span><br><span class="line">    <span class="keyword">for</span> random_negative <span class="keyword">in</span> random_negatives:</span><br><span class="line">        <span class="comment"># Insert the audio clip on the background </span></span><br><span class="line">        background, _ = insert_audio_clip(background, random_negative, previous_segments)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Standardize the volume of the audio clip </span></span><br><span class="line">    background = match_target_amplitude(background, <span class="number">-20.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Export new training example </span></span><br><span class="line">    file_handle = background.export(<span class="string">"train"</span> + <span class="string">".wav"</span>, format=<span class="string">"wav"</span>)</span><br><span class="line">    print(<span class="string">"File (train.wav) was saved in your directory."</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get and plot spectrogram of the new recording (background with superposition of positive and negatives)</span></span><br><span class="line">    x = graph_spectrogram(<span class="string">"train.wav"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure><p>实现model()</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm6amjp4j21hc1jkn36.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function creating the model's graph in Keras.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the model's input data (using Keras conventions)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- Keras model instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    X_input = Input(shape = input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: CONV layer (≈4 lines)</span></span><br><span class="line">    X = Conv1D(filters=<span class="number">196</span>,kernel_size=<span class="number">15</span>,strides=<span class="number">4</span>)(X_input)                                 <span class="comment"># CONV1D</span></span><br><span class="line">    X = BatchNormalization()(X)                                 <span class="comment"># Batch normalization</span></span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)                                 <span class="comment"># ReLu activation</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                 <span class="comment"># dropout (use 0.8)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: First GRU Layer (≈4 lines)</span></span><br><span class="line">    X = GRU(units = <span class="number">128</span>, return_sequences = <span class="keyword">True</span>)(X)                                 <span class="comment"># GRU (use 128 units and return the sequences)</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                  <span class="comment"># dropout (use 0.8)</span></span><br><span class="line">    X = BatchNormalization()(X)                                 <span class="comment"># Batch normalization</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Second GRU Layer (≈4 lines)</span></span><br><span class="line">    X = GRU(units = <span class="number">128</span>, return_sequences = <span class="keyword">True</span>)(X)                                 <span class="comment"># GRU (use 128 units and return the sequences)</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                  <span class="comment"># dropout (use 0.8)</span></span><br><span class="line">    X = BatchNormalization()(X)                                 <span class="comment"># Batch normalization</span></span><br><span class="line">    X = Dropout(<span class="number">0.8</span>)(X)                                 <span class="comment"># dropout (use 0.8)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 4: Time-distributed dense layer (≈1 line)</span></span><br><span class="line">    X = TimeDistributed(Dense(<span class="number">1</span>, activation = <span class="string">"sigmoid"</span>))(X) <span class="comment"># time distributed  (sigmoid)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    model = Model(inputs = X_input, outputs = X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>这里载入预训练好的模型，不需要自己训练那么久了，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = load_model(<span class="string">'./models/tr_model.h5'</span>)</span><br><span class="line">opt = Adam(lr=<span class="number">0.0001</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, decay=<span class="number">0.01</span>)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=opt, metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line">model.fit(X, Y, batch_size = <span class="number">5</span>, epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;这周作业分为了两部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;机器翻译&lt;/li&gt;
&lt;li&gt;触发关键字&lt;/li&gt;
      
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(5-3) -- 序列模型和注意力机制</title>
    <link href="http://fangzh.top/2018/dl-ai-5-3/"/>
    <id>http://fangzh.top/2018/dl-ai-5-3/</id>
    <published>2018-10-18T10:39:10.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><h1 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h1><p><strong>sequence to sequence 模型：</strong></p><p>sequence to sequence 模型最为常见的就是机器翻译，假如这里我们要将法语翻译成英文。</p><p>对于机器翻译的序列对序列模型，如果我们拥有大量的句子语料，则可以得到一个很有效的机器翻译模型。模型的前部分使用一个编码网络来对输入的法语句子进行编码，后半部分则使用一个解码网络来生成对应的英文翻译。网络结构如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69m7lkj20ky0aggm7.jpg" alt></p><p>还有输入图像，输出描述图片的句子的：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69wi4xj20n00br0up.jpg" alt></p><h1 id="挑选最可能的句子"><a href="#挑选最可能的句子" class="headerlink" title="挑选最可能的句子"></a>挑选最可能的句子</h1><p>机器翻译：条件语言模型</p><p>对于机器翻译来说和之前几节介绍的语言模型有很大的相似性但也有不同之处。</p><p>在语言模型中，我们通过估计句子的可能性，来生成新的句子。语言模型总是以零向量开始，也就是其第一个时间步的输入可以直接为零向量；</p><p>在机器翻译中，包含了编码网络和解码网络，其中解码网络的结构与语言模型的结构是相似的。机器翻译以句子中每个单词的一系列向量作为输入，所以相比语言模型来说，机器翻译可以称作条件语言模型，其输出的句子概率是相对于输入的条件概率。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69mfoyj20mu0addgm.jpg" alt></p><h1 id="集束搜索（Beam-search）"><a href="#集束搜索（Beam-search）" class="headerlink" title="集束搜索（Beam search）"></a>集束搜索（Beam search）</h1><p>Beam search 算法：</p><p>这里我们还是以法语翻译成英语的机器翻译为例：</p><ul><li><p>Step 1：对于我们的词汇表，我们将法语句子输入到编码网络中得到句子的编码，通过一个softmax层计算各个单词（词汇表中的所有单词）输出的概率值，通过设置集束宽度（beam width）的大小如3，我们则取前3个最大输出概率的单词，并保存起来。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69luhlj20mi0bvq3c.jpg" alt></p></li><li><p>Step 2：在第一步中得到的集束宽度的单词数，我们分别对第一步得到的每一个单词计算其与单词表中的所有单词组成词对的概率。并与第一步的概率相乘，得到第一和第二两个词对的概率。有3×10000个选择，（这里假设词汇表有10000个单词），最后再通过beam width大小选择前3个概率最大的输出对；</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69x7nhj20mv0cpac9.jpg" alt></p><ul><li>Step 3~Step T：与Step2的过程是相似的，直到遇到句尾符号结束。</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69pggbj20mo0cm759.jpg" alt></p><h1 id="集束搜索的改进"><a href="#集束搜索的改进" class="headerlink" title="集束搜索的改进"></a>集束搜索的改进</h1><p>上面的集束搜索有个问题，就是因为每一项的概率都很小，所以句子越长，概率越小，因此会倾向于选择比较短的句子，这样是不太好的。</p><p>首先，为了保证不会太小而导致数值下溢，先取对数，把连乘变成求和。</p><p>然后在前面加上一个系数</p><p>$$\frac{1}{T_{y}^{\alpha}}$$</p><p>当$\alpha$ 为 1 时，就表示概率为句子长度的平均；为0时，就表示没有系数；在这里一般取$\alpha = 0.7$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69ohpnj20lp0camym.jpg" alt></p><p>集束搜索讨论：</p><p>Beam width：B的选择，B越大考虑的情况越多，但是所需要进行的计算量也就相应的越大。在常见的产品系统中，一般设置B = 10，而更大的值（如100，1000，…）则需要对应用的领域和场景进行选择。</p><p>相比于算法范畴中的搜索算法像BFS或者DFS这些精确的搜索算法，Beam Search 算法运行的速度很快，但是不能保证找到目标准确的最大值。</p><h1 id="集束搜索的误差分析"><a href="#集束搜索的误差分析" class="headerlink" title="集束搜索的误差分析"></a>集束搜索的误差分析</h1><p>集束搜索算法是一种近似搜索算法，也被称为启发式搜索算法。而不是一种精确的搜索。</p><p>如果我们的集束搜素算法出现错误了要怎么办呢？如何确定是算法出现了错误还是模型出现了错误呢？此时集束搜索算法的误差分析就显示出了作用。</p><p>模型分为两个部分：</p><ul><li>RNN 部分：编码网络 + 解码网络</li><li>Beam Search 部分：选取最大的几个值</li></ul><h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>计算人类翻译的概率P(y∗|x)以及模型翻译的概率P(ŷ |x)</p><ul><li><p>P(y∗|x) &gt; P(ŷ |x)：Beam search算法选择了ŷ ，但是y∗ 却得到了更高的概率，所以Beam search 算法出错了；</p></li><li><p>P(y∗|x) &lt;= P(ŷ |x) 的情况：翻译结果y∗相比ŷ 要更好，但是RNN模型却预测P(y∗|x)</p></li></ul><h1 id="Bleu-得分（选修）"><a href="#Bleu-得分（选修）" class="headerlink" title="Bleu 得分（选修）"></a>Bleu 得分（选修）</h1><p>PASS</p><h1 id="注意力模型直观理解"><a href="#注意力模型直观理解" class="headerlink" title="注意力模型直观理解"></a>注意力模型直观理解</h1><p>之前我们的翻译模型分为编码网络和解码网络，先记忆整个句子再翻译，这对于较短的句子效果不错，但是对于很长的句子，翻译结果就会变差。</p><p>回想当我们人类翻译长句子时，都是一部分一部分的翻译，翻译每个部分的时候也会顾及到该部分周围上下文对其的影响。同理，引入注意力机制，一部分一部分的翻译，每次翻译时给该部分及上下文不同的注意力权重以及已经译出的部分，直至翻译出整个句子。</p><h1 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h1><p>以一个双向的RNN模型来对法语进行翻译，得到相应的英语句子。其中的每个RNN单元均是LSTM或者GRU单元。</p><p>对于双向RNN，通过前向和后向的传播，可以得到每个时间步的前向激活值和反向激活值，我们用一个符号来表示前向和反向激活值的组合。 </p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69oymvj205l01i742.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69ohpnj20lp0camym.jpg" alt></p><p>然后得到每个输入单词的注意力权重：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69pircj20qx01w0sr.jpg" alt></p><p>计算公式为：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69rhu3j206q02sdfn.jpg" alt></p><p>这里的$e^{&lt;t,t^{\prime}>}$则是通过一层神经网络来进行计算得到的，其值取决于输出RNN中前一步的激活值$s^{&lt;t-1>}$和输入RNN当前步的激活值$a^{&lt;t^{\prime}>}$。我们可以通过训练这个小的神经网络模型，使用反向传播算法来学习一个对应的关系函数。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69rsmuj20lr0b23zg.jpg" alt></p><h1 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h1><p>语音识别就是将一段音频转化为相应文本。</p><p>之前用音位来识别，现在 end-to-end 模型中已经不需要音位了，但是需要大量的数据常见的语音数据大小为300h、3000h或者更大。</p><h4 id="注意力模型的语音识别"><a href="#注意力模型的语音识别" class="headerlink" title="注意力模型的语音识别"></a>注意力模型的语音识别</h4><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69x4ttj20l40b774l.jpg" alt></p><h4 id="CTC-损失函数的语音识别"><a href="#CTC-损失函数的语音识别" class="headerlink" title="CTC 损失函数的语音识别"></a>CTC 损失函数的语音识别</h4><p>另外一种效果较好的就是使用CTC损失函数的语音识别模型（CTC，Connectionist temporal classification）</p><p>模型会有很多个输入和输出，对于一个10s的语音片段，我们就能够得到1000个特征的输入片段，而往往我们的输出仅仅是几个单词。</p><p>在CTC损失函数中，允许RNN模型输出有重复的字符和插入空白符的方式，强制使得我们的输出和输入的大小保持一致。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69s111j20kn0bdjs1.jpg" alt></p><h1 id="触发字检测"><a href="#触发字检测" class="headerlink" title="触发字检测"></a>触发字检测</h1><p>触发字检测：关键词语音唤醒。</p><p>一种可以简单应用的触发字检测算法，就是使用RNN模型，将音频信号进行声谱图转化得到图像特征或者使用音频特征，输入到RNN中作为我们的输入。而输出的标签，我们可以以触发字前的输出都标记为0，触发字后的输出则标记为1。</p><p>一种简单应用的触发字检测算法，就是使用RNN模型，将音频信号进行声谱图转化音频特征，输入到RNN中作为我们的输入。而输出的标签，非触发字的输出都标记为0，触发字的输出则标记为1。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcm69uuhqj20lr0bfdh5.jpg" alt></p><p>上面方法的缺点就是0、1标签的不均衡，0比1多很多。一种简单粗暴的方法就是在触发字及其之后多个目标标签都标记为1，在一定程度上可以提高系统的精确度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;h1 id=&quot;基础模型&quot;&gt;&lt;a href=&quot;#基础模型&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-2) -- 自然语言处理与词嵌入(NLP and Word Embeddings)</title>
    <link href="http://fangzh.top/2018/dl-ai-5-2h/"/>
    <id>http://fangzh.top/2018/dl-ai-5-2h/</id>
    <published>2018-10-18T09:00:21.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>本周作业分为两部分：</p><ul><li>词向量运算</li><li>emoji表情包</li></ul><a id="more"></a><h1 id="Part1-词向量运算"><a href="#Part1-词向量运算" class="headerlink" title="Part1:词向量运算"></a>Part1:词向量运算</h1><p>由于词嵌入的训练计算量庞大切耗费时间长，绝大部分机器学习人员都会导入一个预训练的词嵌入模型。</p><p>本作业中，我们使用50维的 Glove 向量来表示词。导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words, word_to_vec_map = read_glove_vecs(<span class="string">'data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure><ul><li>words: 词典中的词集合</li><li>word_to_vec_map: 表示单词到向量映射的map。</li></ul><p>one-hot向量不擅长表示向量相似度(内积为0), Glove 向量包含了单词更多的信息，下面看看如何使用 Glove 向量计算相似度。</p><p>$$\text{CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta)$$</p><p>分子表示两个向量的内积，分母是向量的模的乘积，θθ表示向量夹角，向量越近夹角越小，cos 值越大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: cosine_similarity</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similarity</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Cosine similarity reflects the degree of similariy between u and v</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        u -- a word vector of shape (n,)          </span></span><br><span class="line"><span class="string">        v -- a word vector of shape (n,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cosine_similarity -- the cosine similarity between u and v defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    distance = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Compute the dot product between u and v (≈1 line)</span></span><br><span class="line">    dot = np.dot(u,v)</span><br><span class="line">    <span class="comment"># Compute the L2 norm of u (≈1 line)</span></span><br><span class="line">    norm_u = np.sqrt(np.dot(u,u))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the L2 norm of v (≈1 line)</span></span><br><span class="line">    norm_v = np.sqrt(np.dot(v,v))</span><br><span class="line">    <span class="comment"># Compute the cosine similarity defined by formula (1) (≈1 line)</span></span><br><span class="line">    cosine_similarity = dot / (norm_u * norm_v)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure><h2 id="单词类比推理"><a href="#单词类比推理" class="headerlink" title="单词类比推理"></a>单词类比推理</h2><p>类比推理任务中需要实现”a is to b as c is to __” 比如”man is to woman as king is to queen”。我们需要找到单词 d,使得”e_b−e_a ≈ e_d−e_c”<br>也就是两组的差向量应该相似(仍然用 cos 来衡量)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: complete_analogy</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_analogy</span><span class="params">(word_a, word_b, word_c, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the word analogy task as explained above: a is to b as c is to ____. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_a -- a word, string</span></span><br><span class="line"><span class="string">    word_b -- a word, string</span></span><br><span class="line"><span class="string">    word_c -- a word, string</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary that maps words to their corresponding vectors. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert words to lower case</span></span><br><span class="line">    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Get the word embeddings v_a, v_b and v_c (≈1-3 lines)</span></span><br><span class="line">    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    words = word_to_vec_map.keys()</span><br><span class="line">    max_cosine_sim = <span class="number">-100</span>              <span class="comment"># Initialize max_cosine_sim to a large negative number</span></span><br><span class="line">    best_word = <span class="keyword">None</span>                   <span class="comment"># Initialize best_word with None, it will help keep track of the word to output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over the whole word vector set</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:        </span><br><span class="line">        <span class="comment"># to avoid best_word being one of the input words, pass on them.</span></span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> [word_a, word_b, word_c] :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="comment"># Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)</span></span><br><span class="line">        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the cosine_sim is more than the max_cosine_sim seen so far,</span></span><br><span class="line">            <span class="comment"># then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> cosine_sim &gt; max_cosine_sim:</span><br><span class="line">            max_cosine_sim = cosine_sim</span><br><span class="line">            best_word = w</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> best_word</span><br></pre></td></tr></table></figure><h2 id="消除词向量偏见-可选"><a href="#消除词向量偏见-可选" class="headerlink" title="消除词向量偏见 (可选)"></a>消除词向量偏见 (可选)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neutralize</span><span class="params">(word, g, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Removes the bias of "word" by projecting it on the space orthogonal to the bias axis. </span></span><br><span class="line"><span class="string">    This function ensures that gender neutral words are zero in the gender subspace.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        word -- string indicating the word to debias</span></span><br><span class="line"><span class="string">        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)</span></span><br><span class="line"><span class="string">        word_to_vec_map -- dictionary mapping words to their corresponding vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        e_debiased -- neutralized word vector representation of the input "word"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line)</span></span><br><span class="line">    e = word_to_vec_map[word]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute e_biascomponent using the formula give above. (≈ 1 line)</span></span><br><span class="line">    e_biascomponent = np.dot(e, g) / np.square(np.linalg.norm(g)) * g</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Neutralize e by substracting e_biascomponent from it </span></span><br><span class="line">    <span class="comment"># e_debiased should be equal to its orthogonal projection. (≈ 1 line)</span></span><br><span class="line">    e_debiased = e - e_biascomponent</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e_debiased</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equalize</span><span class="params">(pair, bias_axis, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Debias gender specific words by following the equalize method described in the figure above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor") </span></span><br><span class="line"><span class="string">    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their corresponding vectors</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    e_1 -- word vector corresponding to the first word</span></span><br><span class="line"><span class="string">    e_2 -- word vector corresponding to the second word</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines)</span></span><br><span class="line">    w1, w2 = pair</span><br><span class="line">    e_w1, e_w2 = word_to_vec_map[w1, w2]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)</span></span><br><span class="line">    mu = (e_w1 + e_w2) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)</span></span><br><span class="line">    mu_B = np.dot(mu, bias_axis) / np.square(np.linalg.norm(bias_axis)) * bias_axis</span><br><span class="line">    mu_orth = mu - mu_B</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)</span></span><br><span class="line">    e_w1B = np.dot(e_w1, bias_axis) / np.square(np.linalg.norm(bias_axis)) * bias_axis</span><br><span class="line">    e_w2B = np.dot(e_w2, bias_axis) / np.square(np.linalg.norm(bias_axis)) * bias_axis</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)</span></span><br><span class="line">    corrected_e_w1B = np.sqrt(np.abs(<span class="number">1</span>-np.sum(mu_orth**<span class="number">2</span>))) * (e_w1B - mu_B)/np.linalg.norm(e_w1-mu_orth-mu_B)</span><br><span class="line">    corrected_e_w2B = np.sqrt(np.abs(<span class="number">1</span>-np.sum(mu_orth**<span class="number">2</span>))) * (e_w2B - mu_B)/np.linalg.norm(e_w2-mu_orth-mu_B)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)</span></span><br><span class="line">    e1 = corrected_e_w1B + mu_orth</span><br><span class="line">    e2 = corrected_e_w2B + mu_orth</span><br><span class="line">                                                                </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e1, e2</span><br></pre></td></tr></table></figure><h1 id="Part2-Emojify"><a href="#Part2-Emojify" class="headerlink" title="Part2:Emojify!"></a>Part2:Emojify!</h1><p>你有没有想过让你的短信更具表现力？ emojifier APP将帮助你做到这一点。 所以不是写下”Congratulations on the promotion! Lets get coffee and talk. Love you!” emojifier可以自动转换为 “Congratulations on the promotion! ? Lets get coffee and talk. ☕️ Love you! ❤️”</p><p>另外，如果你对emojis不感兴趣，但有朋友向你发送了使用太多表情符号的疯狂短信，你还可以使用emojifier来回复他们。</p><p>你将实现一个模型，输入一个句子（“Let’s go see the baseball game tonight!”），并找到最适合这个句子的表情符号（⚾️）。 在许多表情符号界面中，您需要记住❤️是”heart”符号而不是”love”符号。 但是使用单词向量，你会发现即使你的训练集只将几个单词明确地与特定的表情符号相关联，你的算法也能够将测试集中相关的单词概括并关联到相同的表情符号上，即使这些词没有出现在训练集中。这使得即使使用小型训练集，你也可以建立从句子到表情符号的精确分类器映射。</p><p>在本练习中，您将从使用词嵌入的基本模型（Emojifier-V1）开始，然后构建进一步整合LSTM的更复杂的模型（Emojifier-V2）。</p><h2 id="先用average试试"><a href="#先用average试试" class="headerlink" title="先用average试试"></a>先用average试试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sentence_to_avg</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_to_avg</span><span class="params">(sentence, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word</span></span><br><span class="line"><span class="string">    and averages its value into a single vector encoding the meaning of the sentence.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    sentence -- string, one training example from X</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Split sentence into list of lower case words (≈ 1 line)</span></span><br><span class="line">    words = sentence.lower().split()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the average word vector, should have the same shape as your word vectors.</span></span><br><span class="line">    avg = np.zeros(word_to_vec_map[words[<span class="number">0</span>]].shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: average the word vectors. You can loop over the words in the list "words".</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        avg += word_to_vec_map[w]</span><br><span class="line">    avg = avg / len(words)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure><h2 id="再用RNN"><a href="#再用RNN" class="headerlink" title="再用RNN"></a>再用RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, word_to_vec_map, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">400</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Model to train word vector representations in numpy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, numpy array of sentences as strings, of shape (m, 1)</span></span><br><span class="line"><span class="string">    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    learning_rate -- learning_rate for the stochastic gradient descent algorithm</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    pred -- vector of predictions, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    W -- weight matrix of the softmax layer, of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">    b -- bias of the softmax layer, of shape (n_y,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define number of training examples</span></span><br><span class="line">    m = Y.shape[<span class="number">0</span>]                          <span class="comment"># number of training examples</span></span><br><span class="line">    n_y = <span class="number">5</span>                                 <span class="comment"># number of classes  </span></span><br><span class="line">    n_h = <span class="number">50</span>                                <span class="comment"># dimensions of the GloVe vectors </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters using Xavier initialization</span></span><br><span class="line">    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)</span><br><span class="line">    b = np.zeros((n_y,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert Y to Y_onehot with n_y classes</span></span><br><span class="line">    Y_oh = convert_to_one_hot(Y, C = n_y) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_iterations):                       <span class="comment"># Loop over the number of iterations</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                                <span class="comment"># Loop over the training examples</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">            <span class="comment"># Average the word vectors of the words from the i'th training example</span></span><br><span class="line">            avg = sentence_to_avg(X[i], word_to_vec_map)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagate the avg through the softmax layer</span></span><br><span class="line">            z = np.dot(W, avg) + b</span><br><span class="line">            a = softmax(z)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax)</span></span><br><span class="line">            cost = -np.sum(Y_oh[i] * np.log(a))</span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute gradients </span></span><br><span class="line">            dz = a - Y_oh[i]</span><br><span class="line">            dW = np.dot(dz.reshape(n_y,<span class="number">1</span>), avg.reshape(<span class="number">1</span>, n_h))</span><br><span class="line">            db = dz</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters with Stochastic Gradient Descent</span></span><br><span class="line">            W = W - learning_rate * dW</span><br><span class="line">            b = b - learning_rate * db</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch: "</span> + str(t) + <span class="string">" --- cost = "</span> + str(cost))</span><br><span class="line">            pred = predict(X, Y, W, b, word_to_vec_map)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred, W, b</span><br></pre></td></tr></table></figure><h2 id="Emojifier-V2-Using-LSTMs-in-Keras"><a href="#Emojifier-V2-Using-LSTMs-in-Keras" class="headerlink" title="Emojifier-V2: Using LSTMs in Keras:"></a>Emojifier-V2: Using LSTMs in Keras:</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sentences_to_indices</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)</span></span><br><span class="line">    X_indices = np.zeros((m, max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words =X[i].lower().split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] = word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: pretrained_embedding_layer</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    vocab_len = len(word_to_index) + <span class="number">1</span>                  <span class="comment"># adding 1 to fit Keras embedding (requirement)</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]      <span class="comment"># define dimensionality of your GloVe word vectors (= 50)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. </span></span><br><span class="line">    embedding_layer = Embedding(vocab_len,emb_dim, trainable=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure><h1 id="Building-the-Emojifier-V2"><a href="#Building-the-Emojifier-V2" class="headerlink" title="Building the Emojifier-V2"></a>Building the Emojifier-V2</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: Emojify_V2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Emojify_V2</span><span class="params">(input_shape, word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    sentence_indices = Input(shape= input_shape, dtype=<span class="string">'int32'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    embeddings = embedding_layer(sentence_indices)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line">    X = LSTM(<span class="number">128</span>, return_sequences=<span class="keyword">True</span>)(embeddings)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    X = LSTM(<span class="number">128</span>, return_sequences=<span class="keyword">False</span>)(X)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    X =  Dense(<span class="number">5</span>, activation=<span class="string">'softmax'</span>)(X)</span><br><span class="line">    <span class="comment"># Add a softmax activation</span></span><br><span class="line">    X = Activation(<span class="string">'softmax'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    model = Model(inputs=sentence_indices ,outputs=X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本周作业分为两部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;词向量运算&lt;/li&gt;
&lt;li&gt;emoji表情包&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(5-2) -- 自然语言处理与词嵌入(NLP and Word Embeddings)</title>
    <link href="http://fangzh.top/2018/dl-ai-5-2/"/>
    <id>http://fangzh.top/2018/dl-ai-5-2/</id>
    <published>2018-10-18T09:00:17.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>本周主要讲了NLP和词嵌入的问题。</p><a id="more"></a><h1 id="词汇表征"><a href="#词汇表征" class="headerlink" title="词汇表征"></a>词汇表征</h1><p>在前面学习的内容中，我们表征词汇是直接使用英文单词来进行表征的，但是对于计算机来说，是无法直接认识单词的。为了让计算机能够能更好地理解我们的语言，建立更好的语言模型，我们需要将词汇进行表征。下面是几种不同的词汇表征方式：</p><p><strong>one-hot 表征：</strong></p><p>在前面的一节课程中，已经使用过了one-hot表征的方式对模型字典中的单词进行表征，对应单词的位置用1表示，其余位置用0表示，如下图所示： </p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv3hd7j20n30cuaaw.jpg" alt></p><p>one-hot表征的缺点：这种方法将每个词孤立起来，使得模型对相关词的泛化能力不强。每个词向量之间的距离都一样，乘积均为0，所以无法获取词与词之间的相似性和关联性。</p><p><strong>特征表征：词嵌入</strong></p><p>用不同的特征来对各个词汇进行表征，相对与不同的特征，不同的单词均有不同的值。如下例所示：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv3tlfj20nc0crwfj.jpg" alt></p><p>这样差不多的词汇就会聚在一起：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv3trej20mf0bl3z7.jpg" alt></p><h1 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h1><p>Word Embeddings对不同单词进行了实现了特征化的表示，那么如何将这种表示方法应用到自然语言处理的应用中呢？</p><p>以下图为例，该图表示的是输入一段话，判断出人名。通过学习判断可以知道<strong>orange farmer</strong>指的应该是人，所以其对应的主语<strong>Sally Johnson</strong>就应该是人名了，所以其对应位置输出为1。</p><p>那如果把<strong>orange</strong>换成<strong>apple</strong>呢？通过词嵌入算法可以知道二者词性类似，而且后面跟着<strong>farmer</strong>，所以也能确认<strong>Robert Lin</strong>是人名。</p><p>我们继续替换，我们将<strong>apple farmer</strong>替换成不太常见的<strong>durian cultivator(榴莲繁殖员)</strong>。此时词嵌入中可能并没有<strong>durian</strong>这个词，<strong>cultivator</strong>也是不常用的词汇。这个时候怎么办呢？我们可以用到迁移学习。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv40grj20n40cqt9g.jpg" alt></p><ol><li><p>学习含有大量文本语料库的词嵌入(一般含有10亿到1000亿单词)，或者下载预训练好的词嵌入</p></li><li><p>将学到的词嵌入迁移到相对较小规模的训练集(例如10万词汇)，这个时候就能体现出相比于使&gt; 用one hot表示法，使用词嵌入的优势了。如果是使用one hot，那么每个单词是1×100000表&gt; 示，而用词嵌入后，假设特征维度是300，那么只需要使用 1×300的向量表示即可。</p></li><li><p>(可选) 这一步骤就是对新的数据进行fine-tune。</p></li></ol><p>词嵌入和人脸编码之间有很奇妙的联系。在人脸识别领域，我们会将人脸图片预编码成不同的编码向量，以表示不同的人脸，进而在识别的过程中使用编码来进行比对识别。词嵌入则和人脸编码有一定的相似性。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv4sacj20mv0aq75r.jpg" alt></p><p>但是不同的是，对于人脸识别，我们可以将任意一个<strong>没有见过的人脸照片</strong>输入到我们构建的网络中，则可输出一个对应的人脸编码。而在词嵌入模型中，所有词汇的编码是在一个<strong>固定的词汇表</strong>中进行学习单词的编码以及其之间的关系的。</p><h1 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h1><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv4zr5j20mz0cvt9u.jpg" alt></p><p>可以得到 man to woman ，正如 King to Queen。</p><p>可以通过词嵌入，计算词之间的距离，从而实现类比。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv6rr1j20mt0crjs7.jpg" alt></p><p>关于词相似度的计算，可以使用余弦公式。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv6r3rj209i0380sl.jpg" alt></p><p>当然也可以使用距离公式：</p><p>$$||u - v||^2$$</p><h1 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h1><p>如下图示，左边是词嵌入矩阵，每一列表示该单词的特征向量，每一行表示所有单词在某一特征上的值的大小，这个矩阵用$E$表示，假设其维度是<strong>(300,10000)</strong>。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv88frj20n10b0dgw.jpg" alt></p><p>在原来的one-hot中每个词是维度为10000的向量，而现在在嵌入矩阵中，每个词变成了维度为300的向量。</p><h1 id="学习词嵌入"><a href="#学习词嵌入" class="headerlink" title="学习词嵌入"></a>学习词嵌入</h1><p>下图展示了预测单词的方法，即给出缺少一个单词的句子：</p><p>“<strong>I want a glass of orange ___</strong>”</p><p>计算方法是将已知单词的特征向量都作为输入数据送到神经网络中去，然后经过一系列计算到达 Softmax分类层，在该例中输出节点数为10000个。经过计算<strong>juice</strong>概率最高，所以预测为</p><p>“<strong>I want a glass of orange juice</strong>”</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv86vmj20n30cpgmv.jpg" alt></p><p>在这个训练模式中，是通过全部的单词去预测最后一个单词然后反向传播更新词嵌表E</p><p>假设要预测的单词为W，词嵌表仍然为E，需要注意的是训练词嵌表和预测W是两个不同的任务。</p><p>如果任务是预测W，最佳方案是使用W前面n个单词构建语境。</p><p>如果任务是训练E，除了使用W前全部单词还可以通过：前后各4个单词、前面单独的一个词、前面语境中随机的一个词（这个方式也叫做 <strong>Skip Gram</strong> 算法），这些方法都能提供很好的结果。</p><h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>“<strong>word2vec</strong>” 是指将词语word 变成向量vector 的过程，这一过程通常通过浅层的神经网络完成，例如<strong>CBOW</strong>或者<strong>skip gram</strong>，这一过程同样可以视为构建词嵌表E的过程”。</p><h2 id="Skip-grams"><a href="#Skip-grams" class="headerlink" title="Skip-grams"></a><strong>Skip-grams</strong></h2><p>下图详细的展示了<strong>Skip-grams</strong>。即先假设<strong>Context(上下文)</strong>是<strong>orange</strong>，而<strong>Target(预测词)</strong>则是通过设置窗口值得到的，例如设置为紧邻的后一个单词，此时<strong>Target</strong>则为<strong>juice</strong>，设置其他窗口值可以得到其他预测词。</p><p>注意这个过程是用来构建<strong>词嵌表</strong>的，而不是为了真正的去预测，所以如果预测效果不好并不用担心。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv8qmmj20mr0bq74j.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv9e21j20mx0cljs7.jpg" alt></p><p>上面在使用Softmax的时候有一个很明显的问题，那就是计算量过于繁琐，所以为了解决计算量大的问题，提出了如下图所示的方法，即<strong>Hierachical Softmax(分层的Softmax)</strong></p><p>简单的来说就是通过使用二叉树的形式来减少运算量。</p><p>例如一些常见的单词，如<strong>the</strong>、<strong>of</strong>等就可以在很浅的层次得到，而像<strong>durian</strong>这种少用的单词则在较深的层次得到。</p><h1 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h1><p>对于skip gram model而言，还要解决的一个问题是如何取样（选择）有效的随机词 c 和目标词 t 呢？如果真的按照自然随机分布的方式去选择，可能会大量重复的选择到出现次数频率很高的单词比如说“the, of, a, it, I, …” 重复的训练这样的单词没有特别大的意义。</p><p>如何有效的去训练选定的词如 orange 呢？在设置训练集时可以通过“<strong>负取样</strong>”的方法, 下表中第一行是通过和上面一<br>样的窗口法得到的“正”（1）结果，其他三行是从字典中随机得到的词语，结果为“负”（0）。通过这样的负取样法<br>可以更有效地去训练<strong>skip gram model</strong>.</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv964kj208a082dfz.jpg" alt></p><p>负取样的个数<strong>k</strong>由数据量的大小而定，上述例子中为4. 实际中数据量大则 <strong>k = 2 ~ 5</strong>，数据量小则可以相对大一些<strong>k = 5 ~ 20</strong></p><p>通过负取样，我们的神经网络训练从softmax预测每个词出现的频率变成了<strong>经典binary logistic regression</strong>问题，概率公式用 <strong>sigmoid</strong> 代替 <strong>softmax</strong>从而大大提高了速度。</p><p>选词概率的经验公式：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuv9hyfj208p0400sl.jpg" alt></p><h1 id="GloVe词向量"><a href="#GloVe词向量" class="headerlink" title="GloVe词向量"></a>GloVe词向量</h1><p><strong>GloVe(Global vectors for word representation)</strong>虽然不想<strong>Word2Vec</strong>模型那样流行，但是它也有自身的优点，即简单。</p><p>这里就不介绍了，看不太懂。</p><h1 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h1><p>情感分类就是通过一段文本来判断这个文本中的内容是否喜欢其所讨论的内容，这是NLP中最重要的模块之一。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuvb16tj20n00craap.jpg" alt></p><p>可以看到下图中的模型先将评语中各个单词通过 <strong>词嵌表(数据量一般比较大，例如有100Billion的单词数)</strong> 转化成对应的特征向量，然后对所有的单词向量<strong>做求和</strong>或者<strong>做平均</strong>，然后构建Softmax分类器，最后输出星级评级。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuvedrsj20mv0cmaat.jpg" alt></p><p>但是上面的模型存在一个问题，一般而言如果评语中有像”<strong>good、excellent</strong>“这样的单词，一般都是星级评分较高的评语，但是该模型对下面这句评语就显得无能为力了：</p><p>“<strong>Completely lacking in good taste, good service, and good ambience.</strong>”</p><p>之所以上面的模型存在那样的缺点，就是因为它没有把单词的时序考虑进去，所以我们可以使用RNN构建模型来解决这种问题。</p><p>另外使用RNN模型还有另一个好处，假设测试集中的评语是这样的</p><p>“<strong>Completely absent of good taste, good service, and good ambience.</strong>”</p><p>该评语只是将<strong>lacking in</strong>替换成了<strong>absent of</strong>，而且我们即使假设<strong>absent</strong>并没有出现在训练集中，但是因为词嵌表很庞大，所以词嵌表中包含<strong>absent</strong>，所以算法依旧可以知道<strong>absent</strong>和<strong>lacking</strong>有相似之处，最后输出的结果也依然可以保持正确。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuvbtcqj20n20d4jrv.jpg" alt></p><h1 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a><strong>词嵌入除偏</strong></h1><p>现如今机器学习已经被用到了很多领域，例如银行贷款决策，简历筛选。但是因为机器是向人们学习，所以好的坏的都会学到，例如他也会学到一些偏见或者歧视。</p><p>如下图示</p><p>当说到<strong>Man：程序员</strong>的时候，算法得出<strong>Woman：家庭主妇</strong>，这显然存在偏见。</p><p>又如<strong>Man：Doctor</strong>，算法认为<strong>Woman：Nurse</strong>。这显然也存在其实和偏见。</p><p>上面提到的例子都是性别上的歧视，词嵌入也会反映出年龄歧视、性取向歧视以及种族歧视等等。</p><p>人类在这方面已经做的不对了，所以机器应当做出相应的调整来减少歧视。</p><p><strong>消除偏见的方法：</strong></p><ul><li>定义偏见的方向：如性别 <ul><li>对大量性别相对的词汇进行相减并求平均：$e_{he}−e_{she}、e_{male}−e_{female}$⋯；</li><li>通过平均后的向量，则可以得到一个或多个偏见趋势相关的维度，以及大量不相关的维度；</li></ul></li><li><p>中和化：对每一个定义不明确的词汇，进行偏见的处理，如像doctor、babysitter这类词；通过减小这些词汇在得到的偏见趋势维度上值的大小；</p></li><li><p>均衡：将如gradmother和gradfather这种对称词对调整至babysitter这类词汇平衡的位置上，使babysitter这类词汇处于一个中立的位置，进而消除偏见。</p></li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcjuvp5goj20ts0fndmf.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本周主要讲了NLP和词嵌入的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-1)-- 循环神经网络（Recurrent Neural Networks）（3）</title>
    <link href="http://fangzh.top/2018/dl-ai-5-1h3/"/>
    <id>http://fangzh.top/2018/dl-ai-5-1h3/</id>
    <published>2018-10-18T08:20:36.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>第三个作业是用LSTM来生成爵士乐。</p><a id="more"></a><h1 id="Part3-Improvise-a-Jazz-Solo-with-an-LSTM-Network"><a href="#Part3-Improvise-a-Jazz-Solo-with-an-LSTM-Network" class="headerlink" title="Part3:Improvise a Jazz Solo with an LSTM Network"></a>Part3:Improvise a Jazz Solo with an LSTM Network</h1><p>我们已经对音乐数据做了预处理，以”values”来表示。可以非正式地将每个”value”看作一个音符，它包含音高和持续时间。 例如，如果您按下特定钢琴键0.5秒，那么您刚刚弹奏了一个音符。 在音乐理论中，”value” 实际上比这更复杂。 特别是，它还捕获了同时播放多个音符所需的信息。 例如，在播放音乐作品时，可以同时按下两个钢琴键（同时播放多个音符生成所谓的“和弦”）。 但是这里我们不需要关系音乐理论的细节。对于这个作业，你需要知道的是，我们获得一个”values”的数据集，并将学习一个RNN模型来生成一个序列的”values”。</p><p>我们的音乐生成系统将使用78个独特的值。</p><ul><li>X: 这是一个（m，Tx，78）维数组。 m 表示样本数量，Tx 表示时间步(也即序列的长度)，在每个时间步，输入是78个不同的可能值之一，表示为一个one-hot向量。 因此，例如，X [i，t，：]是表示第i个示例在时间t的值的one-hot向量。</li><li>Y: 与X基本相同，但向左（向前）移动了一步。 与恐龙分配类似，使用先前值预测下一个值，所以我们的序列模型将尝试预测给定的x⟨t⟩。 但是，Y中的数据被重新排序为维（Ty，m，78），其中Ty = Tx。 这种格式使得稍后进入LSTM更方便。</li><li>n_value: 数据集中独立”value”的个数，这里是78</li><li>indices_values: python 字典：key 是0-77，value 是特定音符</li></ul><p>模型结构如下：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolo43mj21sm0ryq6x.jpg" alt></p><p>这里用了3个keras函数来定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reshapor = Reshape((<span class="number">1</span>, <span class="number">78</span>))                        <span class="comment"># Used in Step 2.B of djmodel(), below</span></span><br><span class="line">LSTM_cell = LSTM(n_a, return_state = <span class="keyword">True</span>)         <span class="comment"># Used in Step 2.C</span></span><br><span class="line">densor = Dense(n_values, activation=<span class="string">'softmax'</span>)     <span class="comment"># Used in Step 2.D</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: djmodel</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">djmodel</span><span class="params">(Tx, n_a, n_values)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Tx -- length of the sequence in a corpus</span></span><br><span class="line"><span class="string">    n_a -- the number of activations used in our model</span></span><br><span class="line"><span class="string">    n_values -- number of unique values in the music data </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a keras model with the </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define the input of your model with a shape </span></span><br><span class="line">    X = Input(shape=(Tx, n_values))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define s0, initial hidden state for the decoder LSTM</span></span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">'a0'</span>)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">'c0'</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    <span class="comment"># Step 1: Create empty list to append the outputs while you iterate (≈1 line)</span></span><br><span class="line">    outputs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Tx):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2.A: select the "t"th time step vector from X. </span></span><br><span class="line">        x = Lambda(<span class="keyword">lambda</span> x: X[:,t,:])(X)</span><br><span class="line">        <span class="comment"># Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)</span></span><br><span class="line">        x = reshapor(x)</span><br><span class="line">        <span class="comment"># Step 2.C: Perform one step of the LSTM_cell</span></span><br><span class="line">        a, _, c = LSTM_cell(x, initial_state=[a, c])</span><br><span class="line">        <span class="comment"># Step 2.D: Apply densor to the hidden state output of LSTM_Cell</span></span><br><span class="line">        out = densor(a)</span><br><span class="line">        <span class="comment"># Step 2.E: add the output to "outputs"</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Create model instance</span></span><br><span class="line">    model = Model(inputs=[X, a0, c0], outputs=outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = djmodel(Tx = 30 , n_a = 64, n_values = 78)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=opt, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = 60</span><br><span class="line">a0 = np.zeros((m, n_a))</span><br><span class="line">c0 = np.zeros((m, n_a))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit([X, a0, c0], list(Y), epochs=100)</span><br></pre></td></tr></table></figure><h2 id="生成音乐的模型"><a href="#生成音乐的模型" class="headerlink" title="生成音乐的模型"></a>生成音乐的模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: music_inference_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">music_inference_model</span><span class="params">(LSTM_cell, densor, n_values = <span class="number">78</span>, n_a = <span class="number">64</span>, Ty = <span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Uses the trained "LSTM_cell" and "densor" from model() to generate a sequence of values.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    LSTM_cell -- the trained "LSTM_cell" from model(), Keras layer object</span></span><br><span class="line"><span class="string">    densor -- the trained "densor" from model(), Keras layer object</span></span><br><span class="line"><span class="string">    n_values -- integer, umber of unique values</span></span><br><span class="line"><span class="string">    n_a -- number of units in the LSTM_cell</span></span><br><span class="line"><span class="string">    Ty -- integer, number of time steps to generate</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    inference_model -- Keras model instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input of your model with a shape </span></span><br><span class="line">    x0 = Input(shape=(<span class="number">1</span>, n_values))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define s0, initial hidden state for the decoder LSTM</span></span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">'a0'</span>)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">'c0'</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line">    x = x0</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Create an empty list of "outputs" to later store your predicted values (≈1 line)</span></span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Loop over Ty and generate a value at every time step</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Ty):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.A: Perform one step of LSTM_cell (≈1 line)</span></span><br><span class="line">        a, _, c = LSTM_cell(x, initial_state=[a, c])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)</span></span><br><span class="line">        out = densor(a)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2.C: Append the prediction "out" to "outputs". out.shape = (None, 78) (≈1 line)</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.D: Select the next value according to "out", and set "x" to be the one-hot representation of the</span></span><br><span class="line">        <span class="comment">#           selected value, which will be passed as the input to LSTM_cell on the next step. We have provided </span></span><br><span class="line">        <span class="comment">#           the line of code you need to do this. </span></span><br><span class="line">        x = Lambda(one_hot)(out)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 3: Create model instance with the correct "inputs" and "outputs" (≈1 line)</span></span><br><span class="line">    inference_model = Model(inputs=[x0, a0, c0], outputs=outputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> inference_model</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_initializer = np.zeros((1, 1, 78))</span><br><span class="line">a_initializer = np.zeros((1, n_a))</span><br><span class="line">c_initializer = np.zeros((1, n_a))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict_and_sample</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_and_sample</span><span class="params">(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, </span></span></span><br><span class="line"><span class="function"><span class="params">                       c_initializer = c_initializer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Predicts the next value of values using the inference model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    inference_model -- Keras model instance for inference time</span></span><br><span class="line"><span class="string">    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation</span></span><br><span class="line"><span class="string">    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell</span></span><br><span class="line"><span class="string">    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated</span></span><br><span class="line"><span class="string">    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.</span></span><br><span class="line">    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])</span><br><span class="line">    <span class="comment"># Step 2: Convert "pred" into an np.array() of indices with the maximum probabilities</span></span><br><span class="line">    indices = np.argmax(pred, axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 3: Convert indices to one-hot vectors, the shape of the results should be (1, )</span></span><br><span class="line">    results = to_categorical(indices, num_classes=x_initializer.shape[<span class="number">-1</span>])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> results, indices</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out_stream = generate_music(inference_model)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;第三个作业是用LSTM来生成爵士乐。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-1)-- 循环神经网络（Recurrent Neural Networks）（2）</title>
    <link href="http://fangzh.top/2018/dl-ai-5-1h2/"/>
    <id>http://fangzh.top/2018/dl-ai-5-1h2/</id>
    <published>2018-10-18T08:20:33.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>作业2搭建了一个字符级的语言模型，来生成恐龙的名字。</p><a id="more"></a><h1 id="Part2-Character-level-language-model-Dinosaurus-land"><a href="#Part2-Character-level-language-model-Dinosaurus-land" class="headerlink" title="Part2:Character level language model - Dinosaurus land"></a>Part2:Character level language model - Dinosaurus land</h1><p>模型结构</p><ul><li>初始化参数</li><li>执行最优化循环<ul><li>计算前向传播的损失函数</li><li>计算反向传播的梯度及损失函数</li><li>剪裁梯度避免梯度爆炸</li><li>使用梯度更新梯度下降中的各参数</li></ul></li><li>返回学习到的参数</li></ul><p><strong>梯度裁剪</strong></p><p>确保不会梯度爆炸</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### GRADED FUNCTION: clip</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients, maxValue)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Clips the gradients' values between minimum and maximum.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    gradients -- a dictionary containing the gradients "dWaa", "dWax", "dWya", "db", "dby"</span></span><br><span class="line"><span class="string">    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    gradients -- a dictionary with the clipped gradients.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    dWaa, dWax, dWya, db, dby = gradients[<span class="string">'dWaa'</span>], gradients[<span class="string">'dWax'</span>], gradients[<span class="string">'dWya'</span>], gradients[<span class="string">'db'</span>], gradients[<span class="string">'dby'</span>]</span><br><span class="line">   </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)</span></span><br><span class="line">    <span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax, dWaa, dWya, db, dby]:</span><br><span class="line">        np.clip(gradient, <span class="number">-1</span> * maxValue, maxValue,out=gradient)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dWaa"</span>: dWaa, <span class="string">"dWax"</span>: dWax, <span class="string">"dWya"</span>: dWya, <span class="string">"db"</span>: db, <span class="string">"dby"</span>: dby&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p><strong>采样</strong></p><p>现在假设你的模型已经训练好了，你需要以此生成新的字母，过程如下:</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgol8ej9j21ks0lw0wq.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sample</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters, char_to_ix, seed)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Sample a sequence of characters according to a sequence of probability distributions output of the RNN</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. </span></span><br><span class="line"><span class="string">    char_to_ix -- python dictionary mapping each character to an index.</span></span><br><span class="line"><span class="string">    seed -- used for grading purposes. Do not worry about it.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    indices -- a list of length n containing the indices of the sampled characters.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters and relevant shapes from "parameters" dictionary</span></span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'b'</span>]</span><br><span class="line">    vocab_size = by.shape[<span class="number">0</span>]</span><br><span class="line">    n_a = Waa.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)</span></span><br><span class="line">    x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Step 1': Initialize a_prev as zeros (≈1 line)</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)</span></span><br><span class="line">    indices = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Idx is a flag to detect a newline character, we initialize it to -1</span></span><br><span class="line">    idx = <span class="number">-1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over time-steps t. At each time-step, sample a character from a probability distribution and append </span></span><br><span class="line">    <span class="comment"># its index to "indices". We'll stop if we reach 50 characters (which should be very unlikely with a well </span></span><br><span class="line">    <span class="comment"># trained model), which helps debugging and prevents entering an infinite loop. </span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    newline_character = char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (idx != newline_character <span class="keyword">and</span> counter != <span class="number">50</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2: Forward propagate x using the equations (1), (2) and (3)</span></span><br><span class="line">        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)</span><br><span class="line">        z = np.dot(Wya, a) + by</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for grading purposes</span></span><br><span class="line">        np.random.seed(counter+seed) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3: Sample the index of a character within the vocabulary from the probability distribution y</span></span><br><span class="line">        idx = np.random.choice(range(len(y)),p = y.ravel())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append the index to "indices"</span></span><br><span class="line">        indices.append(idx)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 4: Overwrite the input character as the one corresponding to the sampled index.</span></span><br><span class="line">        x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">        x[idx] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update "a_prev" to be "a"</span></span><br><span class="line">        a_prev = a</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for grading purposes</span></span><br><span class="line">        seed += <span class="number">1</span></span><br><span class="line">        counter +=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (counter == <span class="number">50</span>):</span><br><span class="line">        indices.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> indices</span><br></pre></td></tr></table></figure><h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><p>函数都已经给你了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: optimize</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Execute one step of the optimization to train the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.</span></span><br><span class="line"><span class="string">    Y -- list of integers, exactly the same as X but shifted one index to the left.</span></span><br><span class="line"><span class="string">    a_prev -- previous hidden state.</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        b --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- value of the loss function (cross-entropy)</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        db -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dby -- Gradients of output bias vector, of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Forward propagate through time (≈1 line)</span></span><br><span class="line">    loss, cache = rnn_forward(X, Y, a_prev, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backpropagate through time (≈1 line)</span></span><br><span class="line">    gradients, a = rnn_backward(X, Y, parameters, cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Clip your gradients between -5 (min) and 5 (max) (≈1 line)</span></span><br><span class="line">    gradients = clip(gradients, <span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update parameters (≈1 line)</span></span><br><span class="line">    parameters = update_parameters(parameters, gradients, learning_rate)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss, gradients, a[len(X)<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data, ix_to_char, char_to_ix, num_iterations = <span class="number">35000</span>, n_a = <span class="number">50</span>, dino_names = <span class="number">7</span>, vocab_size = <span class="number">27</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Trains the model and generates dinosaur names. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    data -- text corpus</span></span><br><span class="line"><span class="string">    ix_to_char -- dictionary that maps the index to a character</span></span><br><span class="line"><span class="string">    char_to_ix -- dictionary that maps a character to an index</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to train the model for</span></span><br><span class="line"><span class="string">    n_a -- number of units of the RNN cell</span></span><br><span class="line"><span class="string">    dino_names -- number of dinosaur names you want to sample at each iteration. </span></span><br><span class="line"><span class="string">    vocab_size -- number of unique characters found in the text, size of the vocabulary</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- learned parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve n_x and n_y from vocab_size</span></span><br><span class="line">    n_x, n_y = vocab_size, vocab_size</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(n_a, n_x, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize loss (this is required because we want to smooth our loss, don't worry about it)</span></span><br><span class="line">    loss = get_initial_loss(vocab_size, dino_names)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Build list of all dinosaur names (training examples).</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"dinos.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        examples = f.readlines()</span><br><span class="line">    examples = [x.lower().strip() <span class="keyword">for</span> x <span class="keyword">in</span> examples]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle list of all dinosaur names</span></span><br><span class="line">    np.random.seed(<span class="number">0</span>)</span><br><span class="line">    np.random.shuffle(examples)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the hidden state of your LSTM</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use the hint above to define one training example (X,Y) (≈ 2 lines)</span></span><br><span class="line">        index = j % len(examples)</span><br><span class="line">        X = [<span class="keyword">None</span>] + [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]]</span><br><span class="line">        Y = X[<span class="number">1</span>:] + [char_to_ix[<span class="string">'\n'</span>]]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -&gt; Update parameters</span></span><br><span class="line">        <span class="comment"># Choose a learning rate of 0.01</span></span><br><span class="line">        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate=<span class="number">0.01</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use a latency trick to keep the loss smooth. It happens here to accelerate the training.</span></span><br><span class="line">        loss = smooth(loss, curr_loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Every 2000 Iteration, generate "n" characters thanks to sample() to check if the model is learning properly</span></span><br><span class="line">        <span class="keyword">if</span> j % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">            print(<span class="string">'Iteration: %d, Loss: %f'</span> % (j, loss) + <span class="string">'\n'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># The number of dinosaur names to print</span></span><br><span class="line">            seed = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> range(dino_names):</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Sample indices and print them</span></span><br><span class="line">                sampled_indices = sample(parameters, char_to_ix, seed)</span><br><span class="line">                print_sample(sampled_indices, ix_to_char)</span><br><span class="line">                </span><br><span class="line">                seed += <span class="number">1</span>  <span class="comment"># To get the same result for grading purposed, increment the seed by one. </span></span><br><span class="line">      </span><br><span class="line">            print(<span class="string">'\n'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;作业2搭建了一个字符级的语言模型，来生成恐龙的名字。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(5-1)-- 循环神经网络（Recurrent Neural Networks）（1）</title>
    <link href="http://fangzh.top/2018/dl-ai-5-1h1/"/>
    <id>http://fangzh.top/2018/dl-ai-5-1h1/</id>
    <published>2018-10-18T02:26:56.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>本周作业分为三部分：</p><ul><li>手动建一个RNN模型</li><li>搭建一个字符级的语言模型来生成恐龙的名字</li><li>用LSTM生成爵士乐</li></ul><a id="more"></a><h1 id="Part1-Building-a-recurrent-neural-network-step-by-step"><a href="#Part1-Building-a-recurrent-neural-network-step-by-step" class="headerlink" title="Part1:Building a recurrent neural network - step by step"></a>Part1:Building a recurrent neural network - step by step</h1><p>来构建一个RNN的神经网络。</p><h2 id="1-Forward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#1-Forward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="1 - Forward propagation for the basic Recurrent Neural Network"></a>1 - Forward propagation for the basic Recurrent Neural Network</h2><p>先来进行前向传播的构建，要构建这个网络，先构建每个RNN的传播单元：</p><p><strong>RNN cell</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgoliv4pj21he0o2q6i.jpg" alt></p><ol><li>Compute the hidden state with tanh activation: $a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)$</li><li>Using your new hidden state $a^{\langle t \rangle}$, compute the prediction $\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)$. We provided you a function: <code>softmax</code>.</li><li>Store $(a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)$ in cache</li><li>Return $a^{\langle t \rangle}$ , $y^{\langle t \rangle}$ and cache</li></ol><p>We will vectorize over $m$ examples. Thus, $x^{\langle t \rangle}$ will have dimension $(n_x,m)$, and $a^{\langle t \rangle}$ will have dimension $(n_a,m)$. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: rnn_cell_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt, a_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈2 lines)</span></span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya, a_next) + by)    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure><p><strong>RNN forward pass</strong></p><p>思路是：</p><ul><li>先把 a ,y_pred置为0</li><li>然后初始化a_next = a0</li><li>然后经过Tx个循环，求得每一步的a和y以及cache</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: rnn_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "caches" which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters["Wya"]</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wya"</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a" and "y" with zeros (≈2 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    y_pred = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next (≈1 line)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Append "cache" to "caches" (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure><h2 id="2-Long-Short-Term-Memory-LSTM-network"><a href="#2-Long-Short-Term-Memory-LSTM-network" class="headerlink" title="2 - Long Short-Term Memory (LSTM) network"></a>2 - Long Short-Term Memory (LSTM) network</h2><p>接下来构建一个LSTM的网络</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolqlbvj21d80mqn24.jpg" alt></p><p><strong>遗忘门：</strong></p><p>假设我们正在阅读一段文字中的单词，并且希望使用LSTM来跟踪语法结构，例如主语是单数还是复数。 如果主语从单个单词变成复数单词，我们需要找到一种方法来摆脱先前存储的单数/复数状态的记忆值。</p><p>在LSTM中，遗忘门让我们做到这一点： </p><p>$$\Gamma_f^{\langle t \rangle} = \sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f)$$</p><p><strong>更新门:</strong></p><p>一旦我们忘记所讨论的主题是单数的，我们需要找到一种方法来更新它，以反映新主题现在是复数。</p><p>$$\Gamma_u^{\langle t \rangle} = \sigma(W_u[a^{\langle t-1 \rangle}, x^] + b_u)$$ </p><p>所以两个门结合起来可以更新单元值：</p><p>$$ \tilde{c}^{\langle t \rangle} = \tanh(W_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c) $$</p><p>$$ c^{&lt;t>} = \Gamma_f^{&lt;t>} c^{&lt;t-1>} +  \Gamma_u ^{&lt;t>} \tilde {c}^{&lt;t>} $$</p><p><strong>输出门：</strong></p><p>为了决定输出，我们将使用以下两个公式：</p><p>$$ \Gamma_o^{\langle t \rangle}=  \sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)$$<br>$$ a^{\langle t \rangle} = \Gamma_o^{\langle t \rangle}* \tanh(c^{\langle t \rangle}) $$</p><p><strong>LSTM 单元</strong></p><ul><li>先将$a^{\langle t-1 \rangle}$ and $x^{\langle t \rangle}$连接在一起变成$concat = \begin{bmatrix} a^{\langle t-1 \rangle} \ x^{\langle t \rangle} \end{bmatrix}$</li><li>计算以上的6个公式</li><li>然后预测输出y</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: lstm_cell_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt, a_prev, c_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_next -- next memory state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),</span></span><br><span class="line"><span class="string">          c stands for the memory value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wf = parameters[<span class="string">"Wf"</span>]</span><br><span class="line">    bf = parameters[<span class="string">"bf"</span>]</span><br><span class="line">    Wi = parameters[<span class="string">"Wi"</span>]</span><br><span class="line">    bi = parameters[<span class="string">"bi"</span>]</span><br><span class="line">    Wc = parameters[<span class="string">"Wc"</span>]</span><br><span class="line">    bc = parameters[<span class="string">"bc"</span>]</span><br><span class="line">    Wo = parameters[<span class="string">"Wo"</span>]</span><br><span class="line">    bo = parameters[<span class="string">"bo"</span>]</span><br><span class="line">    Wy = parameters[<span class="string">"Wy"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt (≈3 lines)</span></span><br><span class="line">    concat = np.zeros((n_x + n_a, m))</span><br><span class="line">    concat[: n_a, :] = a_prev  </span><br><span class="line">    concat[n_a :, :] = xt </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)</span><br><span class="line">    it = sigmoid(np.dot(Wi, concat) + bi)</span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)</span><br><span class="line">    c_next = ft * c_prev + it * cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)</span><br><span class="line">    a_next = ot * np.tanh(c_next)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell (≈1 line)</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wy, a_next) + by)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure><p><strong>Forward pass for LSTM</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolirp8j21bs0bcwgj.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: lstm_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize "caches", which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">'Wy'</span>].shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize "a", "c" and "y" with zeros (≈3 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    c = np.zeros((n_a, m, T_x))</span><br><span class="line">    y = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next (≈2 lines)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros((n_a, m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, c_next, yt, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y[:,:,t] = yt</span><br><span class="line">        <span class="comment"># Save the value of the next cell state (≈1 line)</span></span><br><span class="line">        c[:,:,t]  = c_next</span><br><span class="line">        <span class="comment"># Append the cache into caches (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure><h2 id="3-Backpropagation-in-recurrent-neural-networks"><a href="#3-Backpropagation-in-recurrent-neural-networks" class="headerlink" title="3 - Backpropagation in recurrent neural networks"></a>3 - Backpropagation in recurrent neural networks</h2><p>接下来是RNN的反向传播，但是一般框架都会帮我们实现，这里看看就好了。公式也比较复杂。</p><p><strong>RNN backward pass</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass for the RNN-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradient of loss with respect to next hidden state</span></span><br><span class="line"><span class="string">    cache -- python dictionary containing useful values (output of rnn_cell_forward())</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradients of input data, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from cache</span></span><br><span class="line">    (a_next, a_prev, xt, parameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from parameters</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># compute the gradient of tanh with respect to a_next (≈1 line)</span></span><br><span class="line">    dtanh = (<span class="number">1</span> - a_next**<span class="number">2</span>) * da_next</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of the loss with respect to Wax (≈2 lines)</span></span><br><span class="line">    dxt = np.dot(Wax.T, dtanh)</span><br><span class="line">    dWax = np.dot(dtanh, xt.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to Waa (≈2 lines)</span></span><br><span class="line">    da_prev = np.dot(Waa.T, dtanh)</span><br><span class="line">    dWaa = np.dot(dtanh, a_prev.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to b (≈1 line)</span></span><br><span class="line">    dba = np.sum(dtanh, keepdims=<span class="keyword">True</span>, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa, <span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for a RNN over an entire sequence of input data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple containing information from the forward pass (rnn_forward)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches (≈2 lines)</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, a0, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈6 lines)</span></span><br><span class="line">    dx = np.zeros((n_x, m, T_x))</span><br><span class="line">    dWax = np.zeros((n_a, n_x))</span><br><span class="line">    dWaa = np.zeros((n_a, n_a))</span><br><span class="line">    dba = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    da0 = np.zeros((n_a, m))</span><br><span class="line">    da_prevt = np.zeros((n_a, m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop through all the time steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step. (≈1 line)</span></span><br><span class="line">        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Retrieve derivatives from gradients (≈ 1 line)</span></span><br><span class="line">        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[<span class="string">"dxt"</span>], gradients[<span class="string">"da_prev"</span>], gradients[<span class="string">"dWax"</span>], gradients[<span class="string">"dWaa"</span>], gradients[<span class="string">"dba"</span>]</span><br><span class="line">        <span class="comment"># Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)</span></span><br><span class="line">        dx[:, :, t] = dxt</span><br><span class="line">        dWax += dWaxt</span><br><span class="line">        dWaa += dWaat</span><br><span class="line">        dba += dbat</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) </span></span><br><span class="line">    da0 = da_prevt</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa,<span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p><strong>LSTM backward pass</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_backward</span><span class="params">(da_next, dc_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the LSTM-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradients of next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    dc_next -- Gradients of next cell state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    cache -- cache storing information from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from xt's and a_next's shape (≈2 lines)</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_a, m = a_next.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)</span></span><br><span class="line">    dot = da_next * np.tanh(c_next) * ot * (<span class="number">1</span>-ot)</span><br><span class="line">    dcct = (dc_next*it+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*it*da_next)*(<span class="number">1</span>-np.square(cct))</span><br><span class="line">    dit = (dc_next*cct+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*cct*da_next)*it*(<span class="number">1</span>-it)</span><br><span class="line">    dft = (dc_next*c_prev+ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*c_prev*da_next)*ft*(<span class="number">1</span>-ft) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Code equations (7) to (10) (≈4 lines)</span></span><br><span class="line">    <span class="comment"># dit = None</span></span><br><span class="line">    <span class="comment"># dft = None</span></span><br><span class="line">    <span class="comment"># dot = None</span></span><br><span class="line">    <span class="comment"># dcct = None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines)</span></span><br><span class="line">    dWf = np.dot(dft, np.concatenate((a_prev, xt), axis=<span class="number">0</span>).T)</span><br><span class="line">    dWi = np.dot(dit, np.concatenate((a_prev, xt), axis=<span class="number">0</span>).T)</span><br><span class="line">    dWc = np.dot(dcct, np.concatenate((a_prev, xt), axis=<span class="number">0</span>).T)</span><br><span class="line">    dWo = np.dot(dot, np.concatenate((a_prev, xt), axis=<span class="number">0</span>).T)</span><br><span class="line">    dbf = np.sum(dft, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dbi = np.sum(dit, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dbc = np.sum(dcct, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dbo = np.sum(dot, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)</span></span><br><span class="line">    da_prev = np.dot(parameters[<span class="string">'Wf'</span>][:,:n_a].T, dft) + np.dot(parameters[<span class="string">'Wi'</span>][:,:n_a].T, dit) + np.dot(parameters[<span class="string">'Wc'</span>][:,:n_a].T, dcct) + np.dot(parameters[<span class="string">'Wo'</span>][:,:n_a].T, dot)</span><br><span class="line">    dc_prev = dc_next*ft + ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ft*da_next</span><br><span class="line">    dxt = np.dot(parameters[<span class="string">'Wf'</span>][:,n_a:].T,dft)+np.dot(parameters[<span class="string">'Wi'</span>][:,n_a:].T,dit)+np.dot(parameters[<span class="string">'Wc'</span>][:,n_a:].T,dcct)+np.dot(parameters[<span class="string">'Wo'</span>][:,n_a:].T,dot) </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save gradients in dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dc_prev"</span>: dc_prev, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- cache storing information from the forward pass (lstm_forward)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient of inputs, of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches.</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈12 lines)</span></span><br><span class="line">    dx = np.zeros((n_x, m, T_x))</span><br><span class="line">    da0 = np.zeros((n_a, m))</span><br><span class="line">    da_prevt = np.zeros((n_a, m))</span><br><span class="line">    dc_prevt = np.zeros((n_a, m))</span><br><span class="line">    dWf = np.zeros((n_a, n_a+n_x))</span><br><span class="line">    dWi = np.zeros((n_a, n_a+n_x))</span><br><span class="line">    dWc = np.zeros((n_a, n_a+n_x))</span><br><span class="line">    dWo = np.zeros((n_a, n_a+n_x))</span><br><span class="line">    dbf = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    dbi = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    dbc = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    dbo = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop back over the whole sequence</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute all gradients using lstm_cell_backward</span></span><br><span class="line">        gradients = lstm_cell_backward(da[:, :, t] + da_prevt, dc_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Store or add the gradient to the parameters' previous step's gradient</span></span><br><span class="line">        dx[:,:,t] = gradients[<span class="string">'dxt'</span>]</span><br><span class="line">        dWf = dWf + gradients[<span class="string">'dWf'</span>]</span><br><span class="line">        dWi = dWi + gradients[<span class="string">'dWi'</span>]</span><br><span class="line">        dWc = dWc + gradients[<span class="string">'dWc'</span>]</span><br><span class="line">        dWo = dWo + gradients[<span class="string">'dWo'</span>]</span><br><span class="line">        dbf = dbf + gradients[<span class="string">'dbf'</span>]</span><br><span class="line">        dbi = dbi + gradients[<span class="string">'dbi'</span>]</span><br><span class="line">        dbc = dbc + gradients[<span class="string">'dbc'</span>]</span><br><span class="line">        dbo = dbo + gradients[<span class="string">'dbo'</span>]</span><br><span class="line">    <span class="comment"># Set the first activation's gradient to the backpropagated gradient da_prev.</span></span><br><span class="line">    da0 = gradients[<span class="string">'da_prev'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本周作业分为三部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;手动建一个RNN模型&lt;/li&gt;
&lt;li&gt;搭建一个字符级的语言模型来生成恐龙的名字&lt;/li&gt;
&lt;li&gt;用LSTM生成爵士乐&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(5-1)-- 循环神经网络（Recurrent Neural Networks）</title>
    <link href="http://fangzh.top/2018/dl-ai-5-1/"/>
    <id>http://fangzh.top/2018/dl-ai-5-1/</id>
    <published>2018-10-18T02:26:52.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>第五门课讲的是序列模型，主要是对RNN算法的应用，如GRU，LSTM算法，应用在词嵌入模型，情感分类，语音识别等领域。</p><p>第一周讲的是RNN的基本算法。</p><a id="more"></a><h1 id="序列模型的应用"><a href="#序列模型的应用" class="headerlink" title="序列模型的应用"></a>序列模型的应用</h1><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokszq4j20og0dkq54.jpg" alt></p><p>序列模型用在了很多的地方，如语音识别，音乐生成，情感分类，DNA序列分析，机器翻译，视频内容检测，名字检测等等。</p><h1 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h1><p>先讲一下NG在课程中主要用到的数学符号。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokftypj20o507kwes.jpg" alt></p><p>对于输入一个$x$的句子序列，可以细分为一个个的词，每一个词记为$x^{&lt;t>}$，对应的输出$y$记为$y^{&lt;t>}$</p><p>其中，输入x的序列长度为 $T_x$，输出$y$的序列长度为$T_y$</p><p>而针对很多个不同的序列，$X^{(i)&lt;t>}$表示第$i$个样本的第t的词。</p><p>那么如何用数学的形式表示这个$x^{&lt;t>}$呢？这里用到了one-hot编码，假设词表中一共有10000个词汇，那么$x^{&lt;t>}$就是一个长度为10000的向量，在这之中只有一个维度是1，其他都是0</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokgq4pj20o80dkjsc.jpg" alt></p><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>如果用传统的神经网络，经过一个N层的神经网络得到输出y。</p><p>效果并不是很好，因为：</p><ul><li>输入和输出在不同的样本中是可以不同长度的（每个句子可以有不同的长度）</li><li>这种朴素的神经网络结果并不能共享从文本不同位置所学习到的特征。（如卷积神经网络中学到的特征的快速地推广到图片其他位置）</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokgsxfj20n40cfdgj.jpg" alt></p><p>所以循环神经网络采用每一个时间步来计算，输入一个$x^{&lt;t>}$和前面留下来的记忆$a^{&lt;t-1>}$，来得到这一层的输出$y^{&lt;t>}$和下一层的记忆$a^{&lt;t>}$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokk8tvj20n108s0tp.jpg" alt></p><p>这里需要注意在零时刻，我们需要编造一个激活值，通常输入一个零向量，有的研究人员会使用随机的方法对该初始激活向量进行初始化。同时，上图中右边的循环神经网络的绘制结构与左边是等价的。</p><p>循环神经网络是从左到右扫描数据的，同时共享每个时间步的参数。</p><ul><li>$W_{ax}$管理从输入$x^{&lt;t>}$到隐藏层的连接，每个时间步都使用相同的$W_{ax}$，同下；</li><li>$W_{aa}$管理激活值$a^{&lt;t>}$到隐藏层的连接；</li><li>$W_{ya}$管理隐藏层到激活值$y^{&lt;t>}$的连接。</li></ul><p><strong>RNN的前向传播</strong></p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokk24pj20np0ckmya.jpg" alt></p><p>前向传播公式如图，这里可以把$W_{aa}，W_{ax}$合并成一项，为$W_a$，而后将$[a^{&lt;t-1>},x^{&lt;t>}]$合并成一项。</p><p><strong>RNN的反向传播</strong></p><p>定义一个loss function，然后倒回去计算。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgoksqpcj20nf0cxdhj.jpg" alt></p><h1 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h1><p>对于RNN，不同的问题需要不同的输入输出结构。</p><ul><li>One to many：如音乐生成，输入一个音乐类型或者空值，生成一段音乐</li><li>Many to one：如情感分类问题，输入某个序列，输出一个值来判断得分。</li><li>many to many（$T_x = T_y$）：输入和输出的序列长度相同</li><li>many to many（$T_x != T_y$）：如机器翻译这种，先输入一段，然后自己生成一段，输入和输出长度不一定相同的。</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokh75rj20nm0d274y.jpg" alt></p><h1 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h1><p><strong>什么是语言模型？</strong></p><p>对于下面的例子，两句话有相似的发音，但是想表达的意义和正确性却不相同，如何让我们的构建的语音识别系统能够输出正确地给出想要的输出。也就是对于语言模型来说，从输入的句子中，评估各个句子中各个单词出现的可能性，进而给出整个句子出现的可能性。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokqwy9j20ln0b83yw.jpg" alt></p><p> <strong>使用RNN构建语言模型：</strong></p><ul><li>训练集：一个很大的语言文本语料库；</li><li>Tokenize：将句子使用字典库标记化；其中，未出现在字典库中的词使用“UNK”来表示；</li><li>第一步：使用零向量对输出进行预测，即预测第一个单词是某个单词的可能性；</li><li>第二步：通过前面的输入，逐步预测后面一个单词出现的概率；</li></ul><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokk6ckj20n70cx75i.jpg" alt></p><h1 id="对新序列采样"><a href="#对新序列采样" class="headerlink" title="对新序列采样"></a>对新序列采样</h1><p>当我们训练得到了一个模型之后，如果我们想知道这个模型学到了些什么，一个非正式的方法就是对新序列进行采样。具体方法如下：</p><p>在每一步输出$y$时，通常使用 softmax 作为激活函数，然后根据输出的分布，随机选择一个值，也就是对应的一个字或者英文单词。</p><p>然后将这个值作为下一个单元的x输入进去(即$x^{&lt;t>}=y^{&lt;t−1>}$), 直到我们输出了终结符，或者输出长度超过了提前的预设值n才停止采样。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokk4rpj20ic063jre.jpg" alt></p><h1 id="RNN的梯度消失"><a href="#RNN的梯度消失" class="headerlink" title="RNN的梯度消失"></a>RNN的梯度消失</h1><p>RNN存在一个梯度消失问题，如：</p><ul><li>The cat, which already ate ………..，was full；</li><li>The cats, which already ate ………..，were full.</li></ul><p>cat 和 cats要经过很长的一系列词汇后，才对应 was 和 were，但是我们在传递过程中$a^{&lt;t>}$很难记住前面这么多词汇的内容，往往只和前面最近几个词汇有关而已。</p><p>当然，也有可能是每一层的梯度都很大，导致的梯度爆炸问题，不过这个问题可以通过设置阈值来解决，关键是要解决梯度消失问题。我们知道一旦神经网络层次很多时，反向传播很难影响前面层次的参数。</p><h1 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU(Gated Recurrent Unit)"></a>GRU(Gated Recurrent Unit)</h1><p>那么如何解决梯度消失问题了，使用GRU单元可以有效的捕捉到更深层次的连接，来改善梯度消失问题。</p><p>原本的RNN单元如图：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokkehij20m00aiglw.jpg" alt></p><p>而GRU单元多了一个c（memory cell）变量，用来提供长期的记忆能力。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgoktgchj20n20cwjtf.jpg" alt></p><p>具体过程为：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokn137j20pg04m0t4.jpg" alt></p><p>完整的GRU还存在另一个门，用来控制$\bar c$和 $c^{&lt;t-1>}$之间的联系强弱：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokn4qrj20dp06maaj.jpg" alt></p><h1 id="LSTM-Long-short-term-memory"><a href="#LSTM-Long-short-term-memory" class="headerlink" title="LSTM(Long short-term memory)"></a>LSTM(Long short-term memory)</h1><p>GRU能够让我们在序列中学习到更深的联系，长短期记忆（long short-term memory, LSTM）对捕捉序列中更深层次的联系要比GRU更加有效。</p><p>GRU只有两个门，而LSTM有三个门，分别是更新门、遗忘门、输出门：$\Gamma_u,\Gamma_f, \Gamma_o$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgoko16mj20n50c174n.jpg" alt="GRU和LSTM公式对比"></p><p>更新门：用来决定是否更新$\bar c^{&lt;t>}$</p><p>遗忘门：来决定是否遗忘上一个$c^{&lt;t-1>}$</p><p>输出门：来决定是否输出$c^{&lt;t>}$</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolrcslj20wl0idjth.jpg" alt></p><h1 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h1><p>双向RNN（bidirectional RNNs）模型能够让我们在序列的某处，不仅可以获取之间的信息，还可以获取未来的信息。</p><p>对于下图的单向RNN的例子中，无论我们的RNN单元是基本的RNN单元，还是GRU，或者LSTM单元，对于例子中第三个单词”Teddy”很难判断是否是人名，仅仅使用前面的两个单词是不够的，需要后面的信息来进行判断，但是单向RNN就无法实现获取未来的信息。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokp4z4j20mn0boaas.jpg" alt></p><p>而双向RNN则可以解决单向RNN存在的弊端。在BRNN中，不仅有从左向右的前向连接层，还存在一个从右向左的反向连接层。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgolueabj20n20cwq4i.jpg" alt></p><h1 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h1><p>与深层的基本神经网络结构相似，深层RNNs模型具有多层的循环结构，但不同的是，在传统的神经网络中，我们可能会拥有很多层，几十层上百层，但是对与RNN来说，三层的网络结构就已经很多了，因为RNN存在时间的维度，所以其结构已经足够的庞大。如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fwcgokxta0j20mu0cuab9.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;第五门课讲的是序列模型，主要是对RNN算法的应用，如GRU，LSTM算法，应用在词嵌入模型，情感分类，语音识别等领域。&lt;/p&gt;
&lt;p&gt;第一周讲的是RNN的基本算法。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai作业:(4-4)-- 特殊应用:人脸识别和神经风格转换</title>
    <link href="http://fangzh.top/2018/dl-ai-4-4h/"/>
    <id>http://fangzh.top/2018/dl-ai-4-4h/</id>
    <published>2018-10-12T10:55:20.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>本周作业分为了两个部分:</p><ul><li>人脸识别</li><li>风格迁移</li></ul><a id="more"></a><h1 id="Part1：人脸识别"><a href="#Part1：人脸识别" class="headerlink" title="Part1：人脸识别"></a>Part1：人脸识别</h1><p>训练FaceNet很不现实，所以模型已经都训练好了，我们只是学习一下loss函数，然后调用模型来进行简单的识别而已。</p><p>先计算triplet_loss函数，分为4步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: triplet_loss</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(y_true, y_pred, alpha = <span class="number">0.2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the triplet loss as defined by formula (3)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.</span></span><br><span class="line"><span class="string">    y_pred -- python list containing three objects:</span></span><br><span class="line"><span class="string">            anchor -- the encodings for the anchor images, of shape (None, 128)</span></span><br><span class="line"><span class="string">            positive -- the encodings for the positive images, of shape (None, 128)</span></span><br><span class="line"><span class="string">            negative -- the encodings for the negative images, of shape (None, 128)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- real number, value of the loss</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    anchor, positive, negative = y_pred[<span class="number">0</span>], y_pred[<span class="number">1</span>], y_pred[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines)</span></span><br><span class="line">    <span class="comment"># Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1</span></span><br><span class="line">    pos_dist = tf.reduce_sum(tf.square(anchor - positive),axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1</span></span><br><span class="line">    neg_dist = tf.reduce_sum(tf.square(anchor - negative),axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 3: subtract the two previous distances and add alpha.</span></span><br><span class="line">    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.</span></span><br><span class="line">    loss = tf.reduce_sum(tf.maximum(basic_loss, <span class="number">0.</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>进行单个人脸验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: verify</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verify</span><span class="params">(image_path, identity, database, model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function that verifies if the person on the "image_path" image is "identity".</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    image_path -- path to an image</span></span><br><span class="line"><span class="string">    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.</span></span><br><span class="line"><span class="string">    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).</span></span><br><span class="line"><span class="string">    model -- your Inception model instance in Keras</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dist -- distance between the image_path and the image of "identity" in the database.</span></span><br><span class="line"><span class="string">    door_open -- True, if the door should open. False otherwise.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)</span></span><br><span class="line">    encoding = img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute distance with identity's image (≈ 1 line)</span></span><br><span class="line">    dist = np.linalg.norm(encoding-database[identity])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Open the door if dist &lt; 0.7, else don't open (≈ 3 lines)</span></span><br><span class="line">    <span class="keyword">if</span> dist &lt; <span class="number">0.7</span>:</span><br><span class="line">        print(<span class="string">"It's "</span> + str(identity) + <span class="string">", welcome home!"</span>)</span><br><span class="line">        door_open = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"It's not "</span> + str(identity) + <span class="string">", please go away"</span>)</span><br><span class="line">        door_open = <span class="keyword">False</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dist, door_open</span><br></pre></td></tr></table></figure><p>进行人脸识别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: who_is_it</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">who_is_it</span><span class="params">(image_path, database, model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements face recognition for the happy house by finding who is the person on the image_path image.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    image_path -- path to an image</span></span><br><span class="line"><span class="string">    database -- database containing image encodings along with the name of the person on the image</span></span><br><span class="line"><span class="string">    model -- your Inception model instance in Keras</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    min_dist -- the minimum distance between image_path encoding and the encodings from the database</span></span><br><span class="line"><span class="string">    identity -- string, the name prediction for the person on image_path</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 1: Compute the target "encoding" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)</span></span><br><span class="line">    encoding = img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 2: Find the closest encoding ##</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "min_dist" to a large value, say 100 (≈1 line)</span></span><br><span class="line">    min_dist = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over the database dictionary's names and encodings.</span></span><br><span class="line">    <span class="keyword">for</span> (name, db_enc) <span class="keyword">in</span> database.items():</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute L2 distance between the target "encoding" and the current "emb" from the database. (≈ 1 line)</span></span><br><span class="line">        dist = np.linalg.norm(encoding-database[name])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> dist &lt; min_dist:</span><br><span class="line">            min_dist = dist</span><br><span class="line">            identity = name</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> min_dist &gt; <span class="number">0.7</span>:</span><br><span class="line">        print(<span class="string">"Not in the database."</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"it's "</span> + str(identity) + <span class="string">", the distance is "</span> + str(min_dist))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> min_dist, identity</span><br></pre></td></tr></table></figure><h1 id="Part2：风格迁移"><a href="#Part2：风格迁移" class="headerlink" title="Part2：风格迁移"></a>Part2：风格迁移</h1><p>模型也都是训练好的了，用的是VGG-19的网络。这里只是体验一下cost function的实现罢了。</p><p><strong>计算J_content(C,G)</strong></p><p>$$J_{content}(C,G) =  \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{(C)} - a^{(G)})^2 $$</p><p>在这过程中需要把三维的矩阵先展开成2维的矩阵进行计算（虽然不展开也是可以计算的，但是风格损失函数需要计算）</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2x5rgjj218g0kpaxc.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_content_cost</span><span class="params">(a_C, a_G)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the content cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_content -- scalar that you compute using equation 1 above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape a_C and a_G (≈2 lines)</span></span><br><span class="line">    a_C_unrolled = tf.reshape(a_C,[n_H * n_W, n_C])</span><br><span class="line">    a_G_unrolled = tf.reshape(a_G,[n_H * n_W, n_C])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute the cost with tensorflow (≈1 line)</span></span><br><span class="line">    J_content = tf.reduce_sum(tf.square(a_C_unrolled - a_G_unrolled)) / (n_H * n_W * n_C * <span class="number">4</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_content</span><br></pre></td></tr></table></figure><p><strong>计算J_style(S,G)</strong></p><p>需要把三维矩阵展开，然后转置，做矩阵乘法，才能得到相关系数矩阵</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2xlfa3j218g0epqm7.jpg" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: gram_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    A -- matrix of shape (n_C, n_H*n_W)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    GA -- Gram matrix of A, of shape (n_C, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    GA = tf.matmul(A,tf.transpose(A))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> GA</span><br></pre></td></tr></table></figure><p>$$J_{style}^{[l]}(S,G) = \frac{1}{4 \times n_{C}^{2} \times (n_H \times n_W)^2} \sum_{i=1}^{n_C} \sum_{j=1}^{n_C} (G^{(S)}_{ij} - G^{(G)} _ {ij})^{2} $$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_layer_style_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_layer_style_cost</span><span class="params">(a_S, a_G)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)</span></span><br><span class="line">    a_S = tf.transpose(tf.reshape(a_S,[n_H*n_W, n_C]))</span><br><span class="line">    a_G = tf.transpose(tf.reshape(a_G,[n_H*n_W, n_C]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing gram_matrices for both images S and G (≈2 lines)</span></span><br><span class="line">    GS = gram_matrix(a_S)</span><br><span class="line">    GG = gram_matrix(a_G)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing the loss (≈1 line)</span></span><br><span class="line">    J_style_layer = <span class="number">1</span> / (<span class="number">4</span> * (n_C*n_W*n_H)**<span class="number">2</span>) * tf.reduce_sum(tf.square(tf.subtract(GS,GG)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_style_layer</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: total_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the total cost function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    J_content -- content cost coded above</span></span><br><span class="line"><span class="string">    J_style -- style cost coded above</span></span><br><span class="line"><span class="string">    alpha -- hyperparameter weighting the importance of the content cost</span></span><br><span class="line"><span class="string">    beta -- hyperparameter weighting the importance of the style cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J -- total cost as defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    J = alpha * J_content + beta * J_style</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">J = total_cost(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_nn</span><span class="params">(sess, input_image, num_iterations = <span class="number">200</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize global variables (you need to run the session on the initializer)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the noisy input image (initial generated image) through the model. Use assign().</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    generated_image = sess.run(model[<span class="string">'input'</span>].assign(input_image))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Run the session on the train_step to minimize the total cost</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the generated image by running the session on the current model['input']</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">        generated_image = sess.run(model[<span class="string">'input'</span>])</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print every 20 iteration.</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            Jt, Jc, Js = sess.run([J, J_content, J_style])</span><br><span class="line">            print(<span class="string">"Iteration "</span> + str(i) + <span class="string">" :"</span>)</span><br><span class="line">            print(<span class="string">"total cost = "</span> + str(Jt))</span><br><span class="line">            print(<span class="string">"content cost = "</span> + str(Jc))</span><br><span class="line">            print(<span class="string">"style cost = "</span> + str(Js))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save current generated image in the "/output" directory</span></span><br><span class="line">            save_image(<span class="string">"output/"</span> + str(i) + <span class="string">".png"</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save last generated image</span></span><br><span class="line">    save_image(<span class="string">'output/generated_image.jpg'</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_image</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本周作业分为了两个部分:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人脸识别&lt;/li&gt;
&lt;li&gt;风格迁移&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="homework" scheme="http://fangzh.top/tags/homework/"/>
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai笔记:(4-4)-- 特殊应用:人脸识别和神经风格转换</title>
    <link href="http://fangzh.top/2018/dl-ai-4-4/"/>
    <id>http://fangzh.top/2018/dl-ai-4-4/</id>
    <published>2018-10-12T10:55:15.000Z</published>
    <updated>2018-10-23T12:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg" alt></p><p>本周讲了CNN的两个特殊应用：人脸识别和神经风格转换。</p><a id="more"></a><h1 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h1><h2 id="Face-Verification-and-Face-Recognition"><a href="#Face-Verification-and-Face-Recognition" class="headerlink" title="Face Verification and Face Recognition"></a>Face Verification and Face Recognition</h2><p>人脸识别和人脸验证不一样。</p><p>人脸验证是输入一张图片，和这个人的ID或者名字，然后根据输入的图片判断这个人是不是对应这个ID，是个1对1的问题。</p><p>人脸识别是有K个人的数据库，然后输入一张人脸的图片，不确定他是哪一位，然后输出在K个人的数据库中对应的那个人，是1对K的问题。</p><p>所以人脸识别难度更高，而且精度要求更高，因为如果每张图片都是99%的精度，那么K个人就是K倍了，所以应该有99.9%以上的精度。</p><h2 id="One-shot-learning"><a href="#One-shot-learning" class="headerlink" title="One shot learning"></a>One shot learning</h2><p>人脸识别系统，通常都是只有一个人脸的样例，然后就能够成功的识别是不是这个人。这就是one shot learning，一次学习，单单通过一张照片就能识别这个人。</p><p>因此，在只有单个样本的情况下，并不能用之前的方法来实现这个识别系统。这里就需要有一个相似性函数。</p><p><strong>similarity函数：</strong></p><p>通过$d(img1,img2)$来表示两张图片的差异程度，如果d大于某个阈值，那么就表示差别很大，如果小于某个阈值，则认为是同一个人。</p><h1 id="Siamese网络"><a href="#Siamese网络" class="headerlink" title="Siamese网络"></a>Siamese网络</h1><p>那么如何计算这个$d(img1,img2)$呢？</p><p>可以利用Siamese网络来实现。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2v3cqwj20ou0dx0um.jpg" alt></p><p>如图，输入两张图片$x^{(1)},x^{(2)}$，经过一个卷积神经网络，去掉最后的softmax层，可以得到N维的向量，$f(x^{(1)}),f(x^{(2)})$，假设是128维，而N维的向量就相当于是对输入图片的的<strong>编码(encoding)。</strong></p><p>然后比较这两个向量之间的差值：</p><p>$$d(x1,x2) = ||f(x1) - f(x2)||^{2}_{2}$$</p><p>如果距离$d$很小，那表示这两张图片很相近，认为是同一个人。</p><p>如果距离$d$很大，那么表示这两张图片差别很大，不是同一个人。</p><h2 id="Triplet-loss"><a href="#Triplet-loss" class="headerlink" title="Triplet loss"></a>Triplet loss</h2><p>那么，我们之前说到，要得到输入图片的向量编码$f(x)$，是需要经过卷积神经网络的，那么卷积神经网络的参数如何确定呢？使用的方法就是Triplet loss损失函数，而后用梯度下降法进行迭代。</p><p>我们需要比较两组成对的图像 <strong>(Anchor, Positive, Negative)，简写(A,P,N)</strong></p><p>Anchor：表示要检测的目标图片</p><p>Positive：表示与anchor同个人的图片</p><p>Negative：表示与anchor不同个人的图片</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u6lkzj20j8072mz6.jpg" alt></p><p>所以我们希望A和P的距离小，A和N的距离大，因此有了如下不等式：</p><p>$$||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha \leq 0$$</p><p>这里这个公式与SVM的损失函数很类似，$\alpha$是表示margin边界，也就是增加$d(A,P)$和$d(A,N)$之间的差距。</p><p>而如果上面的不等式小于0，那说明是符合我们的要求的，如果是大于0，则要计入损失函数中，所以得到了Triplet loss的公式是：</p><p>$$L(A,P,N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha,0)$$</p><p>整个网络的代价函数就是把所有的图片损失加起来：</p><p>$$J = \sum L(A,P,N)$$</p><h2 id="三元组的选择"><a href="#三元组的选择" class="headerlink" title="三元组的选择"></a>三元组的选择</h2><p>每个三元组的选择是有讲究的，如果你要识别的是一个女人，然后对比的Negative是个老大爷，那么条件就很容易满足，学不到什么东西。所以应该尽量选择那些相似的图片进行每一组的训练，也就是：</p><p>$$d(A,P) \approx d(A,N) $$</p><p>选择的例子如下图，可以看到，每一个三元组对比的都是一些比较相似的图片：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u59x9j20kg0bv41m.jpg" alt></p><h2 id="脸部验证和二分类"><a href="#脸部验证和二分类" class="headerlink" title="脸部验证和二分类"></a>脸部验证和二分类</h2><p>除了之前说的用Triplet loss进行训练以外，还有别的方法来进行训练，也就是可以把Siamese网络当做一个二分类的问题。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u74asj20m50brgnh.jpg" alt></p><p>如图，输入两张图片，当计算得到了两个图片的向量编码后，求两张图片的距离，然后通过一个sigmoid函数，把他变成一个二分类问题，如果同个人，输出1，不同个人则输出0。其中，权重$W,b$都可以通过训练来得到。</p><p>这个时候，人脸识别问题就变成了一个<strong>监督学习</strong>的问题，在创建每一对训练集的时候，应该有对应的输出标签y。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2vnh63j20kx0bv41k.jpg" alt></p><h1 id="神经风格迁移"><a href="#神经风格迁移" class="headerlink" title="神经风格迁移"></a>神经风格迁移</h1><p>神经风格的迁移，就是输入两张图片，一张当做内容图片content，另一张当做风格图片style，输出的图片g兼具有一张的内容，和另一张的风格。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2wcuc4j20ow0ctqg6.jpg" alt></p><h2 id="卷积神经网络学什么？"><a href="#卷积神经网络学什么？" class="headerlink" title="卷积神经网络学什么？"></a>卷积神经网络学什么？</h2><p>在进行风格迁移前，我们需要了解我们的神经网络到底在学些什么东西，把中间的隐藏单元拎出来看看。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2wgh0ej21360l214v.jpg" alt></p><p>如上图，假设我们有一个卷积神经网络，要看到不同层的隐藏单元计算结果，怎么办？依次对各个层进行如下操作：</p><ul><li>在当前层挑选一个隐藏单元；</li><li>遍历训练集，找到最大化地激活了该运算单元的图片或者图片块；</li><li>对该层的其他运算单元执行操作。</li></ul><p>对于在第一层的隐藏单元中，其只能看到卷积网络的小部分内容，也就是最后我们找到的那些最大化激活第一层隐层单元的是一些小的图片块。我们可以理解为第一层的神经单元通常会寻找一些简单的特征，如边缘或者颜色阴影等。</p><p>而后随着层数的增加，隐藏层单元看到的东西就越来越复杂了：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2v7oo0j20ok06sagx.jpg" alt></p><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>对于神经风格迁移，我们的目标是由内容图片C和风格图片S，生成最终的风格迁移图片G。所以定义代价函数为：</p><p>$$J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G)$$</p><ul><li>$J_{content}(C, G)$: 代表生成图片G的内容和内容图片C的内容的相似度</li><li>$J_{style}(S, G)$: 代表生成图片G的内容和风格图片S的内容的相似度</li><li>$\alpha, \beta$: 两个超参数用来表示以上两者之间的权重</li></ul><p>首先随机初始化G的像素，然后进行梯度下降：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2v5on0j20or0dcte1.jpg" alt></p><h2 id="内容代价函数"><a href="#内容代价函数" class="headerlink" title="内容代价函数"></a>内容代价函数</h2><ul><li>首先假设我们使用第$l$层隐藏层来计算$J_{content}(C, G)$注意这里的$l$一般取在中间层，而不是最前面的层，或者最后层。因为太浅了啥也看不到，太深了就太像原图了。</li><li>使用一个预训练的卷积网络。（如，VGG或其他）</li><li>$a^{[l] (C)}$和$a^{[l] (G)}$分别代表内容图片C和生成图片G的$l$层的激活值；</li><li>内容损失函数$J_{content} = \frac{1}{2}||a^{[l] (C)} - a^{[l] (G)}||^2$</li></ul><h2 id="风格代价函数"><a href="#风格代价函数" class="headerlink" title="风格代价函数"></a>风格代价函数</h2><p>对于一个卷积网络中，我们选择网络的中间层$l$， 定义“Style”表示$l$层的各个通道激活项之间的相关性。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u35a3j20oa08wdgh.jpg" alt></p><p>那如何计算这个相关性呢？</p><p>假设我们在第$l$层有5个通道：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2undn5j20nu0d5ada.jpg" alt></p><p><strong>不同的通道之间代表着不同的神经元学习到的特征，</strong>如第一个通道（红色）可以表示含有垂直纹理的特征，第二个通道（黄色）表示区域中出现橙色的特征。</p><p>那么两个通道的相关性就表示图片中出现垂直纹理又出现橙色的可能性大小。</p><p>所以可以得到相关系数的矩阵<strong>“Gram Matrix</strong>：</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u2s3rj209f068q2x.jpg" alt></p><p>$i,j.k$表示神经元所在的高度，宽度和通道。也就是每个通道的神经元分别乘上另一个通道对应位置的神经元再求和即可得到这两个通道$k,k^{\prime}$的相关系数。这个矩阵的维度是$(n_{c}^{[l]},n_{c}^{[l]})$的，也就是第$l$层的通道数乘通道数的大小。</p><p>而代价函数即为两张图片中相关系数矩阵的差值求和，再取平均。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u1z3wj20im03mmx4.jpg" alt></p><h2 id="1D-to-3D-卷积"><a href="#1D-to-3D-卷积" class="headerlink" title="1D to 3D 卷积"></a>1D to 3D 卷积</h2><p>图片都是2D的卷积运算，其实还可以推广到1D和3D的情况。</p><p>典型的1D情况就是信号处理。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29ly1fw5q2u42q0j20o90dqabb.jpg" alt></p><p>3D情况就像CT的切片，是一层一层叠加起来的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/d40b6c29gy1fvrl8dyhm4j218w0nstdc.jpg&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;本周讲了CNN的两个特殊应用：人脸识别和神经风格转换。&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="http://fangzh.top/categories/AI/"/>
    
      <category term="Deep Learning" scheme="http://fangzh.top/categories/AI/Deep-Learning/"/>
    
    
      <category term="dl.ai" scheme="http://fangzh.top/tags/dl-ai/"/>
    
  </entry>
  
</feed>
