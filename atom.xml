<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yuanquanquan的个人博客 | 我愿做你光华中淡淡的一笔</title>
  
  <subtitle>我愿做你光华中淡淡的一笔</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanquanquan.top/"/>
  <updated>2020-04-13T04:30:43.598Z</updated>
  <id>http://yuanquanquan.top/</id>
  
  <author>
    <name>理科生写给世界的情书</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>EfficientDet:Scalable and Efficient Object Detection</title>
    <link href="http://yuanquanquan.top/2020/20200323/"/>
    <id>http://yuanquanquan.top/2020/20200323/</id>
    <published>2020-03-23T09:54:14.000Z</published>
    <updated>2020-04-13T04:30:43.598Z</updated>
    
    <content type="html"><![CDATA[<p>​    <img src="http://q8pl344z8.bkt.clouddn.com/EfficientDet-3.jpg" alt> 是Google Brain于去年11月份公布的目标检测算法族，涵盖轻量级到高精度的多个模型，提出了7种不同的网络结构<img src="http://q8pl344z8.bkt.clouddn.com/EfficientDet-2.jpg" alt>根据其复杂度不同，可以适应不同计算能力的平台，COCO数据集上达到 50.9 mAP，在coco榜单上算前无古人了，可以说是直接把YOLOV3，Mask-R CNN按在地上摩擦……..</p><p>​      前两天，Google Brain终于官方开源了，开源地址：(<a href="https://github.com/google/automl/tree/master/efficientdet)，位于Google新开的automl项目内，看样子以后这个项目还会有其他自动机器学习的算法开源。EfficientDet原出于论文" target="_blank" rel="noopener">https://github.com/google/automl/tree/master/efficientdet)，位于Google新开的automl项目内，看样子以后这个项目还会有其他自动机器学习的算法开源。EfficientDet原出于论文</a> EfficientDet: Scalable and Efficient Object Detection，开源页面显示，这篇论文已经被CVPR 2020接收</p><p>EfficientDet与EfficientNet的第一作者是同一人，可以说Google Brain在EfficientNet的基础上提出了针对于物体检测的可扩展模型架构EfficientDet。EfficientDet主要包括两方面贡献：</p><ol><li><p>双向FPN <strong><em>BiFPN（Bi-directional feature pyramid network</em></strong>，在simplified PANet上引入了lateral shortcut）和weighted-BiFPN（在不同scale的特征进行融合时引入注意力机制对不同来源的feature进行权重调整（per-feature / per-channel / pei-pixel），由实验来看带来的性能提升相比BiFPN较小，看论文总结BiFPN提高了4个百分点）</p></li><li><p>仿照EfficientNet中的Compound Scaling方法，对检测网络中的各个部分进行Compound Scaling（输入图像大小，backbone的深度、宽度，BiFPN的深度（侧向级联层数），cls/reg head的深度）。个人觉得这是最大的亮点，提出了目标检测网络联合调整复杂度的策略。</p></li></ol><p>除此之外值得一提的是EfficienDet中使用的模型缩放，作者结合BiFPN和特征融合策略设计了与YOLOv3精度相仿的EfficientDet-D0，按照一定的优化规则，在网络的深度、宽度、输入图像的分辨率上进行模型缩放，可在统一架构下得到适合移动端和追求高精度的多个模型，根据其复杂度不同，可以适应不同计算能力的平台。</p><h1 id="EfficientDet结构"><a href="#EfficientDet结构" class="headerlink" title="EfficientDet结构"></a><strong><em>EfficientDet结构</em></strong></h1><p><img src="http://q8pl344z8.bkt.clouddn.com/EfficientDet-1.png" alt="EfficientDet结构"><br>EfficientDet组合了backbone（使用了EfficientNet）和BiFPN（特征网络）和Box prediction net，上图整个框架就是EfficientDet的基本模型</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    &lt;img src=&quot;http://q8pl344z8.bkt.clouddn.com/EfficientDet-3.jpg&quot; alt&gt; 是Google Brain于去年11月份公布的目标检测算法族，涵盖轻量级到高精度的多个模型，提出了7种不同的网络结构&lt;img s
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CV" scheme="http://yuanquanquan.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>红外循迹传感器</title>
    <link href="http://yuanquanquan.top/2020/20200321/"/>
    <id>http://yuanquanquan.top/2020/20200321/</id>
    <published>2020-03-21T12:37:40.000Z</published>
    <updated>2020-04-13T04:25:48.833Z</updated>
    
    <content type="html"><![CDATA[<p>​      这次尝试制作了一个简单的红外循迹传感器，红外循迹传感器是专为轮式机器人设计的一款距离可调式避障传感器。其具有一对红外线发射与接收管，发射管发射出一定频率的红外线，当检测方向遇到障碍物（反射面）时，红外线反射回来被接收管接收，经过比较器电路处理之后，开关指示灯会亮起，同时信号输出接口输出数字信号（一个低电平信号），可通过电位器旋钮调节检测距离，有效距离范围2～30cm，工作电压为3.3V-5V，由于工作电压范围宽泛，在电源电压波动比较大的情况下仍能稳定工作，该传感器的探测距离可以通过电位器调节、具有干扰小、使用方便等特点，可以广泛应用于机器人避障、避障小车和黑白线循迹等众多场合。适合多种单片机、Arduino控制器、树莓派使用，安装到机器人上即可感测周围环境的变化。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/%E7%BA%A2%E5%A4%96-3.jpg" alt></p><p>电路工作说明</p><p>1、 当模块检测到前方障碍物信号时，开关指示灯点亮电平，同时OUT端口持续输出低电平信号,该模块检测距离2～30cm，检测距离可以通过电位器进行调节。</p><p>2、传感器模块输出端口OUT可直接与单片机IO口连接即可，也可以直接驱动一个5V继电器。</p><p>3、比较器采用LM393。</p><p>4、可采用3-5V直流电源对模块进行供电。当电源接通时，电源指示灯点亮。</p><p>接口说明</p><p>1 、VCC 外接3.3V-5V电压（可以直接与5v单片机和3.3v单片机相连）；</p><p>2 、GND 外接电源负极；</p><p>3 、OUT 为数字量输出接口（输出0或1高低电平）</p><p>Altium Designer画的原理图和PCB图如下：</p><p><img src="http://q8pl344z8.bkt.clouddn.com/%E7%BA%A2%E5%A4%96-1.png" alt></p><p><img src="http://q8pl344z8.bkt.clouddn.com/%E7%BA%A2%E5%A4%96-2.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​      这次尝试制作了一个简单的红外循迹传感器，红外循迹传感器是专为轮式机器人设计的一款距离可调式避障传感器。其具有一对红外线发射与接收管，发射管发射出一定频率的红外线，当检测方向遇到障碍物（反射面）时，红外线反射回来被接收管接收，经过比较器电路处理之后，开关指示灯会
      
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="单片机" scheme="http://yuanquanquan.top/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>SqueezeNAS:Fast neural architecture search for faster semantic segmentation</title>
    <link href="http://yuanquanquan.top/2020/20200313/"/>
    <id>http://yuanquanquan.top/2020/20200313/</id>
    <published>2020-03-13T03:32:41.000Z</published>
    <updated>2020-04-13T04:38:27.340Z</updated>
    
    <content type="html"><![CDATA[<h3 id="用NAS做语义分割，1-不使用代理，MobileNetV3先搜索分类任务作为代理，SqueezeNAS直接搜索语义分割。-2-使用资源感知损失，直接优化目标平台上的延迟，而不是优化MAC指标，它对于运行速度没有必然关系"><a href="#用NAS做语义分割，1-不使用代理，MobileNetV3先搜索分类任务作为代理，SqueezeNAS直接搜索语义分割。-2-使用资源感知损失，直接优化目标平台上的延迟，而不是优化MAC指标，它对于运行速度没有必然关系" class="headerlink" title="用NAS做语义分割，1.不使用代理，MobileNetV3先搜索分类任务作为代理，SqueezeNAS直接搜索语义分割。 2.使用资源感知损失，直接优化目标平台上的延迟，而不是优化MAC指标，它对于运行速度没有必然关系"></a>用NAS做语义分割，1.不使用代理，MobileNetV3先搜索分类任务作为代理，SqueezeNAS直接搜索语义分割。 2.使用资源感知损失，直接优化目标平台上的延迟，而不是优化MAC指标，它对于运行速度没有必然关系</h3><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>真实场景下，需要DNN在目标任务上准确率高，在目标计算平台上推断延迟低。NAS已经被用于低延迟的图像分类，但是其他任务还很少。这是第一篇用于密集语义分割的无代理硬件感知的搜索。在Cityscapes数据集上达到最好的性能。我们证明了利用NAS对任务和推断同时优化可以获得极大地性能提升。</p><h2 id="1-Introduction-and-Motivation"><a href="#1-Introduction-and-Motivation" class="headerlink" title="1. Introduction and Motivation"></a>1. Introduction and Motivation</h2><p>在做非图像分类任务（如语义分割或目标检测）时，流行的方法是结构迁移：从一个图像分类网络开始，在网络末尾添加一些针对特定任务的层。</p><p>这种结构迁移的主流主要是一些传统假设，我们罗列一些并证明为什么他们已经过时了。</p><ul><li><strong>假设1： 在ImageNet上准确率最高的网络也应该是目标任务上准确率最好的骨干网络</strong></li></ul><p>实际上，ImageNet的准确率与目标任务准确率关系不太大。SqueezeNet在ImageNet上准确率低于VGG，但是对上定位图像相似部分的任务上更好。正确的网络设计取决于目标任务。</p><ul><li><strong>假设2： NAS成本过高</strong></li></ul><p>实际上，有些方法的确需要数千个GPU天，但是最近的“supernetwork”方法例如DARTS和FBNet可以在10个GPU天上取得最优结果。</p><ul><li><strong>假设3： 低MAC（Fewer multiply-accumulate）运算在目标平台上有较低的延迟</strong></li></ul><p>实际上，有工作证明了相同的平台上，同样的MAC可以有10x的延迟差异。取决于处理器和内核实现，即使是相同的MAC也有不同的速度。</p><p>为了在目标计算平台和任务上获得更低的延迟，更高的准确率：</p><ol><li>直接对目标任务运行NAS，例如目标检测、语义分割，不要去优化代理任务，如图像分类。</li><li>使用现代的基于supernetwork的NAS，相信搜索可以快速收敛。</li><li>让NAS同时优化准确率和延迟。</li></ol><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h2><p>【略】</p><h2 id="3-Architecture-Search-Space"><a href="#3-Architecture-Search-Space" class="headerlink" title="3. Architecture Search Space"></a>3. Architecture Search Space</h2><p>我们探索了顺序反向残差模块的语义分割网络编码器的空间。这些块参数化如图2。每次结构搜索，我们约束宏观结构，对每个块寻找最优参数。搜索空间和FBNet，MobileNetV2，MobileNetV3相似，这样可以直接比较对分割优化的网络和他们对分类优化的网络。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/squeeze-nas-fig2.png" alt></p><p>我们网络的基本结构如图1，解码器使用编码器的输出和低层特征。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/squeeze-nas-fig1.png" alt></p><h3 id="3-1-Constrained-Macro-Architecture"><a href="#3-1-Constrained-Macro-Architecture" class="headerlink" title="3.1. Constrained Macro-Architecture"></a>3.1. Constrained Macro-Architecture</h3><p>搜索三个空间，Small, Large, XLarge。为了定义结构空间，先约束编码网络的宏观结构。宏观结构描述了编码器的模块数量N，解码器也是这么多。对于每个模块，固定输入和输出通道，$c_{in} $和$c_{out}$每个块的深度卷积层步长使用1或2。由于运行跳跃连接，所以最终的层数可能小于N。</p><p>在Small和Large的搜索空间，使用LR-ASPP解码器。在XLarge搜索空间，我们使用完全深度卷积的ASPP变种。</p><h3 id="3-2-Block-Search-Space"><a href="#3-2-Block-Search-Space" class="headerlink" title="3.2. Block Search Space"></a>3.2. Block Search Space</h3><p>在每个宏观搜索空间，NAS挑选每个块最优的超参数，或者替换为无运算的跳跃连接。如图2，超参数定义了 1*1 卷积是否分组，深度绝技是否膨胀2倍，深度卷积的核大小k，膨胀率e。如图7，我们挑选12个可能的配置。</p><h2 id="4-Neural-Architecture-Search-Algorithm"><a href="#4-Neural-Architecture-Search-Algorithm" class="headerlink" title="4. Neural Architecture Search Algorithm"></a>4. Neural Architecture Search Algorithm</h2><p>把结构搜索看做是复杂supernetwork的路径选择问题，这样确定的结构可以看做是supernetwork的某些路径。如图3，我们定义supernetwork为superblocks的序列，图4是一个例子。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/squeeze-nas-fig3-4.png" alt></p><p>同时优化卷积权重<em>w</em>和结构参数<em>θ</em>, 优化损失函数为L<em>(</em>θ<em>,</em>w<em>)=</em>L<strong>P<em>(</em>θ<em>,</em>w<em>)+</em>α<em>∗</em>L</strong>E<em>(</em>θ*)</p><p>$L_p$表示特定问题的损失，$L_E$是资源感知损失项，超参数<em>α</em>控制两个平衡。由于本工作关注与语义分割，是$L_p$像素级别的交叉熵。对于，$L_E$我们同时实验了目标平台的推断延迟和Multiply-Accumulates的估计值。</p><p><code>表示特定问题的损失，</code><em>α</em><code>LP</code>是像素级别的交叉熵。对于</p><p><code></code>我们同时实验了目标平台的推断延迟和Multiply-Accumulates的估计值。</p><h3 id="4-1-Gumbel-Softmax"><a href="#4-1-Gumbel-Softmax" class="headerlink" title="4.1. Gumbel-Softmax"></a>4.1. Gumbel-Softmax</h3><p>为了让supernetwork的优化和计算可跟踪，每个superblock独立挑选一个候选块。这样，可以把这个选择看成是对独立类别分布的采样，把superblock i的候选块j概率记作<em>p</em>(<em>i</em>,<em>j</em>) , 用softmax定义：$p ( i , j | \theta ) = \frac { e ^ { \theta _ { i , j } } } { \sum _ { j } ^ { 13 } e ^ { \theta _ { i , j } } }$</p><p>类别分布很难高效优化，所以我们使用Gumbel-Softmax松弛。Gumbel-Softmax分布由稳定参数t控制，t趋近于0，Gumbel-Softmax分布等价于类别分布，稳定参数从5.0到1.0退火。</p><h2 id="4-2-Early-Stopping"><a href="#4-2-Early-Stopping" class="headerlink" title="4.2. Early Stopping"></a>4.2. Early Stopping</h2><p>我们的supernetwork方法中，优化过程需要计算每一个候选的block，无论学到的结构分布是什么。当最优的网络结构收敛的时候，性能差的候选块虽然选择的概率低，但是依然需要继续计算。所以当候选概率小于0.5%的时候，直接移除。虽然有可能低概率的候选块可能后面是最优的，但是实践中没有发现这种情况。该优化可以减少一半的搜索时间。</p><h2 id="4-3-Resource-Aware-Architecture-Search"><a href="#4-3-Resource-Aware-Architecture-Search" class="headerlink" title="4.3. Resource-Aware Architecture Search"></a>4.3. Resource-Aware Architecture Search</h2><p>定义资源感知损失为：</p><p>$L _ { E } ( \theta ) = \sum _ { j } ^ { N } \sum _ { i } ^ { 13 } p \left( i , j | \theta _ { i } \right) C ( i , j )$</p><p><em>C</em>(<em>i</em>, <em>j</em>)表示网络块i选择候选j的资源成本。对每个块独立建模资源成本。</p><h2 id="5-Experiments-and-Results"><a href="#5-Experiments-and-Results" class="headerlink" title="5. Experiments and Results"></a>5. Experiments and Results</h2><p>证明两个关键点：第一，NAS是可以产生搞准确率低延迟网络的工具。第二，优化与硬件无直接联系的指标如MAC不是个合适的代理，可能导致局部最优。</p><p>沿用Small, Large, XLarge三种搜索空间。先用NAS在每个空间上对MAC进行搜索，然后在嵌入式设备上查看这些低MAC的网络的延迟作为基线。最后再搜索优化硬件感知的延迟找到3个新的网络。</p><h3 id="5-1-Hardware-Agnostic-Search"><a href="#5-1-Hardware-Agnostic-Search" class="headerlink" title="5.1. Hardware-Agnostic Search"></a>5.1. Hardware-Agnostic Search</h3><p>对于与硬件无关的结构搜索，使用NAS对MAC进行优化，在MAC和mIOU上找到pareto最优，根据查找表计算每个候选块j的MAC，使得<em>C</em>(<em>i</em>,<em>j</em>)=<em>M<strong>A</strong>C<strong>S</strong>i</em>,<em>j</em></p><p>。对每个搜索空间搜索找到最优的SqueezeNAS-MAC网络，结果如下表。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/squeeze-nas-tab1.png" alt></p><h3 id="5-2-Hardware-Aware-Search"><a href="#5-2-Hardware-Aware-Search" class="headerlink" title="5.2. Hardware-Aware Search"></a>5.2. Hardware-Aware Search</h3><p>使用相同的NAS算法和搜索空间，但是使用延迟优化目标：<em>C</em>(<em>i</em>,<em>j</em>)=<em>L<strong>a</strong>t<strong>e</strong>n<strong>c</strong>y**i</em>,<em>j</em></p><p>。为了计算每个模块j在候选j上的延迟，我们测量了目标平台上所有候选的推断时间，最后得到三个SqueezeNAS-LAT网络。结果见表1。</p><h3 id="5-3-Implementation"><a href="#5-3-Implementation" class="headerlink" title="5.3. Implementation"></a>5.3. Implementation</h3><h3 id="5-4-Results"><a href="#5-4-Results" class="headerlink" title="5.4. Results"></a>5.4. Results</h3><h2 id="6-Network-Analysis"><a href="#6-Network-Analysis" class="headerlink" title="6. Network Analysis"></a>6. Network Analysis</h2><p>比较三者的模块选择：优化延迟的网络（Latency-aware），优化MAC的网络（MAC-aware），MobileNetV3。由于这三个都使用反向残差块，可以把MobileNetV3的块放入13个候选中，如图7。这里没有考虑SE块。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/squeeze-nas-fig7.png" alt></p><p>可视化网络如图8，【略】</p><p><img src="http://q8pl344z8.bkt.clouddn.com/squeeze-nas-fig8.png" alt> <img src="http://q8pl344z8.bkt.clouddn.com/squeeze-nas-fig9.png" alt> <img src="http://q8pl344z8.bkt.clouddn.com/squeeze-nas-fig10.png" alt></p><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>第一、做无代理的语义分割搜索，我们的NAS生成的SqueezeNAS系列模型，相比MobileNet V3达到优越的延迟-准确率平衡。这种优越（至少部分）来自于MobileNetV3是用NAS优化图像分类作为代理实现语义分割任务的。</p><p>第二、虽然MobileNetV3作者使用了几千个GPU天，但是我们的方法每次搜索只需要7-15个GPU天。也就是说，基于supernetwork的NAS在8个GPU上不需要一个周末就可以跑出来最好的结果。</p><p>第三、我们做了两类NAS实验，一个搜索低MAC模型，一个搜索目标平台上低延迟模型。第二个我们获得了非常快同时准确率高的模型。最后，考虑到芯片的速度提升和计算平台的省级，NAS可以继续获得更低的延迟</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;用NAS做语义分割，1-不使用代理，MobileNetV3先搜索分类任务作为代理，SqueezeNAS直接搜索语义分割。-2-使用资源感知损失，直接优化目标平台上的延迟，而不是优化MAC指标，它对于运行速度没有必然关系&quot;&gt;&lt;a href=&quot;#用NAS做语义分割，1
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CV" scheme="http://yuanquanquan.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>【Object Detection】-YOLOV1</title>
    <link href="http://yuanquanquan.top/2020/20200312/"/>
    <id>http://yuanquanquan.top/2020/20200312/</id>
    <published>2020-03-12T02:36:03.000Z</published>
    <updated>2020-04-13T04:38:17.249Z</updated>
    
    <content type="html"><![CDATA[<p><strong>YOLO算法简介</strong></p><p>YOLO十分简单，一个网络同时对多个物体进行分类和定位，没有proposal的概念，是<strong><em>one-stage</em></strong>实时检测网络的里程碑，标准版在TitanX达到45 fps，快速版达到150fps，但精度不及当时的SOTA网络</p><p>YOLO算法使用深度神经网络进行对象的位置检测以及分类，主要的特点是速度够快，而且准确率也很高，采用直接预测目标对象的边界框的方法，将候选区和对象识别这两个阶段合二为一。</p><p>Yolo算法不再是窗口滑动了，而是直接将原始图片分割成互不重合的小方块，然后通过卷积最后生产这样大小的特征图，基于上面的分析，可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标，这就是Yolo算法的朴素思想，而YOLOv3算法再以往的结构上做出了改进，增加了多尺度检测，以及更深的网络结构darknet53,这是比较主要的改进，还有某些细节上的变动。</p><p><em>Unified Detection</em></p><p><img src="http://q8pl344z8.bkt.clouddn.com/yolov1%EF%BC%881%EF%BC%89.png" alt></p><p>将输入分为S*S的格子，如果GT的中心点在格子中，则格子负责该GT的预测：</p><p>对于PASCAL VOC数据集来说，设定s=7如图所示，分为7*7个小格子，每个格子预测两个bounding box。</p><p>如果一个目标的中心落入一个网格单元中，该网格单元负责检测该目标。</p><p>对每一个切割的小单元格预测（置信度，边界框的位置），每个bounding  box需要4个数值来表示其位置，(Center_x,Center_y,width,height)，即bounding  box的中心点的x坐标，y坐标，bounding box的宽度，高度)</p><p>置信度定义为<strong>是否存在目标</strong>与<strong>iou值</strong>的乘积，置信度可以反应格子是否包含物体以及包含物体的概率，无物体则为0，有则为IOU</p><p>$\text { Confidence } = \operatorname { Pr } ( \text { Object } ) * \text { IOU } _ { \text {pred } } ^ { \text {truth } }$</p><p>还要得到分类的概率结果；20个分类每个类别的概率。在测试时，将单独的bbox概率乘以类的条件概率得到最终类别的概率，综合了类别和位置的准确率</p><p> <strong>Network Design</strong></p><p>YOLO采用单个的卷积神经网络进行预测，YOLO的整个结构就是输入图片经过神经网络的变换得到一个输出的张量 。   步骤如下：</p><p>（1）骨干网络前20层接average-pooling层和全连接层进行ImageNet预训练，检测网络训练将输入从224×224增加到448×448</p><p>（2）在图像 上运行单个卷积网络</p><p>（3）由模型的置信度对所得到的检测进行阈值处理</p><p>首先，YOLO速度非常快。由于我们将检测视为回归问题，所以不需要复杂的流程。测试时在一张新图像 上简单的运行我们的神经网络来预测检测</p><p>其次，YOLO在进行预测时，会对图像进行全面地推理。与基于滑动窗口和区域提出的技术不同，YOLO在训练期间和测试时会看到整个图像，所以它隐式地编码了</p><p>关于类的上下文信息以及它们的外观。快速R-CNN是一种顶级的检测方法，但是它看不到更大的上下文信息，所以在图像中会将背景块误检为目标。与快速R-CNN相比，YOLO的背景误检数量少了一半</p><p>然后，由于YOLO具有高度泛化能力，因此在应用于新领域或碰到意外的输入时不太可能出故障。</p><p> <img src="http://q8pl344z8.bkt.clouddn.com/yolov1%EF%BC%882%EF%BC%89.png" alt></p><p>所使用的卷积结构如图所示：受到GoogLeNet图像分类模型的启发。网络有24个卷积层，后面是2个全连接层，最后输出层用线性函数做激活函数，其它层激活函数都是Leaky ReLU。</p><p>我们 只使用1×1降维层，后面是3×3卷积层，</p><p><strong>Training</strong></p><p>最后一层使用ReLU，其它层使用leaky ReLU</p><p>YOLO的损失函数定义如下：</p><p>$\begin{array} { c } \lambda _ { \text {coord } } \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } \mathbb { 1 } _ { i j } ^ { \text {obj } } \left[ \left( x _ { i } - \hat { x } _ { i } \right) ^ { 2 } + \left( y _ { i } - \hat { y } _ { i } \right) ^ { 2 } \right] \ \quad + \lambda _ { \text {coord } } \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } \mathbb { 1 } _ { i j } ^ { \text {obj } } \left[ ( \sqrt { w _ { i } } - \sqrt { \hat { w } _ { i } } ) ^ { 2 } + ( \sqrt { h _ { i } } - \sqrt { \hat { h } _ { i } } ) ^ { 2 } \right] \end{array}$</p><p>$\begin{array} { l } \quad + \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } 1 _ { i j } ^ { \mathrm { obj } } \left( C _ { i } - \hat { C } _ { i } \right) ^ { 2 } \ + \lambda _ { \mathrm { nobbj } } \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } 1 _ { i j } ^ { \mathrm { noobj } } \left( C _ { i } - \hat { C } _ { i } \right) ^ { 2 } \end{array}$</p><p>$+ \sum _ { i = 0 } ^ { S ^ { 2 } } \mathbb { 1 } _ { i } ^ { \mathrm { obj } } \sum _ { c \in \text { classes } } \left( p _ { i } ( c ) - \hat { p } _ { i } ( c ) \right) ^ { 2 }$</p><p> 损失函数如上图，一个GT只对应一个bounding box。由于训练时非目标很多，定位的训练样本较少，所以使用权重$\begin{array} { c } \lambda _ { \text {coord } }\end{array} $和$\begin{array} { c } \lambda _ { \text {nobjd} }\end{array} $来加大定位的训练粒度，包含3个部分：</p><p><strong>第一部分</strong>为坐标回归，使用平方差损失，为了使得模型更关注小目标的小误差，而不是大目标的小误差，对宽高使用了平方根损失进行变相加权。这里$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$指代当前b box是否负责GT的预测，需要满足2个条件，首先GT的中心点在该b box对应的格子中，其次该b box要是对应的格子的个box中与GT的IOU最大</p><p><strong>第二部分</strong>为b box置信度的回归，$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$跟上述一样，$\mathbb { 1 } _ { i j } ^ { \mathrm { nooobj } }$为$$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$$的b box，由于负样本数量较多，所以给了个低权重。若有目标，$\begin{array} hat { C }\end{array}$实际为IOU，虽然很多实现直接取1.</p><p><strong>第三部</strong>分为分类置信度，相对于格子而言，$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$指代GT中心是否在格子中</p><p><img src="http://q8pl344z8.bkt.clouddn.com/yolov1%EF%BC%883%EF%BC%89.png" alt="如图所示"></p><p>YOLO在ImageNet分类任务上以一半的分辨率（224*224的输入图像）预训练卷积层，然后将分辨 率加倍来进行检测。</p><p>训练中采用了drop out和数据增强（data augmentation）来防止过拟合.</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>  开创性的one-stage detector，在卷积网络后面接两个全连接层进行定位和置信度的预测，并设计了一个新的轻量级主干网络，虽然准确率与SOTA有一定距离，但是模型的速度真的很快<br>  作者提到了YOLO的几点局限性：</p><ul><li>每个格子仅预测一个类别，两个框，对密集场景预测不好</li><li>对数据依赖强，不能泛化到不常见的宽高比物体中，下采样过多，导致特征过于粗糙</li><li>损失函数没有完成对大小物体进行区别对待，应该更关注小物体的误差，因为对IOU影响较大，定位错误是模型错误的主要来源</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;YOLO算法简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;YOLO十分简单，一个网络同时对多个物体进行分类和定位，没有proposal的概念，是&lt;strong&gt;&lt;em&gt;one-stage&lt;/em&gt;&lt;/strong&gt;实时检测网络的里程碑，标准版在TitanX达到45 
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="复健计划" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%A4%8D%E5%81%A5%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="Object Detection" scheme="http://yuanquanquan.top/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>【Object Detection】-Faster R-CNN</title>
    <link href="http://yuanquanquan.top/2020/20200311403/"/>
    <id>http://yuanquanquan.top/2020/20200311403/</id>
    <published>2020-03-11T03:40:03.000Z</published>
    <updated>2020-04-16T03:58:32.066Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>写在前面</em></strong>：之前两个月没有看过cv方向的东西…..所以打算开个系列来写目标检测，图像分割的爆款算法，算是回顾一遍吧，当然一些太经典鼻祖级的模型就懒得看了……于是这个系列就决定叫复健计划了….目标检测的第一篇就从Faster R-CNN开始吧，Faster R-CNN 是目标检测中的一个很经典的two stage算法，许多其他的目标检测算法都会运用到Faster R-CNN的部分结构或思想。而且了解Faster R-CNN对理解其他R-CNN系列网络都有一定的帮助，包括Mask R-CNN，Stereo R-CNN 等等。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20200416114808.png" alt></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Faster R-CNN由R-CNN，Fast R-NN改进演变而来，相对于前两者，Faster R-CNN具有训练速度快，消耗内存减少，精度与检测速度都有大幅提升的优点。</p><p>先来一张Faster R-CNN的网络基本结构图</p><p><img src="http://q8pl344z8.bkt.clouddn.com/faster%20r-cnn%281%29.webp" alt></p><p>个人认为网络主要分为五个部分：</p><ul><li>提取feature map（特征图）的<strong>Conv layer</strong>。该部分使用VGG-net作为预处理网络，运用多个conv，relu，pooling层提取图像特征图，为后面的网络提供图像信息。</li><li><strong>anchor</strong>的生成，Faster R-CNN对图像生成一系列的anchor，作为目标检测的先验框，用于多尺度预测，并在后面使用bounding box regression对其位置进行修正。</li><li><strong>RPN</strong>（Region Proposal Network），在feature map上的每个点生成anchor，然后将其映射回原图，对原图中的anchor进行修正、筛选，提取该区域的图像(region proposal)，也就是所谓的ROI，送进ROI pooing层。</li><li><strong>ROI pooing</strong>，对featuer map中的ROI划分为Pool_h*Poo_w(ROI pooling后特征图的高和宽)）个网格，对每一个网格进行maxpooling。</li><li>使用<strong>full connection</strong>，<strong>softmax</strong>对ROI进行分类与bounding box回归，确定bounding box的位置。</li></ul><p>下面从这五个部分进行说明。先上一个网上总结的Faster R-CNN的结构图。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/faster%20r-cnn%EF%BC%882%EF%BC%89.webp" alt></p><h2 id="Conv-layer"><a href="#Conv-layer" class="headerlink" title="Conv layer"></a>Conv layer</h2><p>Conv layers包含3种层，分别是conv层，pooling层，relu层。<strong>conv</strong>层的卷积核大小都是3 * 3 , 步长都为1，并且都做了扩边处理，也就是经过conv层后图像的大小没有改变，只是深度改变。<strong>pooling</strong> 层的kernel_size=2,步长也为2，也就是说每经过一次pooling层后，图像尺寸减小一半。</p><p>Faster RCNN在将图片传入网络之前，会将图片缩放为<strong>M * N</strong>(VGG 为 800 * 600），从原图到feature map一共经过4次pooling，也就是feature map的大小为（int(M/16)，int(N/16)）。</p><h2 id="anchor"><a href="#anchor" class="headerlink" title="anchor"></a>anchor</h2><p>anchor其实是在图像上的一个个先验框，用来对后面检测框的修正以及region proposal。这里说一下anchor的生成过程。</p><p>先来看看作者的图</p><p><img src="http://q8pl344z8.bkt.clouddn.com/faster%20r-cnn%EF%BC%883%EF%BC%89.webp" alt></p><p>可以看到，anchor的长宽比有[0.5, 1, 2]三种比例，每种anchor有[8, 16, 32]三个尺度比例，所以anchors一共有3 * 3 = 9个anchor。</p><p>生成anchor的主要分为一下步骤：</p><ul><li>首先设置一个16 <em> 16的窗口（因为feature map尺寸为原图的1/16，所以一个feature map上的点对应原图上16 </em> 16 的区域），计算的到[x_ctr, y_ctr, w, h]，也就是anchor的中心点坐标以及长宽4个量。</li><li>然后计算anchor的面积size = w <em> h，将size分别除[0.5, 1, 2]3种比例，再分别对3个新的sizes开根号作为新的anchor的3个w，再将w </em> [0.5, 1, 2]得到h，这样就得到3个anchor的长宽。</li><li>将3个anchor长宽分别再乘3个尺度比例，这样就得到9个anchor，再将anchor的表示转换为左上角和右下角的坐标[x_l, y_l, x_r, y_r]。</li></ul><p>此时每个feature map上的点都有9个anchor，也就是一共有（800/16）<em>（600/16） </em> 9=17100个anchor。再将这些anchor通过原图与feature map的映射关系，将其anchor映射回原图。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/faster%20r-cnn%EF%BC%884%EF%BC%89.webp" alt></p><h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><p>RPN的主要结构如图</p><p><img src="http://q8pl344z8.bkt.clouddn.com/faster%20r-cnn%EF%BC%885%EF%BC%89.webp" alt></p><p>首先将feature map在经过一次conv卷积（这里设得到的网络层为conv5_3)，然后分开两条线进行：</p><ul><li>先将conv5_3使用1 <em> 1 </em> 18的卷积核对anchor进行softmax分类，将anchor分为postivte和negative（rpn_cls_prob)，二分类。（18 = 2（positiv and negative）* 9（9个anchor）。</li><li>用于预测bounding box的坐标偏移值。</li><li>在Proposal层综合im_info（主要用来计算proposal的坐标以及限制proposal的大小以免超出图像边框）、rpn_box_pred 和rpn_cls_prob选择和提取ROI。</li></ul><h3 id="softmax-分类"><a href="#softmax-分类" class="headerlink" title="softmax 分类"></a>softmax 分类</h3><p>conv5_3经过1 <em> 1 </em> 18 卷积后，维度变为[W, H, 18]。softmax就是要将每个点的9个anchor进行二分类（positive和negative）。softmax前后各有一次reshape，其实只是为了让分类更方便而已，这是训练的一些trick。</p><h3 id="bounding-box-predict"><a href="#bounding-box-predict" class="headerlink" title="bounding box predict"></a>bounding box predict</h3><p>conv5_3经过1 <em> 1 </em> 36 卷积变为[W, H, 36]，第三个维度为每个anchor的2个坐标的偏移量，用于后面的bounding box regression。</p><h3 id="bounding-box-regression"><a href="#bounding-box-regression" class="headerlink" title="bounding box regression"></a>bounding box regression</h3><p>在训练时需要对anchor进行转换才能贴合GT_bbox。怎样转换呢，最简单的做法就是平移加缩放。</p><p>下面是转换的关系式：<br>$$<br>\begin{aligned}<br>&amp;t_{x}=\left(x-x_{\mathrm{a}}\right) / w_{\mathrm{a}}, \quad t_{\mathrm{y}}=\left(y-y_{\mathrm{a}}\right) / h_{\mathrm{a}}\<br>&amp;t_{\mathrm{w}}=\log \left(w / w_{\mathrm{a}}\right), \quad t_{\mathrm{h}}=\log \left(h / h_{\mathrm{a}}\right)\<br>&amp;\begin{aligned}<br>t_{\mathrm{x}}^{<em>} &amp;=\left(x^{</em>}-x_{\mathrm{a}}\right) / w_{\mathrm{a}}, \quad t_{\mathrm{y}}^{<em>}=\left(y^{</em>}-y_{\mathrm{a}}\right) / h_{\mathrm{a}} \<br>t_{\mathrm{w}}^{<em>} &amp;=\log \left(w^{</em>} / w_{\mathrm{a}}\right), \quad t_{\mathrm{h}}^{<em>}=\log \left(h^{</em>} / h_{\mathrm{a}}\right)<br>\end{aligned}<br>\end{aligned}<br>$$<br>损失函数为：<br>$$<br>\begin{aligned}<br>L\left(\left{p_{i}\right},\left{t_{i}\right}\right) &amp;=\frac{1}{N_{c l s}} \sum_{i} L_{c l s}\left(p_{i}, p_{i}^{<em>}\right) \<br>&amp;+\lambda \frac{1}{N_{r e g}} \sum_{i} p_{i}^{</em>} L_{r e g}\left(t_{i}, t_{i}^{*}\right)<br>\end{aligned}<br>$$</p><h2 id="ROI-proposal"><a href="#ROI-proposal" class="headerlink" title="ROI proposal"></a>ROI proposal</h2><p>ROI prosposal负责综合所有的关于anchor的变换和对softmax的分类positive anchor，在feature map上计算出精确的ROI，将其送入后面的ROI Pooling层。</p><p>主要步骤为：</p><ul><li>对softmax后的anchor按score进行排序，提取前N个score的anchor。</li><li>对这些anchor进行修正。</li><li>修正大于图像边缘的anchor。</li><li>对w或h小于设定阈值的anchor剔除。</li></ul><p><img src="http://q719r0aui.bkt.clouddn.com/fast%20r-cnn%288%29.webp" alt></p><p>​         <em>最后传入的ROI类似这样</em></p><p>至此，RPN的任务到此完成。</p><h2 id="ROI-pooling"><a href="#ROI-pooling" class="headerlink" title="ROI pooling"></a>ROI pooling</h2><p>Faster RCNN最后的Classification和bounding box 的预测需要用到全连接层，所以在将图片传入全连接层时需要将其变为固定大小。但是一般输入的ROI大小都不固定，如果利用采样的方法进行变换为所需要的大小，会对图像的结构产生影响。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/fast%20r-cnn%289%29.jpg" alt></p><p><img src="http://q8pl344z8.bkt.clouddn.com/fast%20r-cnn%2810%29.jpg" alt></p><p>为了解决这个问题，Faster RCNN提出了ROI pooling的方法。</p><p>具体方法为：</p><ul><li><p>将ROI划分为pool_h和pool_w个网格。</p></li><li><p>每个网格的起始和结束坐标计算方法为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int hstart = static_cast&lt;int&gt;(floor(ph * bin_size_h));        int wstart = static_cast&lt;int&gt;(floor(pw * bin_size_w));        int hend = static_cast&lt;int&gt;(ceil((ph + 1) * bin_size_h));        int wend = static_cast&lt;int&gt;(ceil((pw + 1) * bin_size_w));         //其中pw，ph是每个网格的坐标值</span><br></pre></td></tr></table></figure></li><li><p>计算完之后的每个网格可能会有重叠</p></li><li><p>将每个网格进行max pooling操作，这样就得到固定大小的图了。</p><p><strong>假设输出为2 * 2 大小</strong></p></li></ul><p><img src="http://q8pl344z8.bkt.clouddn.com/fast%20r-cnn%2811%29.jpg" alt></p><p><img src="http://q8pl344z8.bkt.clouddn.com/fast%20r-cnn%2812%29.webp" alt></p><h2 id="Classification-，bounding-box-predict"><a href="#Classification-，bounding-box-predict" class="headerlink" title="Classification ，bounding box predict"></a>Classification ，bounding box predict</h2><p>这个没什么好说的了，就是使用全连接层和softmax层进行分类和预测bounding box坐标值。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/fast%20r-cnn%2813%29.webp" alt></p><table><thead><tr><th>方法</th><th>创新</th><th>缺点</th><th>改进</th></tr></thead><tbody><tr><td>R-CNN (Region-based Convolutional Neural Networks)</td><td>1、SS提取RP； 2、CNN提取特征； 3、SVM分类； 4、BB盒回归。</td><td>1、 训练步骤繁琐（微调网络+训练SVM+训练bbox）； 2、 训练、测试均速度慢 ； 3、 训练占空间</td><td>1、 从DPM HSC的34.3%直接提升到了66%（mAP）； 2、 引入RP+CNN</td></tr><tr><td>Fast R-CNN (Fast Region-based Convolutional Neural Networks)</td><td>1、SS提取RP； 2、CNN提取特征； 3、softmax分类； 4、多任务损失函数边框回归。</td><td>1、 依旧用SS提取RP(耗时2-3s，特征提取耗时0.32s)； 2、 无法满足实时应用，没有真正实现端到端训练测试； 3、 利用了GPU，但是区域建议方法是在CPU上实现的。</td><td>1、 由66.9%提升到70%； 2、 每张图像耗时约为3s。</td></tr><tr><td>Faster R-CNN (Fast Region-based Convolutional Neural Networks)</td><td>1、RPN提取RP； 2、CNN提取特征； 3、softmax分类； 4、多任务损失函数边框回归</td><td>1、 还是无法达到实时检测目标； 2、 获取region proposal，再对每个proposal分类计算量还是比较大。</td><td>1、 提高了检测精度和速度； 2、 真正实现端到端的目标检测框架； 3、 生成建议框仅需约10ms。</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;&lt;em&gt;写在前面&lt;/em&gt;&lt;/strong&gt;：之前两个月没有看过cv方向的东西…..所以打算开个系列来写目标检测，图像分割的爆款算法，算是回顾一遍吧，当然一些太经典鼻祖级的模型就懒得看了……于是这个系列就决定叫复健计划了….目标检测的第一篇就从Faster 
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="复健计划" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%A4%8D%E5%81%A5%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="Object Detection" scheme="http://yuanquanquan.top/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>iMX287A多种方法实现流水灯效果</title>
    <link href="http://yuanquanquan.top/2020/20200310/"/>
    <id>http://yuanquanquan.top/2020/20200310/</id>
    <published>2020-03-10T15:15:24.000Z</published>
    <updated>2020-04-13T04:13:04.342Z</updated>
    
    <content type="html"><![CDATA[<p>序：鸽了快两个月没写博客，其实也是因为这两月都没怎么学习（逃，不过确实接触了一些新东西，在B站和知乎看到了一个电科毕业的大佬，一个人能抵一个项目组……于是也燃起了学习单片机的兴趣，想在做程序员的同时做一个业余电子工程师，这段时间零零散散做了一些东西</p><p>一张用来装逼的PCB名片(<a href="https://github.com/Bazingaliu/PCB-card" target="_blank" rel="noopener">https://github.com/Bazingaliu/PCB-card</a>)</p><p>一个根据经典开源飞控pix2.4.6原理图，用ad设计了一个仿制版(<a href="https://github.com/Bazingaliu/PCB-px41" target="_blank" rel="noopener">https://github.com/Bazingaliu/PCB-px41</a>)</p><p>一个用EMMC芯片实现的U盘(<a href="https://github.com/Bazingaliu/PCB_EMMC" target="_blank" rel="noopener">https://github.com/Bazingaliu/PCB_EMMC</a>)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;序：鸽了快两个月没写博客，其实也是因为这两月都没怎么学习（逃，不过确实接触了一些新东西，在B站和知乎看到了一个电科毕业的大佬，一个人能抵一个项目组……于是也燃起了学习单片机的兴趣，想在做程序员的同时做一个业余电子工程师，这段时间零零散散做了一些东西&lt;/p&gt;
&lt;p&gt;一张用来装
      
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="单片机" scheme="http://yuanquanquan.top/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>2019年计算机视觉综述论文汇聚</title>
    <link href="http://yuanquanquan.top/2019/20191230/"/>
    <id>http://yuanquanquan.top/2019/20191230/</id>
    <published>2019-12-30T13:23:38.000Z</published>
    <updated>2019-12-30T14:12:41.751Z</updated>
    
    <content type="html"><![CDATA[<p>​        <img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=4102261544,420722754&amp;fm=26&amp;gp=0.jpg" alt="img"></p><p>​        本文整理了2019年计算机视觉方面的综述论文，包含<strong>目标检测</strong>、<strong>图像分割(含语义/实例分割)</strong>、<strong>目标跟踪</strong>、<strong>医学图像分割</strong>、<strong>显著性目标检测</strong>、<strong>行为识别</strong>、<strong>深度估计</strong>等。可以使读者对相关领域有一个系统的了解。很适合初学者以及相关领域的研究人员。</p><p><strong><em>object detection</em></strong></p><ol><li>Imbalance Problems in Object Detection: A Reviewintro: under review at TPAMI</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1909.00169" target="_blank" rel="noopener">https://arxiv.org/abs/1909.00169</a></p><ol start="2"><li>Recent Advances in Deep Learning for Object Detectionintro: From 2013 (OverFeat) to 2019 (DetNAS)</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1908.03673" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03673</a></p><ol start="3"><li>A Survey of Deep Learning-based Object Detectionintro：From Fast R-CNN to NAS-FPN</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1907.09408" target="_blank" rel="noopener">https://arxiv.org/abs/1907.09408</a></p><ol start="4"><li>Object Detection in 20 Years: A Surveyintro：This work has been submitted to the IEEE TPAMI for possible publication</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1905.05055" target="_blank" rel="noopener">https://arxiv.org/abs/1905.05055</a></p><p><strong><em>图像分割</em></strong></p><ol><li>Deep Semantic Segmentation of Natural and Medical Images: A Reviewintro</li></ol><p>从 FCN(2014) 到 Auto-DeepLab(2019)，本综述共含179篇语义分割和医学图像分割参考文献</p><p>arXiv: <a href="https://arxiv.org/abs/1910.07655" target="_blank" rel="noopener">https://arxiv.org/abs/1910.07655</a></p><ol start="2"><li>Understanding Deep Learning Techniques for Image Segmentationintro</li></ol><p>本综述介绍了从2013年到2019年，主流的30多种分割算法（含语义/实例分割），50多种数据集，共计224篇参考文献</p><p>arXiv: <a href="https://arxiv.org/abs/1907.06119" target="_blank" rel="noopener">https://arxiv.org/abs/1907.06119</a></p><p><strong><em>目标跟踪</em></strong></p><ol><li>A Review of Visual Trackers and Analysis of its Application to Mobile Robotintro</li></ol><p>本目标跟踪综述共含185篇参考文献！从传统方法到最新的深度学习网络</p><p>arXiv: <a href="https://arxiv.org/abs/1910.09761" target="_blank" rel="noopener">https://arxiv.org/abs/1910.09761</a></p><ol start="2"><li>Deep Learning in Video Multi-Object Tracking: A Surveyintro</li></ol><p>38页目标跟踪综述，含30多种主流算法，共计174篇参考文献</p><p>arXiv: <a href="https://arxiv.org/abs/1907.12740" target="_blank" rel="noopener">https://arxiv.org/abs/1907.12740</a></p><p><strong><em>超分辨率</em></strong></p><ol><li>A Deep Journey into Super-resolution: A survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1904.07523" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07523</a></p><ol start="2"><li>Deep Learning for Image Super-resolution: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1902.06068" target="_blank" rel="noopener">https://arxiv.org/abs/1902.06068</a></p><p><strong><em>医学图像分割</em></strong></p><ol><li>Deep learning for cardiac image segmentation: A reviewintro</li></ol><p>本医学图像分割综述从FCN(2014)到Dense U-net(2019)，超过250篇的参考文献（论文中光画图的工作量就超级大）</p><p>arXiv: <a href="https://arxiv.org/abs/1911.03723" target="_blank" rel="noopener">https://arxiv.org/abs/1911.03723</a></p><ol start="2"><li>Machine  Learning Techniques for Biomedical Image Segmentation: An Overview of  Technical Aspects and Introduction to State-of-Art Applications</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1911.02521" target="_blank" rel="noopener">https://arxiv.org/abs/1911.02521</a></p><p><strong><em>显著性目标检测</em></strong></p><ol><li>Salient Object Detection in the Deep Learning Era: An In-Depth Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1904.09146" target="_blank" rel="noopener">https://arxiv.org/abs/1904.09146</a></p><p>github: <a href="https://github.com/wenguanwang/SODsurvey" target="_blank" rel="noopener">https://github.com/wenguanwang/SODsurvey</a></p><p><strong><em>行为识别</em></strong></p><ol><li>Spatio-temporal Action Recognition: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1901.09403" target="_blank" rel="noopener">https://arxiv.org/abs/1901.09403</a></p><p><strong><em>深度估计</em></strong></p><ol><li>Monocular Depth Estimation: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1901.09402" target="_blank" rel="noopener">https://arxiv.org/abs/1901.09402</a></p><p>地址连接：<a href="https://github.com/FranxYao/Deep-Generative-Models-for-Natural-Language-Processing" target="_blank" rel="noopener">https://github.com/FranxYao/Deep-Generative-Models-for-Natural-Language-Processing</a></p><p><strong><em>AutoML + NAS</em></strong></p><ol><li>Neural Architecture Search: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/pdf/1808.05377" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.05377</a></p><p><strong><em>GAN</em></strong></p><ol><li>The Six Fronts of the Generative Adversarial Networks</li></ol><p>arXiv: <a href="https://arxiv.org/pdf/1910.13076" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.13076</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​        &lt;img src=&quot;https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=4102261544,420722754&amp;amp;fm=26&amp;amp;gp=0.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CV" scheme="http://yuanquanquan.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>leetcode 483.最小好进制</title>
    <link href="http://yuanquanquan.top/2019/2019122011/"/>
    <id>http://yuanquanquan.top/2019/2019122011/</id>
    <published>2019-12-20T03:09:20.000Z</published>
    <updated>2019-12-20T03:29:34.939Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static.leetcode-cn.com/cn-mono-assets/production/head/f2ece5fe978d056f5a425ad3387216ee.svg" alt></p><h1 id="最小好进制"><a href="#最小好进制" class="headerlink" title="最小好进制"></a>最小好进制</h1><h1 id="题目链接"><a href="#题目链接" class="headerlink" title="题目链接"></a>题目链接</h1><p>英文链接：<a href="https://leetcode.com/problems/smallest-good-base/" target="_blank" rel="noopener">https://leetcode.com/problems/smallest-good-base/</a></p><p>中文链接：<a href="https://leetcode-cn.com/problems/smallest-good-base/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/smallest-good-base/</a></p><h1 id="题目详述"><a href="#题目详述" class="headerlink" title="题目详述"></a>题目详述</h1><p>对于给定的整数 n, 如果n的k（k&gt;=2）进制数的所有数位全为1，则称 k（k&gt;=2）是 n 的一个<strong>好进制</strong>。</p><p>以字符串的形式给出 n, 以字符串的形式返回 n 的最小好进制。</p><p>   示例 1：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;13&quot;输出：&quot;3&quot;解释：13 的 3 进制是 111。</span><br></pre></td></tr></table></figure><p>示例 2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;4681&quot;输出：&quot;8&quot;解释：4681 的 8 进制是 11111。</span><br></pre></td></tr></table></figure><p>示例 3：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;1000000000000000000&quot;输出：&quot;999999999999999999&quot;解释：1000000000000000000 的 999999999999999999 进制是 11。</span><br></pre></td></tr></table></figure><p>提示：</p><ol><li><p>n的取值范围是 [3, 10^18]。</p></li><li><p>输入总是有效且没有前导 0。、</p></li></ol><h1 id="题目详解"><a href="#题目详解" class="headerlink" title="题目详解"></a>题目详解</h1><p>   ​        本题是寻找一个数最小的good base。其定义是对于一个数y，其x进制表示为全1，则称x是y的good base。应该比较好理解，其实就是将y写成1+x+x^2 +…+x^(n-1)，就是一个等比数列求和，于是我们可以将其转化为y = (x^n - 1)/(x - 1)，其中x&gt;=2, 3&lt;y&lt;10^18,为了寻找最小的x，我们可以先来确定一下n的取值范围，很明显x越小n越大，所以当x=2时，n最大为log2(y+1)。从第三个例子可以看出来，当x=y-1时，n最小为2。所以有了n的取值范围我们就可以遍历所有可能的n，然后每次循环中y和n都是确定值，在对x使用二叉搜索确定其值即可。</p><p>   这里有两个变量，一个是进制，一个是对应进制下的长度。可以固定一个变量，然后判断是否满足条件即可。进制的取值范围为 <code>[2, n - 1]</code>，范围太大，所以应该固定长度，判断是否存在满足条件的进制。进制越小，长度越长；进制越大，长度越短。我们可以从大到小枚举长度，判断是否存在满足条件的进制，这一步可以运用二分查找。</p><p>枚举长度的时间复杂度为 <code>O(logn)</code>，二分查找的时间复杂度为 <code>O(logn)</code>，辅助计算的时间复杂度为 <code>O(logn)</code>，总的时间复杂度为 <code>O(logn ^ 3)</code>。</p><p>另外一个需要注意的问题就是，因为本题中的数都比较大，所以要注意溢出问题，之前也做过一到这种题，可以使用java内置的BigInteger类进行处理。代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.math.BigInteger;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">smallestGoodBase</span><span class="params">(String n)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//现将字符串解析成long型数据</span></span><br><span class="line">        <span class="keyword">long</span> s = Long.parseLong(n);</span><br><span class="line">        <span class="comment">//对所有可能的指数n进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> max_e = (<span class="keyword">int</span>) (Math.log(s) / Math.log(<span class="number">2</span>)) + <span class="number">1</span>; max_e &gt;= <span class="number">2</span>; max_e--) &#123;</span><br><span class="line">            <span class="keyword">long</span> low = <span class="number">2</span>, high = s, mid;</span><br><span class="line">            <span class="comment">//进行二叉搜索，寻找最小的good base。</span></span><br><span class="line">            <span class="keyword">while</span> (low &lt;= high) &#123;</span><br><span class="line">                mid = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">                <span class="comment">//一开始没有使用BigInteger，会报错</span></span><br><span class="line">                BigInteger left = BigInteger.valueOf(mid);</span><br><span class="line">                left = left.pow(max_e).subtract(BigInteger.ONE);</span><br><span class="line">                BigInteger right = BigInteger.valueOf(s).multiply(BigInteger.valueOf(mid).subtract(BigInteger.ONE));</span><br><span class="line">                <span class="keyword">int</span> cmr = left.compareTo(right);</span><br><span class="line">                <span class="keyword">if</span> (cmr == <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">return</span> String.valueOf(mid);</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (cmr &gt; <span class="number">0</span>)</span><br><span class="line">                    high = mid - <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    low = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> String.valueOf(s - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>不过看题解的时候看到一个大佬的神仙解法，如下</p><p>有点类似于梯度下降</p><p>假设有两个变量x, y，求解方程 f(x, y) = z<br>当f(x, y)相对于x和y都是单调的时候<br>则可以先固定x，求下一个满足条件的y边界，然后固定y求下一个满足条件的x边界，</p><p>这样不断逼近最终解</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> ll = <span class="keyword">long</span> <span class="keyword">long</span>;</span><br><span class="line">    <span class="function">ll <span class="title">nextK</span><span class="params">(ll k, ll m, ll N, <span class="keyword">bool</span>&amp; match)</span> </span>&#123;</span><br><span class="line">        match = <span class="literal">false</span>;</span><br><span class="line">        ll lo = k + <span class="number">1</span>;</span><br><span class="line">        ll hi = N - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (lo &lt;= hi) &#123;</span><br><span class="line">            ll md = lo + (hi - lo) / <span class="number">2</span>;</span><br><span class="line">            ll s = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (ll i = <span class="number">0</span>; i &lt;= m; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s &gt; (N - <span class="number">1</span>) / md) &#123;</span><br><span class="line">                    s = N + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                s = s * md + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s == N) &#123;</span><br><span class="line">                match = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">return</span> md;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s &gt; N) hi = md - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> lo = md + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> lo;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">ll <span class="title">nextM</span><span class="params">(ll k, ll m, ll N, <span class="keyword">bool</span>&amp; match)</span> </span>&#123;</span><br><span class="line">        match = <span class="literal">false</span>;</span><br><span class="line">        ll lo = <span class="number">1</span>;</span><br><span class="line">        ll hi = m - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (lo &lt;= hi) &#123;</span><br><span class="line">            ll md = lo + (hi - lo) / <span class="number">2</span>;</span><br><span class="line">            ll s = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (ll i = <span class="number">0</span>; i &lt;= md; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s &gt; (N - <span class="number">1</span>) / k) &#123;</span><br><span class="line">                    s = N + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                s = s * k + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s == N) &#123;</span><br><span class="line">                match = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">return</span> md;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s &gt; N) hi = md - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> lo = md + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> hi;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">smallestGoodBase</span><span class="params">(<span class="built_in">string</span> n)</span> </span>&#123;</span><br><span class="line">        ll N = stoll(n);</span><br><span class="line">        ll k = <span class="number">2</span>;</span><br><span class="line">        ll m = N; <span class="comment">// m为N表示为k进制的最高次幂</span></span><br><span class="line">        <span class="keyword">bool</span> match = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">while</span> (k &lt; N &amp;&amp; m &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            m = nextM(k, m, N, match);</span><br><span class="line">            <span class="keyword">if</span> (match) <span class="keyword">break</span>;</span><br><span class="line">            k = nextK(k, m, N, match);</span><br><span class="line">            <span class="keyword">if</span> (match) <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> to_string(k);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://static.leetcode-cn.com/cn-mono-assets/production/head/f2ece5fe978d056f5a425ad3387216ee.svg&quot; alt&gt;&lt;/p&gt;
&lt;h1 id=&quot;最小好进制&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="算法" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="LeetCode" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="http://yuanquanquan.top/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>CSRNet：Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes</title>
    <link href="http://yuanquanquan.top/2019/2019121899/"/>
    <id>http://yuanquanquan.top/2019/2019121899/</id>
    <published>2019-12-18T11:49:36.000Z</published>
    <updated>2019-12-19T11:03:28.679Z</updated>
    
    <content type="html"><![CDATA[<h3 id="主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域，-生成高质量的人群分布密度图。"><a href="#主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域，-生成高质量的人群分布密度图。" class="headerlink" title="主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1/8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域， 生成高质量的人群分布密度图。"></a>主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1/8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域， 生成高质量的人群分布密度图。</h3><h5 id="1、MCNN的多列设计没有显著作用："><a href="#1、MCNN的多列设计没有显著作用：" class="headerlink" title="1、MCNN的多列设计没有显著作用："></a>1、MCNN的多列设计没有显著作用：</h5><p>​    以前的拥挤场景分析工作主要基于<code>multi-scale architectures</code>。它们在该领域取得了很高的性能，但是当网络变得更深时，它们使用的设计也带来了两个显着的缺点：大量的训练时间和无效的分支结构（例如，MCNN ）。我们设计了一个实验来证明MCNN与表1中更深入的常规网络相比表现不佳。</p><p><img src="http://i1.fuimg.com/706216/912fff8efa8ec71f.png" alt></p><pre><code>如我们先前所知，MCNN的每列专用于某一级别的拥塞场景。但是，使用MCNN的有效性可能并不突出。我们在图2中展示了MCNN中三个独立列（代表大，中，小的感受野）所学习的特征，并用ShanghaiTech Part A [18]数据集进行评估。该图中的三条曲线与具有不同拥塞密度的50个测试案例共享非常相似的模式（估计的错误率），这意味着这种分支结构中的每个列学习几乎相同的特征。它违背了MCNN设计的初衷，用于学习每列的不同功能。</code></pre><p><img src="http://i1.fuimg.com/706216/c0b69c6df04e6684.png" alt></p><h5 id="2、膨胀卷积优于反卷积"><a href="#2、膨胀卷积优于反卷积" class="headerlink" title="2、膨胀卷积优于反卷积"></a>2、膨胀卷积优于反卷积</h5><p>​    已经在分割任务中证明了膨胀卷积层，其精度得到显着提高，并且它是池化层的良好替代方案。 尽管池化层（例如，最大和平均池化）被广泛用于维持不变性和控制过度拟合，但是它们还显着地降低了空间分辨率，这意味着丢失了特征映射的空间信息。 反卷积层可以减轻信息的丢失，但额外的复杂性和执行延迟可能并不适合所有情况。 膨胀卷积是一个更好的选择，它使用稀疏内核（如图3所示）来交替汇集和卷积层。 该字符在不增加参数数量或计算量的情况下扩大了感受野（例如，添加更多卷积层可以产生更大的感受野但引入更多操作）。<br>​     为了保持特征图的分辨率，与使用<code>卷积+池化+反卷积</code>的方案相比，<code>膨胀卷积</code>显示出明显的优点。我们在图4中选择一个例子用于说明。输入是人群的图像，并且它分别通过两种方法处理以产生具有相同大小的输出。在第一种方法中，输入由具有因子2的最大池化层进行下采样，然后将其传递到具有3X3 Sobel内核的卷积层。由于生成的特征映射仅是原始输入的1/2，因此需要通过解卷积层（双线性插值）<code>bilinear interpolation</code>对其进行上采样。在另一种方法中，我们尝试扩张卷积并使相同的3X3 Sobel内核适应具有因子= 2步幅的扩张内核。输出与输入共享相同的维度。最重要的是，扩张卷积的输出包含更详细的信息。</p><p><img src="http://i1.fuimg.com/706216/39f9273771846a6b.png" alt></p><h3 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h3><p>​    在本文中，我们设计了一个更深入的网络，称为<code>CSR-Net</code>，用于计算人群和生成高质量的密度图。我们的模型使用<code>纯卷积层</code>作为主干，以灵活的分辨率支持输入图像。为了限制网络的复杂性，我们在所有层中使用<code>小尺寸</code>的卷积滤波器（如3x3）。我们将VGG-16 [21]的前10层作为前端和<code>膨胀卷积层</code>作为后端部署，以扩大感受域并提取更深的特征而不会丢失分辨率（因为不使用池化层）。</p><h3 id="主要实现："><a href="#主要实现：" class="headerlink" title="主要实现："></a>主要实现：</h3><h5 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h5><p>​    此前端网络的输出大小是原始输入大小的<code>1/8</code>。如果我们继续堆叠更多卷积层和池化层（VGG-16中的基本组件），输出大小将进一步缩小，并且很难生成高质量的密度映射。 我们尝试将膨胀卷积层作为后端来提取更深层的显着性信息以及维持输出分辨率。<br>​     我们在表3中提出了四种CSRNet网络配置，它们具有相同的前端结构但后端的扩展速率不同。 关于前端，我们采用VGG-16网络（全连接层除外）并仅使用3X3内核。 根据VGG的论文，当使用相同大小的感受野时，使用具有小内核的更多卷积层比使用具有更大内核的更少层更有效。<br>​     通过移除完全连接的层，我们尝试确定需要从VGG-16使用的层数。 最关键的部分是在准确性和资源开销（包括训练时间，内存消耗和参数数量）之间进行权衡。 实验表明，在保持前十层VGG-16 [21]只有3个池化层而不是五层时，可以实现最佳权衡，以抑制由池化操作引起的对输出精度的不利影响。 由于CSRNet的输出（密度图）较小（输入尺寸的1/8），我们选择因子为8的双线性插值进行缩放，并确保<code>输出与输入图像具有相同的分辨率</code>。 使用相同的大小，CSRNet生成的结果与使用<code>PSNR</code>（峰值信噪比）和<code>SSIM</code>（图像中的结构相似性）的基础事实结果相当。</p><p><img src="http://i1.fuimg.com/706216/5a6ecfa4bb73acef.png" alt="Markdown"></p><h5 id="高斯核："><a href="#高斯核：" class="headerlink" title="高斯核："></a>高斯核：</h5><p><img src="http://i1.fuimg.com/706216/7bf155f476a47312.png" alt="Markdown"></p><h5 id="数据增强："><a href="#数据增强：" class="headerlink" title="数据增强："></a>数据增强：</h5><p>​    我们从不同位置的每个图像裁剪9个patches，原始图像的大小为1/4。 前四个patches包含四分之三的图像而没有重叠，而其他五个patches则从输入图像中随机裁剪。 之后，我们镜像patches，以便我们将训练集加倍。</p><h5 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h5><p>​    使用简单的方法将CSRNet作为端到端结构进行培训。 前10个卷积层由训练有素的VGG-16进行微调[21]。 对于其他层，初始值来自具有0.01标准偏差的高斯初始化。 随机梯度下降（SGD）在训练期间以1e-6的固定学习率应用。 此外，我们选择欧氏距离来测量地面实况与我们生成的估计密度图之间的差异，这与其他工作类似[19,18,4]。 损失函数如下：</p><p><img src="http://i1.fuimg.com/706216/aa66b1e5a19c197b.png" alt="Markdown"></p><h3 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h3><h5 id="评估指标："><a href="#评估指标：" class="headerlink" title="评估指标："></a>评估指标：</h5><p><img src="http://i1.fuimg.com/706216/f329a0f89d08cab5.png" alt="Markdown"></p><pre><code>我们还使用和来评估ShanghaiTech Part A数据集上输出密度图的质量。 为了计算和，我们遵循[5]给出的预处理，其中包括密度图调整大小（与原始输入相同的大小），对地面实况和预测密度图进行插值和归一化。     除了人群计数之外，我们在TRANCOS数据集[44]上设置了一个实验，用于车辆计数，以证明我们的方法的稳健性和一般化。 TRANCOS是一个公共交通数据集，包含由监控摄像机捕获的1244个不同拥挤交通场景的图像，其中包含46796个带注释的车辆。 此外，提供感兴趣区域（ROI）用于评估。 图像的视角不固定，图像是从非常不同的场景中收集的。 网格平均值平均绝对误差（GAME）[44]用于此测试中的估值。 GAME定义如下：</code></pre><p><img src="http://i1.fuimg.com/706216/faee7fbfe515c69a.png" alt="Markdown"></p><h5 id="代码：https-github-com-leeyeehoo-CSRNet-pytorch"><a href="#代码：https-github-com-leeyeehoo-CSRNet-pytorch" class="headerlink" title="代码：https://github.com/leeyeehoo/CSRNet-pytorch"></a>代码：<a href="https://github.com/leeyeehoo/CSRNet-pytorch" target="_blank" rel="noopener">https://github.com/leeyeehoo/CSRNet-pytorch</a></h5><p>我在根据作者的github（<a href="https://github.com/leeyeehoo/CSRNet-pytorch）" target="_blank" rel="noopener">https://github.com/leeyeehoo/CSRNet-pytorch）</a>, 构建环境时遇到了一些问题。我调试过，百度花了很长时间才解决。写这一章的目的是帮助大家学得更好，少走弯路。</p><p><img src="http://i1.fuimg.com/706216/ddc3d6661db91f8f.png" alt></p><hr><h2 id="step1-install"><a href="#step1-install" class="headerlink" title="step1. install"></a>step1. install</h2><p>For the specific installation process, you can refer to the author’s github. Here I simply show the command line of my operation.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda create -n CSRNet python=3.6</span><br><span class="line">source activate CSRNet</span><br><span class="line">unzip CSRNet-pytorch-master.zip</span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torch torchvision</span><br><span class="line">pip install decorator cloudpickle&gt;=0.2.1 dask[array]&gt;=1.0.0 matplotlib&gt;=2.0.0 networkx&gt;=1.8 scipy&gt;=0.17.0 bleach python-dateutil&gt;=2.1 decorator</span><br><span class="line">unzip ShanghaiTech_Crowd_Counting_Dataset.zip</span><br><span class="line">jupyter nbconvert --to script make_dataset.ipynb  #Convert .ipynb file to .py file</span><br></pre></td></tr></table></figure><h2 id="step2-make-dataset-py"><a href="#step2-make-dataset-py" class="headerlink" title="step2.  make_dataset.py"></a>step2.  make_dataset.py</h2><p>I just run the command to convert the <strong>make_dataset.ipynb</strong> file to a <strong>make_dataset.py</strong> file.Now you need to modify the contents of the <strong>make_dataset.py</strong> file.</p><p>Find the location where <strong>root</strong> is, add <strong>def main()</strong> in the above line</p><p><img src="http://i1.fuimg.com/706216/ffae848e61da7722.png" alt="Markdown"></p><p>Add these two lines at the end of the <strong>make_dataset.py</strong>, adjust the format of the code</p><p>!<img src="http://i1.fuimg.com/706216/c44917f78b1ebe7c.png" alt="Markdown"></p><p>There is an error in the author’s source code, you need to change the code</p><p>Replace <strong>pts = np.array(zip(np.nonzero(gt)[1], np.nonzero(gt)[0]))</strong> with <strong>pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))</strong> </p><p><img src="http://i1.fuimg.com/706216/03aebf60107f0747.png" alt="Markdown"></p><p>Then run the <strong>make_dataset.py</strong> file</p><p><img src="http://i1.fuimg.com/706216/9f9edee518e60290.png" alt="Markdown"></p><hr><p><strong>The above is just a general summary, then we will run and visualize the line-by-line code.</strong></p><p>I will use this image as an example.</p><p><img src="http://i1.fuimg.com/706216/531fb21fadbb8159.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import h5py</span><br><span class="line">import scipy.io as io</span><br><span class="line">import PIL.Image as Image</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from scipy.ndimage.filters import gaussian_filter </span><br><span class="line">import scipy</span><br><span class="line">import json</span><br><span class="line">from matplotlib import cm as CM</span><br><span class="line">from image import *</span><br><span class="line">from model import CSRNet</span><br><span class="line">import torch</span><br><span class="line">img_path=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\images\\IMG_21.jpg&apos;</span><br><span class="line">mat=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\ground_truth\\GT_IMG_21.mat&apos;</span><br><span class="line">mat = io.loadmat(mat)</span><br><span class="line">img= plt.imread(img_path)</span><br><span class="line">k = np.zeros((img.shape[0],img.shape[1]))</span><br></pre></td></tr></table></figure><p>The following is the information of <strong>k</strong></p><p><img src="http://i1.fuimg.com/706216/8f52b469b646095f.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gt = mat[&quot;image_info&quot;][0,0][0,0][0]</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/e7183bd8ca8fbcd2.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in range(0,len(gt)):</span><br><span class="line">    if int(gt[i][1])&lt;img.shape[0] and int(gt[i][0])&lt;img.shape[1]:</span><br><span class="line">        k[int(gt[i][1]),int(gt[i][0])]=1</span><br><span class="line">gt = k</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/789b9238a1f7b923.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">density = np.zeros(gt.shape, dtype=np.float32)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/1dd52b1f4c007a06.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gt_count = np.count_nonzero(gt)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/0639687d384f19e1.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/b3707c96d284a2cd.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">leafsize = 2048</span><br><span class="line"># build kdtree</span><br><span class="line">tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/7ad717c37f07d714.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distances, locations = tree.query(pts, k=4)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/2328bb5f36ae1d4d.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/75af4ca25f9e41ea.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;generate density...&apos;)</span><br><span class="line">for i, pt in enumerate(pts):</span><br><span class="line">    pt2d = np.zeros(gt.shape, dtype=np.float32)</span><br><span class="line">    pt2d[pt[1],pt[0]] = 1.</span><br><span class="line">    if gt_count &gt; 1:</span><br><span class="line">        sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1</span><br><span class="line">    else:</span><br><span class="line">        sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point</span><br><span class="line">    density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode=&apos;constant&apos;)</span><br><span class="line">print(&apos;done.&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/851af41223995264.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k = density</span><br><span class="line">with h5py.File(img_path.replace(&apos;.jpg&apos;,&apos;.h5&apos;).replace(&apos;images&apos;,&apos;ground_truth&apos;), &apos;w&apos;) as hf:</span><br><span class="line">        hf[&apos;density&apos;] = k</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/7044109c81cb61e0.png" alt="Markdown"></p><p>So far, we have generated true values for the image.  At this point I will sort the above code as follows</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import h5py</span><br><span class="line">import scipy.io as io</span><br><span class="line">import PIL.Image as Image</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from scipy.ndimage.filters import gaussian_filter </span><br><span class="line">import scipy</span><br><span class="line">import json</span><br><span class="line">from matplotlib import cm as CM</span><br><span class="line">from image import *</span><br><span class="line">from model import CSRNet</span><br><span class="line">import torch</span><br><span class="line">def gaussian_filter_density(gt):</span><br><span class="line">    print(gt.shape)</span><br><span class="line">    density = np.zeros(gt.shape, dtype=np.float32)</span><br><span class="line">    gt_count = np.count_nonzero(gt)</span><br><span class="line">    if gt_count == 0:</span><br><span class="line">        return density</span><br><span class="line"></span><br><span class="line">    # pts = np.array(zip(np.nonzero(gt)[1], np.nonzero(gt)[0]))</span><br><span class="line">    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))</span><br><span class="line">    leafsize = 2048</span><br><span class="line">    # build kdtree</span><br><span class="line">    tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)</span><br><span class="line">    # query kdtree</span><br><span class="line">    distances, locations = tree.query(pts, k=4)</span><br><span class="line"></span><br><span class="line">    print(&apos;generate density...&apos;)</span><br><span class="line">    for i, pt in enumerate(pts):</span><br><span class="line">        pt2d = np.zeros(gt.shape, dtype=np.float32)</span><br><span class="line">        pt2d[pt[1],pt[0]] = 1.</span><br><span class="line">        if gt_count &gt; 1:</span><br><span class="line">            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1</span><br><span class="line">        else:</span><br><span class="line">            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point</span><br><span class="line">        density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode=&apos;constant&apos;)</span><br><span class="line">    print(&apos;done.&apos;)</span><br><span class="line">    return density</span><br><span class="line">img_path=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\images\\IMG_21.jpg&apos;</span><br><span class="line">mat=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\ground_truth\\GT_IMG_21.mat&apos;</span><br><span class="line">img_paths = []</span><br><span class="line">img_paths.append(img_path)</span><br><span class="line">    for img_path in img_paths:</span><br><span class="line">        print(img_path)</span><br><span class="line">        mat = io.loadmat(mat)</span><br><span class="line">        img= plt.imread(img_path)        </span><br><span class="line">        k = np.zeros((img.shape[0],img.shape[1]))</span><br><span class="line">        gt = mat[&quot;image_info&quot;][0,0][0,0][0]</span><br><span class="line">        for i in range(0,len(gt)):</span><br><span class="line">            if int(gt[i][1])&lt;img.shape[0] and int(gt[i][0])&lt;img.shape[1]:</span><br><span class="line">                k[int(gt[i][1]),int(gt[i][0])]=1</span><br><span class="line">        k = gaussian_filter_density(k)        </span><br><span class="line">        with h5py.File(img_path.replace(&apos;.jpg&apos;,&apos;.h5&apos;).replace(&apos;images&apos;,&apos;ground_truth&apos;), &apos;w&apos;) as hf:</span><br><span class="line">                hf[&apos;density&apos;] = k</span><br></pre></td></tr></table></figure><hr><p>And….then, let’s plot the true values of the image:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gt_file = h5py.File(img_paths[0].replace(&apos;.jpg&apos;,&apos;.h5&apos;).replace(&apos;images&apos;,&apos;ground_truth&apos;),&apos;r&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/4b4772b34da8e598.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groundtruth = np.asarray(gt_file[&apos;density&apos;])</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/138df2675cd215de.png" alt="Markdown"></p><p>plot the true values of the image</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(groundtruth,cmap=CM.jet)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/4f44077d831c59b4.png" alt="Markdown"></p><p>Calculate how many people are in this picture</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(groundtruth)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/b77eec89076ec0cf.png" alt="Markdown"></p><hr><p><strong>Based on the same operation above, I generated true values for all images in the dataset. The following operations are performed on the gpu server.</strong></p><p>That is, run the command line <strong>python make_dataset.py</strong> on the server to get the true value of all the pictures.</p><h2 id="step3-Training"><a href="#step3-Training" class="headerlink" title="step3.  Training"></a>step3.  Training</h2><p><strong>Note</strong>: if you use the python3.x</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. In model.py, change the xrange in line 18 to range</span><br><span class="line">2. In model.py, change line 19 to: list(self.frontend.state_dict().items())[i][1].data[:] = list(mod.state_dict().items())[i][1].data[:]</span><br><span class="line">3. In image.py, change line 40 to: target = cv2.resize(target,(target.shape[1]//8,target.shape[0]//8),interpolation = cv2.INTER_CUBIC)*64</span><br></pre></td></tr></table></figure><ul><li>In  part_A_train.json:change the path of images</li><li>In  part_A_val.json: change the path of images</li></ul><p>run </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py part_A_train.json part_A_val.json 0 0</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/eddfc2ba078f8a53.png" alt="Markdown"></p><h2 id="step4-Testing"><a href="#step4-Testing" class="headerlink" title="step4. Testing"></a>step4. Testing</h2><p>These are our test images. number：182</p><p><img src="http://i1.fuimg.com/706216/107a86f92d987f8e.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter nbconvert --to script val.ipynb</span><br></pre></td></tr></table></figure><p>Finally, the performance of this model on invisible data is tested. We will use the val.py file to verify the results. Remember to change the path to pre-train weights and images.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python val.py</span><br></pre></td></tr></table></figure><p>The average absolute error value that can be obtained by running this val.py file code</p><p>total ：182</p><p><img src="http://i1.fuimg.com/706216/1c9eda9b4fc41cd4.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/6f3f6d53bae9be0f.png" alt="Markdown"></p><p>The average absolute error value obtained is 65.96636956602663, which is very good.</p><hr><p>Now let’s examine the predicted values on a single image:</p><p>run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_single-image.py</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/276e24fbb7b1ad08.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/6f3eb74c5cff3391.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/1a4cf5705f7327be.png" alt="Markdown"></p><p>another one</p><p><img src="http://i1.fuimg.com/706216/4fcbbebaee45e321.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/a9c749406c53061e.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/7299a595907b7ec1.png" alt="Markdown"></p><p>The effect is not too good，maybe the model is not trained enough，i guess。</p><hr><h2 id="Reading-paper-https-arxiv-org-pdf-1802-10062-pdf"><a href="#Reading-paper-https-arxiv-org-pdf-1802-10062-pdf" class="headerlink" title="Reading paper   https://arxiv.org/pdf/1802.10062.pdf"></a>Reading paper   <a href="https://arxiv.org/pdf/1802.10062.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.10062.pdf</a></h2><p>先说数据集用的ShanghaiTech dataset，根据.jpg和.mat处理之后生成train_den文件夹下.csv文件和图片一一对应</p><p><img src="http://i1.fuimg.com/706216/048f00ca3a0fcb9e.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/e92cb793c250e9cd.png" alt="Markdown"></p><p>数据集扩充处理再补充一下：使用的是高斯模糊作用于图像中的每个人的头部。所有图像都被裁剪成9块，每块的大小是图像原始大小的1/4。</p><hr><p>空洞卷积也有的博客翻译成膨胀卷积、扩张卷积啥的，anyway，使用扩张卷积是在不增加参数的情况下扩大内核。因此，如果扩张率为1就是中间的图在整个图像上进行卷积。将膨胀率增加到2最右边图它可以替代pooling层</p><p><img src="http://i1.fuimg.com/706216/2b2b3697b32d9623.png" alt="Markdown"></p><p>接下来再说它的数学公式上怎么计算的，</p><p><img src="http://i1.fuimg.com/706216/0d271181ba1fac76.png" alt="Markdown"></p><p>由上公式得到这个([k + (k-1)<em>(r-1)] </em> [k + (k-1)*(r-1)])</p><p><img src="http://i1.fuimg.com/706216/7caceeba60d24f5a.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/0d271181ba1fac76.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/cd9855760288c342.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/c5f5832af28b79ac.png" alt="Markdown"></p><p>这个过程是：首先预测出给定图像的密度图。如果没有人，像素值pixel value设为0。如果该像素对应于人，则将分配某个预定义值。所以图像中的人数就是总共有的像素值total pixel values </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Fast Online Object Tracking and Segmentation:A Unifying Approach</title>
    <link href="http://yuanquanquan.top/2019/20191213002/"/>
    <id>http://yuanquanquan.top/2019/20191213002/</id>
    <published>2019-12-12T23:41:41.000Z</published>
    <updated>2019-12-12T23:54:08.321Z</updated>
    
    <content type="html"><![CDATA[<p>用单个方法实时做到同时目标跟踪和半监督视频目标分割。SiamMask通过二值分割任务增强损失提升了全卷积Siamense目标跟踪方法的训练过程。训练完成后，SiamMask只依赖于单个边界框初始值，可以做到旋转的边界框分割未知类别目标，达到每秒55帧。</p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>展示了如何用单个方法实时做到同时目标跟踪和半监督视频目标分割。我们的方法SiamMask，通过二值分割任务增强损失提升了全卷积Siamense目标跟踪方法的训练过程。训练完成后，SiamMask只依赖于单个边界框初始值，可以做到旋转的边界框分割未知类别目标，达到每秒55帧。性能也有很大的优势【略】。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig1.png" alt></p><p>本文希望通过SiamMask拉近任意目标跟踪和视频目标分割（VOS）的关系。SiamMask是一个简单的多任务方法，可以同时解决上述两个问题。我们希望保持离线的训练能力，以及在线的速度，同时对目标的表示进行快速调节。</p><p>为了达到这个目标，我们同时在三个任务上训练一个Siamese网络，每个对应于不同的目标对象和候选区域的策略。在Bertinetto的全卷积方法中，一个任务是学习目标对象和滑动窗口中多个候选的相似性度量。它的输出是密度响应图，表示目标的未知，但是不包含空间信息。为了提炼这个信息，我们同时训练另外两个任务：用Region Proposal Network训练边界框回归，未知类别二值分割。需要注意的是：二值标签只需要在离线训练时给出，在线分割和跟踪的时候不需要。每个任务用单独的分支表示，最终的损失对三个输出求和。</p><p>在训练后，SiamMask只依赖于单个初始边界框，不需要更新即可在线运算，最后生成目标分割的mask和旋转的边界框，达到每秒55帧。在VOT-2018上达到最好的效果。另外，相同的方法在半监督VOS上也很有竞争力。</p><h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><p>为了能在线运算，速度更快，我们采用了全卷积Siamese网络。我们实验了SiamFC和SiamRPN。</p><h3 id="3-1-Fully-convolutional-Siamese-networks"><a href="#3-1-Fully-convolutional-Siamese-networks" class="headerlink" title="3.1. Fully-convolutional Siamese networks"></a>3.1. Fully-convolutional Siamese networks</h3><p><strong>SiamFC</strong> Bertinetto提出作为跟踪系统的基本块，离线训练的全卷积Siamese网络在（大的）搜索图像x中比较一个模板图像z，获得一个密集响应图。z和x分别是$w \times h$大小的中心剪切的目标对象，和更大的中心剪切的目标位置目标估计。两个输入通过相同的CNN$f_{\theta}$处理，返回两个交叉相关的特征图：</p><p>$$g_{\theta}(z, x)=f_{\theta}(z) \star f_{\theta}(x) \qquad (1)$$</p><p>我们把响应图（式1左）的每个空间元素看做候选窗的响应（ROW）。例如$g_{\theta}^{n}(z, x)$表示一个模板z和x第n个候选窗的相似性。对于SiamFC而言，目标是响应图最大值对应于在搜索区域x中的目标位置。为了让每个ROW编码更丰富的目标对象信息，我们把式1的交叉相关替换为深度方向的交叉相关，生成多通道响应图。SiamFC在几百万视频帧上用logistic损失（记作$\mathcal{L}_{s i m}$）离线训练。</p><p><strong>SiamRPN</strong> Li等通过区域提议网络（RPN提上SiamFC的性能，可以用一个不同比例的边界框预测目标位置。在SiamRPN中，每个ROW表示k个archor box提议的集合，以及对应的目标/背景分数。两个输出的分支用平滑$L_{1}$和交叉熵训练。记作$\mathcal{L}<em>{b o x}$和$\mathcal{L}</em>{\text { score }}$。</p><h3 id="3-2-SiamMask"><a href="#3-2-SiamMask" class="headerlink" title="3.2. SiamMask"></a>3.2. SiamMask</h3><p>不同于现在的依赖于低保真度目标表示的方法，我们认为生成先前帧的目标二值分割mask是很重要的。除了相似性分数和边界框坐标，ROW也包含了生成二值mask的信息。可以在现有的网络基础上增加新的分支和损失。</p><p>通过参数为$\phi$的两层神经网络$h_{\phi}$预测$w \times h$的二值mask。令$m_{n}$表示第n个ROW上预测的mask：</p><p>$$m_{n}=h_{\phi}\left(g_{\theta}^{n}(z, x)\right) \qquad (2)$$</p><p>从式2可以看出mask的预测是图像到分割x以及目标对象z的函数。这里z可以用于指导分割过程：给定一个不同的参考图像，网络会对x生成不同的分割mask。</p><p><strong>Loss function</strong> 训练的时候，每个ROW由真实二值标签$y_{n} \in{ \pm 1}$标记，同时还有$w \times h$大小的像素级真实mask$c_{n}$。令$c_{n}^{i j} \in{ \pm 1}$表示第n个候选ROW像素$(i, j)$上的标签。损失$\mathcal{L}_{\text { mask }}$是二元logistic回归损失：</p><p>$$\mathcal{L}<em>{\operatorname{mask}}(\theta, \phi)=\sum</em>{n}\left(\frac{1+y_{n}}{2 w h} \sum_{i j} \log \left(1+e^{-c_{n}^{i j} m_{n}^{i j}}\right)\right) \qquad (3)$$</p><p>因此，$h_{\phi}$的分类层包括$w \times h$个分类器，每个表示给定像素是否候选窗中的对象。$\mathcal{L}<em>{\text { mask }}$只计算正的ROW（$y</em>{n}=1$）。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig2.png" alt></p><p><strong>Mask representation</strong> </p><p>$f_{\theta}(z)$和$f_{\theta}(x)$生成的深度方向交叉相关的（$17 \times 17$）ROW之一为mask的表示。分割任务的网络$h_{\phi}$由两个$1 \times 1$卷积层表示，一个256通道，一个$63^{2}$通道。这可以让每个像素分类器利用整个ROW包含的信息，也可以对x中的候选窗有完整的视角，这对于区分目标对象是很关键的。</p><p><strong>Two variants</strong> 【略】</p><p><strong>Box generation</strong> 【略】</p><h3 id="3-3-Implementation-details"><a href="#3-3-Implementation-details" class="headerlink" title="3.3. Implementation details"></a>3.3. Implementation details</h3><p>【略】</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Evaluation-for-visual-object-tracking"><a href="#4-1-Evaluation-for-visual-object-tracking" class="headerlink" title="4.1. Evaluation for visual object tracking"></a>4.1. Evaluation for visual object tracking</h3><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig3.png" alt><br><img src="D:/BazingaliuBlog/Bazingaliu.github.io-master/blog/papers/2019/R/SiamMask-fig3.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab2.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab3-4.png" alt></p><h3 id="4-2-Evaluation-for-semi-supervised-VOS"><a href="#4-2-Evaluation-for-semi-supervised-VOS" class="headerlink" title="4.2. Evaluation for semi-supervised VOS"></a>4.2. Evaluation for semi-supervised VOS</h3><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab5-6.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig4.png" alt></p><h3 id="4-3-Further-analysis"><a href="#4-3-Further-analysis" class="headerlink" title="4.3. Further analysis"></a>4.3. Further analysis</h3><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab7.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig5.png" alt></p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>我们提出了SiamMask方法，让全卷积Siamese跟踪器生成任意类别二值分割mask。我们把它成功运用到了目标跟踪和半监督视频目标分割中，速度快，性能高。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用单个方法实时做到同时目标跟踪和半监督视频目标分割。SiamMask通过二值分割任务增强损失提升了全卷积Siamense目标跟踪方法的训练过程。训练完成后，SiamMask只依赖于单个边界框初始值，可以做到旋转的边界框分割未知类别目标，达到每秒55帧。&lt;/p&gt;
&lt;h2 id
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>jupyter远程连接炼丹</title>
    <link href="http://yuanquanquan.top/2019/2019121111/"/>
    <id>http://yuanquanquan.top/2019/2019121111/</id>
    <published>2019-12-11T03:55:00.000Z</published>
    <updated>2019-12-11T04:16:43.667Z</updated>
    
    <content type="html"><![CDATA[<p><img src="data:image/jpeg;base64,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" alt></p><h1 id="一、前提："><a href="#一、前提：" class="headerlink" title="一、前提："></a>一、前提：</h1><ol><li>安装Python3</li><li>安装Anaconda</li><li>配置jupyter notebook 并启动（重点）</li></ol><h1 id="二、配置jupyter文件"><a href="#二、配置jupyter文件" class="headerlink" title="二、配置jupyter文件"></a>二、配置jupyter文件</h1><p>因为服务器已经安装好anaconda和jupyter，python版本为python3.6，以下主要讲如何配置jupyter文件</p><h2 id="1、设置jupyter-的登录密码"><a href="#1、设置jupyter-的登录密码" class="headerlink" title="1、设置jupyter 的登录密码"></a>1、设置jupyter 的登录密码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config  # 生成jupyter notebook配置文件</span><br></pre></td></tr></table></figure><p>会生成有默认配置文件 jupyter_notebook_config.py</p><h2 id="2、然后打开ipython"><a href="#2、然后打开ipython" class="headerlink" title="2、然后打开ipython"></a>2、然后打开ipython</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from notebook.auth import passwd</span><br><span class="line">passwd() #生成密码</span><br></pre></td></tr></table></figure><h2 id="3、配置文件"><a href="#3、配置文件" class="headerlink" title="3、配置文件"></a>3、配置文件</h2><p>然后会让你输入密码，确认密码，。（这里面的密码是后面在本地打开jupyter时需要输入的，要记住，如设置密码为123456）<br> 然后会输出一长串哈希密码”sha1:XXXXX”  复制这一段密码，。后面要用<br> 然后就开始配置刚才生成的jupyter_notebook_config.py文件。，<br> 使用vim打开：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><p>将以下文字复制进jupyter_notebook_config.py中，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;*&apos;</span><br><span class="line">c.NotebookApp.password = u&apos;把上面的文本粘贴到这里&apos;</span><br><span class="line">c.NotebookApp.open_browser = False</span><br><span class="line">c.NotebookApp.port =8888</span><br></pre></td></tr></table></figure><p>编辑好后按esc键，</p><p>输入:wq保存并退出。</p><h2 id="4、安装ipykernel使得jupyter能访问远程的虚拟环境。"><a href="#4、安装ipykernel使得jupyter能访问远程的虚拟环境。" class="headerlink" title="4、安装ipykernel使得jupyter能访问远程的虚拟环境。"></a>4、安装ipykernel使得jupyter能访问远程的虚拟环境。</h2><p>[1] 启动虚拟环境</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`source` `activate &lt;your virtualenv&gt;`</span><br></pre></td></tr></table></figure><p>[2] 在虚拟环境安装jupyter</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`pip ``install` `jupyter`</span><br></pre></td></tr></table></figure><p>[3] 在虚拟环境安装ipykernel</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`pip ``install` `ipykernel`</span><br></pre></td></tr></table></figure><p>[4] 配置ipykernel</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`python -m ipykernel ``install` `--user --name testenv --display-name ``&quot;Python2 (py2env)&quot;`</span><br></pre></td></tr></table></figure><p>其中，–name的参数和–display-name的参数根据配置更改。</p><p>上面就是配置服务端jupyter的以及激活虚拟环境的全过程，总结一下就是：</p><ol><li>安装jupyter，生成key，修改配置文件，按照ip:端口号登陆。</li><li>在激活的虚拟环境中安装ipykernel并配置。</li></ol><p><img src="https://ask.qcloudimg.com/http-save/4069933/qxikwzn1oj.png?imageView2/2/w/1620" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0
      
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="jupyter" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/jupyter/"/>
    
    
  </entry>
  
  <entry>
    <title>THE LOTTERY TICKET HYPOTHESIS： FINDING SPARSE, TRAINABLE NEURAL NETWORKS</title>
    <link href="http://yuanquanquan.top/2019/2019120912/"/>
    <id>http://yuanquanquan.top/2019/2019120912/</id>
    <published>2019-12-09T06:02:23.000Z</published>
    <updated>2019-12-09T06:11:50.026Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>神经网络剪枝可以减少超过90%的参数量，同时准确率没有太大影响。但是剪枝后的结构很难从头开始训练，不然就能近似提高训练性能了。</p><p>有一些子网络，在初始情况下就可以高效的训练网络，但我们发现标准的剪枝技术天然的没有包括它们。基于这些结果，我们提出了彩票假说：密集的，随机初始化的前馈网络包括一些子网络（中奖者），这些子网络独立训练时可以在相近的迭代次数达到相近的测试准确率。这些中奖者赢得了初始彩票：它们初始权重就能特别高效的训练。</p><p>我们提出了一个算法来找这些中奖者，一系列的实验也支持了我们的假说和那些偶然初始化的重要性。我们不断的在MNIST和CIFAR10数据集上发现，中奖者的大小只有全连接网络和卷积网络的10%到20%。除了尺寸，我们发现这些中奖者训练更快，测试准确率更高。</p><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><p>有一些方法可以减少90%的参数但是没有准确率影响，但是为什么开始时不用这个更小的结构来训练呢？研究发现这样做的结果比较差。</p><p>在图1中，我们从MNIST的全连接网络和CIFAR10的卷积网络中随机采样训练一个子网络。随机采样建模了非结构化剪枝。在不同的稀疏程度上，虚线表示迭代中的最小验证损失和准确率。网络越稀疏，学的越慢，最后的准确率越低。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig1.png" alt></p><p>本文中，我们展示总存在是个更小的网络，在从头训练时至少和原始网络一样快，测试准确率相近。图1实线是我们找到的网络。</p><p><strong>The Lottery Ticket Hypothesis</strong> 随机初始化的，密集的网络包括一个子网络，这个网络的初始化使得它能在单独训练时达到相似的准确率，需要的迭代次数也相近。</p><p>严格来说，考虑一个密集的前馈网络$f(x ; \theta)$，初始参数$\theta=\theta_{0} \sim \mathcal{D}<em>{\theta}$。在训练集中用SGD优化，$f$在迭代$j$次后达到最小验证损失$l$和测试准确率$a$。另外，考虑带参数mask $m \in{0,1}^{|\theta|}$训练$f(x ; m \odot \theta)$，所以初始化参数为$m \odot \theta</em>{0}$。当用SGD优化时，$f$在迭代$j‘$次后达到最小验证损失$l‘$和测试准确率$a’$。彩票假说预测，对于$\exists m$使得$j^{\prime} \leq j$，$a^{\prime} \geq a$，$|m|_{0} \ll|\theta|$参数量更少。</p><p>我们发现标准化的剪枝方法自动的略过了这样的网络。我们称这样的一个子网络为中奖者。在参数随机初始化后，$f\left(x ; m \odot \theta_{0}^{\prime}\right)$ where $\theta_{0}^{\prime} \sim \mathcal{D}_{\theta}$，它们就无法中奖了，除非初始化得好。</p><p><strong>Identifying winning tickets</strong> 我们通过训练，减去梯度最小的权重来定为这个网络。剩下的，没有剪枝的就是我们的中奖者。每个未剪枝的连接值在重新训练时在次填回原来初始化的值。这就是我们的实验核心：</p><ol><li>随机初始化网络$f\left(x ; \theta_{0}\right)\left(\text { where } \theta_{0} \sim \mathcal{D}_{\theta}\right)$</li><li>迭代j次，得到参数$\theta_{j}$</li><li>从参数$\theta_{j}$中减去$p \%$参数，生成mask $m$。</li><li>把剩下的参数值设为原来的值$\theta_{0}$，生成中奖者$f\left(x ; m \odot \theta_{0}\right)$</li></ol><p>可以看到，这个剪枝过程是one-shot：网络训练一次，$p \%$的权重剪枝，剩下的重新设置值。但是本文关注于迭代剪枝，不断的进行训练，剪枝，重设。结果显示，迭代剪枝比one-shot剪枝找到的网络更小。</p><p>结果：我们在MNIST的全连接网络和CIFAR10的卷积网络中找到了中奖者。我们使用非结构化的剪枝方法，因此这些中奖者是稀疏的。在更深的网络中，我们的基于剪枝的方法对学习率敏感，高学习率需要warmup来找中奖者。中奖者是原始网络的10-20%大小，准确率更高，迭代次数相近。随机初始化后，效果就很差了，意味着单独说结构不能解释中奖者的成功。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig2.png" alt></p><p><strong>The Lottery Ticket Conjecture</strong> 密集的随机初始化的网络相对于稀疏网络更容易训练，因为有更多可能的子网络。</p><p><strong>Contributions</strong></p><ul><li>我们证明了可以剪枝得到一个子网络，在相近的迭代后可以达到相似的测试准确率</li><li>我们展示了中奖者相比于原来学的更快，可以达到更高的准确率，泛化性更好</li><li>我们提出了彩票假说可以用来解释这个发现</li></ul><p><strong>Implications</strong> 我们可以探索：</p><ul><li><em>Improve training performance</em> 由于中奖者可以独立从头训练，所以我们可以设计一个训练方法来寻找中奖者，更容易的进行剪枝。</li><li>Design better networks* 中奖者揭示了稀疏结构和初始化对学习尤其重要。我们可以试着设计新的网络结构和初始化方案。甚至可以迁移到其他任务上。</li><li><em>Improve our theoretical understanding of neural networks</em></li></ul><h2 id="2-WINNING-TICKETS-IN-FULLY-CONNECTED-NETWORKS"><a href="#2-WINNING-TICKETS-IN-FULLY-CONNECTED-NETWORKS" class="headerlink" title="2 WINNING TICKETS IN FULLY-CONNECTED NETWORKS"></a>2 WINNING TICKETS IN FULLY-CONNECTED NETWORKS</h2><p>我们试验了MNIST上的全连接网络。使用Lenet-300-100。在随机初始化和训练网络后，我们剪枝，然后重新设置参数。我们使用了简单的修建方法，移除最小梯度的权重。</p><p><strong>Notation</strong> $P_{m}=\frac{|m|<em>{0}}{|\theta|}$表示m的稀疏性，例如$P</em>{m}=25 \%$表示75%的权重减掉了。</p><p><strong>Iterative pruning</strong> 图3表示迭代剪枝的结果。第一测修建，网络学得更快，准确率更高（图3左）。51.3%权重有最好的测试准确率，比原始网络快，但是慢于21.1%。3.6%是达到原始网络一样的性能。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig3.png" alt></p><p>图4a是每次迭代剪枝20%的情况。左侧是早停与权重比例的情况。中间是测试准确率。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig4.png" alt></p><p>中奖者从$P-m$为100%到21%学得越来越快，最后早了38%。进一步的剪枝学习变慢，在3.6%的时候早停性能和原始网络一致。测试准确率在13.5%下提升了0.3个百分点，后续就降低了，在3.6%的时候返回原始网络的水平。</p><p><strong>Random reinitialization</strong> 为了验证中奖者初始化的重要性，我们保持了中奖者的结构，但重新初始化。在图5中对每个中奖者随机初始化3次，发现初始化是极其重要的。图3右展示了迭代剪枝的情况。</p><p>【略】</p><p><strong>One-shot pruning</strong> 图4中也展示了one-shot的实验。【略】</p><h2 id="3-WINNING-TICKETS-IN-CONVOLUTIONAL-NETWORKS"><a href="#3-WINNING-TICKETS-IN-CONVOLUTIONAL-NETWORKS" class="headerlink" title="3 WINNING TICKETS IN CONVOLUTIONAL NETWORKS"></a>3 WINNING TICKETS IN CONVOLUTIONAL NETWORKS</h2><p>【略】</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ABSTRACT&quot;&gt;&lt;a href=&quot;#ABSTRACT&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT&quot;&gt;&lt;/a&gt;ABSTRACT&lt;/h2&gt;&lt;p&gt;神经网络剪枝可以减少超过90%的参数量，同时准确率没有太大影响。但是剪枝后的结构很难从头开始
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="ICLR" scheme="http://yuanquanquan.top/tags/ICLR/"/>
    
  </entry>
  
  <entry>
    <title>艾萨克的英雄主义</title>
    <link href="http://yuanquanquan.top/2019/2019120800/"/>
    <id>http://yuanquanquan.top/2019/2019120800/</id>
    <published>2019-12-07T17:33:52.000Z</published>
    <updated>2019-12-07T18:00:39.151Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1575751702447&amp;di=043ba885e29e0e29604a1930003995af&amp;imgtype=0&amp;src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201605%2F11%2F20160511213352_42sRW.thumb.700_0.jpeg" alt></p><p>再有一个月2019年就要过去了，这一年里好像什么都发生了，又好像什么都没改变。</p><p>我的生活在发生着什么呢？大概…..我变得喜欢听汪峰、Adele、还有Johnny Cash的歌；我变得会望着车窗外飞逝的光景没来由地泪湿眼眶；我变得越来越喜欢仰望天空；我变得越来越不敢在深夜推演公式。</p><p>各种ddl日常砸脸，论文也是毫无头绪，也做不出理想的实验数据；但最难的其实是生活，作为一个人去生活。</p><p>把自己的生活打理得井井有条，敢爱，敢恨，有宽阔的胸怀。</p><p>释怀那些曾经伤害过自己的人和事。</p><p>有罗曼罗兰的英雄主义，看清生活的真相之后仍然热爱它。</p><p>就像民谣里唱的</p><p>“如果天黑之前来得及</p><p>我要忘了你的眼睛。”</p><p>有什么不是时间能冲淡的东西呢</p><p>晚年的艾萨克.牛顿在英国铸币局做了二十多年局长，帮助改革了英国的货币，后来获封勋爵，国葬在西敏寺。</p><p>涉足金融的牛顿炒股亏得一塌糊涂，他因此也留下一句名言：</p><p>我可以计算天体运行的轨道，但我无法计算疯狂的人性。</p><p>这句名言大概就像数学家费马本人证明不出费马大定理就写下一句：“我已经知道如何证明，但这里空白太少写不下”，这样略带戏谑和桀骜的托词。他杂糅着牛顿对这个世界的失望，他的傲慢，他的无奈，他的对和谐唯美自然秩序图景的渴望和混乱的人类社会现实的冲突。</p><p>而他自己的人生，在早年的贫穷、困顿、瘦弱中养成了多疑、好胜、自卑的性格，一生未娶，他并没有获得人性美好的眷顾。如果他不是自然科学之父，历史不会记住他。假设会记住他，牛顿的注脚也顶多是一个矮小怪异的加西莫多式的英格兰人，哪里会有艾丝梅拉达们给他爱情的憧憬，遑论天伦之乐。</p><p>他的这句“托词”，在过去几个月一直萦绕在我心头。也萦绕在我看向天空的视角。</p><p>苦难、贫穷、抑或孤独，它们可以塑造我们的性格，可以影响我们的外貌，可以让我们成为丑陋的加西莫多，可以让我们永远也得不到艾丝梅拉达的爱情。但它们唯一不可影响的就是我们的英雄主义。这种英雄主义不单纯基于罗曼罗兰的愿景，它更像牛顿们的勇气和韧劲，乘着思想的羽翼，去飞翔，去探索，去无畏地开疆拓土，发现未知，塑造未来，哪怕倒下，也像海中之鲸，富饶了整个群落。而每当我们看向天空，至少在那一刻，是神圣的，是唯美的，是纯粹的。</p><p>下一刻低头，便只管往前走，不回头。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://timgsa.baidu.com/timg?image&amp;amp;quality=80&amp;amp;size=b9999_10000&amp;amp;sec=1575751702447&amp;amp;di=043ba885e29e0e29604a193000
      
    
    </summary>
    
      <category term="生活" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="随笔" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活" scheme="http://yuanquanquan.top/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>初次面试-字节跳动</title>
    <link href="http://yuanquanquan.top/2019/2019120722/"/>
    <id>http://yuanquanquan.top/2019/2019120722/</id>
    <published>2019-12-06T16:28:29.000Z</published>
    <updated>2019-12-07T14:39:38.757Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容."><div class="hbe-input-container"><input type="password" id="hbePass" placeholder="您好, 这里需要密码." /><label>您好, 这里需要密码.</label><div class="bottom-line"></div></div><script id="hbeData" type="hbeData" data-hmacdigest="a296fa966cb50dff13d4fb1e220b73658b7aad0fc9132a98e531be52573f80bd">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</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      有东西被加密了, 请输入密码查看.
    
    </summary>
    
      <category term="生活" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="随笔" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="招聘" scheme="http://yuanquanquan.top/tags/%E6%8B%9B%E8%81%98/"/>
    
  </entry>
  
  <entry>
    <title>Two-armed Bandit</title>
    <link href="http://yuanquanquan.top/2019/2019120615/"/>
    <id>http://yuanquanquan.top/2019/2019120615/</id>
    <published>2019-12-06T15:40:15.000Z</published>
    <updated>2019-12-07T17:31:50.583Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdn.net/20170225171914631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY29mZmVlX2NyZWFt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>RL问题有以下几个方面：</p><ul><li>不同的actions导致不同的rewards</li><li>rewards具有时延性</li><li>action的reward取决于环境状态</li></ul><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h3 id="Learning-a-Policy"><a href="#Learning-a-Policy" class="headerlink" title="Learning a Policy"></a>Learning a <em>Policy</em></h3><p>Learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones.</p><h3 id="Policy-Gradients"><a href="#Policy-Gradients" class="headerlink" title="Policy Gradients"></a>Policy Gradients</h3><p>Simple neural network learns a policy for picking actions by adjusting it’s weights through gradient descent using feedback from the environment.</p><h3 id="Value-functions"><a href="#Value-functions" class="headerlink" title="Value functions"></a>Value functions</h3><p>Instead of learning the optimal action in a given state, the agent learns to predict how good a given state or action will be for the agent to be in.</p><h3 id="e-greedy-policy"><a href="#e-greedy-policy" class="headerlink" title="e-greedy policy"></a>e-greedy policy</h3><p>This means that most of the time our agent will choose the action that corresponds to the largest expected value, but occasionally, with e probability, it will choose randomly.</p><h3 id="policy-loss-equation"><a href="#policy-loss-equation" class="headerlink" title="policy loss equation"></a>policy loss equation</h3><blockquote><p><strong>Loss = -log(π)A</strong></p></blockquote><p>A is advantage, and is an essential aspect of all reinforcement learning algorithms. Intuitively it corresponds to how much better an action was than some baseline.</p><p>π is the policy. In this case, it corresponds to the chosen action’s weight.</p><h2 id="The-Multi-armed-bandit"><a href="#The-Multi-armed-bandit" class="headerlink" title="The Multi-armed bandit"></a>The Multi-armed bandit</h2><p><a href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149" target="_blank" rel="noopener">代码来源</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">The Bandits</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Here we define our bandits. For this example we are using a four-armed bandit. The pullBandit function generates a random number from a normal distribution with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit that will give that positive reward.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#List out our bandits. Currently bandit 4 (index#3) is set to most often provide a positive reward.</span></span><br><span class="line">bandits = [<span class="number">0.2</span>,<span class="number">0</span>,<span class="number">-0.2</span>,<span class="number">-5</span>]</span><br><span class="line">num_bandits = len(bandits)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pullBandit</span><span class="params">(bandit)</span>:</span></span><br><span class="line">    <span class="comment">#Get a random number.</span></span><br><span class="line">    result = np.random.randn(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> result &gt; bandit:</span><br><span class="line">        <span class="comment">#return a positive reward.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#return a negative reward.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">The Agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment">#These two lines established the feed-forward part of the network. This does the actual choosing.</span></span><br><span class="line">weights = tf.Variable(tf.ones([num_bandits]))</span><br><span class="line">chosen_action = tf.argmax(weights,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#The next six lines establish the training proceedure. We feed the reward and chosen action into the network</span></span><br><span class="line"><span class="comment">#to compute the loss, and use it to update the network.</span></span><br><span class="line">reward_holder = tf.placeholder(shape=[<span class="number">1</span>],dtype=tf.float32)</span><br><span class="line">action_holder = tf.placeholder(shape=[<span class="number">1</span>],dtype=tf.int32)</span><br><span class="line">responsible_weight = tf.slice(weights,action_holder,[<span class="number">1</span>])</span><br><span class="line">loss = -(tf.log(responsible_weight)*reward_holder)</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">update = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Training the Agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">We will train our agent by taking actions in our environment, and recieving rewards. Using the rewards and actions, we can know how to properly update our network in order to more often choose actions that will yield the highest rewards over time.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">total_episodes = <span class="number">1000</span> <span class="comment">#Set total number of episodes to train agent on.</span></span><br><span class="line">total_reward = np.zeros(num_bandits) <span class="comment">#Set scoreboard for bandits to 0.</span></span><br><span class="line">e = <span class="number">0.1</span> <span class="comment">#Set the chance of taking a random action.</span></span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch the tensorflow graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; total_episodes:</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Choose either a random action or one from our network.</span></span><br><span class="line">        <span class="keyword">if</span> np.random.rand(<span class="number">1</span>) &lt; e:</span><br><span class="line">            action = np.random.randint(num_bandits)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = sess.run(chosen_action)</span><br><span class="line"></span><br><span class="line">        reward = pullBandit(bandits[action]) <span class="comment">#Get our reward from picking one of the bandits.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#Update the network.</span></span><br><span class="line">        _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict=&#123;reward_holder:[reward],action_holder:[action]&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Update our running tally of scores.</span></span><br><span class="line">        total_reward[action] += reward</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Running reward for the "</span> + str(num_bandits) + <span class="string">" bandits: "</span> + str(total_reward)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"The agent thinks bandit "</span> + str(np.argmax(ww)+<span class="number">1</span>) + <span class="string">" is the most promising...."</span></span><br><span class="line"><span class="keyword">if</span> np.argmax(ww) == np.argmax(-np.array(bandits)):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"...and it was right!"</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"...and it was wrong!"</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://img-blog.csdn.net/20170225171914631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY29mZmVlX2NyZWFt/font/5a6L5L2T/fontsiz
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="强化学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="强化学习" scheme="http://yuanquanquan.top/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>OpenAI Spinning Up Part 1： Key Concepts in RL</title>
    <link href="http://yuanquanquan.top/2019/20191206/"/>
    <id>http://yuanquanquan.top/2019/20191206/</id>
    <published>2019-12-06T15:31:10.000Z</published>
    <updated>2019-12-06T15:49:40.473Z</updated>
    
    <content type="html"><![CDATA[<p>来自OpenAI Spinning Up <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" target="_blank" rel="noopener">Introduction to RL</a></p><p>简而言之，RL是研究agent(智能体，本文保留英文描述)如何通过反复的尝试来学习。我们通过奖励或惩罚agent的行为，使其在未来能以更高的概率去重复或放弃该行为。</p><h2 id="Key-Concepts-and-Terminology"><a href="#Key-Concepts-and-Terminology" class="headerlink" title="Key Concepts and Terminology"></a>Key Concepts and Terminology</h2><p><img src="https://cugtyt.github.io/blog/rl-notes/R/spinningup-fig1.png" alt></p><p>RL的主要特征是<strong>agent</strong>和<strong>环境</strong>。环境是agent居住并与之互动的世界。在每一次互动中，agent都会对世界有一个(可能是部分的)观察，然后决定要采取的行动。环境会随着agent的互动发生变化，也可能环境会自行更改。</p><p>agent会感知来自环境的<strong>奖励</strong>信号，这个数字告诉它当前世界状态的好坏。agent的目标是最大化其累积奖励，称为<strong>回报</strong>，RL就是让agent学习更好的行为以实现这个目标。</p><p>为了更具体地讨论RL做什么，我们需要引入一些术语：</p><ul><li>状态和观察(states and observations)</li><li>行为空间(action spaces)，</li><li>策略(policies)，</li><li>轨迹(trajectories)，</li><li>不同的回报方式(different formulations of return)，</li><li>RL优化问题(the RL optimization problem)，</li><li>值函数(value functions)。</li></ul><h3 id="States-and-Observations"><a href="#States-and-Observations" class="headerlink" title="States and Observations"></a>States and Observations</h3><p>状态(states)是对世界状况的完整描述，没有隐藏信息。观察(observations)是对状态的部分描述，有一些信息被略掉了。</p><p>在深度RL中，我们几乎总是通过实值向量，矩阵或高阶张量来表示状态和观察。例如，视觉观察可以由其RGB像素矩阵表示，机器人的状态可以用它的关节角度和速度来表示。</p><p>当agent能够观察到环境的完整状态时，我们说环境被<strong>完全观察</strong>(fully observed)，当agent只能看到部分环境时，我们说环境被<strong>部分观察</strong>(partially observed)。</p><blockquote><p>强化学习符号通常把状态表示为$s$，但是表示观察$o$更合适。通常这用于讨论agent去决定采取一个行为：我们通常说动作是以状态为条件的，但是实际上，动作是以观察为条件的，因为agent没有接触到状态。</p><p>在这里，我们将遵循标准约定，但应从上下文中可以明确区分。</p><p>(这里有点绕，说白了就是agent只能观察环境，不能直接得到环境的状态)</p></blockquote><h3 id="Action-Spaces"><a href="#Action-Spaces" class="headerlink" title="Action Spaces"></a>Action Spaces</h3><p>不同的环境允许不同类型的行为，给定环境中的所有有效行为集合通常称为<strong>行为空间</strong>。某些环境(如Atari和Go)具有<strong>离散的行为空间</strong>，agent只有有限数量可采取的行为。其他环境中例如agent控制物理世界中的机器人，具有<strong>连续的行为空间</strong>，在连续空间中，行为是实值向量。</p><p>这个区别对深度RL中的方法有非常深刻的影响。一些算法族只能在某种情况下应用，而对于其他情况则必须重做。</p><h3 id="Policies"><a href="#Policies" class="headerlink" title="Policies"></a>Policies</h3><p><strong>策略</strong>是agent用来决定行为的规则。它可以是确定性的，在这种情况下通常表示为$\mu$：</p><p>$$a_t = \mu(s_t),$$</p><p>或者是随机的，表示为$\pi$：</p><p>$$a_t \sim \pi(\cdot \vert s_t).$$</p><p>策略本质上是agent的大脑，因此将“策略”一词替换为“agent”也比较常见，例如可以说“策略试图最大化奖励”。</p><p>在深度RL中，我们使用的是<strong>参数化策略</strong>：策略的输出是可计算的函数，函数依赖于一组参数(就像神经网络的权重和偏差)，我们可以通过某种优化算法来调整函数进而改变行为。</p><p>我们经常用$\theta$或$\phi$表示这种策略的参数，将其写在策略符号下标：</p><p>$$a_t = \mu_{\theta}(s_t) \ a_t \sim \pi_{\theta}(\cdot \vert s_t).$$</p><h4 id="Deterministic-Policies"><a href="#Deterministic-Policies" class="headerlink" title="Deterministic Policies"></a>Deterministic Policies</h4><p><strong>确定性策略示例</strong>。用Tensorflow在连续行为空间构建简单的确定性策略代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">obs = tf.placeholder(shape=(<span class="keyword">None</span>, obs_dim), dtype=tf.float32)</span><br><span class="line">net = mlp(obs, hidden_dims=(<span class="number">64</span>,<span class="number">64</span>), activation=tf.tanh)</span><br><span class="line">actions = tf.layers.dense(net, units=act_dim, activation=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>其中mlp是一个函数，是用多个dense层堆叠在一起表示的。</p><h4 id="Stochastic-Policies"><a href="#Stochastic-Policies" class="headerlink" title="Stochastic Policies"></a>Stochastic Policies</h4><p>深度RL中两种最常见的随机策略是<strong>类别策略</strong>(Categorical Policies)和<strong>对角高斯策略</strong>(Diagonal Gaussian Policies)。类别策略可用于离散行为空间，而对角高斯策略用于连续行为空间。使用和训练随机策略有两个关键计算：</p><ul><li>从策略中采样行为(sampling actions from the policy)</li><li>计算特定行为的log似然函数$\log \pi_\theta (a \vert s)$(computing log likelihoods of particular actions)</li></ul><blockquote><p><strong>Categorical Policies</strong> 类别策略就像离散动作的分类器。分类策略构建神经网络的方式与分类器相同：输入是观察，然后是一些层(可能是卷积或全连接，取决于输入的类型)，最后是线性层，提供每个行为的logits，接着是softmax，将logits转换为概率。</p><p><em>Sampling</em> 基于每个行为的概率，Tensorflow等框架有内置的采样工具来采样。例如tf.distributions.Categorical文档或tf.multinomial。</p><p><em>Log-Likelihood</em>。将最后一层概率表示为$P _ {\ theta}(s)$。它是一个向量，我们可以将行为视为向量的索引，然后通过向量索引来获取动作$a$的对数似然：<br>$$\log \pi_{\theta}(a \vert s) = \log \left[P_{\theta}(s)\right]_a.$$</p></blockquote><blockquote><p><strong>Diagonal Gaussian Policies</strong> 多元高斯分布(或多元正态分布)由平均向量$μ$和协方差矩阵$\Sigma$来描述。对角高斯分布是一种特殊情况，其中协方差矩阵仅在对角线上具有值，因此我们可以用向量表示它。</p><p>对角高斯策略总是用一个神经网络将观察映射到动作均值，$\mu _ {\theta}(s)$。通常有两种不同的表示协方差矩阵的方式。</p><p><em>第一种方式</em>：用一个对数标准差向量，$\log \sigma$，它不是状态函数，是独立参数。 (VPG，TRPO和PPO的实现方式是这样做的)</p><p><em>第二种方式</em>：用神经网络将状态映射到对数标准差，$\log \sigma _ {\theta}(s)$。它可以选择与均值网络共享一些层。</p><p>注意，在这两种情况下，我们都直接输出对数标准差而不是标准差。这是因为对数标准差可以容易的取到$(-  \infty，\infty)$中的任何值，而标准差必须是非负的，没有这些约束会更容易训练参数。标准差可以通过对对数标准差取幂获得，所以这种表示没有任何损失。</p><p><em>Sampling</em> 给定动作均值$\mu _ {\theta}(s)$和标准差$\sigma _ {\theta}(s)$，以及来自球形高斯的噪声向量z($z \sim \mathcal {N}(0,I)$ )，行为采样可以用如下计算获得：<br>$$a = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot z$$<br>其中$\odot$表示两个向量的元素乘积。框架都具有计算噪声向量的内置方法，例如tf.random_normal。</p><p><em>Log-Likelihood</em> 对于具有均值$\mu = \mu _ {\theta}(s)$和标准差$\sigma = \sigma _ {\theta}(s)$的对角高斯分布，k维行为a的对数似然性由下式给出：<br>$$\log \pi_{\theta}(a \vert s) = -\frac{1}{2}\left(\sum_{i=1}^k \left(\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2 \log \sigma_i \right) + k \log 2\pi \right).$$</p></blockquote><h3 id="Trajectories"><a href="#Trajectories" class="headerlink" title="Trajectories"></a>Trajectories</h3><p>轨迹$\tau$是在世界中的一系列状态和行为，</p><p>$$\tau = (s_0, a_0, s_1, a_1, …).$$</p><p>世界的第一个状态$s_0$是从<strong>起始状态分布</strong>中随机抽样的，有时用$\rho_0$表示：</p><p>$$s_0 \sim \rho_0(\cdot).$$</p><p>状态转换(在时间t的状态$s_t$和t+1的状态$s_ {t + 1}$之间发生的事情)受环境的自然规律支配，并且仅依赖于最近的行为$a_t$。它们可以是确定性的:</p><p>$$s_{t+1} = f(s_t, a_t)$$</p><p>也可以是随机的：</p><p>$$s_{t+1} \sim P(\cdot \vert s_t, a_t).$$</p><p>行动由agent根据其策略采取。</p><blockquote><p>trajectories也经常被称为episodes或rollouts。</p></blockquote><h3 id="Reward-and-Return"><a href="#Reward-and-Return" class="headerlink" title="Reward and Return"></a>Reward and Return</h3><p>奖励函数R在强化学习中至关重要。它取决于当前的世界状态，采取的行动以及世界的下一个状态：</p><p>$$r_t = R(s_t, a_t, s_{t+1})$$</p><p>虽然经常将其简化为仅依赖于当前状态$r_t = R(s_t)$或状态-行为$r_t = R(s_t，a_t)$。</p><p>agent的目标是在轨迹上最大化累积奖励，我们将用$R(\tau)$来表示所有的奖励。</p><p>一种回报是<strong>有限期未折现的回报</strong>(finite-horizon undiscounted return)，它就是在固定的步骤窗口中获得的奖励总和：</p><p>$$R(\tau) = \sum_{t=0}^T r_t.$$</p><p>另一种回报是<strong>无限期折现回报</strong>(infinite-horizon discounted return)，它是agent获得的所有奖励的总和，但是对获得奖励的距离进行折现。这种奖励形式有一个折现因子$\gamma \in(0,1)$：</p><p>$$R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t.$$</p><p>为什么要一个折现因子呢？不是要获得所有奖励吗？虽然如此，但折现因子不仅直观而且在数学也方便。在直观的层面上：当前的奖励比后续的更好。数学上：无限期的奖励总和可能不会收敛到有限值，并且很难在方程中处理，但是在折现因子以及一定的条件下，这个求和是收敛的。</p><blockquote><p>虽然这两种回报方式之间的界限非常明显，但这个界限在深度RL实践中往往没那么清晰，例如我们经常会设置算法去优化未折现的回报，但在估算值函数时却又使用折现因子。</p></blockquote><h3 id="The-RL-Problem"><a href="#The-RL-Problem" class="headerlink" title="The RL Problem"></a>The RL Problem</h3><p>无论回报方式选什么(无限期折现，还是有限期未折现)，无论策略的选择如何，RL的目标都是选择一种策略，当agent根据它采取行动时能最大化预期回报。</p><p>谈到预期回报，我们首先要讨论轨迹上的概率分布。</p><p>我们假设环境转换和策略都是随机的。 在这种情况下，T步轨迹的概率为：</p><p>$$P(\tau \vert \pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} \vert s_t, a_t) \pi(a_t \vert s_t).$$</p><p>由$J(\pi)$表示预期收益为：</p><p>$$J ( \pi ) = \int _ { \tau } P ( \tau \vert \pi ) R ( \tau ) = \underset { \tau \sim \pi } { \mathrm { E } } [ R ( \tau ) ]$$</p><p>RL中的核心优化问题可以表示为：</p><p>$$\pi^* = \arg \max_{\pi} J(\pi),$$</p><p>其中$\pi ^ *$是<strong>最优策略</strong>。</p><h3 id="Value-Functions"><a href="#Value-Functions" class="headerlink" title="Value Functions"></a>Value Functions</h3><p>了解状态或状态-动作对的<strong>值</strong>通常很有用。值指的是从该状态或状态-行动对开始的预期回报，然后一直根据特定策略行事。几乎每种RL算法都以这样或那样的方式在使用<strong>值函数</strong>。</p><p>这里有四个主要函数：</p><ul><li><p><strong>On-Policy Value Function</strong>，$V ^ {\pi}(s)$，从状态s开始并且总是根据策略$\pi$执行行为，它会给出预期的回报为：</p><p>$$V^{\pi}(s) = \underset {\tau \sim \pi}E [R(\tau)\left\vert s_0 = s \right]$$</p></li><li><p><strong>On-Policy Action-Value Function</strong>，$Q ^ {\ pi}(s，a)$，从状态s开始，采取任意行动a(可能不是来自策略),总是根据策略$\pi$执行行为,它给出预期的回报：</p><p>$$Q^{\pi}(s,a) = \underset {\tau \sim \pi}E[R(\tau)\left\vert s_0 = s, a_0 = a\right.]$$</p></li><li><p><strong>Optimal Value Function</strong>, $V ^ <em>(s)$，从状态s开始并且始终根据环境中的</em>最优*策略执行，则给出预期的回报：</p><p>$$V^*(s) = \max_{\pi} \underset {\tau \sim \pi}E[R(\tau)\left\vert s_0 = s\right.]$$</p></li><li><p><strong>Optimal Action-Value Function</strong>，$Q ^ <em>(s，a)$，从状态s开始，采取任意动作a，然后根据环境中的</em>最优*策略采取行动，给出预期回报：</p><p>$$Q^*(s,a) = \max_{\pi} \underset{\tau \sim \pi}E[R(\tau)\left\vert s_0 = s, a_0 = a\right.]$$</p></li></ul><blockquote><p>在谈论价值函数时，如果不考虑时间依赖性，那就只是在表示无限期折现回报的期望均值。有限期无折现回报的值函数需要有时间作为参数。</p></blockquote><blockquote><p>值函数和动作-值函数之间有两个关键的联系经常出现：<br>$$V^{\pi}(s) = \underset {a\sim \pi}E[Q^{\pi}(s,a)],$$<br>$$V^<em>(s) = \max_a Q^</em> (s,a).$$<br>这些关系直接来自刚刚给出的定义：你能证明它们吗？</p></blockquote><h3 id="The-Optimal-Q-Function-and-the-Optimal-Action"><a href="#The-Optimal-Q-Function-and-the-Optimal-Action" class="headerlink" title="The Optimal Q-Function and the Optimal Action"></a>The Optimal Q-Function and the Optimal Action</h3><p>最优动作-值函数$Q ^ <em>(s，a)$与最优策略选择的动作之间存在重要联系。根据定义，$Q ^ </em>(s，a)$给出了在状态s中开始,采取(任意)动作a，然后一直根据最优策略行动的预期回报。</p><p>s中的最优策略将选择从s开始最大化预期回报的行为。因此如果我们有$Q ^ <em>$，我们可以通过这个式子直接获得最优动作$a ^ </em>(s)$：</p><p>$$a^<em>(s) = \arg \max_a Q^</em> (s,a).$$</p><p>注意：可能存在多个最大化$Q ^ *(s，a)$的行为，在这种情况下，所有动作都是最优的，最优策略可以随机选择它们中的任何一个，但总存在一种确定性选择行为的最优策略。</p><h3 id="Bellman-Equations"><a href="#Bellman-Equations" class="headerlink" title="Bellman Equations"></a>Bellman Equations</h3><p>所有的四个值函数都遵循称为<strong>Bellman方程</strong>的特殊自洽方程。Bellman方程背后的基本思想是：</p><blockquote><p>起始点的值是从当前位置获得的奖励期望，在加上下次落脚点值的期望。(The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.)</p></blockquote><p>On-Policy Value Function的Bellman方程是</p><p>$$V^{\pi}(s) = \underset {a \sim \pi \ s’\sim P}E[r(s,a) + \gamma V^{\pi}(s’)], \<br>Q^{\pi}(s,a) = \underset{s’\sim P}E[r(s,a) + \gamma \underset{a’\sim \pi}E[Q^{\pi}(s’,a’)],$$</p><p>其中$s’\sim P$是$s’\sim P(\cdot \vert s，a)$的简写，表示从环境转换规则中采样的下一个状态$s’$; $a \sim \pi$是$\sim \pi(\cdot \vert s)$的简写; 而$a’\sim \pi$是$a’\sim \pi(\cdot \vert s’)$的简写。</p><p>Optimal Value Function的Bellman方程是</p><p>$$V^<em>(s) = \max_a \underset{s’\sim P}E[r(s,a) + \gamma V^</em>(s’)], \ Q^<em>(s,a) = \underset {s’\sim P}E[r(s,a) + \gamma \max_{a’} Q^</em>(s’,a’)].$$</p><p>On-Policy Value Function与Optimal Value Function的Bellman方程之间的关键区别在于在行为上有没有取$\max$,它的意思是，为了达到最优的目标，agent必须选择那个具有最高值的行为。</p><h3 id="Advantage-Functions"><a href="#Advantage-Functions" class="headerlink" title="Advantage Functions"></a>Advantage Functions</h3><p>在RL中，我们不去描述行为绝对好多少，而只需要说它比其它行为平均好多少。也就是说，我们想知道该行为的<strong>相对优势</strong>，优势函数将精确描述这个概念。</p><p>对于策略$\pi$的优势函数$A ^ {\pi}(s，a)$描述了在状态s中采取特定动作a比根据$\pi(\cdot \vert s)$随机选择动作要好多少,这里假设一直按照$\pi$采取行动。在数学上优势函数定义为：</p><p>$$A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s).$$</p><blockquote><p>我们稍后会对此进行讨论，但优势函数对于策略梯度方法至关重要。</p></blockquote><h2 id="Optional-Formalism"><a href="#Optional-Formalism" class="headerlink" title="(Optional) Formalism"></a>(Optional) Formalism</h2><p>到目前为止，我们已经以非正式的方式讨论了agent的环境，但是如果深入研究文献，你很可能会遇到一个标准数学形式：<strong>马尔可夫决策过程(MDP)</strong>。 MDP是一个5元组，$\langle S，A，R，P，\rho_0 \rangle$，其中</p><ul><li>$S$ is the set of all valid states,</li><li>$A$ is the set of all valid actions,</li><li>$R$ : $S \times A \times S \to \mathbb{R}$ is the reward function, with $r_t = R(s_t, a_t, s_{t+1})$,</li><li>$P$ : $S \times A \to \mathcal{P}(S)$ is the transition probability function, with $P(s’\vert s,a)$ being the probability of transitioning into state s’ if you start in state s and take action a,</li><li>and $\rho_0$ is the starting state distribution.</li></ul><p>马尔可夫决策过程这个名称指的是系统服从马尔可夫属性：状态转换只取决于最近的状态和行动，而不是先前的历史。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来自OpenAI Spinning Up &lt;a href=&quot;https://spinningup.openai.com/en/latest/spinningup/rl_intro.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Introducti
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="强化学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="强化学习" scheme="http://yuanquanquan.top/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Connecting Generative Adversarial Networks and Actor-Critic Methods</title>
    <link href="http://yuanquanquan.top/2019/20190112899/"/>
    <id>http://yuanquanquan.top/2019/20190112899/</id>
    <published>2019-11-28T07:28:01.000Z</published>
    <updated>2019-11-28T08:18:47.404Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>无监督的GAN和强化学习的actor-critic方法在优化困难方面声名显著。这两个领域的研究人员积累了大量的缓解不稳定和改善训练情况的策略。这里我们展示了GAN可以看作是一个特定环境中的actor-critic方法，在这个环境中actor无法影响奖励。我们对这两类模型稳定训练方法做了综述，不仅包括可以同时对二者使用的方法，还包括只针对特定模型的方法。我们也对一系列信息流更复杂的GAN和RL算法进行了综述。我们希望强调二者之间的联系来激励GAN和RL两个领域开发出通用，可扩展以及稳定的算法。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>大部分机器学习问题可以表述为对于一个目标的优化问题。但是，很多问题缺乏一个统一的代价函数，并且混合着多个模型，虽然模型间彼此传递信息，但是却试图最小化各自内部的损失函数。这对大部分学习算法的假设造成了影响，并且应用传统方法如梯度下降等通常会导致病态行为，例如波动或坍塌为退化解。尽管有这些实践中的问题，但是在具有混合或多级损失的模型中存在巨大潜力，同时不同局部损失的组合也是大脑功能的基础。</p><p>Actor-critic（AC）方法和GAN是两类多层次优化问题并且具有相似之处。在这两种情况中，信息流都是一个简单的前向传递。从一个模型，不管它接受一个Action（AC）还是生成的样本（GAN），到另一个模型来衡量前一个模型的输出质量。在这两类中，第二个模型都是唯一对环境特殊信息直接接触的模型，这个信息或许是奖励信息（AC）或是真实样本（GAN），并且第一个模型必须从第二个模型的错误信号中学习。当然二者也有其不同之处，第二部分讨论。这些模型都有稳定性方面的问题，而两个领域都形成了各自独立的稳定训练的方法。</p><h2 id="2-Algorithms"><a href="#2-Algorithms" class="headerlink" title="2 Algorithms"></a>2 Algorithms</h2><p>GAN和AC都可以看作是双层（bilevel）或双时间尺度（two-time-scale）的优化问题，一个模型通过另一个模型来优化自身：<br>$$<br>x^{<em>}=\arg \min _{x \in \mathcal{X}} F\left(x, y^{</em>}(x)\right) \qquad (1)<br>y^{*}(x)=\arg \min _{y \in \mathcal{Y}} f(x, y) \qquad (2)<br>$$<br>双层优化问题在运筹学方面已经得到广泛研究，包括AC方法，但是都是在线性或凸的设置上。</p><h3 id="2-1-Generative-Adversarial-Networks"><a href="#2-1-Generative-Adversarial-Networks" class="headerlink" title="2.1 Generative Adversarial Networks"></a>2.1 Generative Adversarial Networks</h3><p>GAN可以表述为包含两个部分的无监督问题：一个生成器G从一个分布中采样，一个判别器D分类一个样本为真或假。通常生成器是一个前向神经网络，接收一个固定噪声源$z \sim \mathcal{N}(0, I)$输入，判别器是另一个神经网络，把一个图像映射为一个二分类的概率。GAN的博弈可以表述为零和博弈，它的值表示为交叉熵损失：<br>$$<br>\min _{G} \max <em>{D} \mathbb{E}</em>{w, y}[y \log D(w)+(1-y) \log (1-D(w))]\=\min _{G} \max <em>{D} \mathbb{E}</em>{w \sim p_{\text { data }}}[\log D(w)]+\mathbb{E}<em>{z \sim \mathcal{N}(0, I)}[\log (1-D(G(z)))] \qquad (3)<br>$$<br>为了保证生成器在判别器准确率很高的情况下也能有梯度来学习，生成器的损失函数通常表示为最大化分类为真的概率，而不是最小化分类为假的概率。修改后的损失依旧是双层优化问题：<br>$$<br>F(D, G)=-\mathbb{E}</em>{w \sim p_{\text { data }}}[\log D(w)]-\mathbb{E}<em>{z \sim \mathcal{N}(0, I)}[\log (1-D(G(z)))] \qquad (4) \ f(D, G)=-\mathbb{E}</em>{z \sim \mathcal{N}(0, I)}[\log D(G(z))] \qquad (5)<br>$$</p><h3 id="2-2-Actor-Critic-Methods"><a href="#2-2-Actor-Critic-Methods" class="headerlink" title="2.2 Actor-Critic Methods"></a>2.2 Actor-Critic Methods</h3><p>AC方法在强化学习中很早就有了。大部分强化学习算法或是学习一个值函数，例如值迭代和TD学习，或直接学习一个策略，例如策略梯度方法。AC方法是同步的：actor学习策略与critic学习值函数同步进行。在很多AC方法中，critic为策略梯度方法提供了一个低方差的基准，而不用去从返回中估计值。在这种情况下，即使是一个坏的值函数估计也是有帮助的，因为无论使用什么基准策略梯度都会被修正偏置。在其他AC方法中，策略通过近似值函数来更新，这种情形与GAN很相似。如果策略通过一个不正确的值函数来优化，可能会导致一个坏的策略，从而不能完全探索整个空间，那么就不能发现好的值函数，从而出现退化解。有一系列的方法来缓解这个问题。</p><p>考虑一个RL的MDP问题，有一个状态S，行为A的集合，一个初始状态为</p><p>p_0(s)的分布，转移函数$\mathcal{P}\left(s_{t+1} \vert s_{t}, a_{t}\right)$,奖励分布$\mathcal{R}\left(s_{t}\right)$，还有折扣因子$\gamma \in[0,1]$。AC方法的主要目的是同步学习一个行为值函数$Q^{\pi}(s, a)$，来预测折扣奖励期望：<br>$$<br>Q^{\pi}(s, a)=\mathbb{E}{s{t+k} \sim \mathcal{P}, r{t+k} \sim \mathcal{R}, a{t+k} \sim \pi}\left[\sum{k=1}^{\infty} \gamma^{k} r{t+k} | s{t}=s, a{t}=a\right] \qquad (6)<br>$$<br>然后根据值函数学一个最优的策略：<br>$$<br>\pi^{*}=\arg \max <em>{\pi} \mathbb{E}</em>{s_{0} \sim p_{0}, a_{0} \sim \pi}\left[Q^{\pi}\left(s_{0}, a_{0}\right)\right]  \qquad (7)<br>$$<br>我们可以把$Q^{\pi}$表示为最小化问题的解：<br>$$<br>Q^{\pi}=\arg \min <em>{Q} \mathbb{E}</em>{s_{t}, a_{t} \sim \pi}\left[\mathcal{D}\left(\mathbb{E}_{s_{t+1}, r_{t}, a_{t+1}}\left[r_{t}+\gamma Q\left(s_{t+1}, a_{t+1}\right)\right] | Q\left(s_{t}, a_{t}\right)\right)\right] \qquad (8)<br>$$<br>其中$\mathcal{D}(\cdot | \cdot)$是任意的散度。现在AC问题可以表示为一个双层优化问题：<br>$$<br>F(Q, \pi)=\mathbb{E}_{s_{t}, a_{t} \sim \pi}\left[\mathcal{D}\left(\mathbb{E}_{s_{t+1}, r_{t}, a_{t+1}}\left[r_{t}+\gamma Q\left(s_{t+1}, a_{t+1}\right)\right]\right] | Q\left(s_{t}, a_{t}\right)\right) ] \qquad (9)<br>\ f(Q, \pi)=-\mathbb{E}_{s_{0} \sim p_{0}, a_{0} \sim \pi}\left[Q^{\pi}\left(s_{0}, a_{0}\right)\right]<br>$$<br>有很多解决这个问题的AC方法。传统的AC方法通过策略梯度来优化策略，通过TD错误来缩放策略梯度，而行为值函数通过普通的TD学习来更新。我们主要关注于确定性策略梯度（DPG），以及它在随机策略的扩展SVG（0），和连续行为的neurally-fitted Q-learning（NFQCA）。这些算法都是针对于行为和观察值为连续的情况，使用神经网络来近似行为值函数和策略。这是个早就存在的解决连续行为RL的方法，所有的方法通过传回估计值对行为的梯度来更新策略，而不是直接传入TD错误。这些方法的区别主要是训练过程。在NFQCA中，actor和critic在每个episode中以batch模式训练，而DPG和SVG（0）中，网络使用查分法在线更新。</p><h3 id="2-3-GANs-as-a-kind-of-Actor-Critic"><a href="#2-3-GANs-as-a-kind-of-Actor-Critic" class="headerlink" title="2.3 GANs as a kind of Actor-Critic"></a>2.3 GANs as a kind of Actor-Critic</h3><p>GAN和AC方法总结在图1中。在这两种情形中，模型都可以从环境（GAN的判别器和AC中的critic）中获取错误信息，而另一个模型必须从第一个模型的梯度信息中更新。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/GAN-AC-fig1.png" alt></p><p>我们可以使这种联系更加精确，把GAN看作是一种修改过的actor-critic方法来描述一个MDP。考虑这样一个MDP，其中动作为图像中的每个像素。环境随机选择显示生成器生成的图像或显示真实图像。如果环境选择真实图像，则环境奖励为1，否则为0。此MDP是无状态的，因为actor生成的图像不会影响将来的数据。</p><p>在这种环境中进行actor-critic的学习显然非常类似于GAN。但必须进行一些调整才能使其完全相同。如果actor可以获取到环境的state，它可以轻而易举地产生真实的图像，所以actor必须是“盲”的，不知道state。虽然MDP是无状态的，但这并不妨碍actor学习。均方Bellman残差通常用作critic的损失，但为了匹配GAN损失应使用交叉熵。这仍然提供了具有常量最小值的损失函数。由于actor接收值的梯度而不是RL中Bellman残差的梯度，因此要在给actor的梯度中包含与$\frac{\partial \mathcal{D}}{\partial Q}$成比例的缩放项，其中D是交叉熵（在实践中，生成器使用其他损失来细致地处理这个缩放项的复杂性）。最后，如果环境显示真实图像，那么不应更新actor的参数。在奖励为1时，可以通过让critic的梯度归零来实现。因此，GAN可以被视为在无状态MDP中“盲”actor的actor-critic方法。</p><p>此MDP有几个独特的方面会导致一些与actor-critic方法无关的行为。首先，为什么actor-critic算法导致了对抗性行为这点并不是很明显。通常，actor和critic试图优化互补损失函数，而非在不同方向上优化相同的损失。GAN中的对抗是因为其MDP是玩家不能对奖励产生影响的。从本质上讲，它是一个真实政策梯度始终为零的MDP。然而，critic不能单独从输入示例中学习游戏的因果结构，而是要朝着预测奖励的特征方向移动。actor根据critic的最佳估计向某个方向移动以增加奖励，但这种变化并不会导致真实奖励的增加，因此critic将很快学会在actor移动的方向上赋予较低的值。因此，对于actor和critic的更新而言，它们在理想情况下是正交的（如在兼容性的actor-critic中），而不是对抗。值得注意的是，这对于只能部分观察到actor的环境会产生重要影响。而对于完全可观察的MDP，最优策略始终是确定性的。然而，对于GAN，能匹配真实分布的生成器是极小极大问题的固定点。尽管GAN与RL问题之间存在这些差异，但我们认为二者有足够的相似之处，也值得研究两种情况下通用的训练技术。</p><h2 id="3-Stabilizing-Strategies"><a href="#3-Stabilizing-Strategies" class="headerlink" title="3 Stabilizing Strategies"></a>3 Stabilizing Strategies</h2><p>在回顾了GAN的基本知识，actor-critic算法及其扩展之后，我们这里讨论每个领域各自的“交易技巧”。表1总结了不同的方法。虽然并不是一个详尽的清单，我们已经包括那些我们认为已经在其领域产生最大影响或最有可能在各个领域之间交叉的东西。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/GAN-AC-tab1.png" alt></p><h3 id="3-1-GANs"><a href="#3-1-GANs" class="headerlink" title="3.1 GANs"></a>3.1 GANs</h3><ol><li>冻结学习（Freezing learning）。通常当生成器或判别器超过对方时，GAN将陷入退化的情况。一个简单的补救措施是在一个模型开始过于强大时冻结学习。在actor-critic学习中已经成功采用了一种非常类似的方法，即当TD误差的幅度低于或高于某个阈值时，actor或critic的学习被冻结。</li><li>标签平滑（Label smoothing）。当判别器的预测非常有信心时，一种防止梯度消失的简单技巧是，用平滑标签$\epsilon / 1-\epsilon$替换0/1标签，这保证了生成器始终具有信息梯度。虽然这是针对分类的，但没有理由不能于奖励为0/1并且critic梯度消失的RL情形。</li><li>历史平均（Historical averaging）。受到游戏理论中虚构游戏的启发，历史平均为梯度下降增加了一个限制项，惩罚超过参数历史平均值的步骤。即使参数平均值不具有直观含义，此方法也可有效地防止因两个模型优化不同目标而引起的振荡。历史平均也与Polyak-Ruppert平均及其扩展密切相关。尽管Polyak-Ruppert类型的平均已经被RL领域的人严格分析过，但据我们所知，它还没有被采纳为标准的“交易工具”。在DPG中使用replay buffer在概念上也类似于虚构游戏，但仅适用于actor。</li><li>小批量判别（Minibatch discrimination）。为了防止生成器坍塌到单个样本上，小批量判别将鉴别器的作用从对单个图像分类到整个小批量分类，这有助于增加生成样本的熵。在RL中，通过向actor添加惩罚，进而鼓励更高熵策略的行为来解决类似的探索不足问题。直接计算熵对于复杂的随机策略是不切实际的，批量判别可能是鼓励在连续空间进行探索的有效替代方案。</li><li>批量标准化（Batch normalization）。批量标准化对于将GAN扩展到深度卷积网络至关重要，虚拟批量标准化对其进行了扩展，使用常量引用批处理来防止由于小批量的其他元素导致的预测相关性。批量标准化在DPG已经尝试过的大部分环境中也很有用，但尚未有研究虚拟批量标准化能否会带来更大的改进。</li></ol><h3 id="3-2-Actor-Critic"><a href="#3-2-Actor-Critic" class="headerlink" title="3.2 Actor-Critic"></a>3.2 Actor-Critic</h3><ol><li>回放缓冲（Replay buffers）。在离散和连续RL中，重放缓冲对于从训练数据中去除相关性非常有效。它们在概念上也类似于博弈论中的虚构游戏，一个玩家对另一个玩家的平均政策起到最佳响应。在虚构游戏中，两个玩家都与对手的历史平均对战，而在AC设置中，回放缓冲只能用于critic。critic可以根据actor过去选择的行动进行离线政策更新，但actor不能从过去critic的梯度中学习，因为这些梯度是针对不同的行为而采取的。类似地，对于GAN，我们可以保留先前生成图像的缓冲器，以防止判别器与当前生成器拟合太紧密。我们已经尝试使用GAN的回放缓冲，但即使对于简单的分布也无法生成渐近正确的样本。</li><li>目标网络（Target networks）。由于动作值函数在Bellman递归中出现两次，因此稳定性可能是Q函数近似中的难题。目标网络将TD更新中的一个网络固定来解决此问题。由于GAN博弈可以被视为无状态MDP，因此动作值函数的二次出现便消失了，并且判别器的学习变成普通的回归问题。因此，我们不考虑适用于GAN的目标网络。然而，将Q-learning作为子问题的其他多层优化问题可能会受益。同样值得注意的是，自相对估计在概念上与目标网络类似，因此目标网络的思想仍然可以适用于其他密度估计问题。</li><li>熵正则化（Entropy regularization）。actor-critic方法往往无法充分探索行动空间。为了解决这个问题，有时会增加额外奖励以鼓励高熵政策。当动作空间是离散的时候很容易实现，但在连续控制情况中稍微困难一些。值得注意的是，GAN经常遇到类似的问题，这时候生成器坍塌到某几种模式。在连续控制中任何鼓励进行探索的方法都可能增加GAN中的样本多样性。</li><li>兼容性（Compatibility）。actor-critic方法的理论发展之一是兼容性critic的概念。如果critic是策略梯度相对于其参数的线性函数，并且critic是最优的，那么使用它来代替预期回报的梯度真实值可以给出无偏近似。这种兼容性的critic也与预期收益的自然梯度密切相关，并可用于策略的高效自然梯度下降。虽然优雅，但仍不清楚兼容性的概念是否可以自然地扩展到GAN中。由于任何策略的真实值在GAN MDP中始终为0.5，因此真实政策梯度始终为零。通常我们更希望GAN是对抗而不是兼容。</li></ol><h2 id="4-Conclusions"><a href="#4-Conclusions" class="headerlink" title="4 Conclusions"></a>4 Conclusions</h2><p>将深度学习与多层优化相结合，可以为机器学习和人工智能中的各种问题带来巨大希望。尽管优化和探索存在固有的困难，但GAN和actor-critic方法已经对各自的领域产生了巨大的影响。我们希望通过指出两者之间的深层联系，能鼓励在不同领域之间开发和采用更通用的技术，让不同的思想自由流动。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;无监督的GAN和强化学习的actor-critic方法在优化困难方面声名显著。这两个领域的研
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="GAN" scheme="http://yuanquanquan.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>statapy——在jupyter里写stata代码</title>
    <link href="http://yuanquanquan.top/2019/2019090522/"/>
    <id>http://yuanquanquan.top/2019/2019090522/</id>
    <published>2019-11-24T11:24:11.000Z</published>
    <updated>2019-11-24T11:46:22.695Z</updated>
    
    <content type="html"><![CDATA[<p><img src="data:image/jpeg;base64,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" alt="img"></p><p>Stata是做生物统计/计量经济学的重要统计工具, 而python是做数据科学的利器, ipystata将stata和python结合在一起, 并能够在jupyter notebook中使用, 使得我们的工作效率大大提升。下面我们介绍一下, 如何安装stata,<br>如何在python中使用stata, 并进行stata的一些自动化操作。</p><p>目前来看, 在jupyternotebook中使用stata有两种方案:</p><ul><li>方案一: 使用ipystata模块, 这个模块提供了<code>%%stata</code>魔法函数, 可以把notebook的cell可以执行stata语句</li><li>方案二: 使用stata_kernel, 它实际上是一个notebook kernel, 使用stata kernel创建的notebook, 只能执行stata语句</li></ul><p>下面我们分别介绍两种方案。</p><p><strong>命令行注册</strong></p><p>这是windows的安装方法, 如果你是linux, 也是类似的道理, 需要运行stata命令来注册。</p><p>使用管理员模式打开powershell:</p><p><img src="https://mlln.cn/2018/11/01/%E5%AE%89%E8%A3%85stata%E5%B9%B6%E5%9C%A8jupyter-notebook%E4%B8%AD%E8%B0%83%E7%94%A8/powershell.png" alt></p><p>工作目录调整到stata的安装目录, 然后执行命令<code>.\StataSE-64.exe /Register</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1 PS C:\Users\syd&gt; cd c:/</span><br><span class="line">2 PS C:\&gt; cd &apos;.\Program Files (x86)\&apos;</span><br><span class="line">3 PS C:\Program Files (x86)&gt; cd .\Starth\</span><br><span class="line">4 PS C:\Program Files (x86)\Starth&gt; cd ..</span><br><span class="line">5 PS C:\Program Files (x86)&gt; cd .\Stata15\</span><br><span class="line">6 PS C:\Program Files (x86)\Stata15&gt; .\StataSE-64.exe /Register</span><br><span class="line">7 PS C:\Program Files (x86)\Stata15&gt;</span><br></pre></td></tr></table></figure><h2 id="方案一-使用stata魔法函数"><a href="#方案一-使用stata魔法函数" class="headerlink" title="方案一: 使用stata魔法函数"></a>方案一: 使用stata魔法函数</h2><h3 id="安装python模块"><a href="#安装python模块" class="headerlink" title="安装python模块"></a>安装python模块</h3><p>(假设你已经安装好了jupyter notebook)</p><p>你需要使用pip安装两个模块:</p><p>‘’’<br>pip install ipystata<br>pip install psutil<br>‘’’</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>自己随便新建一个notebook , 然后先设置stata的路径:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import ipystata  from ipystata.config import config_stata  config_stata(r&apos;C:\Program Files (x86)\Stata15\StataSE-64.exe&apos;)</span><br></pre></td></tr></table></figure><p>然后你可以使用魔法函数<code>%%stata</code>运行一个stata的输出命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display &quot;hello world&quot;</span><br></pre></td></tr></table></figure><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>把pandas.DataFrame发送给stata使用:</p><p>在python中提前定义好一个df(DataFrame), 然后:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%%stata -d df</span><br></pre></td></tr></table></figure><p>或者把数据从stata输出到python:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%%stata -o df</span><br></pre></td></tr></table></figure><p>为了调试, 运行stata的时候, 可以设置打开stata的操作界面:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%%stata -os</span><br></pre></td></tr></table></figure><p>输出图表:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%%stata -gr</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>还有很多用法请参考github地址: <a href="https://github.com/TiesdeKok/ipystata/blob/master/ipystata/Example.ipynb" target="_blank" rel="noopener">https://github.com/TiesdeKok/ipystata/blob/master/ipystata/Example.ipynb</a></p><h2 id="方案二-使用stata-kernel"><a href="#方案二-使用stata-kernel" class="headerlink" title="方案二: 使用stata kernel"></a>方案二: 使用stata kernel</h2><h3 id="安装方法"><a href="#安装方法" class="headerlink" title="安装方法"></a>安装方法</h3><p>在<code>powershell</code>中执行下面两条命令即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install stata_kernelpython -m stata_kernel.install</span><br></pre></td></tr></table></figure><p>安装输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simpleCollecting stata_kernel  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/10/5c/b0bebe1214f09d50439622ad812fb165b5a8caba1fd0f83d51b67ebc7e4f/stata_kernel-1.5.5-py3-none-any.whl (60kB)    100% |████████████████████████████████| 61kB 2.2MB/sCollecting requests&gt;=2.19.1 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)    100% |████████████████████████████████| 92kB 1.6MB/sCollecting packaging&gt;=17.1 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/89/d1/92e6df2e503a69df9faab187c684585f0136662c12bb1f36901d426f3fab/packaging-18.0-py2.py3-none-any.whlRequirement already satisfied: jupyter&gt;=1.0.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (1.0.0)Requirement already satisfied: pygments&gt;=2.2.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (2.2.0)Requirement already satisfied: jupyter-client&gt;=5.2.3 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (5.2.3)Requirement already satisfied: pywin32&gt;=223; platform_system == &quot;Windows&quot; in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (224)Collecting pexpect&gt;=4.6.0 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/89/e6/b5a1de8b0cc4e07ca1b305a4fcc3f9806025c1b651ea302646341222f88b/pexpect-4.6.0-py2.py3-none-any.whl (57kB)    100% |████████████████████████████████| 61kB 20.5MB/sCollecting pandas&gt;=0.23.4 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/0e/67/def5bfaf4d3324fdb89048889ec523c0903c5efab1a64c8dbe0ac8eec13c/pandas-0.23.4-cp36-cp36m-win_amd64.whl (7.7MB)    100% |████████████████████████████████| 7.7MB 34.2MB/sCollecting regex&gt;=2018.7.11 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/15/a5/cdb73862c207dcbb2dec5a4c64f850314910c55097dfa12cdfc533892502/regex-2018.08.29-cp36-none-win_amd64.whl (255kB)    100% |████████████████████████████████| 256kB 1.9MB/sRequirement already satisfied: ipykernel&gt;=4.8.2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (4.8.2)Collecting IPython&gt;=6.5.0 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/a0/27/29d66ed395a5c2c3a912332d446a54e2bc3277c36b0bbd22bc71623e0193/ipython-7.0.1-py3-none-any.whl (760kB)    100% |████████████████████████████████| 768kB 3.5MB/sCollecting beautifulsoup4&gt;=4.6.3 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/21/0a/47fdf541c97fd9b6a610cb5fd518175308a7cc60569962e776ac52420387/beautifulsoup4-4.6.3-py3-none-any.whl (90kB)    100% |████████████████████████████████| 92kB 452kB/sCollecting pillow&gt;=5.2.0 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/bd/39/c76eaf781343162bdb1cf4854cb3bd5947a87ee44363e5acd6c48d69c4a1/Pillow-5.3.0-cp36-cp36m-win_amd64.whl (1.6MB)    100% |████████████████████████████████| 1.6MB 11.4MB/sRequirement already satisfied: certifi&gt;=2017.4.17 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from requests&gt;=2.19.1-&gt;stata_kernel) (2018.4.16)Requirement already satisfied: urllib3&lt;1.24,&gt;=1.21.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from requests&gt;=2.19.1-&gt;stata_kernel) (1.22)Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from requests&gt;=2.19.1-&gt;stata_kernel) (3.0.4)Requirement already satisfied: idna&lt;2.8,&gt;=2.5 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from requests&gt;=2.19.1-&gt;stata_kernel) (2.6)Requirement already satisfied: six in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from packaging&gt;=17.1-&gt;stata_kernel) (1.11.0)Requirement already satisfied: pyparsing&gt;=2.0.2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from packaging&gt;=17.1-&gt;stata_kernel) (2.2.0)Requirement already satisfied: notebook in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (5.5.0)Requirement already satisfied: ipywidgets in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (7.2.1)Requirement already satisfied: jupyter-console in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (5.2.0)Requirement already satisfied: nbconvert in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (5.3.1)Requirement already satisfied: qtconsole in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (4.3.1)Requirement already satisfied: python-dateutil&gt;=2.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (2.7.3)Requirement already satisfied: pyzmq&gt;=13 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (17.0.0)Requirement already satisfied: traitlets in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (4.3.2)Requirement already satisfied: jupyter-core in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (4.4.0)Requirement already satisfied: tornado&gt;=4.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (5.0.2)Collecting ptyprocess&gt;=0.5 (from pexpect&gt;=4.6.0-&gt;stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whlRequirement already satisfied: pytz&gt;=2011k in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from pandas&gt;=0.23.4-&gt;stata_kernel) (2018.4)Requirement already satisfied: numpy&gt;=1.9.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from pandas&gt;=0.23.4-&gt;stata_kernel) (1.14.1)Requirement already satisfied: colorama; sys_platform == &quot;win32&quot; in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.3.9)Requirement already satisfied: decorator in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (4.3.0)Requirement already satisfied: pickleshare in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.7.4)Requirement already satisfied: simplegeneric&gt;0.8 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.8.1)Requirement already satisfied: setuptools&gt;=18.5 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (28.8.0)Requirement already satisfied: jedi&gt;=0.10 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.12.0)Collecting prompt-toolkit&lt;2.1.0,&gt;=2.0.0 (from IPython&gt;=6.5.0-&gt;stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/e5/c5/f1ee6698bdcf615f171a77e81ca70293b16a6d82285f1760b388b4348263/prompt_toolkit-2.0.5-py3-none-any.whl (334kB)    100% |████████████████████████████████| 337kB 12.8MB/sRequirement already satisfied: backcall in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.1.0)Requirement already satisfied: Send2Trash in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (1.5.0)Requirement already satisfied: terminado&gt;=0.8.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.8.1)Requirement already satisfied: jinja2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (2.10)Requirement already satisfied: nbformat in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (4.4.0)Requirement already satisfied: ipython-genutils in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.2.0)Requirement already satisfied: widgetsnbextension~=3.2.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from ipywidgets-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (3.2.1)Requirement already satisfied: mistune&gt;=0.7.4 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.8.3)Requirement already satisfied: bleach in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (1.5.0)Requirement already satisfied: entrypoints&gt;=0.2.2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.2.3)Requirement already satisfied: testpath in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.3.1)Requirement already satisfied: pandocfilters&gt;=1.4.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (1.4.2)Requirement already satisfied: parso&gt;=0.2.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jedi&gt;=0.10-&gt;IPython&gt;=6.5.0-&gt;stata_kernel) (0.2.0)Requirement already satisfied: wcwidth in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from prompt-toolkit&lt;2.1.0,&gt;=2.0.0-&gt;IPython&gt;=6.5.0-&gt;stata_kernel) (0.1.7)Requirement already satisfied: pywinpty&gt;=0.5; os_name == &quot;nt&quot; in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from terminado&gt;=0.8.1-&gt;notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.5.1)Requirement already satisfied: MarkupSafe&gt;=0.23 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jinja2-&gt;notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (1.0)Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbformat-&gt;notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (2.6.0)Requirement already satisfied: html5lib!=0.9999,!=0.99999,&lt;0.99999999,&gt;=0.999 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from bleach-&gt;nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.9999999)spacy 2.0.11 has requirement regex==2017.4.5, but you&apos;ll have regex 2018.8.29 which is incompatible.jupyter-console 5.2.0 has requirement prompt-toolkit&lt;2.0.0,&gt;=1.0.0, but you&apos;ll have prompt-toolkit 2.0.5 which is incompatible.Installing collected packages: requests, packaging, ptyprocess, pexpect, pandas, regex, prompt-toolkit, IPython, beautifulsoup4, pillow, stata-kernel  Found existing installation: requests 2.18.4    Uninstalling requests-2.18.4:      Successfully uninstalled requests-2.18.4  Found existing installation: pandas 0.23.0    Uninstalling pandas-0.23.0:      Successfully uninstalled pandas-0.23.0  Found existing installation: regex 2017.4.5    Uninstalling regex-2017.4.5:      Successfully uninstalled regex-2017.4.5  Found existing installation: prompt-toolkit 1.0.15    Uninstalling prompt-toolkit-1.0.15:      Successfully uninstalled prompt-toolkit-1.0.15  Found existing installation: ipython 6.4.0    Uninstalling ipython-6.4.0:      Successfully uninstalled ipython-6.4.0  Found existing installation: Pillow 5.1.0    Uninstalling Pillow-5.1.0:      Successfully uninstalled Pillow-5.1.0Successfully installed IPython-7.0.1 beautifulsoup4-4.6.3 packaging-18.0 pandas-0.23.4 pexpect-4.6.0 pillow-5.3.0 prompt-toolkit-2.0.5 ptyprocess-0.6.0 regex-2018.8.29 requests-2.19.1 stata-kernel-1.5.5You are using pip version 18.0, however version 18.1 is available.You should consider upgrading via the &apos;python -m pip install --upgrade pip&apos; command.</span><br></pre></td></tr></table></figure><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><p>打开你的notebook:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure><p>然后, 创建一个statanotebook:</p><p><img src="https://mlln.cn/2018/11/01/%E5%AE%89%E8%A3%85stata%E5%B9%B6%E5%9C%A8jupyter-notebook%E4%B8%AD%E8%B0%83%E7%94%A8/stata.png" alt="img"></p><p>最后, 你就可以在cell中写stata命令了:</p><p><img src="https://kylebarron.github.io/stata_kernel/img/starting_jupyter_notebook.gif" alt="img"></p><ul><li>在新建的notebook中，通过下图可以初步判断是否关联成功。</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/17516282-c0a1880ab695006c.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200" alt="img"></p><ul><li>在命令行输入stata命令，并点击<code>运行</code>执行。如果关联成功，则会在命令的下方显示stata结果窗口的结果。具体如下图所示：</li></ul><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">dis</span> 1+3</span><br><span class="line"><span class="keyword">sysuse</span> auto,<span class="keyword">clear</span></span><br><span class="line"><span class="keyword">reg</span> price weight</span><br><span class="line"><span class="keyword">scatter</span> price weight</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/17516282-b566cbff21ac6eaf.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0
      
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="jupyter" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/jupyter/"/>
    
    
      <category term="SAS" scheme="http://yuanquanquan.top/tags/SAS/"/>
    
  </entry>
  
  <entry>
    <title>saspy——在jupyter里写sas代码</title>
    <link href="http://yuanquanquan.top/2019/2019090521/"/>
    <id>http://yuanquanquan.top/2019/2019090521/</id>
    <published>2019-11-24T04:59:15.000Z</published>
    <updated>2019-11-24T11:22:49.047Z</updated>
    
    <content type="html"><![CDATA[<p><img src="data:image/jpeg;base64,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" alt="img"></p><p>saspy, 即是一个连接sas服务器和Python解释器的一个API, 适合在jupyter Notebook里面编辑代码，如果你安装了Anaconda，即是安装了Jupyter Notebook。安装saspy API:</p><p>（1）打开Anaconda Prompt环境：pip install saspy</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131019.png" alt></p><p>（2）复制一个sascfg.py, 并重命名为sascfg_personal.py</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131030.png" alt></p><p>（3）改sascfg_personal.py里面的参数：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124135438.png" alt></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131038.png" alt></p><p>改过之后的：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131044.png" alt></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124135506.png" alt></p><p>到此saspy的配置ok了。</p><p>（4）还有一个PATH环境变量需要配置：</p><p>C:\Program Files\SASHome\SASFoundation\9.4\core\sasext</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124135821.png" alt></p><p>（5）安装sas内核，sas kernel, 回到Anaconda Prompt环境：</p><p>命令：</p><p>pip install sas_kernel<br>conda install -c conda-forge saspy<br>jupyter kernelspec list</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124135510.png" alt></p><p>看到这个，说明已经安装好了，接下来体验一下。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131049.jpg" alt></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131056.png" alt></p><p>我们先来进入Python3试试：</p><p>在sas里面成为数据集，在Python里面成为dataframe数据框，主要以pandas标准库来处理数据。由sasdata转化为Python的数据框, 然后后面就是纯Python语言处理了。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131116.jpg" alt></p><p>试试Python作图：</p><p><strong>柱状图：</strong></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131127.jpg" alt></p><p><strong>热力图:</strong></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131131.jpg" alt></p><p>甚至还可以画化学药物结构：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131135.png" alt></p><p>还可以批量画化学药物结构：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131139.jpg" alt></p><p>再切换至SAS服务器内核：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131143.jpg" alt></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131151.png" alt></p><p>比如用sas<strong>作图</strong></p><p><strong>生存分析曲线</strong></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131154.png" alt></p><p><strong>比如森林图：</strong></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131157.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0
      
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="jupyter" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/jupyter/"/>
    
    
      <category term="SAS" scheme="http://yuanquanquan.top/tags/SAS/"/>
    
  </entry>
  
  <entry>
    <title>十问 | 沉思录及其他</title>
    <link href="http://yuanquanquan.top/2019/2019091122/"/>
    <id>http://yuanquanquan.top/2019/2019091122/</id>
    <published>2019-11-22T10:35:03.000Z</published>
    <updated>2019-11-22T10:49:24.082Z</updated>
    
    <content type="html"><![CDATA[<p>​     终于完了这本霍金沉思录，也是霍金先生的遗作，这位伟大的思想家留给世界的最后的礼物。作为多年来的霍金粉丝，读这部作品是酣畅淋漓，但也不乏惆怅满腹。毕竟，我们不会再有机会读到先生的作品了。</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/ml-data/2018/R/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191122184312.jpg?raw=true" alt></p><p>​      史蒂芬 ·  霍金在2018年3月14日逝世，这天恰好是爱因斯坦的生日，霍金出生于1942年1月8日，又恰好是伽利略逝世三百周年，很多人都在感慨历史的惊人巧合；而他自己，在和脊髓侧索硬化症搏斗了55年之后离开了这蓝色的星球，结束了传奇的一生，飞向那浩渺的星空。</p><p>​      爱因斯坦曾说，“If you can’t explain it simply, you don’t really understand  it.”-  即如果你不能简单地解释一件事，那么你就没有真正理解那个东西。从这个角度说，霍金先生是真正理解他研究的宇宙的人，因为他可以用简单明了生动的方式来阐述物理，但很多职业物理学家缺乏这样的能力。这也是为什么鲜有职业物理学家涉足科普，因为实际上用文字去表达本应用数学来表达的东西本身就是一个巨大的挑战，除非你真正理解那件事并且恰好是个充满激情与想象力的思想家。显然，霍金先生正是这样的人。否则在历史上也不会有时间简史的巨大成功。</p><p>​        “十问”中的文字除了严密的科学思维和物理学家一本正经做科普的决心，还有从时间简史以来一以贯之的霍金式幽默，比如说吐槽语音合成器给他带来的美国口音，吐槽英国脱欧，又比如调侃幼年的自己曾经在二战德国V2导弹的爆炸范围内幸免于难，或者是坦率的回忆起中学同学的嘲笑：“史蒂芬·霍金将不会有所作为。”</p><p>​    当然，也有幸福与快乐的流露，比如在夕阳下看着孩子们在草坪上玩耍，又比如在国际空间站体验零重力的纯粹的快乐，那是真正摆脱轮椅束缚的自由时刻。</p><p>​       所以很明显，这本书和以前的作品相比更多的是霍金本人对于个人生命轨迹的回顾和对人类命运的关怀；对于家人和孩子们的爱，特别是对于第一任妻子简·怀尔德多年无私照料与付出的谢意；以及他自己经年累月与脊髓侧索硬化症的斗争、妥协与学会共存的心路历程；还有，不可不提的是，诸多重要物理学贡献，如黑洞熵理论、宇宙奇点、霍金辐射、包括他自己认为的最重要成就  - 宇宙无边界理论的建立过程中 - 那些无数的筚路蓝缕和灵光一现的时刻。</p><p>​      所有这些特质 -  残疾的天才物理学家、超过职业范围的好奇心、对公共事务的关心与表达，从哪个维度上说，都超越了一个物理学家的职业关切，而让霍金成为一个颇具争议性的人物。很少有人质疑他作为一个物理学家的杰出，但我相信，反对他的有关社会和外星人观点的人和赞同他的人一样多。但恐怕每个人都必须承认他热切的好奇心和探索欲，以及深思熟虑的能力，因为我们不得不惊讶于他对那些超过自己专业领域问题的见解仍然是发人深省的。</p><p>​      霍金喜欢问那些大问题，并且愿意去不懈探索它们的答案，正如这本书所展示的十个大问题。提出它们的意义不是去解决这些问题，事实上这些问题甚至我们都不知道是否是可以被解决的，遑论完成的具体步骤。但霍金对它们的思索却给了我们方向和可能性，从本质上说，他有着一种朴素的世界观，既然物理确实描述了这个世界，那么它一定包含着回答这些大问题所需要的信息，即使只是间接的提示。他清楚地知道，长期来说，我们的未来取决于我们对自然的理解程度。显然，他走在正确的道路上。并且正是由无数的霍金先生这样的巨擘不断地提出大问题，我们才越来越深入地认识这个陌生的世界，从而指引我们的生存，发展，寻得与自然的共存之道。</p><p>​       历史不会撒谎，这条道路早已显示出它的轮廓。正是因为对牛顿力学和热力学的掌握，才迎来了蒸汽机和内燃机的时代；正是因为电磁学的发展，才迎来了电力时代和信息时代；而计算机技术和量子技术的发展将带领我们进入新的未知时代，我们的计算能力将获得巨大的提升，我们将以前所未有的精度和速度去研究、模拟这个星球上的极其复杂的重要问题，比如环球大气运动、洋流、还有我们自己的细胞的运作、社会的演化等等。我不是盲目的技术乐观主义者，但我相信，这些时代的开创至少多养活了世界上大多数的人口，极大减少了来自环境的生存压力，虽然它们也带来了特定的问题，但相比于只有极少数高大强壮的人才能躲过野兽追击并存活下来的蛮荒年代，现代的依赖科学和技术维持的社会系统要宽容的多。我们可以依靠自己的思想获得生存的机会，我们不需要那样的身材去生存（虽然那样的人仍然更受异性的欢迎），生命的意义是多元化的，就像霍金先生这样只有眼和手指能动的人，同样对整个人类做出了大的贡献。</p><p>​      事实是，有很多身体残疾的杰出的人，这种东西原本不应该成为评判的一部分标准。但即便如此，你仍然无法将霍金先生的巨大贡献同他所忍受的经年之病痛割离开来。五十五年同病魔的斗争，同时还取得了正常人都难以望其项背的成就，真的是一件叹为观止的事。那个卧在轮椅上，依靠电脑合成发声的智者形象，激励了一代又一代崇尚真理的人努力奋斗，这是一种额外的津贴，或许比他的工作所产生的直接贡献更有意义。</p><p>​      这个额外的津贴，就像他在21岁被诊断患有脊髓侧索硬化症并被告知余日不多时他说的那样：“从那之后，一切都成为了额外津贴”。“我对生活的期望降到了最低，我抓紧所有的时间去做那些最要紧的事”。苹果的开创者乔布斯说，不要去患得患失，永远去做你想做的最重要的事，就好像生命的最后一天。对于霍金先生而言，这个最后一天，推迟了五十五年才来到，而他也成为了那个“不会有所作为的人”的相反面。</p><p>​       在我看来，霍金是绝世聪明的科学家没错，但绝不是同时代最聪明的，正如杨振宁先生常谈论霍金为什么拿不到诺贝尔奖。但霍金绝对是有着最好的科学直觉与远见的人之一，用英语来说，他是一个visionary的人。他晓得哪些东西对于认识世界是真正重要的，而不会囿于旁枝末节。这是一种惊人的能力，这恰恰也是爱因斯坦相比他同时代的智者们更高明的地方。再次引用杨先生的话，这是一种既能看到全景（big   picture）,又能注意到细节的微妙之处的洞察力。而霍金先生就具备这样的能力。或许他只是不够幸运，正年轻事业启航就被命运禁锢到轮椅之上；而或许他已经足够幸运，束缚于轮椅之上还能有着莎士比亚戏剧名言-“即使身处果壳之中，我也是无限空间之王”的乐观。此“王”不是统治压迫和权威的王，而是彻悟的自由。最后他能长眠于牛顿和达尔文的墓碑之间，委实也是世界对他的最高的认可了罢，诺奖之于此，也显得不那么重要了。</p><p>​       于我个人而言，霍金的人生，正如上述文字，启迪太多，我也如千千万万被他的故事感动的人们一样，虔诚地瞻仰与阅读他留给世界的回音。这回音很可能仍会在人类社会中激荡多年，因为它杰出地代表着人类最重要的特质  -  对真理的纯粹探索与理性精神。但最震撼我的，却是霍金在其博士论文封面上写下的这句话，那时的他已经逐渐无法握稳一支笔，歪歪扭扭的字体，却无比感动。完全能够想象他艰难地书写着这几个简单的单词的样子，坚毅而灵性。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/ml-data/2018/R/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191122184812.jpg" alt></p><p>This dissertation is my original work.</p><p>—- S. W. Hawking</p><p>这篇博士论文是我的原创性工作。</p><p>—-S. W. 霍金</p><p>​    其实，科学家能做的最酷的一件事，莫过于，说出这样的话了。因为每一个深刻而原创的理论都是真理在灵童耳畔的轻吟。</p><p>​       我有时候也在想，究竟是什么在支撑着一个有着残缺的躯体，但心灵却异常美丽而完整的灵魂不断地坚持探索下去。难道仅仅是因为对物理学的热爱么？可能答案不止于此。在2014年上映的电影《万物理论》中，“小雀斑”埃迪·雷德梅恩扮演的霍金本尊可能已经回答了这个问题。正是在那个他本人在书中调侃的美国口音的合成声音里，他说：</p><p>There is no boundary in human endeavors<br>However bad life seems<br>Where there is life<br>There is hope</p><p>人类的探索是没有边界的<br>无论生活看起来多糟<br>哪里有生命<br>哪里就有希望</p><p>​     我想这就是问题的答案吧。是电影的高潮，也是霍金先生留给我们最大的启示。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/ml-data/2018/R/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191122184323.jpg" alt></p><p>​     埃迪·雷德梅恩也被邀请给这本十问 | 霍金沉思录作序，在序言的末尾，他引用了奥巴马的一句话：我希望史蒂芬在灿烂的星空中玩得开心。这里快要结束这些零散文字时，我想这句话也正是我内心涌动的音符。</p><p>​     事实上，对于霍金先生而言，是否有我这样一个来自中国的粉丝的欣赏是完全没有意义的。但毕竟像我这样的不够高大强壮无法躲避野兽追击的弱者居然能够在这里欣赏这样美丽的灵魂，并且决定在太阳升起之前依然保持对明天的信心，不能不说是一种奇迹。</p><p>​       最后，也是最重要的，致敬史蒂芬，一个叹为观止的人，以及他传奇的一生。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/ml-data/2018/R/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191122184337.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​     终于完了这本霍金沉思录，也是霍金先生的遗作，这位伟大的思想家留给世界的最后的礼物。作为多年来的霍金粉丝，读这部作品是酣畅淋漓，但也不乏惆怅满腹。毕竟，我们不会再有机会读到先生的作品了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/Ba
      
    
    </summary>
    
      <category term="生活" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="随笔" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活" scheme="http://yuanquanquan.top/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
</feed>
