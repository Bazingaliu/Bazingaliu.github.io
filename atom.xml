<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yuanquanquan的个人博客 | 我愿做你光华中淡淡的一笔</title>
  
  <subtitle>我愿做你光华中淡淡的一笔</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanquanquan.top/"/>
  <updated>2021-03-27T09:04:45.491Z</updated>
  <id>http://yuanquanquan.top/</id>
  
  <author>
    <name>Yann</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>写给憨憨姐姐的远程炼丹</title>
    <link href="http://yuanquanquan.top/2021/202103225/"/>
    <id>http://yuanquanquan.top/2021/202103225/</id>
    <published>2021-03-26T14:14:49.000Z</published>
    <updated>2021-03-27T09:04:45.491Z</updated>
    
    <content type="html"><![CDATA[<p>啦啦啦啦啦啦</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;啦啦啦啦啦啦&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="远程炼丹" scheme="http://yuanquanquan.top/tags/%E8%BF%9C%E7%A8%8B%E7%82%BC%E4%B8%B9/"/>
    
  </entry>
  
  <entry>
    <title>《组合优化》部分结果整理-1</title>
    <link href="http://yuanquanquan.top/2021/20210314/"/>
    <id>http://yuanquanquan.top/2021/20210314/</id>
    <published>2021-03-13T17:11:22.000Z</published>
    <updated>2021-03-14T14:30:26.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>​    《Combinatorial Optimization Theory and Algorithms(6th,2018)》是一本内容相当广泛而全面的教材，但因其证明的晦涩使人望而却步，本文的目的是整理本学期所学内容涉及的相关结果，仅给出概括性描述，如对相关结果感兴趣，可从此书中找到相关内容及引文。</p></blockquote><a id="more"></a>  <ul><li><p>多项式时间算法：算法运行时间为<br>$$<br>O(n^k)<br>$$</p></li><li><p>强多项式时间算法：此问题的运行时间不因输入的数字大小而变动，而是依照输入数据的结构复杂度</p></li><li><p>给定图的极小连通支撑子图是支撑树，我们的目标是找一个最小（权）的树</p></li><li><p><strong>钻孔问题：</strong>在完全图中寻找一条含有所有顶点的最短路(最短的Hamiltonian path)。</p></li><li><p><strong>最小树形图</strong>（带有方向的树）<strong>问题：</strong>在有向图中找一个带有方向的最小支撑树。</p></li><li><p>求解<strong>最小支撑树问题</strong>的“贪婪算法”：</p></li><li><p><strong>Kruskal算法</strong>(多项式时间算法):O(mn) → O(mlogn) → O(mα(m,n))(几乎是线性的)</p></li></ul><p><em>Idea：</em>从一个空图开始，每次增加原图中权最小的边，同时要避开长出圈。</p><ul><li><strong>Prim算法</strong>(多项式时间算法)：O(n2)→O(m+nlogn)（结合Fibonacci堆）→O(mlogβ(n,m))</li></ul><p><em>Idea:</em> 任选一个顶点作为初始集，每次不断选取初始集同剩余集间的最小权边，最终将一个点的初始集长成V(G)。</p><ul><li><p>(<strong>Cheriton, Tarjan 1976</strong>) 平面图的最小支撑树问题能线性时间被求解。</p></li><li><p>平面上n个点的最小支撑树问题能在O(nlogn)时间内被求解。</p></li><li><p><img src="https://i.loli.net/2021/03/14/G7AdgNIac2x1nlR.png" alt></p></li><li><p>(<strong>Karger,Klein,Tarjan 1976</strong>)寻找最小支撑树的一个<strong>随机算法有线性期望</strong>的运行时间。</p></li><li></li><li><p>最小树形图（带有方向的树）问题等价于<strong>最大权分支问题</strong>：</p></li><li><p><strong>Edmonds分支算法</strong> O(mn) →O(m+nlogn)（<strong>Gabow等1986</strong> 用Fibonacci得到）</p></li><li><p>寻求<strong>有向图G中边不交支撑树形图(根节点r)的最大集</strong>的多项式时间算法：目前最有效算法由<strong>Gabow(1995)</strong>给出。</p></li><li><p>处理更困难的组合最优化问题最常见的子问题：<strong>最小支撑树+最短路。</strong></p></li><li><p><strong>非负边权的最短路问题</strong>：</p></li><li><p>如果<strong>所有权值为1</strong>，则最短路问题可直接由BFS求解；</p></li><li><p>对<strong>权值都为正整数</strong>，可将边e替换成长度为c(e)的路，然后再运用BFS(可能增加指数多条边，很笨！)；</p></li><li><p>更为高明的构思是<strong>Dijkstra(1959)算法</strong>(已知最好的强多项式时间算法): O(n2)→ O(m+nlogn)（<strong>Fredman，Tarjan, 1987</strong> 使用Fibonacci堆）；</p></li><li><p>假设权是<strong>固定范围内的整数</strong>（0到C之间的整数），<strong>Dial(1969)</strong>用一个编号为0,…,|V(G)|·C的数组，按照当前的l值去储存顶点，从而得到简单的线性时间算法；</p></li><li><p>对<strong>平面有向图</strong>，<strong>Henzinger等(1997)</strong>给出了线性时间算法；</p></li><li><p>对<strong>非负整数权的无向图</strong>，<strong>Thorup(1999)</strong>找到了线性时间算法。（在无向图中寻找最短路极其困难）</p></li><li><p>一般守恒权（每条边权不需要非负，只要求不存在总权为负的圈(负圈)就好了）</p></li><li><p><strong>Moore-Bellman-Ford算法</strong>(迄今最快的强多项式时间算法)：O(nm)</p></li><li><p>边权为整数且有下界cmin时，<strong>Goldberg(1995)</strong>的缩放尺度法具有运行时间O(√nmlog|cmin|+2)</p></li><li><p>平面图，<strong>Fakcharoenphol和Rao(2006)</strong>阐述了一个O(nlog3n)算法。</p></li><li><p>若G含有负圈，则至今未有多项式时间算法（问题变为NP-hard）</p></li><li><p>对给定一个有向图G，权c:E(G)→R，可在O(nm)时间找到一个可行势，或找到负圈。</p></li></ul><ul><li><p><strong>全点对最短路问题</strong>（找一个有向图的所有顶点有序对(s,t)求出最短的s-t路）：</p></li><li><p>调用n次<strong>Moore-Bellman-Ford算法</strong>，每次选定一个s：O(n2m)→O(mn+n2logn)(<strong>P**</strong>ettie 2004**，目前已知最好的时间界)；</p></li></ul><ul><li><p>对于非负权的稠密图，<strong>Chan(2007)</strong>得到界O(n3log3logn/log2n)；</p></li><li><p>对于<strong>所有边权都是较小的正整数</strong>，<strong>Zwick(2002)</strong>运用快速矩阵乘积可以改进Pettie的时间界。</p></li><li><p><strong>Floyd-Warshall**</strong>算法**：O(n3)</p></li><li><p>在<strong>具有守恒权的无向图G</strong>中，求全部点对之间的最短路问题可以在O(n4)时间内解决。<strong>Gabow(1983)</strong>将运行时间改进到O(min{n3,nmlogn})</p></li></ul><ul><li><p>在一个具有守恒权的有向图中，我们用上述最短路算法容易求出最小总权的圈。</p></li><li><p>如何求平均权为最小的圈？<strong>最小平均圈问题</strong>。</p></li><li><p><strong>Karp(1978)**</strong>最小平均圈算法**:O(nm) 注：对任意边权的无向图，此算法不能用来求最小平均权的圈。</p></li></ul><ul><li><p><strong>最大流问题</strong></p></li><li><p><strong>Ford-Fulkerson(1957**</strong>)最大流算法**：整值容量，指数次增流，存在整值的最大流，所以算法一定在有限步结束。</p></li></ul><p><em>Idea：</em>找可扩路，再沿可扩路增流</p><p>注：如果<strong>允许容量取无理数</strong>，在选择可扩路时运气不好，算法可能不会终止。</p><p><img src="https://i.loli.net/2021/03/14/sIK5dzgX3ZGUfaj.png" alt></p><ul><li><strong>Edmonds-Karp(1972)</strong>算法（最大流问题的第一个多项式时间算法）O(m2n)（至多有mn/2次增流，每次增流运用BFS的时间是O(m)）</li></ul><p><em>Idea.</em> 把任选一条可扩路修改为找一条最短的可扩路。</p><ul><li>分层图容易由BFS在O(m)时间构造出来。</li><li><strong>Dinic算法</strong>（多项式时间算法）：至多进行n-1个阶段，每个阶段O(nm)→O(n2)</li></ul><p><strong>Sleator, Tarjan(1983)</strong>用一种称为动态树的数据结构使得Dinic算法成为最大流问题的O(mnlogn)算法</p><ul><li><p><strong>Fujishige(2003)算法</strong>（弱多项式时间算法，简单，缩放尺度技术）：对简单有向图G及整容量可在O(mnlogumax)时间内正确求解最大流问题。</p></li><li><p><strong>Goldberg,Tarjan(1988)**</strong>推流-重标算法<strong>：O(n2√m</strong>)<strong>→O(nmlog(n2/m))（</strong>Goldberg,Tarjan 1988<strong>）→O(nmlog2+m/(nlogn)n)(</strong>King, Rao, Tarjan 1994**)</p></li><li><p><img src="https://i.loli.net/2021/03/14/v9IJQqWtbCTg4dZ.png" alt></p></li><li><p><img src="https://i.loli.net/2021/03/14/ghpbx12jq6aF593.png" alt></p></li></ul><ul><li><p>一个具有n个顶点及m条边的<strong>最小费用流问题</strong>的实例可以变换为等价的<strong>Hitchcock运输问题</strong>的实例，其中有n+m个顶点及2m条边。</p></li><li><p>求解最小费用流的<strong>最小平均圈消去算法</strong>(强多项式时间算法)：O(m3n2logn)</p></li></ul><p><em>Idea.</em> 首先用最大流算法求出一个b-流，然后逐次沿负权的可扩圈增流，直到没有这样的圈为止，为得到多项式的运行时间，每次选择一个平均权最小的圈是有必要的。</p><ul><li><strong>容量缩放算法</strong>(最小费用流问题的第一个多项式时间算法)：O(n(m+nlogn)logbmax)</li><li><strong>Orlin算法</strong>(已知最快捷的强多项式时间算法)：只能处理无容量限制问题 O(nlogm(m+nlogn))</li><li><p>原始的<strong>网络单纯形法</strong>不是多项式时间算法，但它在实用上是十分有效的。<strong>Orlin(1997)</strong>提出一种变形，可在多项式时间运行。多项式时间的<strong>对偶网路单纯形法</strong>由<strong>Orlin, Plotkin</strong>与<strong>Tardos(1993)</strong>以及<strong>Armstrong与Jim(1997)</strong>得到。</p></li><li><p><strong>Ford与Fulkerson(1958)：</strong>动态最大流（每条边上的流量可以随时间变化，而进入一条边的流要在规定的延迟时间之后才能到达另一端）问题可以用同最小费用流问题一样的运行时间求解。</p></li><li><strong>Hoppe与Tardos(2000)</strong>运用子模函数最小化解决了<strong>最速转运问题</strong>，其中有多源多汇且有整数传输时间。</li><li><strong>Klinz, Woeginger(2004)</strong>证明了求最小费用动态流是NP-hard的。</li></ul><hr><p><strong>附录：一些有价值的结论</strong></p><ul><li><p>关于阶乘的一个不等式</p><p><img src="https://i.loli.net/2021/03/14/niTY3GwRksNFHpv.png" alt></p></li><li><p><img src="https://i.loli.net/2021/03/14/fjGAEcoilMu813v.png" alt></p></li><li><p><img src="https://i.loli.net/2021/03/14/xcrBGktMIHDUAsa.png" alt></p></li><li><p>按照边的权重排边可以使用合并整序算法，时间复杂度O(mlogm)。</p><p><img src="https://i.loli.net/2021/03/14/shqmzIKSuokdPiL.png" alt></p></li><li><p>判定一个至多有n条边的图中的圈能在O(n)内完成(用DFS\BFS来完成。</p></li><li><p>满足<br>$$<br>|E(G)|&gt;\binom{|V(G)|-1}{2}<br>$$<br>的任一无向图G是连通的。</p></li></ul><hr><p><em>就像格罗滕迪克所说：<strong>“构成一个研究者创造力和想象力的本质，是他们聆听事情内部声音的能力。</strong>”这里没有等级高下，没有阶层之分，在对未知的探索前人人平等，每个人都拥有绝对的自由。</em></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;​    《Combinatorial Optimization Theory and Algorithms(6th,2018)》是一本内容相当广泛而全面的教材，但因其证明的晦涩使人望而却步，本文的目的是整理本学期所学内容涉及的相关结果，仅给出概括性描述，如对相关结果感兴趣，可从此书中找到相关内容及引文。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="-优化理论" scheme="http://yuanquanquan.top/tags/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>Jane Street日内高频交易预测</title>
    <link href="http://yuanquanquan.top/2021/20210205/"/>
    <id>http://yuanquanquan.top/2021/20210205/</id>
    <published>2021-02-05T08:28:52.000Z</published>
    <updated>2021-02-05T09:39:48.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一次kaggle比赛，这次比赛的场景主要是Jane Street日内高频交易，是短线模型，使用夏普率衡量模型效果，但是提供的数据并非原始数据，不清楚feature含义。</p></blockquote><a id="more"></a> <h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><h3 id="1-1-Description"><a href="#1-1-Description" class="headerlink" title="1.1 Description"></a><strong>1.1 Description</strong></h3><p>In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their “fair values” and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.</p><p>Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. </p><h3 id="1-2-Evaluation"><a href="#1-2-Evaluation" class="headerlink" title="1.2 Evaluation"></a><strong>1.2 Evaluation</strong></h3><ul><li><p>Utility score</p><p>Each row in the test set represents a trading opportunity for which <strong>you will be predicting an <code>action</code> value, 1 to make the trade and 0 to pass on it.</strong> Each trade <code>j</code> has an associated <code>weight</code> and <code>resp</code>, which represents a return.</p></li></ul><p>$$<br>p_i = \sum_j(weight_{ij} <em> resp_{ij} </em> action_{ij}),<br>$$</p><p>$$<br>t = \frac{\sum p_i }{\sqrt{\sum p_i^2}} * \sqrt{\frac{250}{|i|}},<br>$$<br>                where |i| is the number of unique dates in the test set. The utility is                 then defined as:<br>$$<br>u = min(max(t,0), 6)  \sum p_i.<br>$$</p><blockquote><p> <a href="https://www.kaggle.com/renataghisloti/understanding-the-utility-score-function" target="_blank" rel="noopener">https://www.kaggle.com/renataghisloti/understanding-the-utility-score-function</a></p></blockquote><ul><li><p>_Pi_ </p><p>Each row or trading opportunity can be chosen (action == 1) or not (action == 0). </p><p>The variable _pi_ is a indicator for each day _i_, showing how much return we got for that day.</p><p>Since we want to maximize u, we also want to maximize _pi_. To do that, we have to select the least amount of negative <em>resp</em> values as possible (since this is the only negative value in my equation and only value that would make the total sum of p going down) and maximize the positive number of positive <em>resp</em> transactions we select.</p></li><li><p><em><code>t</code></em> </p><p><strong>_t_</strong> is <strong>larger</strong> when the return for <strong>each day is better distributed and has lower variation.</strong> It is better to have returns uniformly divided among days than have all of your returns concentrated in just one day. It reminds me a little of a <strong>_L1_</strong> over <strong>_L2_</strong> situation, where the <strong>_L2_</strong> norm penalizes outliers more than <strong>_L1_</strong>.</p></li></ul><p>  Basically, we want to select uniformly distributed distributed returns over days, maximizing our return but giving a penalty on choosing too many dates.</p><ul><li>t is simply the annualized sharpe ratio assuming that there are 250 trading days in a year, an important risk adjusted performance measure in investing. If sharpe ratio is negative, utility is zero. A sharpe ratio higher than 6 is very unlikely, so it is capped at 6. The utility function overall try to maximize the product of sharpe ratio and total return.</li></ul><hr><h2 id="2-Implementing"><a href="#2-Implementing" class="headerlink" title="2. Implementing."></a>2. Implementing.</h2><blockquote><p><a href="https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning" target="_blank" rel="noopener">https://www.kaggle.com/vivekanandverma/eda-xgboost-hyperparameter-tuning</a></p><p><a href="https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners" target="_blank" rel="noopener">https://www.kaggle.com/wongguoxuan/eda-pca-xgboost-classifier-for-beginners</a></p><p><a href="https://www.kaggle.com/smilewithme/jane-street-eda-of-day-0-and-feature-importance/edit" target="_blank" rel="noopener">https://www.kaggle.com/smilewithme/jane-street-eda-of-day-0-and-feature-importance/edit</a></p></blockquote><h3 id="2-0-Preprocessing"><a href="#2-0-Preprocessing" class="headerlink" title="2.0 Preprocessing"></a>2.0 Preprocessing</h3><h3 id="2-1-EDA"><a href="#2-1-EDA" class="headerlink" title="2.1. EDA"></a>2.1. EDA</h3><p><strong>Market Basics:</strong> Financial market is a dynamic world where investors, speculators, traders, hedgers understand the market by different strategies and use the opportunities to make profit. They may use fundamental, technical analysis, sentimental analysis,etc. to place their bet. As data is growing, many professionals use data to understand and analyze previous trends and predict the future prices to book profit.</p><p><strong>Competition Description:</strong> The dataset provided contains set of features, <strong>feature_{0…129}</strong>,representing real stock market data. Each row in the dataset represents a trading opportunity, for which we will be predicting an action value: <strong>1</strong> to make the trade and <strong>0</strong> to pass on it. </p><p>Each trade has an associated weight and resp, which together represents a return on the trade. In the training set, <strong>train.csv</strong>, you are provided a <strong>resp</strong> value, as well as several other <strong>resp_{1,2,3,4}</strong> values that represent returns over different time horizons.</p><p>In <strong>Test set</strong> we don’t have <strong>resp</strong> value, and other <strong>resp_{1,2,3,4}</strong> data, so we have to use only <strong>feature_{0…129}</strong> to make prediction.</p><p>Trades with <strong>weight = 0</strong> were intentionally included in the dataset for completeness, although such trades <strong>will not</strong> contribute towards the scoring evaluation. So we will ignore it.</p><h3 id="2-2-Using-XGBoost-Algorithm"><a href="#2-2-Using-XGBoost-Algorithm" class="headerlink" title="2.2 Using XGBoost Algorithm"></a>2.2 Using XGBoost Algorithm</h3><h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost?"></a>XGBoost?</h1><p>it is contents of ensemble learning.</p><blockquote><p><a href="https://www.youtube.com/watch?v=VHky3d_qZ_E" target="_blank" rel="noopener">https://www.youtube.com/watch?v=VHky3d_qZ_E</a></p><p><a href="https://www.youtube.com/watch?v=1OEeguDBsLU&amp;list=PL23__qkpFtnPHVbPnOm-9Br6119NMkEDE&amp;index=4" target="_blank" rel="noopener">https://www.youtube.com/watch?v=1OEeguDBsLU&amp;list=PL23__qkpFtnPHVbPnOm-9Br6119NMkEDE&amp;index=4</a></p><p><a href="https://www.youtube.com/watch?v=4Jz4_IOgS4c" target="_blank" rel="noopener">https://www.youtube.com/watch?v=4Jz4_IOgS4c</a></p><p><a href="https://www.youtube.com/watch?v=VkaZXGknN3g&amp;feature=youtu.be" target="_blank" rel="noopener">https://www.youtube.com/watch?v=VkaZXGknN3g&amp;feature=youtu.be</a></p><p><a href="https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost" target="_blank" rel="noopener">https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost</a></p></blockquote><h2 id="2-2-1-Ensemble-learning"><a href="#2-2-1-Ensemble-learning" class="headerlink" title="2.2.1 Ensemble learning"></a>2.2.1 Ensemble learning</h2><p>x - dataset, y - error rate, each color is algorithm.</p><ul><li>No Free Lunch Theorem.<ul><li>any classification method cannot be superior or inferior overall</li></ul></li></ul><p>Ensemble means harmony or unity.</p><p>When we predict the value of a data, we use one model. But if we learn several models in harmony and use their predictions, we’ll get a more accurate estimate.</p><p>Ensemble learning is a machine learning technique that combines multiple decision trees to perform better than a single decision tree. The key to ensemble learning is to combine several weak classifiers to create a Strong Classifier. This improves the accuracy of the model.</p><h3 id="2-2-1-1-Bagging"><a href="#2-2-1-1-Bagging" class="headerlink" title="2.2.1.1 Bagging"></a>2.2.1.1 Bagging</h3><p>Bagging is Bootstrap Aggregation. Bagging is a method of aggregating results by taking samples multiple times (Bootstrap) each model.</p><p><img src="https://i.loli.net/2021/02/05/m4nEZyg3qUCu1Da.png" alt="bagging"></p><p>First, bootstrap from the data. (Restore random sampling) Examine the bootstrap data to learn the model. It aggregates the results of the learned model to obtain the final result value.</p><p>Categorical data aggregates results in Voting, and Continuous data is averaged.</p><p>When it’s categorical data, voting means that the highest number of values predicted by the overall model is chosen as the final prediction. Let’s say there are six crystal tree models. If you predicted four as A, and two as B, four models will predict A as the final result by voting.</p><p>Aggregating by means literally means that each decision tree model averages the predicted values to determine the predicted values of the final bagging model.</p><p>Bagging is a simple yet powerful method. Random Forest is representative model of using bagging.</p><h3 id="2-2-1-2-Boosting"><a href="#2-2-1-2-Boosting" class="headerlink" title="2.2.1.2 Boosting"></a>2.2.1.2 Boosting</h3><p>Boosting is a method of making weak classifiers into strong classifiers using weights. The Bagging predicts results independently of the Deicison Tree1 and Decision Tree2. This is how multiple independent decision trees predict the values, and then aggregate the resulting values to predict the final outcome. </p><p>Boosting, however, takes place between models. When the first model predicts, the data is weighted according to its prediction results, and the weights given affect the next model. Repeat the steps for creating new classification rules by focusing on misclassified data.</p><p><img src="https://blog.kakaocdn.net/dn/kCejr/btqyghvqEZB/9o3rKTEsuSIDHEfelYFJlk/img.png" alt></p><h2 id="2-2-2-Different-of-Bagging-and-Boosting"><a href="#2-2-2-Different-of-Bagging-and-Boosting" class="headerlink" title="2.2.2 Different of Bagging and Boosting"></a>2.2.2 Different of Bagging and Boosting</h2><p><img src="https://i.loli.net/2021/02/05/slDmWHtPY3quxfp.png" alt></p><p>Bagging is learned in parallel, while boosting is learned sequentially. After learning once, weights are given according to the results. Such weights affect the prediction of the results of the following models.</p><p>High weights are given for incorrect answers and low weights for correct answers. This allows you to focus more on the wrong answers in order to get them right.</p><p>Boosting has fewer errors compared to bagging. That is, performance is good performance. However, the speed is slow and there is a possibility of over fitting. So, which one should you choose between bagging or boosting when you actually use it? It depends on the situation. If the low performance of the individual decision tree is a problem, boosting is a good idea, or over-fitting problem bagging is a good idea.</p><h1 id="2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting"><a href="#2-2-XGBoost-A-scalable-Tree-Boosting-System-eXtreme-Gradient-Boosting" class="headerlink" title="2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)"></a>2.2 XGBoost : A scalable Tree Boosting System(eXtreme Gradient Boosting)</h1><p><a href="https://xgboost.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://xgboost.readthedocs.io/en/latest/</a></p><p><a href="https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d" target="_blank" rel="noopener">https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d</a></p><p>Optimized Gradient Boosting algorithm through parallel processing, tree-pruning, handling missing values and regularization to avoid over-fitting/bias</p><p><strong>XGBoost = GBM + Regularization</strong></p><p>Red part is Regularization term.</p><p>T : weak learner(node)</p><p>w : node score</p><p>Therefore, it can be seen that the regularization term of XGboost prevents overfitting by giving penalty to loss as the tree complexity increases.</p><h2 id="2-2-1-Split-finding-Algorithm"><a href="#2-2-1-Split-finding-Algorithm" class="headerlink" title="2.2.1 Split finding Algorithm"></a>2.2.1 Split finding Algorithm</h2><ul><li><p>Basing exact greedy algorithm</p><ul><li>Pros: <strong>Always find the optimal split</strong> point because it enumerates over <em>all possible</em> splitting points greedily</li><li>Cons: <ul><li>Impossible to efficiently do so when the data does not fit entirely into memory</li><li>Cannot be done under a distributed setting</li></ul></li></ul></li><li><p>Approximate algorithm</p><ul><li>Example<ul><li>Assume that the value is sorted in an ascending order</li><li>Divide the dataset into 10 buckets</li><li>Global variant(Per tree) vs Local variant(per split)</li></ul></li></ul></li></ul><h2 id="2-2-2-Sparsity-Aware-Split-Finding"><a href="#2-2-2-Sparsity-Aware-Split-Finding" class="headerlink" title="2.2.2 Sparsity-Aware Split Finding"></a>2.2.2 Sparsity-Aware Split Finding</h2><ul><li>In many real-world Problems, it is quite common for the input x to be sparse<ul><li>presence of missing values in the data</li><li>frequent zero entries in the statistics</li><li>artifacts of feature engineering such as one-hot encoding</li></ul></li><li>Solution : Set the default direction that is learned from the data</li></ul><h2 id="2-2-3-System-Design-for-Efficient-Computing"><a href="#2-2-3-System-Design-for-Efficient-Computing" class="headerlink" title="2.2.3 System Design for Efficient Computing"></a>2.2.3 System Design for Efficient Computing</h2><ul><li>The most time-consuming part of tree learning<ul><li>to get the data into sorted order</li></ul></li><li>XGBoost propose to store the data in in-memory units called block<ul><li>Data in each block is stored in the compressed column(CSC) format, with each column sorted by the corresponding feature value</li><li>This input data layout only needs to be computed once before training and can be reused in later iterations.</li></ul></li><li>Cache-aware access<ul><li>For the exact greedy algorithm, we can alleviate the problem by a cache-aware prefetching algorithm</li><li>For approximate algorithms, we solve the problem by choosing a correct block size</li></ul></li><li>Out-of-core computing<ul><li>Besides processors and memory, it is important to utilize disk space to handle data that does not fit into main memory</li><li>To enable out-of-core computation, the data is divided into multiple blocks and store each block on disk</li><li>To enable out-of-core computation, we divide the data into multiple blocks and store each block on disk</li><li>It is important to reduce the overhead and increase the throughout of disk IO</li></ul></li><li>Block Compression<ul><li>The block is compressed by columns and decompressed on the fly by an independent thread when loading into main memory</li><li>This helps to trade some of the computation in decompression with the disk reading cost</li></ul></li><li>Block Sharding<ul><li>A pre-fetcher thread is assigned to each disk and fetches the data into an in-memory buffer</li><li>The training thread then alternatively reads the data from each buffer.</li><li>This helps to increase the throughput of disk reading when multiple disks are available.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">'/kaggle/input/jane-street-market-prediction/train.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_memory_usage</span><span class="params">(df)</span>:</span></span><br><span class="line">    </span><br><span class="line">    start_memory = df.memory_usage().sum() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    print(<span class="string">f"Memory usage of dataframe is <span class="subst">&#123;start_memory&#125;</span> MB"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtype</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> col_type != <span class="string">'object'</span>:</span><br><span class="line">            c_min = df[col].min()</span><br><span class="line">            c_max = df[col].max()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> str(col_type)[:<span class="number">3</span>] == <span class="string">'int'</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).min <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).max:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).min <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).max:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[col] = df[col].astype(<span class="string">'category'</span>)</span><br><span class="line">    </span><br><span class="line">    end_memory = df.memory_usage().sum() / <span class="number">1024</span>**<span class="number">2</span></span><br><span class="line">    print(<span class="string">f"Memory usage of dataframe after reduction <span class="subst">&#123;end_memory&#125;</span> MB"</span>)</span><br><span class="line">    print(<span class="string">f"Reduced by <span class="subst">&#123;<span class="number">100</span> * (start_memory - end_memory) / start_memory&#125;</span> % "</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><h3 id="Importing-dataset"><a href="#Importing-dataset" class="headerlink" title="Importing dataset"></a>Importing dataset</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = reduce_memory_usage(train)</span><br></pre></td></tr></table></figure><h2 id="1-import-library"><a href="#1-import-library" class="headerlink" title="1) import library"></a>1) import library</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># This Python 3 environment comes with many helpful analytics libraries installed</span><br><span class="line"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span><br><span class="line"># For example, here&apos;s several helpful packages to load</span><br><span class="line"></span><br><span class="line">#import numpy as np # linear algebra</span><br><span class="line">#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import sklearn</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">import xgboost as xgb</span><br><span class="line">import optuna</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"># Input data files are available in the read-only &quot;../input/&quot; directory</span><br><span class="line"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">for dirname, _, filenames in os.walk(&apos;/kaggle/input&apos;):</span><br><span class="line">    for filename in filenames:</span><br><span class="line">        print(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span><br><span class="line"># You can also write temporary files to /kaggle/temp/, but they won&apos;t be saved outside of the current session</span><br></pre></td></tr></table></figure><h2 id="2-load-and-clean-dataset"><a href="#2-load-and-clean-dataset" class="headerlink" title="2) load and clean dataset"></a>2) load and clean dataset</h2><h3 id="Cleaning-data"><a href="#Cleaning-data" class="headerlink" title="Cleaning data"></a>Cleaning data</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># This Python 3 environment comes with many helpful analytics libraries installed</span><br><span class="line"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span><br><span class="line"># For example, here&apos;s several helpful packages to load</span><br><span class="line"></span><br><span class="line">#import numpy as np # linear algebra</span><br><span class="line">#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import sklearn</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">import xgboost as xgb</span><br><span class="line">import optuna</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"># Input data files are available in the read-only &quot;../input/&quot; directory</span><br><span class="line"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">for dirname, _, filenames in os.walk(&apos;/kaggle/input&apos;):</span><br><span class="line">    for filename in filenames:</span><br><span class="line">        print(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span><br><span class="line"># You can also write temporary files to /kaggle/temp/, but they won&apos;t be saved outside of the current session</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;记录一次kaggle比赛，这次比赛的场景主要是Jane Street日内高频交易，是短线模型，使用夏普率衡量模型效果，但是提供的数据并非原始数据，不清楚feature含义。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="kaggle" scheme="http://yuanquanquan.top/tags/kaggle/"/>
    
      <category term="量化" scheme="http://yuanquanquan.top/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>ORB_SLAM2的Rviz可视化</title>
    <link href="http://yuanquanquan.top/2021/20210109/"/>
    <id>http://yuanquanquan.top/2021/20210109/</id>
    <published>2021-01-09T08:02:16.000Z</published>
    <updated>2021-01-09T08:14:27.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近我在考虑能不能使用RVIZ可视化ORB_SLAM2,突然发现一位前辈的分享,为防止以后大佬删除了,我fork到了自己的github,这里做个笔记,记录一下.</p><p>这篇博客主要分析一下相关代码</p></blockquote><a id="more"></a>  <h2 id="可视化的几个要点"><a href="#可视化的几个要点" class="headerlink" title="可视化的几个要点"></a><strong>可视化的几个要点</strong></h2><p><code>ORB-SLAM</code>中关于<code>Rviz</code>的可视化：</p><ul><li>ORB-SLAM的Rviz可视化使用单独的一个类来完成可视化信息的发布：<code>MapPublisher</code>类.</li><li>所有的可视化信息都是Rviz的Mark类型,根据发布的地图点、关键帧、<code>Covisibility Graph、Spanning Tree</code>和相机轨迹，使用了不同的Mark类型.</li><li>所有的可视化信息,包括地图、轨迹等都是从<code>ORB-SLAM</code>中的<code>Map</code>类中获取的.</li><li>每次获得一帧图像,进行<code>Track</code>后，利用<code>MapPublisher</code>类发布可视化信息.</li><li>在配置相应的<code>Rviz</code>,使其可以接收可视化信息.</li></ul><p>明白了这几点之后，在ORB-SLAM2中添加Rviz可视化模块就很简单了，主要对源代码做以下改动：</p><ul><li>添加<code>MapPublisher</code>类和配置<code>Rviz</code>,可以直接复用<code>ORB-SLAM</code>中的<code>MapPublisher</code>类和<code>Rviz</code>文件,并在每次<code>Track</code>之后(执行完<code>mpSLAM-&gt;TrackStereo()</code>), 利用<code>MapPublisher</code>类发布可视化信息.</li><li>为Map类添加返回相关信息的接口.</li></ul><p>特别要注意:</p><p>ORB-SLAM2的坐标系下，<code>z轴</code>是<strong>朝前</strong>的，而<code>Rviz</code>的坐标系下，<code>z轴</code>是<strong>朝上</strong>的，因此要做相应的转换.</p><h2 id="ROS包建立"><a href="#ROS包建立" class="headerlink" title="ROS包建立"></a><strong>ROS包建立</strong></h2><h3 id="创建相关软件包"><a href="#创建相关软件包" class="headerlink" title="创建相关软件包"></a>创建相关软件包</h3><p>在<code>catkin_ws/src</code>目录下新建软件包并编译：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">catkin_create_pkg my_image_transport image_transport cv_bridge</span><br><span class="line">cd ..</span><br><span class="line">catkin_make -DCATKIN_WHITELIST_PACKAGES=&quot;my_image_transport&quot;</span><br><span class="line">source devel/setup.bash</span><br></pre></td></tr></table></figure><h3 id="创建图像发布者程序"><a href="#创建图像发布者程序" class="headerlink" title="创建图像发布者程序"></a>创建图像发布者程序</h3><p>新建<code>my_image_transport/src/my_publisher.cpp</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;ros/ros.h&gt;</span><br><span class="line">#include &lt;image_transport/image_transport.h&gt;</span><br><span class="line">#include &lt;opencv2/highgui/highgui.hpp&gt;</span><br><span class="line">#include &lt;cv_bridge/cv_bridge.h&gt;</span><br><span class="line"></span><br><span class="line">int main(int argc, char** argv)</span><br><span class="line">&#123;</span><br><span class="line">  ros::init(argc, argv, &quot;image_publisher&quot;);</span><br><span class="line">  ros::NodeHandle nh;</span><br><span class="line">  image_transport::ImageTransport it(nh);</span><br><span class="line">  image_transport::Publisher pub = it.advertise(&quot;camera/image&quot;, 1);</span><br><span class="line">  cv::Mat image = cv::imread(argv[1], CV_LOAD_IMAGE_COLOR);</span><br><span class="line">  cv::waitKey(30);//不断刷新图像，频率时间为delay，单位为ms</span><br><span class="line">  sensor_msgs::ImagePtr msg = cv_bridge::CvImage(std_msgs::Header(), &quot;bgr8&quot;, image).toImageMsg();</span><br><span class="line"></span><br><span class="line">  ros::Rate loop_rate(5);</span><br><span class="line">  while (nh.ok()) &#123;</span><br><span class="line">    pub.publish(msg);</span><br><span class="line">    ros::spinOnce();</span><br><span class="line">    loop_rate.sleep();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码解释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">line 1-4：</span><br><span class="line">ros.h头文件是所有的ros节点中必须要包含的，下面三个分别是实现图像的发布和订阅，调用opencv库，完成opencv图像格式转化为ROS图像格式所要用到的头文件；</span><br><span class="line"></span><br><span class="line">line 11：</span><br><span class="line">告知结点管理器要在camera/image话题发布图像消息，参数1是话题名称，话题2是缓冲区大小（即消息队列的长度，在发布图像消息时消息队列的长度只能是1）；</span><br><span class="line"></span><br><span class="line">line 12：</span><br><span class="line">根据运行时给定的参数（图像文件的路径）读取图像；</span><br><span class="line"></span><br><span class="line">line 14：</span><br><span class="line">将opencv格式的图像转化为ROS所支持的消息类型，从而发布到相应的话题上；</span><br><span class="line"></span><br><span class="line">line 16-21：</span><br><span class="line">发布图片消息，使消息类型匹配的节点订阅该消息。</span><br></pre></td></tr></table></figure><h3 id="创建图像订阅者程序"><a href="#创建图像订阅者程序" class="headerlink" title="创建图像订阅者程序"></a>创建图像订阅者程序</h3><p>新建<code>my_image_transport/src/my_subscriber.cpp</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;ros/ros.h&gt;</span><br><span class="line">#include &lt;image_transport/image_transport.h&gt;</span><br><span class="line">#include &lt;opencv2/highgui/highgui.hpp&gt;</span><br><span class="line">#include &lt;cv_bridge/cv_bridge.h&gt;</span><br><span class="line"></span><br><span class="line">void imageCallback(const sensor_msgs::ImageConstPtr&amp; msg)</span><br><span class="line">&#123;</span><br><span class="line">  try</span><br><span class="line">  &#123;</span><br><span class="line">    cv::imshow(&quot;view&quot;, cv_bridge::toCvShare(msg, &quot;bgr8&quot;)-&gt;image);</span><br><span class="line">    cv::waitKey(30);</span><br><span class="line">  &#125;</span><br><span class="line">  catch (cv_bridge::Exception&amp; e)</span><br><span class="line">  &#123;</span><br><span class="line">    ROS_ERROR(&quot;Could not convert from &apos;%s&apos; to &apos;bgr8&apos;.&quot;, msg-&gt;encoding.c_str());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">  ros::init(argc, argv, &quot;image_listener&quot;);</span><br><span class="line">  ros::NodeHandle nh;</span><br><span class="line">  cv::namedWindow(&quot;view&quot;);</span><br><span class="line">  cv::startWindowThread();</span><br><span class="line">  image_transport::ImageTransport it(nh);</span><br><span class="line">  image_transport::Subscriber sub = it.subscribe(&quot;camera/image&quot;, 1, imageCallback);</span><br><span class="line">  ros::spin();</span><br><span class="line">  cv::destroyWindow(&quot;view&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码解释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">line 6：回调函数，当有新的图像消息到达camera/image时，该函数就会被调用；</span><br><span class="line">line 10：显示捕捉到的图像，其中cv_bridge::toCvShare(msg, &quot;bgr8&quot;)-&gt;image用于将ROS图像消息转化为Opencv支持的图像格式（采用BGR8编码方式）。这部分用法恰好与上一节中发布者节点中的CvImage(std_msgs::Header(), &quot;bgr8&quot;, image).toImageMsg(); 的作用相反</span><br><span class="line">line 11：刷新图像的频率，实践过程中发现如果注释这一行图像将无法在窗口的显示</span><br></pre></td></tr></table></figure><h3 id="相关配置文件"><a href="#相关配置文件" class="headerlink" title="相关配置文件"></a>相关配置文件</h3><p><code>CMakeLists.txt</code>内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 2.8.3)</span><br><span class="line">project(my_image_transport)</span><br><span class="line"></span><br><span class="line">## Compile as C++11, supported in ROS Kinetic and newer</span><br><span class="line">add_compile_options(-std=c++11)</span><br><span class="line"></span><br><span class="line">find_package(catkin REQUIRED COMPONENTS</span><br><span class="line">  cv_bridge</span><br><span class="line">  image_transport</span><br><span class="line">)</span><br><span class="line">find_package(OpenCV REQUIRED)</span><br><span class="line"></span><br><span class="line">set(LIBS</span><br><span class="line"> $&#123;OpenCV_LIBS&#125; </span><br><span class="line"> $&#123;catkin_LIBRARIES&#125;)</span><br><span class="line"> </span><br><span class="line">catkin_package(</span><br><span class="line">#  INCLUDE_DIRS include</span><br><span class="line">#  LIBRARIES my_image_transport</span><br><span class="line">#  CATKIN_DEPENDS cv_bridge image_transport</span><br><span class="line">#  DEPENDS system_lib</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">include_directories(</span><br><span class="line"># include</span><br><span class="line">  $&#123;catkin_INCLUDE_DIRS&#125;</span><br><span class="line">  $&#123;OpenCV_INCLUDE_DIRS&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">add_executable(my_publisher src/my_publisher.cpp)</span><br><span class="line">target_link_libraries(my_publisher $&#123;LIBS&#125;)</span><br><span class="line"></span><br><span class="line">add_executable(my_subscriber src/my_subscriber.cpp)</span><br><span class="line">target_link_libraries(my_subscriber $&#123;LIBS&#125;)</span><br></pre></td></tr></table></figure><p><code>package.xml</code>文件中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;build_depend&gt;opencv2&lt;/build_depend&gt;</span><br><span class="line">&lt;exec_depend&gt;opencv2&lt;/exec_depend&gt;</span><br></pre></td></tr></table></figure><h3 id="编译软件包"><a href="#编译软件包" class="headerlink" title="编译软件包"></a>编译软件包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/catkin_ws</span><br><span class="line">catkin_make -DCATKIN_WHITHELIST_PACKAGES=&quot;my_image_transport&quot;</span><br></pre></td></tr></table></figure><h3 id="运行节点"><a href="#运行节点" class="headerlink" title="运行节点"></a>运行节点</h3><p>单独开启一个终端执行<code>roscore</code>,启动ros节点管理器。</p><p>开启另一个终端，启动发布者节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosrun my_image_transport my_publisher /home/xxx/catkin_ws/src/my_image_transport/000.png</span><br></pre></td></tr></table></figure><p>运行订阅者节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rosrun my_image_transport my_subscriber</span><br></pre></td></tr></table></figure><h2 id="ORB-SLAM2-ROS模块结点的编译"><a href="#ORB-SLAM2-ROS模块结点的编译" class="headerlink" title="ORB_SLAM2 ROS模块结点的编译"></a><strong>ORB_SLAM2 ROS模块结点的编译</strong></h2><p>在环境变量<code>ROS_PACKAGE_PATH</code>中添加<code>Examples/ROS/ORB_SLAM2</code>的路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;source ~/slam/ORB_SLAM2/Examples/ROS/ORB_SLAM2/build/devel/setup.sh&quot; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure><p>在~/slam/ORB_SLAM2目录下执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x build_ros.sh</span><br><span class="line">./build_ros.sh</span><br></pre></td></tr></table></figure><p>等待编译成功.</p><h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a><strong>运行结果</strong></h2><p>执行命令</p><p><img src="https://i.loli.net/2021/01/09/pwP2GjXoCuaWFfL.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roslaunch my_image_transport stereo_image_transport.launch</span><br></pre></td></tr></table></figure><h3 id="使用PCL库显示地图点"><a href="#使用PCL库显示地图点" class="headerlink" title="使用PCL库显示地图点"></a>使用PCL库显示地图点</h3><p>注意：<code>ORB_SLAM2</code>系统<code>Map</code>类中的地图点是世界坐标系中的所有地图点，而不是每个关键</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ROSpointcloud_to_laserscan包pointcloud_to_laserscan_node`节点将点云`PointCloud`转成`2D激光扫描</span><br></pre></td></tr></table></figure><p>订阅节点：<code>cloud_in(sensor_msgs/PointCloud2)</code></p><p>发布节点：<code>scan(sensor_msg/LaserScan)</code></p><p>配置好ROS程序包，在<code>MapPulisher</code>类中稍作修改，参照如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;ros/ros.h&gt;</span><br><span class="line">#include&lt;pcl/point_cloud.h&gt;</span><br><span class="line">#include&lt;pcl_conversions/pcl_conversions.h&gt;</span><br><span class="line">#include&lt;sensor_msgs/PointCloud2.h&gt;</span><br><span class="line"></span><br><span class="line">main (int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">  ros::init (argc, argv, &quot;pcl_create&quot;);</span><br><span class="line">  ros::NodeHandle nh;</span><br><span class="line">  ros::Publisher pcl_pub = nh.advertise&lt;sensor_msgs::PointCloud2&gt; (&quot;pcl_output&quot;, 1);</span><br><span class="line">  pcl::PointCloud&lt;pcl::PointXYZ&gt; cloud;</span><br><span class="line">  sensor_msgs::PointCloud2 output;</span><br><span class="line">  // Fill in the cloud data</span><br><span class="line">  cloud.width = 100;</span><br><span class="line">  cloud.height = 1;</span><br><span class="line">  cloud.points.resize(cloud.width * cloud.height);</span><br><span class="line">  for (size_t i = 0; i &lt; cloud.points.size(); ++i)</span><br><span class="line">  &#123;</span><br><span class="line">    cloud.points[i].x = 1024 * rand () / (RAND_MAX + 1.0f);</span><br><span class="line">    cloud.points[i].y = 1024 * rand () / (RAND_MAX + 1.0f);</span><br><span class="line">    cloud.points[i].z = 1024 * rand () / (RAND_MAX + 1.0f);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  //Convert the cloud to ROS message</span><br><span class="line">  pcl::toROSMsg(cloud, output);</span><br><span class="line">  output.header.frame_id = &quot;odom&quot;;//this has been done in order to be able to visualize our PointCloud2 message on the RViz visualizer</span><br><span class="line">  ros::Rate loop_rate(1);</span><br><span class="line">  while (ros::ok())</span><br><span class="line">  &#123;</span><br><span class="line">    pcl_pub.publish(output);</span><br><span class="line">    ros::spinOnce();</span><br><span class="line">    loop_rate.sleep();</span><br><span class="line">  &#125;</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>效果展示:</p><p><img src="https://i.loli.net/2021/01/09/TEtB9WROelcZk3z.png" alt></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] 2D Grid Mapping and Navigation with ORB SLAM. Abhineet Kumar Singh, Ali Jahani Amiri.</p><p>[2]  <a href="http://ttshun.com/2018/10/14/ORB-SLAM2%E7%B3%BB%E7%BB%9FRviz%E5%8F%AF%E8%A7%86%E5%8C%96%E6%96%B9%E6%A1%88/" target="_blank" rel="noopener">系统Rviz可视化方案</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近我在考虑能不能使用RVIZ可视化ORB_SLAM2,突然发现一位前辈的分享,为防止以后大佬删除了,我fork到了自己的github,这里做个笔记,记录一下.&lt;/p&gt;
&lt;p&gt;这篇博客主要分析一下相关代码&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="slam" scheme="http://yuanquanquan.top/tags/slam/"/>
    
  </entry>
  
  <entry>
    <title>Autoware</title>
    <link href="http://yuanquanquan.top/2020/20201221/"/>
    <id>http://yuanquanquan.top/2020/20201221/</id>
    <published>2020-12-21T08:17:06.000Z</published>
    <updated>2020-12-21T08:18:39.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Autoware最早是由名古屋大学研究小组在加藤伸平教授的领导下于2015年8月正式发布，2015年12月下旬，加藤伸平教授创立了Tier IV，以维护Autoware并将其应用于真正的自动驾驶汽车。</p><p>随着时间流逝，Autoware已成为公认的开源项目，Autoware也是世界上第一个用于自动驾驶“多合一”开源软件，在业内使用也较为广泛。</p></blockquote><a id="more"></a>  <h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>首先看看官网的匹配关系图，由于是在Ubuntu上运行的，官方给出了Autoware与UBuntu的对应关系</p><table><thead><tr><th style="text-align:center">Autoware version</th><th style="text-align:center">Ubuntu 14.04</th><th style="text-align:center">Ubuntu 16.04</th><th style="text-align:center">Ubuntu 18.04</th></tr></thead><tbody><tr><td style="text-align:center">v1.140</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">x</td></tr><tr><td style="text-align:center">v1.130</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">x</td></tr><tr><td style="text-align:center">v1.120</td><td style="text-align:center"></td><td style="text-align:center">x</td><td style="text-align:center">x</td></tr><tr><td style="text-align:center">v1.11.1</td><td style="text-align:center"></td><td style="text-align:center">x</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">v1.11.0</td><td style="text-align:center"></td><td style="text-align:center">x</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">v1.10.0</td><td style="text-align:center"></td><td style="text-align:center">x</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">v1.9.1</td><td style="text-align:center">x</td><td style="text-align:center">x</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">v1.9.0</td><td style="text-align:center">x</td><td style="text-align:center">x</td></tr></tbody></table><p>注：图标中的x号意思是支持，且还需要提前安装好ROS</p><h3 id="系统依赖"><a href="#系统依赖" class="headerlink" title="系统依赖"></a>系统依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install -y python-catkin-pkg python-rosdep ros-$ROS_DISTRO-catkin</span><br><span class="line">$ sudo apt install -y python3-pip python3-colon-common-extensions python3-setuptools python3-vcstool</span><br><span class="line">$ pip3 install -U setuptools</span><br></pre></td></tr></table></figure><p>如果需要GPU支持，那就需要安装CUDA10。CUDA10.0以上不支持。</p><h3 id="安装Eigen3-3-7"><a href="#安装Eigen3-3-7" class="headerlink" title="安装Eigen3.3.7"></a>安装Eigen3.3.7</h3><p>Ubuntu自带的Eigen要卸掉，重新安装此版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cd &amp;&amp; wget http://bitbucket.org/eigen/eigen/get/3.3.7.tar.gz</span><br><span class="line">$ mkdir eigen &amp;&amp; tar build &amp;&amp; tar --strip-compoents=1 -xzvf 3.37.tar.gz -C eigen #Decompress</span><br><span class="line">$ cd eigen &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make &amp;&amp; make install #Build and install</span><br><span class="line">$ cd &amp;&amp; rm -rf 3.37.tar.gz &amp;&amp; rm -rf eigen</span><br></pre></td></tr></table></figure><h3 id="安装Qt5"><a href="#安装Qt5" class="headerlink" title="安装Qt5"></a>安装Qt5</h3><table><thead><tr><th style="text-align:center">product</th><th style="text-align:center">ubuntu 14.04</th><th style="text-align:center">ubuntu 1604</th><th style="text-align:center">ubuntu 18.04</th></tr></thead><tbody><tr><td style="text-align:center">ROS</td><td style="text-align:center">Indigo</td><td style="text-align:center">kinetic</td><td style="text-align:center">Melodic</td></tr><tr><td style="text-align:center">Qt</td><td style="text-align:center">4.8.6 or higher</td><td style="text-align:center">5.2.1 or higher</td><td style="text-align:center">5.95 or higher</td></tr><tr><td style="text-align:center">CUDA(optional)</td><td style="text-align:center">8.0GA(?)</td><td style="text-align:center">9.0</td><td style="text-align:center">10.0</td></tr><tr><td style="text-align:center">FlyCapture2(optional)</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">Armdillo(optional)</td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table><h3 id="可选项"><a href="#可选项" class="headerlink" title="可选项"></a>可选项</h3><ul><li>FlyCapture2：相机的开发套件</li><li>Armdillo：用于线性代数和科学计算的CPP库</li></ul><h2 id="安装编译"><a href="#安装编译" class="headerlink" title="安装编译"></a>安装编译</h2><h3 id="创建工作空间"><a href="#创建工作空间" class="headerlink" title="创建工作空间"></a>创建工作空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p autoware.ai/src</span><br><span class="line">$ cd autoware.ai</span><br></pre></td></tr></table></figure><h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ wget -0 autoware.ai.repos &quot;https://raw.githubusercontent.com/Autoware-AI/autoware.ai/1.14.0/autoware.ai.repos&quot;</span><br><span class="line">$ vcs import src &lt; autoware.ai.repos</span><br></pre></td></tr></table></figure><h3 id="使用rosdep安装依赖项"><a href="#使用rosdep安装依赖项" class="headerlink" title="使用rosdep安装依赖项"></a>使用rosdep安装依赖项</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rosdep update</span><br><span class="line">$ rosdep install -y --from-paths src --ignore-src --rosdistro $ROS_DISTRO</span><br></pre></td></tr></table></figure><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>若已经配置CUDA</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ AUTOWARE_COMPILE_WITH_CUDA=1 colon build --cmake-args -DCMAKE_BUILD_TYPE=Release</span><br></pre></td></tr></table></figure><p>若没有CUDA支持/</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ colon build --cmake-args -DCMAKE_BUILD_TYPE=Release</span><br></pre></td></tr></table></figure><h2 id="数据集测试"><a href="#数据集测试" class="headerlink" title="数据集测试"></a>数据集测试</h2><p>以官方提供的一个数据集为例</p><h3 id="数据集下载"><a href="#数据集下载" class="headerlink" title="数据集下载"></a>数据集下载</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://autoware-ai.s3.us-east-2.amazonaws.com/sample_moriyama_data.tar.gz</span><br><span class="line">wget https://autoware-ai.s3.us-east-2.amazonaws.com/sample_moriyama_150324.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir .autoware</span><br><span class="line">cp /home/xx/Downloads/my_launch.sh ./.autoware</span><br><span class="line">cp /home/xx/Downloads/sample_moriyama_data.tar.gz ./. autoware</span><br><span class="line">cp /home/xx/Downloads/sample_moriyama_150324.tar.gz ./.autoware</span><br><span class="line">cd .autoware/</span><br><span class="line">sh my_launch.sh</span><br><span class="line">tar -xzvf sample_moriyama_data.tar.gz</span><br><span class="line">tar -xzvf sample_moriyama_150324.gz</span><br></pre></td></tr></table></figure><h3 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd autoware.ai</span><br><span class="line">$ source install/seyup.bash</span><br><span class="line">$ roslaunch runitme_manager runtime_manager.launch</span><br></pre></td></tr></table></figure><p>出现如图所示界面以后，进入simulation选项卡，单机ref按钮找到下载好的bag包<img src="https://tva1.sinaimg.cn/large/0081Kckwly1glvgeh2t23j30mh0iv40u.jpg" alt></p><p>加载完毕之后设置开始时间，点击play，pause为暂停播放</p><p>再点击quick start分别点击map和location的右侧ref按钮，选择一下路径对应文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoware.ai/src/autoware/documentation/autoware_quickstart_example/launch/rosbag_demo/</span><br></pre></td></tr></table></figure><p>最后点击左侧的Map和Locations按钮。</p><p>下面就是在Rviz里面显示了，按照视频点击Rviz来启动Rviz，最后加载.rviz文件</p><p>回到simulation点击Pause案件开始播放，rviz里面就会开始运行数据集。</p><h2 id="用户手册"><a href="#用户手册" class="headerlink" title="用户手册"></a><a href="https://github.com/CPFL/Autoware-Manuals" target="_blank" rel="noopener">用户手册</a></h2><h3 id="Quick-Start选项说明"><a href="#Quick-Start选项说明" class="headerlink" title="Quick Start选项说明"></a>Quick Start选项说明</h3><ul><li>Map：地图配置文件，可以点击Ref按钮从系统中选择脚本</li><li>Sensing：传感器配置文件点击后面的Ref按钮可从系统中选择脚本</li><li>Localization：定位文件，点击后面的Ref按钮可从系统中选择脚本</li><li>Detection：目标检测，点击后面的Ref按钮可从系统中选择脚本</li><li>Mission planning：任务规划启动，可以点击后面的Ref按钮可从系统中选择脚本</li><li>Motion Planning：运动规划，可以点击后面的Ref按钮可从系统中选择脚本</li></ul><h3 id="Setup选项说明"><a href="#Setup选项说明" class="headerlink" title="Setup选项说明"></a>Setup选项说明</h3><p>在这个选项卡下面有个TF，包含了车辆基座标位置到激光雷达位置的主题发布，x，y，z，yaw，pitch，roll为激光雷达到达车辆坐标的相对位置输入。</p><p>总的来说，setup为安装设置模块，由于Autoware建图使用的是激光雷达。TF输入的是激光雷达和车辆之间的旋转平移矩阵信息。所以该模块还需要导入车辆的模型文件，配置文件也在下面的选项里有，点击Ref选择即可。</p><h3 id="Map·选项说明"><a href="#Map·选项说明" class="headerlink" title="Map·选项说明"></a>Map·选项说明</h3><ul><li>Point Cloud：点云地图</li><li>AutoUpdate：自动更新指定的场景数据</li><li>Vector Map：矢量地图</li><li>PCD Fillter：PCD滤波器</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Autoware最早是由名古屋大学研究小组在加藤伸平教授的领导下于2015年8月正式发布，2015年12月下旬，加藤伸平教授创立了Tier IV，以维护Autoware并将其应用于真正的自动驾驶汽车。&lt;/p&gt;
&lt;p&gt;随着时间流逝，Autoware已成为公认的开源项目，Autoware也是世界上第一个用于自动驾驶“多合一”开源软件，在业内使用也较为广泛。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="自动驾驶" scheme="http://yuanquanquan.top/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
      <category term="slam" scheme="http://yuanquanquan.top/tags/slam/"/>
    
  </entry>
  
  <entry>
    <title>Lyft Motion Prediction for Autonomous Vehicles</title>
    <link href="http://yuanquanquan.top/2020/20201216/"/>
    <id>http://yuanquanquan.top/2020/20201216/</id>
    <published>2020-12-16T08:04:46.000Z</published>
    <updated>2020-12-16T16:23:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>几个月前，lyft在kaggle平台上组织了一个比赛[1]：<a href="https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles" target="_blank" rel="noopener">Lyft Motion Prediction for Autonomous Vehicles</a>，用算法预测自动驾驶汽车周围的交通参与者的运动轨迹。和几个同学参加了这个比赛排名8%，拿了个铜牌，这篇博客是对比赛的总结。</p></blockquote><a id="more"></a>  <p><strong>赛题背景：</strong>The ridesharing company Lyft started Level 5 to take on the self-driving challenge and build a full self-driving system they’re hiring!). Their previous competition tasked participants with identifying 3D objects, an important step prior to detecting their movement. Now, they’re challenging you to predict the motion of these traffic agents.</p><p><strong>赛题任务：</strong>In this competition, you’ll apply your data science skills to build motion prediction models for self-driving vehicles. You’ll have access to the largest Prediction Dataset ever released to train and test your models. Your knowledge of machine learning will then be required to predict how cars, cyclists,and pedestrians move in the AV’s environment.</p><p><strong>Loss：</strong>The goal of this competition is to predict the trajectories of other traffic participants. You can employ uni-modal models yielding a single prediction per sample, or multi-modal ones generating multiple hypotheses (up to 3) - further described by a confidence vector.</p><p>Due to the high amount of multi-modality and ambiguity in traffic scenes, the used evaluation metric to score this competition is tailored to account for multiple predictions.</p><p><strong>Note:</strong> The following is a brief excerpt of our <a href="https://github.com/lyft/l5kit/blob/master/competition.md" target="_blank" rel="noopener">metrics page in the L5Kit repository</a></p><p>We calculate the negative log-likelihood of the ground truth data given the multi-modal predictions. Let us take a closer look at this. Assume, ground truth positions of a sample trajectory are<br>$$<br>x_{1}, \ldots, x_{T}, y_{1}, \ldots, y_{T}<br>$$</p><h3 id="问题定义和数据集"><a href="#问题定义和数据集" class="headerlink" title="问题定义和数据集"></a>问题定义和数据集</h3><p>自动驾驶中的运动预测或轨迹预测，在这里我们<strong>定义为</strong>：</p><blockquote><p>给定</p><ul><li>交通参与者（例如行人、骑车人、车辆）在过去一段时间的路径/轨迹</li><li>道路信息（例如道路结构、交通灯信息）</li><li>相关交通参与者过去一段时间对路径/轨迹等</li></ul><p>对</p><ul><li>未来一段时间的（一段或多段可能）路径/轨迹</li><li>或运动意图</li><li>或占用格栅图</li></ul><p>进行预测。</p></blockquote><p>上面这段文字便定义了问题的输入（“给定”部分）和输出（“对”部分），基于问题的定义，现存的研究有诸多方法，传统的方法如：</p><ul><li>基于运动学和动力学的预测（如卡尔曼滤波），缺点在于没有考虑到环境中其他交通参与者对被预测目标运动的影响；</li><li>考虑了环境因素的方法，如社会力模型、逆强化学习等。</li></ul><p><strong>基于deep learning的方法可以大致分为</strong>：</p><ul><li>循环神经网络（RNN）方法，由于轨迹具有明显的时序信息，通过RNN对时间维度建模；</li><li>卷积神经网络（CNN）方法，将轨迹和环境信息编码成图的形式（例如多通道的鸟瞰图），用CNN方法进行建模。在比赛中的baseline和大多数参赛者的方法均基于此；</li><li>其他，例如图神经网络、RNN+CNN的结合等；</li></ul><h3 id="Tips："><a href="#Tips：" class="headerlink" title="Tips："></a>Tips：</h3><p>由于这个官方baseline并不弱，并且为参赛选手提供了便利，因此大多数参赛者都是基于此进行了修改。从赛道上top解决方案[5]来看，有一些重要的修改技巧摘选如下（都是图像比赛的基本操作，但根据问题领域的不同有所调整）：</p><ol><li><h4 id="对样本的筛选："><a href="#对样本的筛选：" class="headerlink" title="对样本的筛选："></a>对样本的筛选：</h4><p>对训练样本进行筛选，使其与评测样本保持一致。例如在这个赛题中，评测样本基本在未来十帧都有数据，因此在训练样本筛选时也只保留有未来十帧数据的样本；</p></li><li><h4 id="对数据的编码："><a href="#对数据的编码：" class="headerlink" title="对数据的编码："></a>对数据的编码：</h4><p>历史帧的数目：第一名方案中history_num_frames=30，即构建了66通道的“图像”输入，包括30通道自车运动历史轨迹、30通道他车运动历史轨迹、3通道语义地图、3通道卫星地图；</p><p>其他可以实验的参数：例如“图像”的整体大小、“图像”每个像素点代表的物理范围；</p></li><li><h4 id="数据增强："><a href="#数据增强：" class="headerlink" title="数据增强："></a>数据增强：</h4><p>图像级增强：如 cutout、模糊、下采样；</p><p>栅格级增强：如 随机丢弃他车数据；</p></li><li><h4 id="对训练过程的加速："><a href="#对训练过程的加速：" class="headerlink" title="对训练过程的加速："></a>对训练过程的加速：</h4><p>lyft官方提供的上述数据栅格编码过程复杂，一次训练中的大部分时间都耗费在CPU上的数据处理中。因此可以优化编码部分代码，解决CPU运算瓶颈，提升多GPU训练效率，有助于在比赛中快速实验；</p></li><li><h4 id="单模型的选择和训练："><a href="#单模型的选择和训练：" class="headerlink" title="单模型的选择和训练："></a>单模型的选择和训练：</h4><p>第一名方案：EfficientNetB3模型，先用低分辨率图像预训练4个epoch，而后从第5个epoch开始，在原图上用余弦退火和阶梯下降学习率的方式进行训练；</p></li><li><h4 id="多模型的融合："><a href="#多模型的融合：" class="headerlink" title="多模型的融合："></a>多模型的融合：</h4><p>第一名方案：用不同的数据编码参数训练得到5个模型，用stacking方法进行融合；</p><p>第四名方案：用GMM方法将多模型的多条轨迹采样并拟合成最终的三条轨迹；</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;几个月前，lyft在kaggle平台上组织了一个比赛[1]：&lt;a href=&quot;https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Lyft Motion Prediction for Autonomous Vehicles&lt;/a&gt;，用算法预测自动驾驶汽车周围的交通参与者的运动轨迹。和几个同学参加了这个比赛排名8%，拿了个铜牌，这篇博客是对比赛的总结。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="自动驾驶" scheme="http://yuanquanquan.top/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
      <category term="kaggle" scheme="http://yuanquanquan.top/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>图像语义融合关键技术知识点</title>
    <link href="http://yuanquanquan.top/2020/20201212/"/>
    <id>http://yuanquanquan.top/2020/20201212/</id>
    <published>2020-12-10T11:27:36.000Z</published>
    <updated>2020-12-16T06:32:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>图像融合是将两个或多个具有互补信息和冗余特性的图像进行多层次、多级别、多方向的综合处理，减小图像的冗余性和模糊性，使得融合后的图像信息更丰富、更精准、更可靠，为后续的图像检测、图像识别、图像分类、图像理解等相关研究和应用提供技术基础。</p></blockquote> <a id="more"></a> <p>​     <strong>机器识别图像</strong>主要依赖图像的各种底层视觉特性信息，提取出<em>图像的形状</em>、<em>颜色直方图</em>、<em>纹理</em>、<em>轮廓</em>等底层视觉特征，分析不同的底层视觉特征之间的联系，实现图像的分类，进而达到图像识别的目的。其缺点是：纯粹的依赖图像底层特征的做法，缺乏对图像内容潜在的语义分析，难以捕捉到图像的直接视觉效果以外的人文或情感信息。<strong>图像的底层视觉特性和图像的高级语义之间存在语义鸿沟</strong>，导致图像的检索不理想。在图像检索，图像识别和分类一定程度上依赖于图像语义标注，这使得研究图像的语义标注算法成为图像理解领域的热点，也是人工智能的重要研究课题。</p><h4 id="图像标注算法和模型的涌现"><a href="#图像标注算法和模型的涌现" class="headerlink" title="图像标注算法和模型的涌现"></a>图像标注算法和模型的涌现</h4><ul><li>共现模型。利用统计学建立图像与标注词之间的映射关系。共现模型首先把图像划分成若干规则区域，然后对分割得到的区域进行分类，得到图像的区域与关键词之间的共生概率，然后选择对分割得到的区域进行分类，得到图像的区域与关键词之间的共生概率，然后选择共生概率大的关键词对图像进行标注。</li><li>机器翻译模型。利用传统的语言统计翻译模型，同样基于分割图像的做法，但是机器翻译模型的分割强调有意义的分割，分割过程能够实现对象识别，从标注图像中分割出来的图像区域与现实对象存在对应关系，以便于把分割出的区域能与具体对象建立关联，并将视觉特征转化为语义标注词。</li><li>CMRM模型（ cross modia relevance model）</li><li>CRM模型（continous-space relevance model）</li><li>图学习模型 自然语言处理中的语义上下文关系被用来图像底层视觉特征和高级语义之间建立关联。其中：</li><li>LSA模型（latent semantic analysis）</li><li>PLSA模型（probailistic latent semantic analysis） 这两种模型被用来分析图像和标注词之间的关系。</li></ul><p><strong>图像融合</strong>解释：<strong>将两个或多个具有互补信息和冗余特性的图像进行多层次、多级别、多方向的综合性处理</strong>，减小图像的冗余性和模糊性，使得融合后的图像信息更丰富、更为精准可靠、具有更佳的互补性、更有利于理解，更加适合人类接受和计算机理解，为后续图像的检测和识别、图像分类和图像理解等处理提供了技术支撑。</p><h4 id="图像分割算法现状"><a href="#图像分割算法现状" class="headerlink" title="图像分割算法现状"></a>图像分割算法现状</h4><p><strong>阈值分割算法</strong>，<strong>边缘检测分割算法</strong>，<strong>区域分割算法</strong>，以及近年流行的<strong>图论分割算法</strong>，<strong>聚类分割算法</strong></p><p><em>基于边缘分割算法</em>有：<strong>Prewitt算法</strong>、<strong>Roberts算法</strong>、<strong>Laplacian算法</strong>以及<strong>Sobel算法</strong>等。想要取得理想分割效果经常需要合理运用各种边缘检测算法。</p><p><em>基于区域的分割算法</em>，利用灰度级的不连续性来查找区域的边界。常用的操作是区域生长，以及区域的分离与合并。</p><p><em>基于图论的分割算法</em>，将图像映射成带有权重的无向图，然后对图进行划分处理，得到若干个不同的子图。<strong>归一化割</strong>的图划分方法，简称“<strong>N-cut</strong>”。<strong>最小割算法</strong>（<strong>Min-cut</strong>）是一种典型的N-cut算法。Min-cut利用图像的局部信息计算图结点之间的距离。</p><p><em>基于聚类的分割算法</em>；缺点：图像的空间信息没有得到处理。例如<strong>基于模糊C-均值聚类</strong>（<strong>FCM</strong>）对噪声比较敏感。FCM算法的改进算法如下：针对FCM算法的空间信息的改进：</p><ul><li><strong>FCM_S</strong>算法，加入了领域信息</li><li><strong>FCM_S1</strong>算法，利用预算值进行聚类。</li><li><strong>FCM_S2</strong>算法</li></ul><p>针对FCM_S是像素级的改进</p><ul><li><strong>EnFCM</strong>算法，结合图像的统计信息，把计算工作量降低到灰度级量级。</li><li><strong>FGFCM</strong>算法，建立空间相关性和灰度相关性，提高算法抗造性。</li><li><strong>NWFCM</strong>算法，利用非局部空间信息来计算领域像素到聚类中心的距离。</li></ul><p>针对FCM聚类算法和邻域加权的FCM聚类算法中的参数问题的改进：</p><ul><li><strong>FLICM</strong>算法，使用模糊因子取代参数。</li><li><strong>NDFCM</strong>算法，基于核函数的局部FCM算法。</li></ul><p>基于非局部空间信息的改进：</p><ul><li>FCM_NLS算法，基于非局部空间信息，提前对原图像的非局部空间进行滤波。</li><li>FCM_SNLS算法，基于自调节非局部空间信息的FCM聚类算法。</li></ul><p>新的图像分割理论：</p><ul><li>图像的颜色分布和纹理空间处理方面的经验模态分解EMD。</li></ul><h4 id="图像融合算法现状"><a href="#图像融合算法现状" class="headerlink" title="图像融合算法现状"></a>图像融合算法现状</h4><p>图像融合的主要流程是先进行图像的配准和特征提取，再进行决策，最后进行图像融合。依据所处上述处理流程中的不同阶段，图像融合可分为像素级融合、特征级融合和决策级融合三个层次。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;图像融合是将两个或多个具有互补信息和冗余特性的图像进行多层次、多级别、多方向的综合处理，减小图像的冗余性和模糊性，使得融合后的图像信息更丰富、更精准、更可靠，为后续的图像检测、图像识别、图像分类、图像理解等相关研究和应用提供技术基础。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="cv" scheme="http://yuanquanquan.top/tags/cv/"/>
    
  </entry>
  
  <entry>
    <title>关于脉冲特征提取的想法</title>
    <link href="http://yuanquanquan.top/2020/202011077/"/>
    <id>http://yuanquanquan.top/2020/202011077/</id>
    <published>2020-11-07T05:33:18.000Z</published>
    <updated>2020-11-07T06:58:46.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>选的毕设题目是《脉冲信号的智能提取软件设计》，虽然之前基本没处理过脉冲数据，但是数据处理的套路都差不多，无非先进行数据清洗，然后去直流、降噪，再对信号进行加窗（滤波、分析信号得到想要频段内的信号），再通过一些方法，如FFT、小波或者一些其他变换，通过这些变换得出频率成分，异常特征值。</p></blockquote><a id="more"></a>  <p>关于这个题目我大概有两种思路，一种较为传统，基本就是将之前各种传统算法整合在一起，另一种则是基于特征编码和一维CNN卷积神经网络对信号对脉冲矩阵进行信号提取。</p><h3 id="一、使用传统算法对脉冲信号进行提取："><a href="#一、使用传统算法对脉冲信号进行提取：" class="headerlink" title="一、使用传统算法对脉冲信号进行提取："></a>一、使用传统算法对脉冲信号进行提取：</h3><p><strong>1、首先，在特征提取之前需明确是怎样的信号，怎样的应用，怎样的场景，因为针对不同应用和场景选择的特征提取也不近相同</strong></p><p><strong>2、信号特征的提取往往都是用最简单有效的参数表示信号中的信息，这是根本目的。</strong></p><p><strong>3、针对不同后端模型需要确定特征维度。</strong></p><p><strong>4、开始特征提取前，信号往往需要做一些预处理，如滤波、去均值、去异常等等。</strong></p><h3 id="特征提取有哪些方法："><a href="#特征提取有哪些方法：" class="headerlink" title="特征提取有哪些方法："></a><strong>特征提取有哪些方法：</strong></h3><p><strong>1、时域一维信号简单统计和运算可以得到的特征有：均值，方差，均方根，峰值因子，峭度系数，波形因子，裕度因子、脉冲因子。</strong></p><p><strong>2、估计–分布参数一般服从某一类分布；</strong></p><p><strong>3、频域，特征频率，均方频率，重心频率，频率方差；</strong></p><p><strong>4、小波方法提取的系数，小波滤波后的特征频率等等；</strong></p><p><strong>5、信号熵，谱熵，排列熵，小波熵，EMD熵，包络谱熵等；</strong></p><p><strong>6、谱峭度，快速谱峭度、小波谱峭度等；</strong></p><p><strong>7、基于数学工具和降维的特征，如PCA，矩阵特征向量，矩阵的秩，特征根，SVD-奇异值、ICA等等；</strong></p><p><strong>8、一些基于距离的度量、范数、马氏距离、分形参数，同胚流行等等；</strong></p><p><strong>9、任何能表征信号特征的自定义参数均可以，注意有意义有时是结合实际需求的。</strong></p><h3 id="二、基于特征编码和一维CNN卷积神经网络"><a href="#二、基于特征编码和一维CNN卷积神经网络" class="headerlink" title="二、基于特征编码和一维CNN卷积神经网络"></a>二、基于特征编码和一维CNN卷积神经网络</h3><p><strong>目前已有的脉冲信号识别方法大多是基于脉间特征和使用机器学习方法来实现。整体上来说，已有的方法已经能对大多数的脉冲信号进行较为准确的识别，但是对于一些参数较为相近的信号识别效果并不好。</strong></p><p><strong>我的思路是根据脉冲信号的载频(RF)、重频(PRI)、脉宽(PW)等参数构建脉冲描述矩阵。</strong></p><p><img src="https://i.loli.net/2020/11/07/LX1V6mlEw7WhiHZ.png" alt="脉冲信号及其特征"></p><p><strong>通过传感器获取的脉冲信号，经过信号分选等处理后得到属于一组脉冲描述字(PDW)信息，组成脉冲描述矩阵（PDM）。</strong></p><p><img src="https://i.loli.net/2020/11/07/kQVEKAPt8RsLW3j.png" alt="脉冲描述矩阵"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;选的毕设题目是《脉冲信号的智能提取软件设计》，虽然之前基本没处理过脉冲数据，但是数据处理的套路都差不多，无非先进行数据清洗，然后去直流、降噪，再对信号进行加窗（滤波、分析信号得到想要频段内的信号），再通过一些方法，如FFT、小波或者一些其他变换，通过这些变换得出频率成分，异常特征值。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="毕设" scheme="http://yuanquanquan.top/tags/%E6%AF%95%E8%AE%BE/"/>
    
  </entry>
  
  <entry>
    <title>Effective AER Object Classification Using Segmented Probability-Maximization Learning in Spiking Neural Networks</title>
    <link href="http://yuanquanquan.top/2020/20201103/"/>
    <id>http://yuanquanquan.top/2020/20201103/</id>
    <published>2020-11-02T17:30:48.000Z</published>
    <updated>2020-11-07T04:05:30.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前因为一些emmmm俗事，很久没看论文了，现在因为要做毕设，又久违看起了论文，这次是浙大普适与智能实验室AAAI 2020的论文《基于脉冲神经网络的事件表示的物体分类方法》</p></blockquote><a id="more"></a><p>事件相机是一种神经拟态视觉传感器，通过模拟人类的视网膜机制来记录场景。与传统相机不同，事件相机没有 “帧” 的概念。当它捕捉到视野中任意位置发生了一定程度的光线变化时，就会产生一些像素级的输出（即事件）。一个事件中包括三个信息：事件在2D空间的像素坐标，事件的时间戳，以及事件的极性（1和-1，表示是由光线变亮或变暗触发的）。由于事件相机关注的是场景中的光线变化，因此它只会记录场景中的动态信息，忽略不变的静态冗余信息，从而能显著减少内存使用和能源消耗。</p><p>这种基于事件的数据形式与脉冲神经网络（Spiking Neural Network，SNN）不谋而合。脉冲神经网络通过脉冲传递信息，而脉冲也具有事件的属性。相比传统人工神经网络，脉冲神经网络更具有生物解释性，在处理时空数据的能力上也有更大的潜力。</p><p>然而，将这种新的事件表示与SNN相结合来进行物体分类，还存在着若干挑战。首先，与传统相机的视频流数据形式相比，事件相机产生的事件流是不稳定的。由于事件相机对视觉感受域内的动态信息非常敏感，在场景记录过程中，除了与目标的相关的事件被记录下来，相机抖动和环境光的变化等因素也会产生大量的噪声事件，影响SNN网络中神经元响应的可靠性和网络学习性能。其次，事件相机输出的事件流记录了一段时间内的信息，我们希望SNN的响应能够足够快，以便在提供的测试信息还不完整的情况下，仍达到具有竞争力的准确性水平。</p><p><img src="https://i.loli.net/2020/11/07/e3xkVDZrl2w718d.png" alt></p><p>最近，我们发表在AAAI2020的论文《Effective AER Object Classification Using Segmented Probability-Maximization Learning in Spiking Neural Networks》就针对事件表示的物体提出了一个分类模型，在这个模型中，我们提出了一个新的脉冲神经网络学习算法——分段概率最大化算法（Segmented Probability-MAximization，SPA）。SPA基于脉冲神经元之间的相对响应定义了一个响应概率，利用梯度更新来迭代增加响应正确的概率，从而提高神经元响应的可靠性。同时我们根据神经元的电压将事件流的处理过程动态分段，以便能充分利用一段时长内的大量事件，增加信息的利用率，提高网络的学习的性能。</p><p> <img src="https://i.loli.net/2020/11/07/SgR67PrvBj3GLeW.png" alt="图1 峰值检测示意图"></p><p><strong>分段概率最大化算法</strong></p><p>SPA算法是一个电压驱动的脉冲神经网络监督学习算法。首先，该算法定义了一个脉冲神经元响应与其电压的关系，电压越高，其响应就越积极。在监督学习算法中，每个神经元会对应一个物体类别，并对属于其代表类的样本敏感。因此，SPA算法的训练目标是使得样本对应类的脉冲神经元响应最积极。基于此目的，我们定义了一个响应概率，即样本对应类的脉冲神经元响应占总神经元响应比例。这个比例越高，说明样本对应类的脉冲神经元的相对响应越积极，样本被分到正确类的概率就越高，结果越可靠。为了优化这个概率，SPA算法使用交叉熵定义损失函数，并使用梯度更新迭代优化网络的权重。</p><p>事件表示这种新的数据表达形式是通过一时间段内的事件流来描绘物体的，那如何充分利用期间的信息呢？在这里，我们提出了分段峰值检测（Peak Detection）机制：从tS＝0开始，首先划出一较长的固定时间段tR, 然后在此时间段中检测各神经元的电压峰值，将此峰值带入上述权重更新过程中。更新后，将起始时间tS更新为各神经元峰值中的最大值所对应的时间点，再划出时间段tR，重复上述寻找峰值和权重更新过程。神经元的电压峰值是积累的结果，是由一串激发的输入脉冲触发的，表明神经元在电压峰值时已经接收了大量的信息。因此，通过分段峰值检测算法可以将整个事件流时间窗内的峰值信息有效利用起来进行网络训练。</p><p><strong>实验结果与分析</strong></p><p><img src="https://i.loli.net/2020/11/07/4txYD2QhKeBzkf8.png" alt="表1 四个数据集分类结果的比较"></p><p>我们在若干公开发布的事件表示的物体数据集上评估模型的效果。首先，我们将本文提出的分类模型的结果与其它的事件表示的物体分类框架（特别是基于脉冲的物体分类框架）的结果比较。由表1可以看到，本文提出的分类模型在四个公开数据集都表现较好。</p><p>接下来，我们通过更细致的实验来进一步验证模型的效果。首先，我们研究了事件流时长对算法结果的影响。事件流的长度代表了信息量，事件流越长即提供的信息越丰富。我们在MNIST-DVS数据集四种不同事件流长度上进行实验，并选择了表1中在该数据集上表现较好的Zhao的模型与我们的方法进行比较。如表2所示，我们看到：1）事件流时长逐渐增加，两种方法的准确性都会提高，这也印证了较长的事件流提供了更多的信息；2）在MNIST-DVS的各个时长上，本模型的效果都优于Zhao的方法，甚至在时间长度为100ms的事件流上，本模型就超过了Zhao的方法在全长事件流上实现的精度。这一结果表明在相同甚至更少的信息下，我们的模型可以达到更好的分类效果。</p><p><img src="https://i.loli.net/2020/11/07/u16zsqjmZkWHCa9.png" alt="表2 在MNIST-DVS数据集不同时间长度下的性能"></p><p>接着我们验证模型不完整信息推理的能力，即模型在测试过程中，当描述对象的信息不完整时，是否仍然能够保持一定的识别精度。我们同样在MNIST-DVS数据集上进行验证，使用500ms的事件流进行训练，并在测试时观察前300ms三种不同分类算法的性能（本文的SPA学习算法，Zhao的方法中使用的Tempotron学习算法，和非时序分类器SVM）。如图2所示，随着事件流的流入，三种算法模型的分类精度都不断提高。SPA模型是所有方法中性能最好的，特别是在输入信息极其不完整的前100ms时。这是因为SPA在训练模型时，每个训练样本都会由于峰值检测机制基于片段中的峰值被多次训练，继而增加了训练样本的多样性。因此，我们的模型具有较好的泛化能力，即使在信息不完整的情况下，也可以在早期推断出结果。</p><p><img src="https://i.loli.net/2020/11/07/RKD8G4jzPtwyuJa.png" alt="图2 在MNIST-DVS数据集上不完全信息推理的性能"></p><p><strong>结论和讨论</strong></p><p>综上，我们提出了一种有效的事件表示的物体分类模型，该模型提出了一种新的脉冲神经网络学习算法——SPA算法。SPA学习算法通过迭代增加神经元响应正确的概率这一方式提高神经元响应的可靠性。同时，通过SPA算法中的峰值检测机制提高事件流时长内信息的学习利用率。实验结果验证了本模型的性能，以及模型所具有信息利用效率高、泛化能力强等优点。</p><p>论文链接：<a href="https://arxiv.org/abs/2002.06199" target="_blank" rel="noopener">https://arxiv.org/abs/2002.06199</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前因为一些emmmm俗事，很久没看论文了，现在因为要做毕设，又久违看起了论文，这次是浙大普适与智能实验室AAAI 2020的论文《基于脉冲神经网络的事件表示的物体分类方法》&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="SNN" scheme="http://yuanquanquan.top/tags/SNN/"/>
    
  </entry>
  
  <entry>
    <title>Some Virtual Try-on (VTON) Research</title>
    <link href="http://yuanquanquan.top/2020/20201030/"/>
    <id>http://yuanquanquan.top/2020/20201030/</id>
    <published>2020-10-30T14:51:09.000Z</published>
    <updated>2020-10-29T15:44:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近跟项目在做一些，虚拟试衣Virtual Try-on (VTON）的工作，记录一下调研的数据以及开源的论文以及模型。</p></blockquote><a id="more"></a><h2 id="Clothing-dataset"><a href="#Clothing-dataset" class="headerlink" title="Clothing dataset"></a>Clothing dataset</h2><p>Over 5,000 images of 20 different classes.</p><p>This dataset can be freely used for any purpose, including commercial:</p><p>For example:</p><ul><li>Creating a tutorial or a course (free or paid)</li><li>Writing a book</li><li>Kaggle competitions (as an external dataset)</li><li>Training an internal model at any company</li></ul><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>The <code>images.csv</code> file contains:</p><ul><li><code>image</code> - the ID of the image (use it to load the image from <code>images/&lt;ID&gt;.jpg</code>)</li><li><code>sender_id</code> - the ID of a person who contributed the image</li><li><code>label</code> - the class of the image</li><li><code>kids</code> - flag, <code>True</code> if it’s clothes for kids</li></ul><h3 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h3><ul><li>If you’re looking for a subset of the clothing dataset, check here: <a href="https://github.com/alexeygrigorev/clothing-dataset-small" target="_blank" rel="noopener">https://github.com/alexeygrigorev/clothing-dataset-small</a></li><li>You can read more about this dataset here: <a href="https://medium.com/data-science-insider/clothing-dataset-5b72cd7c3f1f" target="_blank" rel="noopener">https://medium.com/data-science-insider/clothing-dataset-5b72cd7c3f1f</a></li><li>This dataset is also awailable on Kaggle (with images in higher resolution): </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.kaggle.com/agrigorev/clothing-dataset-full/</span><br></pre></td></tr></table></figure><h3 id="Top-10-subset"><a href="#Top-10-subset" class="headerlink" title="Top-10 subset"></a>Top-10 subset</h3><p>Images of some classes don’t appear very often. Training a neural network to predict these classes is quite difficult — we need at least 100-200 images of each class to make a meaningful model.</p><p>That’s why, for educational purposes, we created a subset of the full dataset that covers only the top-10 classes.</p><p>Check it here: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/alexeygrigorev/clothing-dataset-small</span><br></pre></td></tr></table></figure><p>Examples</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.kaggle.com/agrigorev/collage</span><br></pre></td></tr></table></figure><p>Do you use this dataset somewhere? Please submit a PR with a link</p><h3 id="Acknowledgements"><a href="#Acknowledgements" class="headerlink" title="Acknowledgements"></a>Acknowledgements</h3><p>We’d like to thank</p><ul><li><p>Kenes Shangereyev and Tagias.com for helping with 3000 images</p></li><li><p>All the 32 people who contributed their images to the dataset via the forms:</p></li><li><ul><li>Patricia Goldberg</li></ul></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.linkedin.com/in/patricia-goldberg/</span><br></pre></td></tr></table></figure></li><li><p>Everyone who supported the initiative by engaging with the announcements on social media</p></li></ul><p>It wouldn’t be possible to collect this dataset without your help!</p><p>A curated list of awesome research papers, projects, code, dataset, workshops etc. related to virtual try-on (VTON).</p><ul><li><a href="#Image-based-2D-Virtual-Try-on">Image-based (2D) Virtual Try-on</a></li><li><a href="#3D-virtual-try-on">3D Virtual Try-on</a></li><li><a href="#Multi-Pose-Guided-Virtual-Try-on">Multi-Pose Guided Virtual Try-on</a></li><li><a href="#Video-Virtual-Try-on">Video Virtual Try-on</a></li><li><a href="#non-clothing-virtual-try-on">Non-clothing Virtual Try-on</a></li><li><a href="#pose-guided-human-synthesis">Pose-Guided Human Synthesis</a></li><li><a href="#Datasets-for-Virtual-Try-on">Datasets for Virtual Try-on</a></li><li><a href="#Related-Conference-Workshops">Related Conference Workshops</a></li><li><a href="#Related-Repositories">Related Repositories</a></li></ul><h2 id="Image-based-2D-Virtual-Try-on"><a href="#Image-based-2D-Virtual-Try-on" class="headerlink" title="Image-based (2D) Virtual Try-on"></a>Image-based (2D) Virtual Try-on</h2><h4 id="ACCV-2020"><a href="#ACCV-2020" class="headerlink" title="ACCV 2020"></a>ACCV 2020</h4><ul><li><p>CloTH-VTON: Clothing Three-dimensional reconstruction for Hybrid image-based Virtual Try-ON - <a href="https://minar09.github.io/clothvton/" target="_blank" rel="noopener">Project</a></p><h4 id="ECCV-2020"><a href="#ECCV-2020" class="headerlink" title="ECCV 2020"></a>ECCV 2020</h4></li><li><p>Do Not Mask What You Do Not Need to Mask: a Parser Free Virtual Try-On - <a href="https://arxiv.org/pdf/2007.02721.pdf" target="_blank" rel="noopener">Paper</a></p><h4 id="CVPR-2020"><a href="#CVPR-2020" class="headerlink" title="CVPR 2020"></a>CVPR 2020</h4></li><li><p>Towards Photo-Realistic Virtual Try-On by Adaptively Generating↔Preserving Image Content - <a href="https://github.com/switchablenorms/DeepFashion_Try_On" target="_blank" rel="noopener">Paper/Code/Data</a></p></li><li>Image Based Virtual Try-On Network From Unpaired Data - <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Neuberger_Image_Based_Virtual_Try-On_Network_From_Unpaired_Data_CVPR_2020_paper.html" target="_blank" rel="noopener">Paper</a></li><li><p>Semantically Multi-modal Image Synthesis - <a href="https://seanseattle.github.io/SMIS/" target="_blank" rel="noopener">Paper/Code/Model</a></p><h4 id="CVPRW-2020"><a href="#CVPRW-2020" class="headerlink" title="CVPRW 2020"></a>CVPRW 2020</h4></li><li><p>CP-VTON+: Clothing Shape and Texture Preserving Image-Based Virtual Try-On - <a href="https://minar09.github.io/cpvtonplus/" target="_blank" rel="noopener">Paper/Code/Data/Model</a></p></li><li><p>3D Reconstruction of Clothes using a Human Body Model and its Application to Image-based Virtual Try-On - <a href="https://minar09.github.io/c3dvton/" target="_blank" rel="noopener">Paper/Project</a></p><h4 id="ICCV-2019"><a href="#ICCV-2019" class="headerlink" title="ICCV 2019"></a>ICCV 2019</h4></li><li><p>VTNFP: An Image-Based Virtual Try-On Network With Body and Clothing Feature Preservation - <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_VTNFP_An_Image-Based_Virtual_Try-On_Network_With_Body_and_Clothing_ICCV_2019_paper.html" target="_blank" rel="noopener">Paper</a></p></li><li><p>ClothFlow: A Flow-Based Model for Clothed Person Generation - <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.html" target="_blank" rel="noopener">Paper</a></p><h4 id="ICCVW-2019"><a href="#ICCVW-2019" class="headerlink" title="ICCVW 2019"></a>ICCVW 2019</h4></li><li><p>UVTON: UV Mapping to Consider the 3D Structure of a Human in Image-Based Virtual Try-On Network, <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Kubo_UVTON_UV_Mapping_to_Consider_the_3D_Structure_of_a_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></p></li><li>LA-VITON: A Network for Looking-Attractive Virtual Try-On - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Lee_LA-VITON_A_Network_for_Looking-Attractive_Virtual_Try-On_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Robust Cloth Warping via Multi-Scale Patch Adversarial Loss for Virtual Try-On Framework - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/HBU/Ayush_Robust_Cloth_Warping_via_Multi-Scale_Patch_Adversarial_Loss_for_Virtual_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Powering Virtual Try-On via Auxiliary Human Segmentation Learning - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Ayush_Powering_Virtual_Try-On_via_Auxiliary_Human_Segmentation_Learning_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li><p>Generating High-Resolution Fashion Model Images Wearing Custom Outfits - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Yildirim_Generating_High-Resolution_Fashion_Model_Images_Wearing_Custom_Outfits_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></p><h4 id="ECCV-2018"><a href="#ECCV-2018" class="headerlink" title="ECCV 2018"></a>ECCV 2018</h4></li><li><p>Toward Characteristic-Preserving Image-based Virtual Try-On Network - <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Bochao_Wang_Toward_Characteristic-Preserving_Image-based_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/sergeywong/cp-vton" target="_blank" rel="noopener">Code</a></p></li><li><p>SwapNet: Garment Transfer in Single View Images - <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Amit_Raj_SwapNet_Garment_Transfer_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/andrewjong/SwapNet" target="_blank" rel="noopener">Code (unofficial)</a></p><h4 id="CVPR-2018"><a href="#CVPR-2018" class="headerlink" title="CVPR 2018"></a>CVPR 2018</h4></li><li><p>VITON: An Image-based Virtual Try-on Network - <a href="https://arxiv.org/abs/1711.08447" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/xthan/VITON" target="_blank" rel="noopener">Code/Model</a></p><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4></li><li><p>Keypoints-Based 2D Virtual Try-on Network System, JAKO 2020 - <a href="https://www.koreascience.or.kr/article/JAKO202010163508810.pdf" target="_blank" rel="noopener">Paper</a></p></li><li>Virtual Try-On With Generative Adversarial Networks: A Taxonomical Survey - <a href="https://www.igi-global.com/chapter/virtual-try-on-with-generative-adversarial-networks/260791" target="_blank" rel="noopener">Book chapter</a></li><li>LGVTON: A Landmark Guided Approach to Virtual Try-On - <a href="https://arxiv.org/abs/2004.00562v1" target="_blank" rel="noopener">Paper</a></li><li>SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On, WACV 2020 - <a href="http://openaccess.thecvf.com/content_WACV_2020/html/Jandial_SieveNet_A_Unified_Framework_for_Robust_Image-Based_Virtual_Try-On_WACV_2020_paper.html" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/levindabhi/SieveNet" target="_blank" rel="noopener">Code/Model (unofficial)</a></li><li>GarmentGAN: Photo-realistic Adversarial Fashion Transfer - <a href="https://arxiv.org/abs/2003.01894" target="_blank" rel="noopener">Paper</a></li><li>Toward Accurate and Realistic Virtual Try-on Through Shape Matching and Multiple Warps - <a href="https://arxiv.org/abs/2003.10817" target="_blank" rel="noopener">Paper</a></li><li>FashionFit Analysis of Mapping 3D Pose and neural body fit for custom virtual try-on, IEEE Access 2020 - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091008" target="_blank" rel="noopener">Paper</a></li><li>SP-VITON: shape-preserving image-based virtual try-on network, Multimedia Tools and Applications 2019 - <a href="https://link.springer.com/article/10.1007/s11042-019-08363-w" target="_blank" rel="noopener">Paper</a></li><li>VITON-GAN: Virtual Try-on Image Generator Trained with Adversarial Loss, Eurographics 2019 Posters - <a href="https://arxiv.org/abs/1911.07926v1" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/shionhonda/viton-gan" target="_blank" rel="noopener">Code/Model</a></li><li>Image-Based Virtual Try-on Network with Structural Coherence, ICIP 2019 - <a href="https://ieeexplore.ieee.org/document/8803811" target="_blank" rel="noopener">Paper</a></li><li>End-to-End Learning of Geometric Deformations of Feature Maps for Virtual Try-On - <a href="https://arxiv.org/abs/1906.01347v2" target="_blank" rel="noopener">Paper</a></li><li>M2E-Try On Net: Fashion from Model to Everyone - <a href="https://arxiv.org/abs/1811.08599v1" target="_blank" rel="noopener">Paper</a></li></ul><h2 id="3D-Virtual-Try-on"><a href="#3D-Virtual-Try-on" class="headerlink" title="3D Virtual Try-on"></a>3D Virtual Try-on</h2><h4 id="ECCV-2020-1"><a href="#ECCV-2020-1" class="headerlink" title="ECCV 2020"></a>ECCV 2020</h4><ul><li>BCNet: Learning Body and Cloth Shape from A Single Image - <a href="https://arxiv.org/pdf/2004.00214.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/jby1993/BCNet" target="_blank" rel="noopener">Code/Data</a></li><li>GAN-based Garment Generation Using Sewing Pattern Images - <a href="https://gamma.umd.edu/researchdirections/virtualtryon/garmentgeneration/" target="_blank" rel="noopener">Paper/Code/Model/Data</a></li><li>Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images - <a href="https://kv2000.github.io/2020/03/25/deepFashion3DRevisited/" target="_blank" rel="noopener">Paper/Data</a></li><li>SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing - <a href="https://virtualhumans.mpi-inf.mpg.de/sizer/" target="_blank" rel="noopener">Paper/Code/Data</a></li><li><p>CLOTH3D: Clothed 3D Humans - <a href="https://arxiv.org/pdf/1912.02792.pdf" target="_blank" rel="noopener">Paper</a></p><h4 id="CVPR-2020-1"><a href="#CVPR-2020-1" class="headerlink" title="CVPR 2020"></a>CVPR 2020</h4></li><li><p>Learning to Transfer Texture from Clothing Images to 3D Humans - <a href="http://virtualhumans.mpi-inf.mpg.de/pix2surf/" target="_blank" rel="noopener">Paper/Code</a></p></li><li>TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style - <a href="http://virtualhumans.mpi-inf.mpg.de/tailornet/" target="_blank" rel="noopener">Paper/Code/Data</a></li><li><p>Learning to Dress 3D People in Generative Clothing - <a href="https://cape.is.tue.mpg.de/" target="_blank" rel="noopener">Paper/Code/Data</a></p><h4 id="ICCV-2019-1"><a href="#ICCV-2019-1" class="headerlink" title="ICCV 2019"></a>ICCV 2019</h4></li><li><p>Multi-Garment Net: Learning to Dress 3D People from Images - <a href="http://virtualhumans.mpi-inf.mpg.de/mgn/" target="_blank" rel="noopener">Paper/Code/Data</a></p></li><li>3DPeople: Modeling the Geometry of Dressed Humans - <a href="https://arxiv.org/abs/1904.04571" target="_blank" rel="noopener">Paper</a></li><li><p>GarNet: A Two-Stream Network for Fast and Accurate 3D Cloth Draping - <a href="https://www.epfl.ch/labs/cvlab/research/garment-simulation/garnet/" target="_blank" rel="noopener">Paper/Data</a></p><h4 id="ECCV-2018-1"><a href="#ECCV-2018-1" class="headerlink" title="ECCV 2018"></a>ECCV 2018</h4></li><li><p>DeepWrinkles: Accurate and Realistic Clothing Modeling - <a href="https://arxiv.org/abs/1808.03417" target="_blank" rel="noopener">Paper</a></p><h4 id="CVPR-2018-1"><a href="#CVPR-2018-1" class="headerlink" title="CVPR 2018"></a>CVPR 2018</h4></li><li><p>Video Based Reconstruction of 3D People Models - <a href="http://gvv.mpi-inf.mpg.de/projects/wxu/VideoAvatar/" target="_blank" rel="noopener">Paper/Code/Data</a></p><h4 id="Others-1"><a href="#Others-1" class="headerlink" title="Others"></a>Others</h4></li><li><p>CloTH-VTON: Clothing Three-dimensional reconstruction for Hybrid image-based Virtual Try-ON, ACCV 2020 - <a href="https://minar09.github.io/clothvton/" target="_blank" rel="noopener">Project</a></p></li><li>Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On, ACM SCA 2020 - <a href="http://mslab.es/projects/FullyConvolutionalGraphVirtualTryOn" target="_blank" rel="noopener">Paper/Project</a></li><li>DeePSD: Automatic Deep Skinning And Pose Space Deformation For 3D Garment Animation - <a href="https://arxiv.org/pdf/2009.02715.pdf" target="_blank" rel="noopener">Paper</a></li><li>3D Reconstruction of Clothes using a Human Body Model and its Application to Image-based Virtual Try-On, CVPRW 2020 - <a href="https://minar09.github.io/c3dvton/" target="_blank" rel="noopener">Paper/Project</a></li><li>Learning-Based Animation of Clothing for Virtual Try-On, Eurographics 2019 - <a href="http://dancasas.github.io/projects/LearningBasedVirtualTryOn/index.html" target="_blank" rel="noopener">Paper/Project</a></li><li>Learning an Intrinsic Garment Space for Interactive Authoring of Garment Animation, SIGGRAPH Asia 2019 - <a href="http://geometry.cs.ucl.ac.uk/projects/2019/garment_authoring/" target="_blank" rel="noopener">Paper/Code</a></li><li>3D Virtual Garment Modeling from RGB Images, ISMAR 2019 - <a href="https://arxiv.org/abs/1908.00114" target="_blank" rel="noopener">Paper</a></li><li>Deep Garment Image Matting for a Virtual Try-on System, ICCVW 2019 - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Shin_Deep_Garment_Image_Matting_for_a_Virtual_Try-on_System_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Learning a Shared Shape Space for Multimodal Garment Design, SIGGRAPH Asia 2018 - <a href="http://geometry.cs.ucl.ac.uk/projects/2018/garment_design/" target="_blank" rel="noopener">Paper/Code/Data</a></li><li>Detailed Garment Recovery from a Single-View Image, ACM TOG 2018 - <a href="https://arxiv.org/abs/1608.01250" target="_blank" rel="noopener">Paper</a></li><li>ClothCap: Seamless 4D Clothing Capture and Retargeting, SIGGRAPH 2017 - <a href="http://clothcap.is.tue.mpg.de/" target="_blank" rel="noopener">Paper</a></li><li>Virtual Try-On through Image-Based Rendering, IEEE T-VCG 2013 - <a href="https://ieeexplore.ieee.org/document/6487501" target="_blank" rel="noopener">Paper</a></li><li>Markerless Garment Capture, ACM TOG 2008 - <a href="http://www.cs.ubc.ca/labs/imager/tr/2008/MarkerlessGarmentCapture/" target="_blank" rel="noopener">Paper/Data</a></li></ul><h2 id="Multi-Pose-Guided-Virtual-Try-on"><a href="#Multi-Pose-Guided-Virtual-Try-on" class="headerlink" title="Multi-Pose Guided Virtual Try-on"></a>Multi-Pose Guided Virtual Try-on</h2><ul><li><p>Down to the Last Detail: Virtual Try-on with Detail Carving - <a href="https://arxiv.org/abs/1912.06324v2" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/JDAI-CV/Down-to-the-Last-Detail-Virtual-Try-on-with-Detail-Carving" target="_blank" rel="noopener">Code/Model</a></p></li><li><p>Towards Multi-pose Guided Virtual Try-on Network, ICCV 2019 - <a href="https://arxiv.org/abs/1902.11026" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/thaithanhtuan/MyMGVTON" target="_blank" rel="noopener">Code</a></p></li><li><p>FIT-ME: IMAGE-BASED VIRTUAL TRY-ON WITH ARBITRARY POSES, ICIP 2019 - <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803681&amp;casa_token=2CL5K9pwy1IAAAAA:OTa5P-h6RWj9BdQVvkxQURR8tDy4Eg1BZynYOizMyQACnE-zL_EHu2xRzyXBOWijP_cItaO4" target="_blank" rel="noopener">Paper</a></p></li><li><p>Virtually Trying on New Clothing with Arbitrary Poses, ACM MM 2019 - <a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3350946?casa_token=w7EzejnZIaEAAAAA:KvDBsi1xYswuQuzEdJO-rsTDvysnSLYlAYi1J2st5lf8lnyotm5-umPKQupGaMEPUGxyBzijUkA9" target="_blank" rel="noopener">Paper</a></p></li><li><p>FashionOn: Semantic-guided Image-based Virtual Try-on with Detailed Human and Clothing Information, ACM MM 2019 - <a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3351075?casa_token=7y85FCo6B-QAAAAA:diZbVYmcSK13bMQ94MzrMG_-VvVG_oNFoGpI8wCBFJ_dHEzYnLBAPn2ZwbAgj_pmOWFMD6_1hOuk" target="_blank" rel="noopener">Paper</a></p></li></ul><h2 id="Video-Virtual-Try-on"><a href="#Video-Virtual-Try-on" class="headerlink" title="Video Virtual Try-on"></a>Video Virtual Try-on</h2><ul><li>FW-GAN: Flow-Navigated Warping GAN for Video Virtual Try-On, ICCV 2019 - <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_FW-GAN_Flow-Navigated_Warping_GAN_for_Video_Virtual_Try-On_ICCV_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Unsupervised Image-to-Video Clothing Transfer, ICCVW 2019 - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Pumarola_Unsupervised_Image-to-Video_Clothing_Transfer_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li></ul><h2 id="Non-clothing-Virtual-Try-on"><a href="#Non-clothing-Virtual-Try-on" class="headerlink" title="Non-clothing Virtual Try-on"></a>Non-clothing Virtual Try-on</h2><ul><li>CA-GAN: Weakly Supervised Color Aware GAN for Controllable Makeup Transfer - <a href="https://arxiv.org/pdf/2008.10298.pdf" target="_blank" rel="noopener">Paper</a></li><li>Regularized Adversarial Training for Single-Shot Virtual Try-On, ICCVW 2019 - <a href="http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Kikuchi_Regularized_Adversarial_Training_for_Single-Shot_Virtual_Try-On_ICCVW_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Disentangled Makeup Transfer with Generative Adversarial Network - <a href="https://arxiv.org/pdf/1907.01144v1.pdf" target="_blank" rel="noopener">Paper</a></li><li>PIVTONS: Pose Invariant Virtual Try-On Shoe with Conditional Image Completion, ACCV 2018 - <a href="https://winstonhsu.info/wp-content/uploads/2018/09/chou18PIVTONS.pdf" target="_blank" rel="noopener">Paper</a></li><li>Virtual Try-on of Eyeglasses using 3D Model of the Head - <a href="https://dl.acm.org/doi/pdf/10.1145/2087756.2087838" target="_blank" rel="noopener">Paper</a></li><li>A MIXED REALITY SYSTEM FOR VIRTUAL GLASSES TRY-ON - <a href="https://dl.acm.org/doi/pdf/10.1145/2087756.2087816" target="_blank" rel="noopener">Paper</a></li><li>A virtual try-on system in augmented reality using RGB-D cameras for footwear personalization - <a href="https://www.sciencedirect.com/science/article/abs/pii/S0278612514000594" target="_blank" rel="noopener">Paper</a></li></ul><h2 id="Pose-Guided-Human-Synthesis"><a href="#Pose-Guided-Human-Synthesis" class="headerlink" title="Pose-Guided Human Synthesis"></a>Pose-Guided Human Synthesis</h2><ul><li>PoNA: Pose-Guided Non-Local Attention for Human Pose Transfer, IEEE T-IP 2020 - <a href="https://ieeexplore.ieee.org/abstract/document/9222550" target="_blank" rel="noopener">Paper</a></li><li>Pose-Guided High-Resolution Appearance Transfer via Progressive Training - <a href="https://arxiv.org/pdf/2008.11898.pdf" target="_blank" rel="noopener">Paper</a></li><li>Recapture as You Want - <a href="https://arxiv.org/pdf/2006.01435.pdf" target="_blank" rel="noopener">Paper</a></li><li>Generating Person Images with Appearance-aware Pose Stylizer, IJCAI 2020 - <a href="https://arxiv.org/pdf/2007.09077.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/siyuhuang/PoseStylizer" target="_blank" rel="noopener">Code</a></li><li>Controllable Person Image Synthesis with Attribute-Decomposed GAN, CVPR 2020 - <a href="https://arxiv.org/pdf/2003.12267.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/menyifang/ADGAN" target="_blank" rel="noopener">Code</a></li><li>Deep Image Spatial Transformation for Person Image Generation, CVPR 2020 - <a href="https://arxiv.org/pdf/2003.00696v2.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/RenYurui/Global-Flow-Local-Attention" target="_blank" rel="noopener">Code</a></li><li>Neural Pose Transfer by Spatially Adaptive Instance Normalization, CVPR 2020 - <a href="https://arxiv.org/pdf/2003.07254v2.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/jiashunwang/Neural-Pose-Transfer" target="_blank" rel="noopener">Code</a></li><li>Guided Image-to-Image Translation with Bi-Directional Feature Transformation, ICCV 2019 - <a href="https://arxiv.org/pdf/1910.11328v1.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/vt-vl-lab/Guided-pix2pix" target="_blank" rel="noopener">Code</a></li><li>Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis, ICCV 2019 - <a href="https://arxiv.org/pdf/1909.12224.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/svip-lab/impersonator" target="_blank" rel="noopener">Code</a></li><li>ClothFlow: A Flow-Based Model for Clothed Person Generation, ICCV 2019 - <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.html" target="_blank" rel="noopener">Paper</a></li><li>Progressive Pose Attention for Person Image Generation, CVPR 2019 - <a href="https://arxiv.org/pdf/1904.03349.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/tengteng95/Pose-Transfer" target="_blank" rel="noopener">Code</a></li><li>Dense Intrinsic Appearance Flow for Human Pose Transfer, CVPR 2019 - <a href="https://arxiv.org/pdf/1903.11326v1.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/ly015/intrinsic_flow" target="_blank" rel="noopener">Code</a></li><li>Unsupervised Person Image Generation with Semantic Parsing Transformation, CVPR 2019, TPAMI 2020 - <a href="https://arxiv.org/pdf/1904.03379.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/SijieSong/person_generation_spt" target="_blank" rel="noopener">Code</a></li><li>Pose Guided Fashion Image Synthesis Using Deep Generative Model - <a href="https://arxiv.org/pdf/1906.07251.pdf" target="_blank" rel="noopener">Paper</a></li><li>Synthesizing Images of Humans in Unseen Poses, CVPR 2018 - <a href="https://arxiv.org/pdf/1804.07739.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/balakg/posewarp-cvpr2018" target="_blank" rel="noopener">Code</a></li><li>Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis, NeurIPS 2018 - <a href="https://arxiv.org/pdf/1810.11610.pdf" target="_blank" rel="noopener">Paper</a></li><li>Deformable GANs for Pose-based Human Image Generation, CVPR 2018 - <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Siarohin_Deformable_GANs_for_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a></li><li>Pose-Normalized Image Generation for Person Re-identification, ECCV 2018 - <a href="https://arxiv.org/pdf/1712.02225.pdf" target="_blank" rel="noopener">Paper</a></li><li>Disentangled Person Image Generation, CVPR 2018 - <a href="https://homes.esat.kuleuven.be/~liqianma/CVPR18_DPIG/index.html" target="_blank" rel="noopener">Paper/Code/Data</a></li><li>A Variational U-Net for Conditional Appearance and Shape Generation, CVPR 2018 - <a href="https://arxiv.org/pdf/1804.04694.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/CompVis/vunet" target="_blank" rel="noopener">Code</a></li><li>Human Appearance Transfer, CVPR 2018 - <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Human_Appearance_Transfer_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a></li><li>Pose guided person image generation, NeurIPS 2017 - <a href="https://arxiv.org/pdf/1705.09368.pdf" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/charliememory/Pose-Guided-Person-Image-Generation" target="_blank" rel="noopener">Code</a></li></ul><h2 id="Datasets-for-Virtual-Try-on"><a href="#Datasets-for-Virtual-Try-on" class="headerlink" title="Datasets for Virtual Try-on"></a>Datasets for Virtual Try-on</h2><ul><li>VITON - <a href="https://drive.google.com/file/d/1MxCUvKxejnwWnoZ-KoCyMCXo3TLhRuTo/view" target="_blank" rel="noopener">Download</a>, <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Han_VITON_An_Image-Based_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a></li><li>MPV - <a href="https://drive.google.com/drive/folders/1e3ThRpSj8j9PaCUw8IrqzKPDVJK_grcA" target="_blank" rel="noopener">Download</a>, <a href="https://arxiv.org/abs/1902.11026" target="_blank" rel="noopener">Paper</a></li><li>Deep Fashion3D - <a href="https://arxiv.org/abs/2003.12753" target="_blank" rel="noopener">Paper</a></li><li>Digital Wardrobe - <a href="http://virtualhumans.mpi-inf.mpg.de/mgn/" target="_blank" rel="noopener">Download/Paper/Project</a></li><li>TailorNet Dataset - <a href="https://github.com/zycliao/TailorNet_dataset" target="_blank" rel="noopener">Download</a>, <a href="http://virtualhumans.mpi-inf.mpg.de/tailornet/" target="_blank" rel="noopener">Project</a></li><li>CLOTH3D - <a href="https://arxiv.org/abs/1912.02792" target="_blank" rel="noopener">Paper</a></li><li>3DPeople - <a href="https://www.albertpumarola.com/research/3DPeople/index.html" target="_blank" rel="noopener">Project</a></li><li>THUman Dataset - <a href="http://www.liuyebin.com/deephuman/deephuman.html" target="_blank" rel="noopener">Project</a></li><li>Garment Dataset, Wang et al. 2018 - <a href="http://geometry.cs.ucl.ac.uk/projects/2018/garment_design/" target="_blank" rel="noopener">Project</a></li></ul><h2 id="Related-Conference-Workshops"><a href="#Related-Conference-Workshops" class="headerlink" title="Related Conference Workshops"></a>Related Conference Workshops</h2><ul><li>Workshop on Computer Vision for Fashion, Art and Design: <a href="https://sites.google.com/view/cvcreative2020" target="_blank" rel="noopener">CVPR 2020</a>, <a href="https://sites.google.com/view/cvcreative" target="_blank" rel="noopener">ICCV 2019</a>, <a href="https://sites.google.com/view/eccvfashion" target="_blank" rel="noopener">ECCV 2018</a></li><li>Workshop on Towards Human-Centric Image/Video Synthesis: <a href="https://vuhcs.github.io/" target="_blank" rel="noopener">CVPR 2020</a>, <a href="https://vuhcs.github.io/vuhcs-2019/index.html" target="_blank" rel="noopener">CVPR 2019</a></li></ul><h2 id="Related-Repositories"><a href="#Related-Repositories" class="headerlink" title="Related Repositories"></a>Related Repositories</h2><ul><li><a href="https://github.com/ayushidalmia/awesome-fashion-ai" target="_blank" rel="noopener">awesome-fashion-ai</a></li><li><a href="https://github.com/lzhbrian/Cool-Fashion-Papers" target="_blank" rel="noopener">Cool Fashion Papers</a></li><li><a href="https://github.com/lzhbrian/Clothes-3D" target="_blank" rel="noopener">Clothes-3D</a></li><li><a href="https://github.com/lijiaman/awesome-3d-human" target="_blank" rel="noopener">Awesome 3D Human</a></li><li><a href="https://github.com/openMVG/awesome_3DReconstruction_list" target="_blank" rel="noopener">Awesome 3D reconstruction list</a></li><li><a href="https://github.com/chenweikai/Body_Reconstruction_References" target="_blank" rel="noopener">Human Body Reconstruction</a></li></ul><h4 id="Pull-requests-are-welcome"><a href="#Pull-requests-are-welcome" class="headerlink" title="Pull requests are welcome!"></a>Pull requests are welcome!</h4>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近跟项目在做一些，虚拟试衣Virtual Try-on (VTON）的工作，记录一下调研的数据以及开源的论文以及模型。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="VTON" scheme="http://yuanquanquan.top/tags/VTON/"/>
    
  </entry>
  
  <entry>
    <title>树莓派4B+英特尔神经计算棒(Intel Movidius Neural Computing Stick)-YOLOV3目标检测(3)</title>
    <link href="http://yuanquanquan.top/2020/20201029/"/>
    <id>http://yuanquanquan.top/2020/20201029/</id>
    <published>2020-10-28T16:18:02.000Z</published>
    <updated>2020-10-29T02:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列的最后一篇，NCS这个东西感觉上跑demo是可用的，但是用在产品上开发工作量怕是不小，需要转换成IR文件。调用方式比较复杂，支持的东西不太全。Intel的文档已经算可以了，但是还是少。算力的话标准的边缘端算力，被别人的i7完爆，和我的i7一样，哭。</p></blockquote><a id="more"></a><h2 id="模型在树莓派上的部署过程"><a href="#模型在树莓派上的部署过程" class="headerlink" title="模型在树莓派上的部署过程"></a>模型在树莓派上的部署过程</h2><ul><li>选择预训练模型；</li><li>使用模型优化器，来转换模型；</li><li>最后在树莓派上推理模型。</li></ul><h3 id="五、如何训练自己的模型"><a href="#五、如何训练自己的模型" class="headerlink" title="五、如何训练自己的模型"></a>五、如何训练自己的模型</h3><h4 id="准备和优化训练好的模型"><a href="#准备和优化训练好的模型" class="headerlink" title="准备和优化训练好的模型"></a>准备和优化训练好的模型</h4><p>推断引擎可以<em>部署</em>经过任何受支持深度学习框架训练的网络模型：Caffe<em>、TensorFlow</em>、Kaldi<em>、MXNet</em> 或转换为 ONNX* 格式。要执行推断，推断引擎不使用原始模型操作，而是使用其中间表示文件  (IR)，后者已为在目标端点设备上执行进行过优化。要为训练好的模型生成中间表示文件，会使用模型优化器工具。</p><h4 id="模型优化器如何工作"><a href="#模型优化器如何工作" class="headerlink" title="模型优化器如何工作"></a>模型优化器如何工作</h4><p>模型优化器将模型加载到内存中、读取模型、构建模型的内部表示文件、优化模型，并生成中间表示文件。中间表示文件是推断引擎接受的唯一格式。</p><blockquote><p><strong>注意</strong>：模型优化器不推断模型。模型优化器是推断发生之前运行的一种离线工具。</p></blockquote><p>模型优化器有两个主要用途：</p><ul><li><strong>生成有效的中间表示文件</strong>。如果这个主要转换文件无效，则推断引擎无法运行。模型优化器的主要责任是生成构成中间表示文件的两个文件（<code>.xml</code>和<code>.bin</code>）。</li><li><strong>生成优化的中间表示文件</strong>。预训练的模型包含对于训练至关重要的层，如<code>Dropout</code>层。这些层在推断过程中毫无用处，反而可能增加推断时间。在许多情况下，这些操作可以自动从由此产生的中间表示文件中移除。但是，如果可以将一组操作表示为单个数学操作，然后成为模型图表中的单个操作节点，那么模型优化器会识别这一模式并用该单个操作节点代替这一组操作节点。结果就是一个运行节点数量比原始模型少的中间表示文件。这就降低了推断时间。</li></ul><p>要生成有效的中间表示文件，模型优化器必须能够读取原始模型包含的操作、处理它们的属性，并以中间表示文件的格式表示它们，同时保持所产生的中间表示文件的有效性。由此产生的模型由<a href="https://docs.openvinotoolkit.org/cn/latest/_docs_ops_opset.html" target="_blank" rel="noopener">操作规格</a>中描述的操作组成。</p><h2 id="有关模型，需要了解哪些内容"><a href="#有关模型，需要了解哪些内容" class="headerlink" title="有关模型，需要了解哪些内容"></a>有关模型，需要了解哪些内容</h2><p>在已知的框架和神经网络拓扑中存在着许多普通层。这类层的例子包括<code>Convolution</code>、<code>Pooling</code>和<code>Activation</code>。要读取原始模型并生成模型的中间表示文件，模型优化器必须能够处理这些层。</p><p>它们的完整列表取决于框架，并可以在<a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html" target="_blank" rel="noopener">受支持的框架层</a>部分中找到。如果您的拓扑仅包含来自层列表的层，像大多数用户所使用的拓扑一样，那么模型优化器便可以轻松创建中间表示文件。在这之后，您就可以继续使用推断引擎。</p><p>但是，如果您使用的拓扑含有模型优化器无法识别的层，请查看<a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html" target="_blank" rel="noopener">模型优化器中的自定义层</a>以了解如何处理自定义层。</p><h2 id="模型优化器目录结构"><a href="#模型优化器目录结构" class="headerlink" title="模型优化器目录结构"></a>模型优化器目录结构</h2><p>在使用 OpenVINO™ 工具套件或英特尔® Deep Learning Deployment Toolkit 进行安装后，模型优化器文件夹具有以下结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">|-- model_optimizer</span><br><span class="line"></span><br><span class="line">​    |-- extensions</span><br><span class="line"></span><br><span class="line">​        |-- front - Front-End framework agnostic transformations (operations output shapes are not defined yet). </span><br><span class="line"></span><br><span class="line">​            |-- caffe - Front-End Caffe-specific transformations and Caffe layers extractors</span><br><span class="line"></span><br><span class="line">​                |-- CustomLayersMapping.xml.example - example of file for registering custom Caffe layers (compatible</span><br></pre></td></tr></table></figure><p>with the 2017R3 release)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">            |-- kaldi - Front-End Kaldi-specific transformations and Kaldi operations extractors</span><br><span class="line"></span><br><span class="line">​            |-- mxnet - Front-End MxNet-specific transformations and MxNet symbols extractors</span><br><span class="line"></span><br><span class="line">​            |-- onnx - Front-End ONNX-specific transformations and ONNX operators extractors            </span><br><span class="line"></span><br><span class="line">​            |-- tf - Front-End TensorFlow-specific transformations, TensorFlow operations extractors, sub-graph</span><br></pre></td></tr></table></figure><p>replacements configuration files. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">        |-- middle - Middle-End framework agnostic transformations (layers output shapes are defined).</span><br><span class="line"></span><br><span class="line">​        |-- back - Back-End framework agnostic transformations (preparation for IR generation).        </span><br><span class="line"></span><br><span class="line">​    |-- mo</span><br><span class="line"></span><br><span class="line">​        |-- back - Back-End logic: contains IR emitting logic</span><br><span class="line"></span><br><span class="line">​        |-- front - Front-End logic: contains matching between Framework-specific layers and IR specific, calculation of output shapes for each registered layer</span><br><span class="line"></span><br><span class="line">​        |-- graph - Graph utilities to work with internal IR representation</span><br><span class="line"></span><br><span class="line">​        |-- middle - Graph transformations - optimizations of the model</span><br><span class="line"></span><br><span class="line">​        |-- pipeline - Sequence of steps required to create IR for each framework</span><br><span class="line"></span><br><span class="line">​        |-- utils - Utility functions</span><br><span class="line"></span><br><span class="line">​    |-- tf_call_ie_layer - Source code that enables TensorFlow fallback in Inference Engine during model inference</span><br><span class="line"></span><br><span class="line">​    |-- mo.py - Centralized entry point that can be used for any supported framework</span><br><span class="line"></span><br><span class="line">​    |-- mo_caffe.py - Entry point particularly for Caffe</span><br><span class="line"></span><br><span class="line">​    |-- mo_kaldi.py - Entry point particularly for Kaldi</span><br><span class="line"></span><br><span class="line">​    |-- mo_mxnet.py - Entry point particularly for MXNet</span><br><span class="line"></span><br><span class="line">​    |-- mo_onnx.py - Entry point particularly for ONNX</span><br><span class="line"></span><br><span class="line">​    |-- mo_tf.py - Entry point particularly for TensorFlow</span><br></pre></td></tr></table></figure><p>以下部分提供了有关如何使用模型优化器的信息，从配置工具、生成针对给定模型的中间表示文件，到根据您的需求定制工具：</p><ul><li><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Config_Model_Optimizer.html" target="_blank" rel="noopener">配置模型优化器</a></li><li><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model.html" target="_blank" rel="noopener">将模型转换为中间表示文件</a></li><li><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_customize_model_optimizer_Customize_Model_Optimizer.html" target="_blank" rel="noopener">模型优化器中的自定义层</a></li><li><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Model_Optimization_Techniques.html" target="_blank" rel="noopener">模型优化技术</a></li><li><p><a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html" target="_blank" rel="noopener">有关模型优化器的常见问题</a></p></li><li><p>](<a href="https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html" target="_blank" rel="noopener">https://docs.openvinotoolkit.org/cn/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html</a>)</p></li></ul><p><strong>第一步：</strong></p><p>找到label文件</p><p>C:\Users\（你的用户名）\Documents\Intel\OpenVINO\openvino_models\ir\public\squeezenet1.1\FP16\squeezenet1.1.labels。</p><p>找到模型文件</p><p>C:\Users\（你的用户名）\Documents\Intel\OpenVINO\openvino_models\models\public\squeezenet1.1\squeezenet1.1.caffemodel</p><p>找一个input</p><p>C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379\deployment_tools\demo\car.png</p><p><strong>第二步：</strong></p><p>转换模型，用到model_optimizer下的mo.py. 把上面的文件拷到了D:OPENVN下，用下面的代码转换。就能转换出.xml的文件。</p><p><img src="https://pic1.zhimg.com/80/v2-4d0c114cf7184e8a97488e52b54d71cc_720w.png" alt="img"></p><p><strong>第三步：</strong></p><p>跑转换好的文件。。由于这一步骤其实主要是展示如何转换模型。。。所以我们还是用demo自带的业务逻辑。在这个文件夹夹下有刚才建立好的classification_sample_async.exe文件。。。下面这个文件夹下找。</p><p>C:\Users<username>\Documents\Intel\OpenVINO\inference_engine_samples_build\intel64\Release</username></p><p>cd到上面这个文件夹。</p><p>然后运行</p><p> .\classification_sample_async.exe -i “D:\OPENVN\car.png” -m “D:\OPENVN\squeezenet1.1.xml” -d MYRIAD</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本系列的最后一篇，NCS这个东西感觉上跑demo是可用的，但是用在产品上开发工作量怕是不小，需要转换成IR文件。调用方式比较复杂，支持的东西不太全。Intel的文档已经算可以了，但是还是少。算力的话标准的边缘端算力，被别人的i7完爆，和我的i7一样，哭。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Intel-Neural-Compute-stick" scheme="http://yuanquanquan.top/tags/Intel-Neural-Compute-stick/"/>
    
  </entry>
  
  <entry>
    <title>树莓派4B+英特尔神经计算棒(Intel Movidius Neural Computing Stick)-YOLOV3目标检测(2)</title>
    <link href="http://yuanquanquan.top/2020/20201028/"/>
    <id>http://yuanquanquan.top/2020/20201028/</id>
    <published>2020-10-28T10:18:53.000Z</published>
    <updated>2020-10-28T10:45:46.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>接上一章，分别在树莓派和Windows上安装了NCS环境，接下来进行格式转换，在windows训练Caffe或TensorFlow模型，编译成NCS可以执行的graph；测试端则面向ncs python mvnc api编程，可以运行在树莓派上raspbian stretch版本，也可以运行在训练端这种机器上。</p></blockquote><a id="more"></a><h3 id="三、格式转换"><a href="#三、格式转换" class="headerlink" title="三、格式转换"></a>三、格式转换</h3><h4 id="1、下载darknet版官网训练模型"><a href="#1、下载darknet版官网训练模型" class="headerlink" title="1、下载darknet版官网训练模型"></a>1、下载darknet版官网训练模型</h4><p>如果没有现成的，可以从<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">pjreddie网站</a>下载</p><p><a href="https://blog.csdn.net/c20081052/article/details/90056746" target="_blank" rel="noopener">Yolo V3 COCO weights</a>（237MB），<a href="https://pjreddie.com/media/files/yolov3-tiny.weights" target="_blank" rel="noopener">Tiny Yolo V3 COCO weights</a>（34MB）， 标签文件 <a href="https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names" target="_blank" rel="noopener">coco.names</a></p><h4 id="2、将weights文件转换为pb文件"><a href="#2、将weights文件转换为pb文件" class="headerlink" title="2、将weights文件转换为pb文件"></a>2、将weights文件转换为pb文件</h4><p>OpenVINO不支持直接使用Yolo V3的.weights文件，目前仅支持ONNX、TensorFlow、Caffe和MXNet。需要先<a href="https://github.com/PINTO0309/OpenVINO-YoloV3" target="_blank" rel="noopener">把.weights文件转换成TensorFlow的.pb文件。</a><br> 自己在桌面新建一个文件夹例如raspberry，将下载的仓库放进新建的文件夹进行解压，并将前面下载的<strong>两个权重</strong>放进解压的仓库中。</p><p><strong>转换yolov3.weights的指令如下：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert_weights_pb.py --weights_file yolov3-tiny.weights --tiny True --size 416 --output_graph frozen_darknet_yolov3_tiny_model.pb --data_format NHWC</span><br></pre></td></tr></table></figure><p>上面指令是将<strong>yolov3-tiny.weights转为pb文件</strong>，传的参数：–tiny必须指定True, –data_format <strong>必须是NHWC</strong>，否则后面你拿转成功的pb，然后再去转xml和bin，然后做目标检测会遇到此类错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[ ERROR ]  Cannot infer shapes or values for node &quot;detector/yolo-v3/meshgrid_1/mul_1/YoloRegion&quot;.</span><br><span class="line">[ ERROR ]  index 2 is out of bounds for axis 0 with size 2</span><br><span class="line">[ ERROR ]</span><br><span class="line">[ ERROR ]  It can happen due to bug in custom shape infer function &lt;function RegionYoloOp.regionyolo_infer at 0x000001DE82253EA0&gt;.</span><br><span class="line">[ ERROR ]  Or because the node inputs have incorrect values/shapes.</span><br><span class="line">[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).</span><br><span class="line">[ ERROR ]  Run Model Optimizer with --log_level=DEBUG for more information.</span><br><span class="line">[ ERROR ]  Stopped shape/value propagation at &quot;detector/yolo-v3/meshgrid_1/mul_1/YoloRegion&quot; node.</span><br><span class="line"> For more information please refer to Model Optimizer FAQ (&lt;INSTALL_DIR&gt;/deployment_tools/documentation/docs/MO_FAQ.html),</span><br></pre></td></tr></table></figure><p>错误解析就是数据维度格式不对！参考： <a href="https://software.intel.com/en-us/node/802233" target="_blank" rel="noopener">https://software.intel.com/en-us/node/802233</a><br> 所以必须指定数据格式为NHWC （这种格式支持神经计算棒和CPU设备）</p><p><strong>转换yolov3.weights的指令如下：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python convert_weights_pb.py --weights_file yolov3.weights --size 416 --data_format NHWC</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/2020010819375385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt><br> 会在OpenVINO-YoloV3-master目录下生成<strong>frozen_darknet_yolov3_tiny_model.pb</strong> 和 <strong>frozen_darknet_yolov3_model.pb</strong> ；</p><p><img src="https://img-blog.csdnimg.cn/20200110184001358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt></p><h4 id="3、将pb模型文件转为IR文件"><a href="#3、将pb模型文件转为IR文件" class="headerlink" title="3、将pb模型文件转为IR文件"></a>3、将pb模型文件转为IR文件</h4><p>将之前安装好的openvino文件夹里的deployment_tools文件夹拷贝出来。<br> <img src="https://img-blog.csdnimg.cn/20200110182837145.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt><br> 将拷贝的文件夹粘贴进raspberry文件夹。<br> <img src="https://img-blog.csdnimg.cn/20200110183739695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt><br> 切到deploment_tools目录下的model_optimizer并运行以下命令：<br> <strong>转yolov3.pb的指令是：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E:\桌面\raspberry\deployment_tools\model_optimizer&gt;python mo_tf.py --input_model E:\桌面\raspberry\OpenVINO-YoloV3-master\frozen_darknet_yolov3_model.pb --tensorflow_use_custom_operations_config E:\桌面\raspberry\deployment_tools\model_optimizer\extensions\front\tf\yolo_v3.json --input_shape [1,416,416,3] --data_type=FP16</span><br></pre></td></tr></table></figure><p>相应文件的路径切换成自己的~需要说明的是树莓派据说支持数据类型是半浮点型的FP16, 官网很多是FP32的是针对PC的。</p><p><strong>转yolov3-tiny.pb的指令如下：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E:\桌面\raspberry\deployment_tools\model_optimizer&gt;python mo_tf.py --input_model E:\桌面\raspberry\OpenVINO-YoloV3-master\frozen_darknet_yolov3_tiny_model.pb --tensorflow_use_custom_operations_config E:\桌面\raspberry\deployment_tools\model_optimizer\extensions\front\tf\yolo_v3_tiny.json --input_shape [1,416,416,3] --data_type=FP16</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200110191229870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>执行完上述指令会生成以下六个文件：<br> <img src="https://img-blog.csdnimg.cn/20200110192121519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 转换过程中，需要你安装了如下依赖项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. tensorflow&gt;=1.2.0 </span><br><span class="line">2. networkx&gt;=1.11</span><br><span class="line">3. numpy&gt;=1.12.0</span><br><span class="line">4. test-generator==0.1.1</span><br><span class="line">5. defusedxml&gt;=0.5.0</span><br></pre></td></tr></table></figure><p>用pip安装就行</p><h3 id="四、在树莓派上部署"><a href="#四、在树莓派上部署" class="headerlink" title="四、在树莓派上部署"></a>四、在树莓派上部署</h3><h4 id="1、环境激活"><a href="#1、环境激活" class="headerlink" title="1、环境激活"></a>1、环境激活</h4><p>打开树莓派，终端激活OpenVINO工具包环境变量，后续操作都在这个终端窗口下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /home/pi/Downloads/l_openvino_toolkit_runtime_raspbian_p_2019.3.334/bin/setupvars.sh</span><br></pre></td></tr></table></figure><p>当完成如上操作后，会在终端显示如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[setupvars.sh] OpenVINO environment initialized</span><br></pre></td></tr></table></figure><h4 id="2、编译和运行yoloV3"><a href="#2、编译和运行yoloV3" class="headerlink" title="2、编译和运行yoloV3"></a>2、编译和运行yoloV3</h4><p>将上文转换的六个模型文件拷贝到树莓派上的/home/pi/Downloads/yolo_model目录下（可更改成你自己的目录）；并将coco.names复制两份，分别重命名为：frozen_darknet_yolov3_model.labels 和frozen_darknet_yolov3_tiny_model.labels ，所以/home/pi/Downloads/yolo_model目录下共有以下8个文件<br> <img src="https://img-blog.csdnimg.cn/20200110200435754.png" alt="在这里插入图片描述"><br> 接着</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir build &amp;&amp; cd build</span><br></pre></td></tr></table></figure><p><strong>编译</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=&quot;-march=armv7-a&quot; /home/pi/Downloads/l_openvino_toolkit_runtime_raspbian_p_2019.3.334/deployment_tools/inference_engine/samples</span><br></pre></td></tr></table></figure><p><strong>编译目标检测例子</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j2 object_detection_demo_yolov3_async</span><br></pre></td></tr></table></figure><p>编译完成后，会在/build/armv 7l/Release下生成可执行文件：object_detection_demo_yolov3_async；接下来就是给执行文件传入模型文件就可以跑深度学习了！</p><p><strong>（测摄像头）运行如下指令：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./armv7l/Release/object_detection_demo_yolov3_async -m /home/pi/Downloads/yolo_model/frozen_darknet_yolov3_model.xml -d MYRIAD -i cam</span><br></pre></td></tr></table></figure><p>此处-i cam表示读取的是camera；</p><p><strong>（测视频文件）运行如下指令：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./armv7l/Release/object_detection_demo_yolov3_async -m /home/pi/Downloads/yolo_model/frozen_darknet_yolov3_model.xml -d MYRIAD -i /path_to_video</span><br></pre></td></tr></table></figure><p>测试视频需要给-i指定视频路径；</p><h4 id="3、编译与运行yoloV3-tiny"><a href="#3、编译与运行yoloV3-tiny" class="headerlink" title="3、编译与运行yoloV3 tiny"></a>3、编译与运行yoloV3 tiny</h4><p>同样的编译和执行流程，在编译前需要你更改下源cpp文件，先找到yolo  v3的源文件，在：/home/pi/Downloads/l_openvino_toolkit_runtime_raspbian_p_2019.3.334/deployment_tools/inference_engine/samples/object_detection_demo_yolov3_async/这个目录下；你可以发现这个目录下有<br> <strong>main.cpp<br> CMakelists.txt<br> object_detection_demo_yolov3_async.hpp<br> README.md</strong><br> 我们需要更改<strong>main.cpp</strong>,更改前先备份一份main.cpp<br> <strong>更改：</strong> 第126行左右，注释掉原先yolov3的锚点框尺寸，更改成yolo v3 tiny的（共12个数：10, 14, 23, 27, 37, 58, 81, 82, 135, 169, 344, 319）<br> 更改完保存并退出。<br> 然后重新编译:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -j2 object_detection_demo_yolov3_async</span><br></pre></td></tr></table></figure><p>再编译</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./armv7l/Release/object_detection_demo_yolov3_async -m /home/pi/Downloads/yolo_model/frozen_darknet_yolov3_tiny_model.xml -d MYRIAD -i cam</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;接上一章，分别在树莓派和Windows上安装了NCS环境，接下来进行格式转换，在windows训练Caffe或TensorFlow模型，编译成NCS可以执行的graph；测试端则面向ncs python mvnc api编程，可以运行在树莓派上raspbian stretch版本，也可以运行在训练端这种机器上。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Intel-Neural-Compute-stick" scheme="http://yuanquanquan.top/tags/Intel-Neural-Compute-stick/"/>
    
  </entry>
  
  <entry>
    <title>树莓派4B+英特尔神经计算棒(Intel Movidius Neural Computing Stick)-YOLOV3目标检测(1)</title>
    <link href="http://yuanquanquan.top/2020/20201027/"/>
    <id>http://yuanquanquan.top/2020/20201027/</id>
    <published>2020-10-27T02:28:17.000Z</published>
    <updated>2020-10-29T02:07:19.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在看各种AI的加速方案以及边缘计算，jetsonnano、英伟达TX2、华为昇腾啊什么的</p><p>jetson nano 感觉性价比不如rk3399，还有Intel的计算棒。做终端应用成本太高，但是。。。。。3399得1000+了啊</p><p>下面就是关于Intel神经棒二代的上手教程。</p></blockquote><a id="more"></a><h2 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h2><p><a href="https://link.zhihu.com/?target=https%3A//developer.movidius.com/" target="_blank" rel="noopener">Intel Movidius Neural Computing Stick</a> </p><p><img src="https://pic2.zhimg.com/80/v2-68d4491ad612eb93863b60750f56819d_720w.jpg" alt></p><p>NCS是一个专用计算芯片，但能起到类似GPU对神经网络运算的加速作用。</p><p>SDK是开源的：<a href="https://link.zhihu.com/?target=https%3A//github.com/movidius/ncsdk" target="_blank" rel="noopener">https://github.com/movidius/ncsdk</a></p><p>提问不在GitHub issue里，而是在一个专门的论坛：<a href="https://link.zhihu.com/?target=https%3A//ncsforum.movidius.com/" target="_blank" rel="noopener">https://ncsforum.movidius.com/</a></p><p>虽然目前NCSDK支持的框架包含Tensorflow和Caffe，但并不是支持所有的模型，目前已支持的模型列表可以在这里查到：<a href="https://link.zhihu.com/?target=https%3A//github.com/movidius/ncsdk/releases" target="_blank" rel="noopener">https://github.com/movidius/ncsdk/releases</a></p><h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p>ncsdk的环境分为两部分，训练端和测试端。</p><ol><li>训练端通常是一个Ubuntu 带GPU主机，训练Caffe或TensorFlow模型，编译成NCS可以执行的graph；</li><li>测试端则面向ncs python mvnc api编程，可以运行在树莓派上raspbian stretch版本，也可以运行在训练端这种机器上。</li></ol><h2 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h2><h4 id="一、在树莓派上安装NCS环境"><a href="#一、在树莓派上安装NCS环境" class="headerlink" title="一、在树莓派上安装NCS环境"></a>一、在树莓派上安装NCS环境</h4><h4 id="二、在windows上安装NCS环境（格式转换用到）"><a href="#二、在windows上安装NCS环境（格式转换用到）" class="headerlink" title="二、在windows上安装NCS环境（格式转换用到）"></a>二、在windows上安装NCS环境（格式转换用到）</h4><h4 id="三、格式转换"><a href="#三、格式转换" class="headerlink" title="三、格式转换"></a>三、格式转换</h4><h4 id="四、在树莓派上部署"><a href="#四、在树莓派上部署" class="headerlink" title="四、在树莓派上部署"></a>四、在树莓派上部署</h4><h2 id="Step1-在树莓派上安装NCS环境"><a href="#Step1-在树莓派上安装NCS环境" class="headerlink" title="Step1 在树莓派上安装NCS环境"></a>Step1 在树莓派上安装NCS环境</h2><p><strong>Operating Systems</strong></p><ul><li>Raspbian* Buster, 32-bit</li><li>Raspbian* Stretch, 32-bit</li></ul><p><strong>Software</strong></p><ul><li>CMake* 3.7.2 or higher</li><li>Python* 3.5, 32-bit</li></ul><h4 id="1、下载OpenVINO-toolkit-for-Raspbian"><a href="#1、下载OpenVINO-toolkit-for-Raspbian" class="headerlink" title="1、下载OpenVINO toolkit for Raspbian"></a>1、下载<a href="https://download.01.org/opencv/2020/openvinotoolkit/" target="_blank" rel="noopener">OpenVINO toolkit for Raspbian</a></h4><h4 id="2、树莓派上安装OpenVINO工具包"><a href="#2、树莓派上安装OpenVINO工具包" class="headerlink" title="2、树莓派上安装OpenVINO工具包"></a>2、树莓派上安装OpenVINO工具包</h4><p>可以参考官网教程：<a href="https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_raspbian.html" target="_blank" rel="noopener">Install OpenVINO™ toolkit for Raspbian* OS</a><br> 下载完后工具包位于/home/pi/Downloads目录下，如果不是，可以创建一个Downloads目录并把工具包放在此目录下<br> <strong>切换至Downloads目录下:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd  ~/Downloads</span><br></pre></td></tr></table></figure><p><strong>配置路径与环境：</strong><br> 执行以下命令，会自动对setupvars.sh文件做修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s|&lt;INSTALLDIR&gt;|$(pwd)/l_openvino_toolkit_runtime_raspbian_p_2019.3.334|&quot; l_openvino_toolkit_runtime_raspbian_p_2019.3.334/bin/setupvars.sh</span><br></pre></td></tr></table></figure><p>配置虚拟环境：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">suorce l_openvino_toolkit_runtime_raspbian_p_2019.3.334/bin/setupvars.sh</span><br></pre></td></tr></table></figure><p><strong>添加USB规则：</strong><br> 将当前Linux用户添加到users组：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -a -G users &quot;$(whoami)&quot;</span><br></pre></td></tr></table></figure><p>注：1、”$(whoami)”是用户名，2、这里要说的是我们现在是root用户，如果打开新窗口的话起始用户是pi，所以出现[  setupvars.sh] OpenVINO environment  initialized，是对于pi用户来说的。如果在新窗口中用root执行程序，其实并没有成功加载[ setupvars.sh]  OpenVINO environment initialized，需要自己再执行一遍<br>  source/home/pi/Downloads/l_openvino_toolkit_runtime_raspbian_p_2019.3.334/bin/setupvars.sh，才能给root用户配置好OpenVINO environment initialized。<br> 接下来配置USB规则，执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh l_openvino_toolkit_runtime_raspbian_p_2019.3.334/install_dependencies/install_NCS_udev_rules.sh</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20191230164606485.png" alt></p><p><strong>demo测试验证安装是否成功</strong><br> 运行人脸检测的实例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd l_openvino_toolkit_runtime_raspbian_p_2019.3.334/deployment_tools/inference_engine/samples</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS=&quot;-march=armv7-a&quot;</span><br><span class="line">make -j2 object_detection_sample_ssd</span><br></pre></td></tr></table></figure><p>编译完成后，下载网络和权重文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget --no-check-certificate https://download.01.org/opencv/2019/open_model_zoo/R1/models_bin/face-detection-adas-0001/FP16/face-detection-adas-0001.bin</span><br><span class="line">wget --no-check-certificate https://download.01.org/opencv/2019/open_model_zoo/R1/models_bin/face-detection-adas-0001/FP16/face-detection-adas-0001.xml</span><br></pre></td></tr></table></figure><p>然后自己在网上找一张人脸的图片，执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./armv7l/Release/object_detection_sample_ssd -m face-detection-adas-0001.xml -d MYRIAD -i &lt;path_to_image&gt;图片的绝对路径</span><br></pre></td></tr></table></figure><p>如果运行成功，会在build文件夹下输出一种观念out_0.bmp图片，即表示计算棒运行成功。</p><p><strong>Opencv+python api 调用方法</strong><br> 新建一个文件夹，先建立一个face_detection.py文件，写入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="comment"># Load the model </span></span><br><span class="line">net = cv.dnn.readNet(<span class="string">'face-detection-adas-0001.xml'</span>, <span class="string">'face-detection-adas-0001.bin'</span>) </span><br><span class="line"><span class="comment"># Specify target device </span></span><br><span class="line">net.setPreferableTarget(cv.dnn.DNN_TARGET_MYRIAD)</span><br><span class="line"><span class="comment"># Read an image </span></span><br><span class="line">frame = cv.imread(<span class="string">'/path/to/image'</span>)</span><br><span class="line"><span class="comment"># Prepare input blob and perform an inference </span></span><br><span class="line">blob = cv.dnn.blobFromImage(frame, size=(<span class="number">672</span>, <span class="number">384</span>), ddepth=cv.CV_8U) net.setInput(blob) </span><br><span class="line">out = net.forward()</span><br><span class="line"><span class="comment"># Draw detected faces on the frame </span></span><br><span class="line"><span class="keyword">for</span> detection <span class="keyword">in</span> out.reshape(<span class="number">-1</span>, <span class="number">7</span>): </span><br><span class="line">    confidence = float(detection[<span class="number">2</span>]) </span><br><span class="line">    xmin = int(detection[<span class="number">3</span>] * frame.shape[<span class="number">1</span>]) </span><br><span class="line">    ymin = int(detection[<span class="number">4</span>] * frame.shape[<span class="number">0</span>]) </span><br><span class="line">    xmax = int(detection[<span class="number">5</span>] * frame.shape[<span class="number">1</span>]) </span><br><span class="line">    ymax = int(detection[<span class="number">6</span>] * frame.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> confidence &gt; <span class="number">0.5</span>:</span><br><span class="line">        cv.rectangle(frame, (xmin, ymin), (xmax, ymax), color=(<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>))</span><br><span class="line"><span class="comment"># Save the frame to an image file </span></span><br><span class="line">cv.imwrite(<span class="string">'out.png'</span>, frame)</span><br></pre></td></tr></table></figure><p>在文件夹中放入刚刚下载的那两个文件：face-detection-adas-0001.bin和face-detection-adas-0001.xml还有用于检测用的脸的图片，执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 face_detection.py</span><br></pre></td></tr></table></figure><h3 id="Workflow-for-Raspberry-Pi"><a href="#Workflow-for-Raspberry-Pi" class="headerlink" title="Workflow for Raspberry Pi"></a>Workflow for Raspberry Pi</h3><p>If you want to use your model for inference, the model must be  converted to the .bin and .xml Intermediate Representation (IR) files  that are used as input by Inference Engine. OpenVINO™ toolkit support on Raspberry Pi only includes the Inference Engine module of the Intel®  Distribution of OpenVINO™ toolkit. The Model Optimizer is not supported  on this platform. To get the optimized models you can use one of the  following options:</p><ul><li><p>Download a set of ready-to-use pre-trained models for the appropriate version of OpenVINO from the Intel® Open Source  Technology Center:</p><ul><li>Models for the 2020.1 release of OpenVINO are available at <a href="https://download.01.org/opencv/2020/openvinotoolkit/2020.1/open_model_zoo/" target="_blank" rel="noopener">https://download.01.org/opencv/2020/openvinotoolkit/2020.1/open_model_zoo/</a>.</li><li>Models for the 2019 R1 release of OpenVINO are available at <a href="https://download.01.org/opencv/2019/open_model_zoo/R1/" target="_blank" rel="noopener">https://download.01.org/opencv/2019/open_model_zoo/R1/</a>.</li><li>Models for the 2018 R5 release of OpenVINO are available at <a href="https://download.01.org/openvinotoolkit/2018_R5/open_model_zoo/" target="_blank" rel="noopener">https://download.01.org/openvinotoolkit/2018_R5/open_model_zoo/</a>.</li></ul><p>For more information on pre-trained models, see <a href="https://docs.openvinotoolkit.org/latest/omz_models_intel_index.html" target="_blank" rel="noopener">Pre-Trained Models Documentation</a></p></li><li><p>Convert the model using the Model Optimizer from a full installation of Intel® Distribution of OpenVINO™ toolkit on one of the supported platforms. Installation instructions are available:</p><ul><li><a href="https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_macos.html" target="_blank" rel="noopener">Installation Guide for macOS*</a></li><li><a href="https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_windows.html" target="_blank" rel="noopener">Installation Guide for Windows*</a></li><li><a href="https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_linux.html" target="_blank" rel="noopener">Installation Guide for Linux*</a></li></ul></li></ul><h2 id="Step2-在Windows上安装NCS环境"><a href="#Step2-在Windows上安装NCS环境" class="headerlink" title="Step2 在Windows上安装NCS环境"></a>Step2 在Windows上安装NCS环境</h2><p><strong>Operating System</strong></p><ul><li>Microsoft Windows* 10 64-bit</li></ul><p><strong>Software</strong></p><ul><li><p><a href="http://visualstudio.microsoft.com/downloads/" target="_blank" rel="noopener">Microsoft Visual Studio* with C++ <strong>2019 or 2017</strong> with MSBuild</a></p></li><li><p>CMake <strong>3.10 or higher</strong> 64-bit</p><blockquote><p><strong>NOTE</strong>: If you want to use Microsoft Visual Studio 2019, you are required to install CMake 3.14. </p></blockquote></li><li><p><a href="https://www.python.org/downloads/windows/" target="_blank" rel="noopener">Python <strong>3.6</strong> - <strong>3.8</strong> 64-bit</a></p></li></ul><h4 id="1、安装英特尔®分布式OpenVINO™工具包核心组件"><a href="#1、安装英特尔®分布式OpenVINO™工具包核心组件" class="headerlink" title="1、安装英特尔®分布式OpenVINO™工具包核心组件"></a>1、安装英特尔®分布式OpenVINO™工具包核心组件</h4><p>可以进入<a href="https://software.intel.com/zh-cn/openvino-toolkit" target="_blank" rel="noopener">OpenVINO官网</a>参考官方安装步骤，也可以参考我的安装步骤。</p><p><strong>英特尔®分布式OpenVINO™工具包核心组件安装步骤：</strong></p><ol><li>如果您尚未下载英特尔®分布式OpenVINO™工具包，请下载最新版本。默认情况下，该文件将保存到Downloads目录w_openvino_toolkit_p_2019.3.379.exe。</li><li>转到该Downloads文件夹。</li><li>双击w_openvino_toolkit_p_2019.3.379.exe。将打开一个窗口，您可以选择安装目录和组件。默认安装目录是C:\Program Files (x86)\IntelSWTools\openvino_2019.3.379 .为了简便，还会创建 C:\Program  Files (x86)\IntelSWTools\openvino安装目录快捷方式。如果选择其他安装目录，安装程序将为您创建目录：<br> <img src="https://img-blog.csdnimg.cn/20191230211524163.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li></ol><p>4.单击下一步。</p><p>5.系统会询问您是否同意收集信息。选择您选择的选项。单击下一步。</p><p>6.如果您缺少外部依赖项，则会看到警告屏幕。记下您缺少的依赖项。<strong>此时您不需要采取任何行动。</strong>安装英特尔®分布式OpenVINO™工具包核心组件后，将向您提供安装缺少的依赖项说明。下面的屏幕表示您缺少两个依赖项：</p><p>7.单击下一步。</p><p>8.安装的第一部分完成后，最终屏幕会通知您已安装核心组件并仍需执行自他步骤：</p><h4 id="2、设置环境变量"><a href="#2、设置环境变量" class="headerlink" title="2、设置环境变量"></a>2、设置环境变量</h4><p>在编译和运行OpenVINO™应用程序之前，必须更新多个环境变量。打开命令提示符并运行以下批处理文件以临时设置环境变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files(x86)\IntelSWTools\openvino\bin\setupvars.bat</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p><strong>（可选）</strong>：关闭“命令提示符”窗口时，将删除OpenVINO工具箱环境变量。作为选项，您可以手动永久设置环境变量。</p><h4 id="下列步骤进一步确认使用OpenVINO"><a href="#下列步骤进一步确认使用OpenVINO" class="headerlink" title="下列步骤进一步确认使用OpenVINO"></a>下列步骤进一步确认使用OpenVINO</h4><h5 id="1、配置模型优化程序"><a href="#1、配置模型优化程序" class="headerlink" title="1、配置模型优化程序"></a>1、配置模型优化程序</h5><p><strong>重要信息</strong>：这些步骤是必需的。您必须为至少一个框架配置Model Optimizer。如果您未完成本节中的步骤，模型优化程序将失败。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer</span><br></pre></td></tr></table></figure><h5 id="2、模型优化器配置步骤"><a href="#2、模型优化器配置步骤" class="headerlink" title="2、模型优化器配置步骤"></a>2、模型优化器配置步骤</h5><p>您可以一次为所有受支持的框架配置模型优化程序，也可以一次为一个框架配置模型优化程序。选择最适合您需求的选项。如果看到错误消息，请确保已安装所有依赖项。<br> <strong>选项1：同时为所有支持的框架配置Model Optimizer</strong><br> 打开命令提示符，转到Model Optimizer条件目录，执行下列命令以配置Caffe <em>，TensorFlow </em>，MXNet <em>，Kaldi </em>和ONNX *的模型优化器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install_prerequisites.bat</span><br></pre></td></tr></table></figure><p>也可以安装你需要的。比如本次demo用的是caffe.所以装个caffe的。</p><p>把后面那个文件换成install_prerequistites_caffe.bat即可。</p><h5 id="3、测试"><a href="#3、测试" class="headerlink" title="3、测试"></a>3、测试</h5><p>先瞎跑个测试脚本，证明我们把一套openvino装成功了。。。。</p><p>在下面这个地址找到demo_squeezenet_download_convert_run.bat，跑起来。。。</p><p><img src="https://pic3.zhimg.com/80/v2-a28ce9b7f0b3ef6718a5373db803cf62_720w.png" alt></p><p><img src="https://img-blog.csdnimg.cn/20200108191904455.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbmtpblRhbw==,size_16,color_FFFFFF,t_70" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近在看各种AI的加速方案以及边缘计算，jetsonnano、英伟达TX2、华为昇腾啊什么的&lt;/p&gt;
&lt;p&gt;jetson nano 感觉性价比不如rk3399，还有Intel的计算棒。做终端应用成本太高，但是。。。。。3399得1000+了啊&lt;/p&gt;
&lt;p&gt;下面就是关于Intel神经棒二代的上手教程。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Intel-Neural-Compute-stick" scheme="http://yuanquanquan.top/tags/Intel-Neural-Compute-stick/"/>
    
  </entry>
  
  <entry>
    <title>APDS-9960 RGB and Gesture Sensor</title>
    <link href="http://yuanquanquan.top/2020/20200720/"/>
    <id>http://yuanquanquan.top/2020/20200720/</id>
    <published>2020-07-20T00:19:47.000Z</published>
    <updated>2020-07-21T06:48:43.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>通过APDS-9960手势识别芯片在Arduino上实现手势识别。</p></blockquote><a id="more"></a><p> <img src="https://i.loli.net/2020/07/20/V7QfZtiyl1X3kOj.png" alt></p><h1 id="1-HARDWARE"><a href="#1-HARDWARE" class="headerlink" title="1. HARDWARE"></a>1. HARDWARE</h1><p>· Osoyoo UNO Board (Fully compatible with Arduino UNO rev.3) x 1</p><p>· APDS-9960 RGB and Gesture Sensor x 1</p><p>· Jumpers</p><p>· USB Cable x 1</p><p>· PC x 1</p><h1 id="2-SOFTWARE"><a href="#2-SOFTWARE" class="headerlink" title="2. SOFTWARE"></a>2. SOFTWARE</h1><p>· Arduino IDE (version 1.6.4+)</p><p>· Arduino library: <a href="https://codeload.github.com/adafruit/Adafruit_APDS9960/zip/master" target="_blank" rel="noopener">APDS9960.h</a></p><h1 id="3-About-APDS-9960-RGB-and-Gesture-Sensor"><a href="#3-About-APDS-9960-RGB-and-Gesture-Sensor" class="headerlink" title="3. About APDS-9960 RGB and Gesture Sensor"></a>3. About APDS-9960 RGB and Gesture Sensor</h1><p><img src="https://i.loli.net/2020/07/20/6fex8zmILMlo5Ua.png" alt> </p><p>This is the  RGB and Gesture Sensor, a small breakout board with a built in APDS-9960 sensor that offers ambient light and color measuring, proximity detection, and touchless gesture sensing. With this RGB and Gesture Sensor you will be able to control a computer, microcontroller, robot, and more with a simple swipe of your hand! This is, in fact, the same sensor that the Samsung Galaxy S5 uses and is probably one of the best gesture sensors on the market for the price.</p><p> <img src="https://i.loli.net/2020/07/20/Qt2qSDv4sdORk7o.png" alt></p><p>The APDS-9960 is a serious little piece of hardware with built in UV and IR blocking filters, four separate diodes sensitive to different directions, and an I2C compatible interface. For your convenience we have broken out the following pins: VL (optional power to IR LED), GND (Ground), VCC (power to APDS-9960 sensor), SDA (I2C data), SCL (I2C clock), and INT (interrupt). Each APDS-9960 also has a detection range of 4 to 8 inches (10 to 20 cm).</p><p><img src="https://i.loli.net/2020/07/20/Rtq621F7UxkYDlH.png" alt> </p><h2 id="3-1-PIN-DESCRIPTIONS"><a href="#3-1-PIN-DESCRIPTIONS" class="headerlink" title="3.1 PIN DESCRIPTIONS"></a>3.1 PIN DESCRIPTIONS</h2><table><thead><tr><th><strong><em>\</em>Pin Label**</strong></th><th><strong><em>\</em>Description**</strong></th></tr></thead><tbody><tr><td>VL</td><td>Optional power to the IR LED if PS jumper is disconnected. Must be 3.0 - 4.5V</td></tr><tr><td>GND</td><td>Connect to ground.</td></tr><tr><td>VCC</td><td>Used to power the APDS-9960 sensor. Must be 2.4 - 3.6V</td></tr><tr><td>SDA</td><td>I2C data</td></tr><tr><td>SCL</td><td>I2C clock</td></tr><tr><td>INT</td><td>External interrupt pin. Active LOW on interrupt event</td></tr></tbody></table><h2 id="3-2-FEATURES"><a href="#3-2-FEATURES" class="headerlink" title="3.2 FEATURES:"></a>3.2 FEATURES:</h2><p>· Model: GY-APDS 9960-3.3</p><p>· Using chip: APDS-9960</p><p>· Operational Voltage: 3.3V</p><p>· Ambient Light &amp; RGB Color Sensing</p><p>· Proximity Sensing</p><p>· Gesture Detection</p><p>· Operating Range: 4-8in (10-20cm)</p><p>· I2C Interface (I2C Address: 0x39)</p><p>· Size: 20mm * 15.3mm</p><p><strong>Recommended Reading</strong></p><p>Before getting started with the APDS-9960, there are a few concepts that you should be familiar with. Consider <a href="http://osoyoo.com/2017/09/21/osoyoo-advanced-kit-for-arduino/" target="_blank" rel="noopener">reading some of these tutorials before continuing.</a></p><h2 id="3-3-SETTING-THE-JUMPERS"><a href="#3-3-SETTING-THE-JUMPERS" class="headerlink" title="3.3 SETTING THE JUMPERS"></a>3.3 SETTING THE JUMPERS</h2><p><img src="https://i.loli.net/2020/07/20/Qlw6Am5xdPqIeiW.jpg" alt> </p><p>On the front of the breakout board are 2 solder jumpers:</p><p><strong>·</strong> <strong><em>\</em>PS**</strong> – This jumper connects the power supplies of the sensor and IR LED (also located on the APDS-9960) together. When the jumper is closed (i.e. connected), you only need to supply power to the VCC pin to power both the sensor and the IR LED. If the jumper is open, you need to provide power to both the VCC (2.4 - 3.6V) and VL (3.0 - 4.5V) pins separately. This jumper is <strong><em>\</em>closed**</strong> by default.</p><p><strong>·</strong> <strong><em>\</em>I2C PU**</strong> – This is a 3-way solder jumper that is used to connect and disconnect the I2C pullup resistors. By default, this jumper is <strong><em>\</em>closed**</strong>, which means that both SDA and SCL lines have connected pullup resistors on the breakout board. Use some solder wick to open the jumper if you do not need the pullup resistors (e.g. you have pullup resistors that are located on the I2C bus somewhere else).</p><h2 id="3-4-HARDWARE-HOOKUP"><a href="#3-4-HARDWARE-HOOKUP" class="headerlink" title="3.4 HARDWARE HOOKUP"></a>3.4 HARDWARE HOOKUP</h2><h3 id="3-4-1-Add-Headers"><a href="#3-4-1-Add-Headers" class="headerlink" title="3.4.1 Add Headers"></a>3.4.1 Add Headers</h3><p>Solder a row of break away male headers to the 6 headers holes on the board.</p><p><img src="https://i.loli.net/2020/07/20/eVvx14DpdJnFG8M.jpg" alt> </p><h3 id="3-4-2-Connect-the-Breakout-Board"><a href="#3-4-2-Connect-the-Breakout-Board" class="headerlink" title="3.4.2 Connect the Breakout Board"></a>3.4.2 Connect the Breakout Board</h3><p>We will be using the Arduino Pro’s regulated 3.3V power and I2C bus with the APDS-9960. Note that we are leaving VL on the breakout board unconnected. IMPORTANT: You must use 3.3V! If you try to use a 5V power supply  you risk damaging the APDS-9960. Connect the breakout board to the following pins on the Arduino:</p><table><thead><tr><th><strong><em>\</em>APDS-9960 Breakout Board**</strong></th><th><strong><em>\</em>OSOYOO UNO**</strong></th></tr></thead><tbody><tr><td>GND</td><td>GND</td></tr><tr><td>VCC</td><td>3.3V</td></tr><tr><td>SDA</td><td>A4</td></tr><tr><td>SCL</td><td>A5</td></tr></tbody></table><p><strong><em>\</em>NOTE:**</strong></p><p>· Connect the <strong><em>\</em>SCL**</strong> pin to the I2C clock <strong><em>\</em>SCL**</strong> pin on your Arduino. On an UNO &amp; ‘328 based Arduino, this is also known as <strong><em>\</em>A5**</strong>, on a Mega it is also known as <strong><em>\</em>digital 21**</strong> and on a Leonardo/Micro, <strong><em>\</em>digital 3**</strong></p><p>· Connect the <strong><em>\</em>SDA**</strong> pin to the I2C data <strong><em>\</em>SDA**</strong> pin on your Arduino. On an UNO &amp; ‘328 based Arduino, this is also known as <strong><em>\</em>A4**</strong>, on a Mega it is also known as <strong><em>\</em>digital 20**</strong> and on a Leonardo/Micro, <strong><em>\</em>digital 2**</strong></p><p><img src="https://i.loli.net/2020/07/20/Gutm3UWAv95jQgN.png" alt></p><h2 id="3-5-ARDUINO-LIBRARY-INSTALLATION"><a href="#3-5-ARDUINO-LIBRARY-INSTALLATION" class="headerlink" title="3.5 ARDUINO LIBRARY INSTALLATION"></a>3.5 ARDUINO LIBRARY INSTALLATION</h2><p>To use the APDS-9960, you will need some supporting software. If you are using an Arduino, then you are in luck! We created an Arduino library that makes the APDS-9960 easy to use. Click the button below to download the latest version of the APDS-9960 breakout board project, which includes the Arduino library. <a href="https://codeload.github.com/adafruit/Adafruit_APDS9960/zip/master" target="_blank" rel="noopener">DOWNLOAD THE PROJECT FILES!</a> Follow <a href="http://osoyoo.com/2017/05/08/how-to-install-additional-arduino-libraries/" target="_blank" rel="noopener">this guide on installing Arduino libraries</a> to install the files within the APDS9960 directory as an Arduino library. </p><h1 id="Gesture-Sensing-Example"><a href="#Gesture-Sensing-Example" class="headerlink" title="\Gesture Sensing Example**"></a><strong><em>\</em>Gesture Sensing Example**</strong></h1><h1 id="4-UPLOAD-SKETCH"><a href="#4-UPLOAD-SKETCH" class="headerlink" title="4. UPLOAD SKETCH"></a>4. UPLOAD SKETCH</h1><p>After above operations are completed, connect the Arduino board to your computer using the USB cable. The green power LED (labelled <strong><em>\</em>PWR**</strong>) should go on. </p><h1 id="5-CODE-PROGRAM"><a href="#5-CODE-PROGRAM" class="headerlink" title="5.CODE PROGRAM"></a>5.CODE PROGRAM</h1><p>You can copy below code to your Arduino IDE window, then select corresponding board type and port type for your Arduino board. </p><p><img src="https://i.loli.net/2020/07/20/FEjCHOW8l5XB4Nb.png" alt></p><h1 id="6-Running-Result"><a href="#6-Running-Result" class="headerlink" title="6.Running Result"></a>6.Running Result</h1><p>Click the Upload button and wait for the program to finish uploading to the Arduino. Once uploaded to your Adruino, open up the serial monitor at 115200 baud speed.More info on the Serial Terminal can be found <a href="http://osoyoo.com/2017/07/06/arduino-lesson-the-serial-monitor/" target="_blank" rel="noopener">here</a>. You should see a messages noting that “Device initialized! ” Hover your hand 4 to 8 inches (10 to 20 cm) above the sensor but off to one side (i.e. not directly above the sensor). While maintaining the same height, swipe your hand over the sensor (into and then immediately out of range of the sensor). If you move too fast, the sensor will not recognize the gesture.</p><p><img src="https://i.loli.net/2020/07/20/hHpdYazO9xjXJ7w.jpg" alt> </p><p>#</p><h1 id="附：Adafruit-Community-Code-of-Conduct"><a href="#附：Adafruit-Community-Code-of-Conduct" class="headerlink" title="附：Adafruit Community Code of Conduct"></a>附：Adafruit Community Code of Conduct</h1><h2 id="Our-Pledge"><a href="#Our-Pledge" class="headerlink" title="Our Pledge"></a>Our Pledge</h2><p>In the interest of fostering an open and welcoming environment, we as<br>contributors and leaders pledge to making participation in our project and<br>our community a harassment-free experience for everyone, regardless of age, body<br>size, disability, ethnicity, gender identity and expression, level or type of<br>experience, education, socio-economic status, nationality, personal appearance,<br>race, religion, or sexual identity and orientation.</p><h2 id="Our-Standards"><a href="#Our-Standards" class="headerlink" title="Our Standards"></a>Our Standards</h2><p>We are committed to providing a friendly, safe and welcoming environment for<br>all.</p><p>Examples of behavior that contributes to creating a positive environment<br>include:</p><ul><li>Be kind and courteous to others</li><li>Using welcoming and inclusive language</li><li>Being respectful of differing viewpoints and experiences</li><li>Collaborating with other community members</li><li>Gracefully accepting constructive criticism</li><li>Focusing on what is best for the community</li><li>Showing empathy towards other community members</li></ul><p>Examples of unacceptable behavior by participants include:</p><ul><li>The use of sexualized language or imagery and sexual attention or advances</li><li>The use of inappropriate images, including in a community member’s avatar</li><li>The use of inappropriate language, including in a community member’s nickname</li><li>Any spamming, flaming, baiting or other attention-stealing behavior</li><li>Excessive or unwelcome helping; answering outside the scope of the question<br>asked</li><li>Trolling, insulting/derogatory comments, and personal or political attacks</li><li>Public or private harassment</li><li>Publishing others’ private information, such as a physical or electronic<br>address, without explicit permission</li><li>Other conduct which could reasonably be considered inappropriate</li></ul><p>The goal of the standards and moderation guidelines outlined here is to build<br>and maintain a respectful community. We ask that you don’t just aim to be<br>“technically unimpeachable”, but rather try to be your best self. </p><p>We value many things beyond technical expertise, including collaboration and<br>supporting others within our community. Providing a positive experience for<br>other community members can have a much more significant impact than simply<br>providing the correct answer.</p><h2 id="Our-Responsibilities"><a href="#Our-Responsibilities" class="headerlink" title="Our Responsibilities"></a>Our Responsibilities</h2><p>Project leaders are responsible for clarifying the standards of acceptable<br>behavior and are expected to take appropriate and fair corrective action in<br>response to any instances of unacceptable behavior.</p><p>Project leaders have the right and responsibility to remove, edit, or<br>reject messages, comments, commits, code, issues, and other contributions<br>that are not aligned to this Code of Conduct, or to ban temporarily or<br>permanently any community member for other behaviors that they deem<br>inappropriate, threatening, offensive, or harmful.</p><h2 id="Moderation"><a href="#Moderation" class="headerlink" title="Moderation"></a>Moderation</h2><p>Instances of behaviors that violate the Adafruit Community Code of Conduct<br>may be reported by any member of the community. Community members are<br>encouraged to report these situations, including situations they witness<br>involving other community members.</p><p>You may report in the following ways:</p><p>In any situation, you may send an email to <a href="mailto:&#x73;&#117;&#x70;&#112;&#111;&#114;&#x74;&#64;&#97;&#100;&#97;&#102;&#x72;&#x75;&#x69;&#116;&#x2e;&#99;&#111;&#x6d;" target="_blank" rel="noopener">&#x73;&#117;&#x70;&#112;&#111;&#114;&#x74;&#64;&#97;&#100;&#97;&#102;&#x72;&#x75;&#x69;&#116;&#x2e;&#99;&#111;&#x6d;</a>.</p><p>On the Adafruit Discord, you may send an open message from any channel<br>to all Community Helpers by tagging @community helpers. You may also send an<br>open message from any channel, or a direct message to @kattni#1507,<br>@tannewt#4653, @Dan Halbert#1614, @cater#2442, @sommersoft#0222, or<br>@Andon#8175.</p><p>Email and direct message reports will be kept confidential.</p><p>In situations on Discord where the issue is particularly egregious, possibly<br>illegal, requires immediate action, or violates the Discord terms of service,<br>you should also report the message directly to Discord.</p><p>These are the steps for upholding our community’s standards of conduct.</p><ol><li>Any member of the community may report any situation that violates the<br>Adafruit Community Code of Conduct. All reports will be reviewed and<br>investigated.</li><li>If the behavior is an egregious violation, the community member who<br>committed the violation may be banned immediately, without warning.</li><li>Otherwise, moderators will first respond to such behavior with a warning.</li><li>Moderators follow a soft “three strikes” policy - the community member may<br>be given another chance, if they are receptive to the warning and change their<br>behavior.</li><li>If the community member is unreceptive or unreasonable when warned by a<br>moderator, or the warning goes unheeded, they may be banned for a first or<br>second offense. Repeated offenses will result in the community member being<br>banned.</li></ol><h2 id="Scope"><a href="#Scope" class="headerlink" title="Scope"></a>Scope</h2><p>This Code of Conduct and the enforcement policies listed above apply to all<br>Adafruit Community venues. This includes but is not limited to any community<br>spaces (both public and private), the entire Adafruit Discord server, and<br>Adafruit GitHub repositories. Examples of Adafruit Community spaces include<br>but are not limited to meet-ups, audio chats on the Adafruit Discord, or<br>interaction at a conference.</p><p>This Code of Conduct applies both within project spaces and in public spaces<br>when an individual is representing the project or its community. As a community<br>member, you are representing our community, and are expected to behave<br>accordingly.</p><h2 id="Attribution"><a href="#Attribution" class="headerlink" title="Attribution"></a>Attribution</h2><p>This Code of Conduct is adapted from the [Contributor Covenant][homepage],<br>version 1.4, available at<br><a href="https://www.contributor-covenant.org/version/1/4/code-of-conduct.html" target="_blank" rel="noopener">https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</a>,<br>and the <a href="https://www.rust-lang.org/en-US/conduct.html" target="_blank" rel="noopener">Rust Code of Conduct</a>.</p><p>For other projects adopting the Adafruit Community Code of<br>Conduct, please contact the maintainers of those projects for enforcement.<br>If you wish to use this code of conduct for your own project, consider<br>explicitly mentioning your moderation policy or making a copy with your<br>own moderation policy so as to avoid confusion.</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;通过APDS-9960手势识别芯片在Arduino上实现手势识别。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="-手势识别" scheme="http://yuanquanquan.top/tags/%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Arcore-Android</title>
    <link href="http://yuanquanquan.top/2020/20200706/"/>
    <id>http://yuanquanquan.top/2020/20200706/</id>
    <published>2020-07-06T07:56:07.000Z</published>
    <updated>2020-07-06T08:50:03.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ARCore 是 Google 为开发者构建的增强现实平台，如何让虚拟物体和真实世界完美融合，这一直是 Google ARCore 技术所探讨的问题。众所周知，当虚拟物体附近有现实物体时，有可能会出现互相交融、重叠等效果，大大地影响了用户体验。这一直是 AR 技术的难点，也是 Google 不懈努力的方向。</p></blockquote><a id="more"></a>  <h1 id="1-Quickstart-for-Android"><a href="#1-Quickstart-for-Android" class="headerlink" title="1. Quickstart for Android"></a>1. Quickstart for Android</h1><p><img src="https://developers.google.cn/ar/develop/java/images/android-studio.png" alt></p><h2 id="1-2-Set-up-your-development-environment"><a href="#1-2-Set-up-your-development-environment" class="headerlink" title="1.2 Set up your development environment"></a>1.2 Set up your development environment</h2><ul><li>Install <a href="https://developer.android.google.cn/studio/index.html" target="_blank" rel="noopener">Android Studio</a> version 3.1 or higher with Android SDK Platform version 7.0 (API level 24) or higher.</li><li>You will need a basic understanding of Android development. If you are new to Android, see <a href="https://developer.android.google.cn/training/basics/firstapp/index.html" target="_blank" rel="noopener">Building your first Android app for beginners</a>.</li></ul><h2 id="1-3-Open-the-sample-project"><a href="#1-3-Open-the-sample-project" class="headerlink" title="1.3 Open the sample project"></a>1.3 Open the sample project</h2><p>This quickstart uses <a href="https://en.wikipedia.org/wiki/OpenGL" target="_blank" rel="noopener">OpenGL</a>, a programming interface for rendering 2D and 3D vector graphics. Review the <a href="https://developers.google.cn/ar/develop/java/enable-arcore" target="_blank" rel="noopener">Enable ARCore</a> documentation before getting started with the steps below.</p><p>Get the sample project by cloning the repository with the following command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/google-ar/arcore-android-sdk.git</span><br></pre></td></tr></table></figure><p>In Android Studio, open the <strong>HelloAR</strong> sample project, located in the <strong>samples</strong> subdirectory within the <code>arcore-android-sdk</code> directory.</p><h2 id="1-4-Prepare-your-device-or-emulator"><a href="#1-4-Prepare-your-device-or-emulator" class="headerlink" title="1.4 Prepare your device or emulator"></a>1.4 Prepare your device or emulator</h2><p>You can run AR apps on a <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">supported device</a> or in the Android Emulator:</p><ul><li>In the emulator, you must sign into the Google Play Store or <a href="https://developers.google.cn/ar/develop/java/emulator#update-arcore" target="_blank" rel="noopener">update Google Play Services for AR</a> manually.</li></ul><h2 id="1-5-Run-the-sample"><a href="#1-5-Run-the-sample" class="headerlink" title="1.5 Run the sample"></a>1.5 Run the sample</h2><p>Make sure your Android device is connected to the development machine and click <strong>Run</strong> <img src="https://developers.google.cn/ar/develop/java/images/toolbar-run.png" alt="img"> in Android Studio. Then, choose your device as the deployment target and click <strong>OK</strong>.</p><p><img src="https://developers.google.cn/ar/develop/java/images/deployment-target.png" alt></p><p>Android Studio builds your project into a debuggable APK, installs the APK, and then runs the app on your device. For more information, see <a href="https://developer.android.google.cn/studio/run/index.html" target="_blank" rel="noopener">Build and Run Your App</a>.</p><p>You may be prompted to install or update <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> if it is missing or out of date. Select <strong>CONTINUE</strong> to install it from Google Play Store.</p><p>The <strong>HelloAR</strong> app lets you place and manipulate Android figurines on detected AR plane surfaces. It is implemented with <a href="https://developer.android.google.cn/reference/android/opengl/GLSurfaceView" target="_blank" rel="noopener">Android GL SurfaceView</a>, which is used to render the camera preview and basic AR objects such as Planes and Anchors. <strong>HelloAR</strong>‘s render can be found <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples/hello_ar_java/app/src/main/java/com/google/ar/core/examples/java/common/rendering" target="_blank" rel="noopener">here</a>.</p><p><img src="https://developers.google.cn/ar/develop/java/images/helloar-demo.jpg" alt></p><p><strong>Note:</strong> The lifecycle methods in <strong>HelloAR</strong> are different than those normally found in OpenGL applications. To ensure the correct AR setup for your own applications, follow the lifecycle management logic in <strong>HelloAR</strong>.</p><h2 id="1-6-Next-steps"><a href="#1-6-Next-steps" class="headerlink" title="1.6 Next steps"></a>1.6 Next steps</h2><ul><li>Try building and running other <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples" target="_blank" rel="noopener">sample projects</a> in the ARCore SDK.</li><li>Learn how to <a href="https://developers.google.cn/ar/develop/java/enable-arcore" target="_blank" rel="noopener">Enable ARCore</a> in your app.</li><li>Use <a href="https://developers.google.cn/ar/develop/java/augmented-images" target="_blank" rel="noopener">Augmented Images</a> to build apps that can respond to 2D images, such as posters or logos, in the user’s environment.</li><li>Use <a href="https://developers.google.cn/ar/develop/java/cloud-anchors/cloud-anchors-overview-android" target="_blank" rel="noopener">Cloud Anchors</a> to create shared AR experiences across iOS and Android users.</li><li>Review the <a href="https://developers.google.cn/ar/develop/developer-guides/runtime-considerations" target="_blank" rel="noopener">Runtime Considerations</a>.</li><li>Review the <a href="https://developers.google.cn/ar/develop/developer-guides/design-guidelines" target="_blank" rel="noopener">Design Guidelines</a>.</li></ul><h1 id="2-Enable-ARCore"><a href="#2-Enable-ARCore" class="headerlink" title="2. Enable ARCore"></a>2. Enable ARCore</h1><p>This page describes how to enable ARCore functionality in your Android Studio projects. To do this, you need to:</p><ol><li><a href="https://developers.google.cn/ar/develop/java/enable-arcore#manifest" target="_blank" rel="noopener">Add AR Required or AR Optional entries to the manifest</a></li><li><a href="https://developers.google.cn/ar/develop/java/enable-arcore#dependencies" target="_blank" rel="noopener">Add build dependencies</a> to your project</li><li><a href="https://developers.google.cn/ar/develop/java/enable-arcore#runtime" target="_blank" rel="noopener">Perform runtime checks</a> to ensure the device is <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore-supported</a>, that Google Play Services for AR is installed on it, and that camera permission has been granted</li><li>Make sure your app complies with ARCore’s <a href="https://developers.google.cn/ar/distribute/privacy-requirements" target="_blank" rel="noopener">User Privacy Requirements</a></li></ol><h2 id="2-1-Using-Google-Play-Services-for-AR-to-enable-ARCore-functionality"><a href="#2-1-Using-Google-Play-Services-for-AR-to-enable-ARCore-functionality" class="headerlink" title="2.1 Using Google Play Services for AR to enable ARCore functionality"></a>2.1 Using Google Play Services for AR to enable ARCore functionality</h2><p>ARCore SDKs make AR features available on <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore supported devices</a> that have <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> installed. Users can install and update Google Play Services for AR from the Google Play Store.</p><h2 id="2-2-Add-AR-Required-or-AR-Optional-entries-to-the-manifest"><a href="#2-2-Add-AR-Required-or-AR-Optional-entries-to-the-manifest" class="headerlink" title="2.2 Add AR Required or AR Optional entries to the manifest"></a>2.2 Add AR Required or AR Optional entries to the manifest</h2><p>An app that supports AR features can be configured in two ways: <strong>AR Required</strong> and <strong>AR Optional</strong>.</p><h3 id="2-2-1-AR-Required-apps"><a href="#2-2-1-AR-Required-apps" class="headerlink" title="2.2.1 AR Required apps"></a>2.2.1 AR Required apps</h3><p>To be usable, an <em>AR Required</em> app requires an <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore Supported Device</a> that has <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> installed on it.</p><ul><li>The Google Play Store makes AR Required apps available only on <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">devices that support ARCore</a>.</li><li>When users install an AR Required app, the Google Play Store automatically installs <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a>. However, your app must still perform additional <a href="https://developers.google.cn/ar/develop/java/enable-arcore#runtime" target="_blank" rel="noopener">runtime checks</a> in case Google Play Services for AR must be updated or has been manually uninstalled.</li></ul><p>For more information, see <a href="https://developers.google.cn/ar/distribute" target="_blank" rel="noopener">Publishing AR Apps in the Google Play Store</a>.</p><p>To declare your app to be <em>AR Required</em>, modify your <code>AndroidManifest.xml</code> to include the following entries:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;uses-permission android:name=&quot;android.permission.CAMERA&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Limits app visibility in the Google Play Store to ARCore supported devices</span><br><span class="line">     (https://developers.google.com/ar/discover/supported-devices). --&gt;</span><br><span class="line">&lt;uses-feature android:name=&quot;android.hardware.camera.ar&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;application …&gt;</span><br><span class="line">    …</span><br><span class="line"></span><br><span class="line">  &lt;!-- &quot;AR Required&quot; app, requires &quot;Google Play Services for AR&quot; (ARCore)</span><br><span class="line">       to be installed, as the app does not include any non-AR features. --&gt;</span><br><span class="line">    &lt;meta-data android:name=&quot;com.google.ar.core&quot; android:value=&quot;required&quot; /&gt;</span><br><span class="line">&lt;/application&gt;</span><br></pre></td></tr></table></figure><p>Then, modify your app’s <code>build.gradle</code> to specify a <code>minSdkVersion</code> of at least 24:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">android &#123;  defaultConfig &#123;    …    minSdkVersion 24  &#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-2-AR-Optional-apps"><a href="#2-2-2-AR-Optional-apps" class="headerlink" title="2.2.2 AR Optional apps"></a>2.2.2 AR Optional apps</h3><p>An <em>AR Optional</em> app has optional AR features, which are activated only on <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore supported devices</a> that have <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> installed.</p><ul><li>AR Optional apps can be installed and run on devices that don’t support ARCore.</li><li>When users install an AR Optional app, the Google Play Store will <em>not</em> automatically install <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> with the app.</li></ul><p>To declare your app to be <em>AR Optional</em>, modify your <code>AndroidManifest.xml</code> to include the following entries:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;uses-permission android:name=&quot;android.permission.CAMERA&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;application …&gt;</span><br><span class="line">    …</span><br><span class="line"></span><br><span class="line">    &lt;!-- &quot;AR Optional&quot; app, contains non-AR features that can be used when</span><br><span class="line">         &quot;Google Play Services for AR&quot; (ARCore) is not available. --&gt;</span><br><span class="line">    &lt;meta-data android:name=&quot;com.google.ar.core&quot; android:value=&quot;optional&quot; /&gt;</span><br><span class="line">&lt;/application&gt;</span><br></pre></td></tr></table></figure><p>Then, modify your app’s <code>build.gradle</code> to specify a <code>minSdkVersion</code> of at least 14:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">android &#123;</span><br><span class="line">    defaultConfig &#123;</span><br><span class="line">        …</span><br><span class="line">        minSdkVersion 14</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-3-Add-build-dependencies"><a href="#2-3-Add-build-dependencies" class="headerlink" title="2.3 Add build dependencies"></a>2.3 Add build dependencies</h2><p>To add ARCore to your Android Studio project, perform these steps:</p><ul><li><p>Make sure your <strong>project’s</strong> <code>build.gradle</code> file includes Google’s Maven repository:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">allprojects &#123;</span><br><span class="line">    repositories &#123;</span><br><span class="line">        google()</span><br><span class="line">        …</span><br></pre></td></tr></table></figure></li><li><p>Add the latest ARCore library as a dependency in your <strong>app’s</strong> <code>build.gradle</code> file:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dependencies &#123;</span><br><span class="line">    …</span><br><span class="line">    implementation &apos;com.google.ar:core:1.18.0&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-4-Perform-runtime-checks"><a href="#2-4-Perform-runtime-checks" class="headerlink" title="2.4 Perform runtime checks"></a>2.4 Perform runtime checks</h2><h3 id="2-4-1-Check-whether-ARCore-is-supported-AR-Optional-apps-only"><a href="#2-4-1-Check-whether-ARCore-is-supported-AR-Optional-apps-only" class="headerlink" title="2.4.1 Check whether ARCore is supported (AR Optional apps only)"></a>2.4.1 Check whether ARCore is supported (<em>AR Optional</em> apps only)</h3><p><em>AR Optional</em> apps can use <code>ArCoreApk.checkAvailability()</code> to determine if the current device supports ARCore. On devices that do not support ARCore, AR Optional apps should disable AR related functionality and hide associated UI elements:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">protected void onCreate(Bundle savedInstanceState) &#123;</span><br><span class="line">  super.onCreate(savedInstanceState);</span><br><span class="line"></span><br><span class="line">  // Enable AR related functionality on ARCore supported devices only.</span><br><span class="line">  maybeEnableArButton();</span><br><span class="line">  …</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void maybeEnableArButton() &#123;</span><br><span class="line">  ArCoreApk.Availability availability = ArCoreApk.getInstance().checkAvailability(this);</span><br><span class="line">  if (availability.isTransient()) &#123;</span><br><span class="line">    // Re-query at 5Hz while compatibility is checked in the background.</span><br><span class="line">    new Handler().postDelayed(new Runnable() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      public void run() &#123;</span><br><span class="line">        maybeEnableArButton();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, 200);</span><br><span class="line">  &#125;</span><br><span class="line">  if (availability.isSupported()) &#123;</span><br><span class="line">    mArButton.setVisibility(View.VISIBLE);</span><br><span class="line">    mArButton.setEnabled(true);</span><br><span class="line">    // indicator on the button.</span><br><span class="line">  &#125; else &#123; // Unsupported or unknown.</span><br><span class="line">    mArButton.setVisibility(View.INVISIBLE);</span><br><span class="line">    mArButton.setEnabled(false);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Note, <code>checkAvailability()</code> may need to query network resources to determine whether the device supports ARCore. During this time, it will return <code>UNKNOWN_CHECKING</code>. To reduce the perceived latency and pop-in, apps should call <code>checkAvailability()</code> once early in it’s life cycle to initiate the query, ignoring the returned value. This way a cached result will be available immediately when <code>maybeEnableArButton()</code> is called.</p><p>This flowchart illustrates the logic in the preceding code sample:</p><p><img src="https://developers.google.cn/ar/images/check-availability-flowchart.png" alt></p><h3 id="2-4-2-Request-camera-permission-AR-Optional-and-AR-Required-apps"><a href="#2-4-2-Request-camera-permission-AR-Optional-and-AR-Required-apps" class="headerlink" title="2.4.2 Request camera permission (AR Optional and AR Required apps)"></a>2.4.2 Request camera permission (<em>AR Optional</em> and <em>AR Required</em> apps)</h3><p>Both <em>AR Optional</em> and <em>AR Required</em> apps must ensure that the camera permission has been granted before creating an AR Session. The <strong>hello_ar_java</strong> sample includes a <a href="https://github.com/google-ar/arcore-android-sdk/search?q=CameraPermissionHelper.java" target="_blank" rel="noopener"><code>CameraPermissionHelper</code></a> class which can be copied into your project and should be called from your AR activity’s <code>onResume()</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">protected void onResume() &#123;</span><br><span class="line">  super.onResume();</span><br><span class="line"></span><br><span class="line">  // ARCore requires camera permission to operate.</span><br><span class="line">  if (!CameraPermissionHelper.hasCameraPermission(this)) &#123;</span><br><span class="line">    CameraPermissionHelper.requestCameraPermission(this);</span><br><span class="line">    return;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Your AR activity must also implement <code>onRequestPermissionsResult(…)</code>, as seen in <a href="https://github.com/google-ar/arcore-android-sdk/search?q=HelloArActivity.java" target="_blank" rel="noopener"><code>HelloArActivity</code></a>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void onRequestPermissionsResult(int requestCode, String[] permissions, int[] results) &#123;</span><br><span class="line">  if (!CameraPermissionHelper.hasCameraPermission(this)) &#123;</span><br><span class="line">    Toast.makeText(this, &quot;Camera permission is needed to run this application&quot;, Toast.LENGTH_LONG)</span><br><span class="line">        .show();</span><br><span class="line">    if (!CameraPermissionHelper.shouldShowRequestPermissionRationale(this)) &#123;</span><br><span class="line">      // Permission denied with checking &quot;Do not ask again&quot;.</span><br><span class="line">      CameraPermissionHelper.launchPermissionSettings(this);</span><br><span class="line">    &#125;</span><br><span class="line">    finish();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-3-Check-whether-Google-Play-Services-for-AR-is-installed-AR-Optional-and-AR-Required-apps"><a href="#2-4-3-Check-whether-Google-Play-Services-for-AR-is-installed-AR-Optional-and-AR-Required-apps" class="headerlink" title="2.4.3 Check whether Google Play Services for AR is installed (AR Optional and AR Required apps)"></a>2.4.3 Check whether Google Play Services for AR is installed (<em>AR Optional</em> and <em>AR Required</em> apps)</h3><p>To check whether a compatible version of Google Play Services for AR is installed, apps must also call <code>ArCoreApk.requestInstall()</code> before creating an ARCore session. This prompts the user to install or update the service if needed.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">// Set to true ensures requestInstall() triggers installation if necessary.</span><br><span class="line">private boolean mUserRequestedInstall = true;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">protected void onResume() &#123;</span><br><span class="line">  super.onResume();</span><br><span class="line"></span><br><span class="line">  // Check camera permission.</span><br><span class="line">  …</span><br><span class="line"></span><br><span class="line">  // Make sure Google Play Services for AR is installed and up to date.</span><br><span class="line">  try &#123;</span><br><span class="line">    if (mSession == null) &#123;</span><br><span class="line">      switch (ArCoreApk.getInstance().requestInstall(this, mUserRequestedInstall)) &#123;</span><br><span class="line">        case INSTALLED:</span><br><span class="line">          // Success, create the AR session.</span><br><span class="line">          mSession = new Session(this);</span><br><span class="line">          break;</span><br><span class="line">        case INSTALL_REQUESTED:</span><br><span class="line">          // Ensures next invocation of requestInstall() will either return</span><br><span class="line">          // INSTALLED or throw an exception.</span><br><span class="line">          mUserRequestedInstall = false;</span><br><span class="line">          return;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch (UnavailableUserDeclinedInstallationException e) &#123;</span><br><span class="line">    // Display an appropriate message to the user and return gracefully.</span><br><span class="line">    Toast.makeText(this, &quot;TODO: handle exception &quot; + e, Toast.LENGTH_LONG)</span><br><span class="line">        .show();</span><br><span class="line">    return;</span><br><span class="line">  &#125; catch (…) &#123;  // Current catch statements.</span><br><span class="line">    …</span><br><span class="line">    return;  // mSession is still null.</span><br><span class="line">  &#125;</span><br><span class="line">  …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This flowchart illustrates the logic in the preceding code sample:</p><p><img src="upload/image-20200706161631230.png" alt></p><p>If <code>requestInstall()</code> returns <code>INSTALL_REQUESTED</code>, the current activity pauses and the user is prompted to install or update Google Play Services for AR:</p><p><img src="https://developers.google.cn/ar/images/request-install-prompt.png" alt></p><p>The activity’s <code>onResume()</code> executes again once the user returns to the activity.</p><h2 id="2-5-Compliance-with-User-Privacy-Requirements"><a href="#2-5-Compliance-with-User-Privacy-Requirements" class="headerlink" title="2.5 Compliance with User Privacy Requirements"></a>2.5 Compliance with User Privacy Requirements</h2><p>Make sure your app complies with ARCore’s <a href="https://developers.google.cn/ar/distribute/privacy-requirements" target="_blank" rel="noopener">User Privacy Requirements</a>.</p><h2 id="2-6-Next-steps"><a href="#2-6-Next-steps" class="headerlink" title="2.6 Next steps"></a>2.6 Next steps</h2><ul><li>Read the code and comments in the <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples/hello_ar_java" target="_blank" rel="noopener">hello_ar_java</a> sample</li><li>Review the <a href="https://developers.google.cn/ar/reference/java" target="_blank" rel="noopener">Java API Reference</a></li></ul><h1 id="3-Run-AR-Apps-in-Android-Emulator"><a href="#3-Run-AR-Apps-in-Android-Emulator" class="headerlink" title="3. Run AR Apps in Android Emulator"></a>3. Run AR Apps in Android Emulator</h1><p>Use the <a href="https://developer.android.google.cn/studio/run/emulator.html" target="_blank" rel="noopener">Android Emulator</a> to test AR scenarios without a physical device. The Android Emulator lets you run ARCore apps in a virtual environment with an emulated device that you control.</p><p><strong>Warning:</strong> The Android Emulator does not support ARCore APIs for Depth, Augmented Faces, or Augmented Images. When any of these features are enabled, the camera preview image does not render correctly: the GPU camera texture is entirely black, although UI elements drawn on top of the preview image still render correctly.</p><h2 id="3-1-Set-up-your-development-environment"><a href="#3-1-Set-up-your-development-environment" class="headerlink" title="3.1 Set up your development environment"></a>3.1 Set up your development environment</h2><p>Software requirements:</p><ul><li><a href="https://developer.android.google.cn/studio/" target="_blank" rel="noopener">Android Studio</a> <strong>3.1</strong> or later.</li><li><a href="https://developer.android.google.cn/studio/run/emulator.html#Requirements" target="_blank" rel="noopener">Android Emulator</a> <strong>27.2.9</strong> or later.</li></ul><h2 id="3-2-Get-Android-Studio-and-SDK-tools-for-ARCore"><a href="#3-2-Get-Android-Studio-and-SDK-tools-for-ARCore" class="headerlink" title="3.2 Get Android Studio and SDK tools for ARCore"></a>3.2 Get Android Studio and SDK tools for ARCore</h2><ol><li><p>Install <a href="https://developer.android.google.cn/studio/" target="_blank" rel="noopener">Android Studio</a> 3.1 or later.</p></li><li><p>In Android Studio, go to <strong>Preferences &gt; Appearance and Behavior &gt; System Settings &gt; Android SDK</strong>.</p></li><li><p>Select the <strong>SDK Platforms</strong> tab and check <strong>Show Package Details</strong>.</p><p>Under <strong>Android 8.1 (Oreo)</strong>, select:<br><strong>Google APIs Intel x86 Atom System Image</strong> API Level 27, version 4 or later.</p></li><li><p>Select the <strong>SDK Tools</strong> tab and add <strong>Android Emulator</strong> 27.2.9 or later.</p></li><li><p>Click <strong>OK</strong> to install the selected packages and tools.</p></li><li><p>Click <strong>OK</strong> again to confirm changes.</p></li><li><p>Accept the license agreement for the Component Installer.</p></li><li><p>Click <strong>Finish</strong>.</p></li></ol><h2 id="3-3-Create-a-virtual-device-with-AR-support"><a href="#3-3-Create-a-virtual-device-with-AR-support" class="headerlink" title="3.3 Create a virtual device with AR support"></a>3.3 Create a virtual device with AR support</h2><p>For more information, see the Android Studio instructions to <a href="https://developer.android.google.cn/studio/run/managing-avds.html#createavd" target="_blank" rel="noopener">Create a Virtual Device</a>.</p><h3 id="3-3-1-Create-a-new-Android-Virtual-Device-AVD"><a href="#3-3-1-Create-a-new-Android-Virtual-Device-AVD" class="headerlink" title="3.3.1 Create a new Android Virtual Device (AVD)"></a>3.3.1 Create a new Android Virtual Device (AVD)</h3><ol><li>In Android Studio open the <em>AVD Manager</em> by clicking <strong>Tools &gt; AVD Manager</strong>.</li><li>Click <strong>Create Virtual Device</strong>, at the bottom of the <em>AVD Manager</em> dialog.</li><li>Select or create your desired <em>Phone</em> hardware profile and select <strong>Next</strong>.</li><li>Select an <code>x86</code> or <code>x86_64</code> system image running <strong>API Level 27 or later</strong> and select <strong>Next</strong>.<ul><li>While physical ARCore devices are supported on API Level 24 or later, Android Emulator support requires API Level 27 or later.</li><li>Only x86-based Android Emulator architectures are supported. Other architectures such as <code>arm64-v8a</code>, <code>armeabi-v7</code>, are not currently supported.</li><li><strong>macOS only with ARCore SDK 1.16.0 or later:</strong> Due to a <a href="https://issuetracker.google.com/141500087" target="_blank" rel="noopener">known issue</a>, Android Emulator <code>x86_64</code> system images are not supported on macOS with ARCore SDK 1.16.0 or later. As a workaround, use an <code>x86</code> system image.</li></ul></li><li>Verify that your virtual device is configured correctly:<ul><li>Click <strong>Show Advanced Settings</strong>.</li><li>Make sure that <strong>Camera Back</strong> is set to <strong>VirtualScene</strong>.</li></ul></li><li>Click <strong>Finish</strong> to create your AVD.</li></ol><h2 id="3-4-Run-your-app"><a href="#3-4-Run-your-app" class="headerlink" title="3.4 Run your app"></a>3.4 Run your app</h2><p>Test an ARCore app on an AR-supported virtual device in the emulator. To do this, you can follow the Android Studio instructions to <a href="https://developer.android.google.cn/studio/run/emulator.html#runningapp" target="_blank" rel="noopener">Run an app in the Android Emulator</a>.</p><p><strong>Note:</strong> To run apps with NDK components in the Android Emulator, your app must be built with <a href="https://developer.android.google.cn/ndk/guides/abis.html" target="_blank" rel="noopener"><strong>x86 ABIs</strong></a>. For an example, see the <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples/hello_ar_c" target="_blank" rel="noopener"><strong>ARCore HelloAR C sample app</strong></a>.</p><h3 id="3-4-1-Update-Google-Play-Services-for-AR"><a href="#3-4-1-Update-Google-Play-Services-for-AR" class="headerlink" title="3.4.1 Update Google Play Services for AR"></a>3.4.1 Update Google Play Services for AR</h3><p>The version of Google Play Services for AR on the emulator is likely out of date. Follow these instructions to update it:</p><ol><li><p>Download the latest <strong>Google_Play_Services_for_AR_1.18.0_x86_for_emulator.apk</strong> from the GitHub <a href="https://github.com/google-ar/arcore-android-sdk/releases" target="_blank" rel="noopener">releases</a> page.</p></li><li><p>Install the downloaded APK into each AVD you’d like to use:</p><p>Start the desired AVD, then drag the downloaded APK onto the running emulator, or install it using <code>adb</code> while the virtual device is running:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adb install -r Google_Play_Services_for_AR_1.18.0_x86_for_emulator.apk</span><br></pre></td></tr></table></figure></li></ol><p>Repeat these steps process for any additional AVDs you’d like to use.</p><h3 id="3-4-2-Control-the-virtual-scene"><a href="#3-4-2-Control-the-virtual-scene" class="headerlink" title="3.4.2 Control the virtual scene"></a>3.4.2 Control the virtual scene</h3><p>When your app connects to ARCore, you’ll see an overlay describing how to control the camera and a status bar below the emulator window.</p><p><img src="https://developers.google.cn/ar/images/ar_emulator_overlay.png" alt></p><h4 id="Move-the-virtual-camera"><a href="#Move-the-virtual-camera" class="headerlink" title="Move the virtual camera"></a>Move the virtual camera</h4><p>Press and hold <strong>Option</strong> (macOS) or <strong>Alt</strong> (Linux or Windows) to access camera movement controls. Use the following controls to move the camera:</p><table><thead><tr><th style="text-align:left">Platform</th><th style="text-align:left">Action</th><th style="text-align:left">What to do</th></tr></thead><tbody><tr><td style="text-align:left"><strong>macOS</strong></td><td style="text-align:left">Move left or right</td><td style="text-align:left">Hold <strong>Option</strong> + press <strong>A</strong> or <strong>D</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move down or up</td><td style="text-align:left">Hold <strong>Option</strong> + press <strong>Q</strong> or <strong>E</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move forward or back</td><td style="text-align:left">Hold <strong>Option</strong> + press <strong>W</strong> or <strong>S</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Change device orientation</td><td style="text-align:left">Hold <strong>Option</strong> + move mouse</td></tr><tr><td style="text-align:left"><strong>Linux</strong> or <strong>Windows</strong></td><td style="text-align:left">Move left or right</td><td style="text-align:left">Hold <strong>Alt</strong> + press <strong>A</strong> or <strong>D</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move down or up</td><td style="text-align:left">Hold <strong>Alt</strong> + press <strong>Q</strong> or <strong>E</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move forward or back</td><td style="text-align:left">Hold <strong>Alt</strong> + press <strong>W</strong> or <strong>S</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Change device orientation</td><td style="text-align:left">Hold <strong>Alt</strong> + move mouse</td></tr></tbody></table><p>Release <strong>Option</strong> or <strong>Alt</strong> to return to interactive mode in the emulator.</p><p>Use the <strong>Virtual Sensors</strong> tab in <a href="https://developer.android.google.cn/studio/run/emulator.html#extended" target="_blank" rel="noopener">Extended controls</a> for more precise device positioning.</p><h3 id="3-4-3-Add-Augmented-Images-to-the-scene"><a href="#3-4-3-Add-Augmented-Images-to-the-scene" class="headerlink" title="3.4.3 Add Augmented Images to the scene"></a>3.4.3 Add Augmented Images to the scene</h3><p>Load images into the emulator’s simulated environment to test <a href="https://developers.google.cn/ar/develop/java/augmented-images" target="_blank" rel="noopener">Augmented Images</a>.</p><p><img src="https://developers.google.cn/ar/images/augmented-images-emulator.png" alt>Use the <strong>Camera</strong> tab in Extended controls to add or modify <strong>Scene images</strong>. There are two image locations, one on the wall and one on the table.</p><p><img src="https://developers.google.cn/ar/images/augmented-images-emulator-settings.png" alt></p><p>To view these image locations in the scene, launch your emulator, then move the camera to the dining room area through the door behind the camera’s starting position.</p><h3 id="3-4-4-Troubleshooting-tips"><a href="#3-4-4-Troubleshooting-tips" class="headerlink" title="3.4.4 Troubleshooting tips"></a>3.4.4 Troubleshooting tips</h3><ul><li>If your ARCore app launches and you see an “AR Core not supported” message, check the revision on your system image. Make sure you are using <strong>API Level 27 Revision 4</strong>.</li><li>If your ARCore app fails to open the camera when it launches, make sure that <strong>Camera Back</strong> is set to <strong>VirtualScene</strong>, as described in the <a href="https://developers.google.cn/ar/develop/java/emulator#configure_the_virtual_device" target="_blank" rel="noopener">configuration steps above</a>.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ARCore 是 Google 为开发者构建的增强现实平台，如何让虚拟物体和真实世界完美融合，这一直是 Google ARCore 技术所探讨的问题。众所周知，当虚拟物体附近有现实物体时，有可能会出现互相交融、重叠等效果，大大地影响了用户体验。这一直是 AR 技术的难点，也是 Google 不懈努力的方向。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="AR" scheme="http://yuanquanquan.top/tags/AR/"/>
    
  </entry>
  
  <entry>
    <title>PC-DARTS:Partial Channel Connections for Memory-Efficient DifferentiableArchitecture Search</title>
    <link href="http://yuanquanquan.top/2020/20200529/"/>
    <id>http://yuanquanquan.top/2020/20200529/</id>
    <published>2020-05-29T14:34:41.000Z</published>
    <updated>2020-06-09T21:01:40.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>DARTS是可微分网络架构搜索，而本文将要解读的PC-DARTS是DARTS的扩展。DARTS方法速度快，但是由于它需要训练一个超网来寻找最优结构，需要消耗大量的内存和计算资源。因此论文作者提出了Partially-ConnectedDARTS，即部分通道连接的DARTS方法，通过对super-net进行一小部分的采样，能够减少网络搜索过程中计算的内存占用。但是由于通过在通道子集上执行运算搜索，而其他部分不变，这可能导致挑选超大网络边时出现不一致，为了解决这个问题，作者提出了边正则化，在搜索中添加边级别的超参数集合，来减少搜索的不确定性。</p></blockquote><a id="more"></a><p><img src="https://i.loli.net/2020/05/30/aWSh36meoy2DkIL.png" alt="PC-DARTS"></p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>DARTS把运算操作进行连续松弛，可以让网络的超参数搜索可微，进而达到端对端的网络搜索。但是由于在一个很大的超网上进行搜索，导致它的搜索空间有大量的冗余，使得计算量和内存占用很大。作者为了减少内存和计算量，采用这样的思路：不用把全部通道都送入运算选择中，而是对通道子集进行随机采样进行运算，其他的通道直接通过。但是这种思路带来了一种问题：由于采样的随机性，网络连接的选择可能是不稳定的。由此作者又引入了边正则化（edgenormalization）进行稳定，添加一个额外的边选择超参数集合。同时得益于部分通道连接的策略，选择1/K的通道可以减少K倍内存，那么就可以增大K倍的batchsize，不仅可以加速K倍，还可以稳定搜索。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>DARTS把网络的搜索拆分为L个cell，每个cell为N个节点的有向无环边，每个节点是一个网络层。Cell的类型有两种：ReductionCell和NormalCell，在整个超网中共享。</p><p>一般网络的每一层代表着一种操作，这个操作可能是卷积、池化、激活等函数，但在超网络SuperNet中，每一层网络是由多种运算组合起来的，每一种运算对应一个系数$\alpha$,，混合运算的加权公式如下：$f_{i, j}\left(\mathbf{x}<em>{i}\right)=\sum</em>{o \in \mathcal{O}} \frac{\exp \left{\alpha_{i, j}^{o}\right}}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left{\alpha_{i, j}^{o^{\prime}}\right}} \cdot o\left(\mathbf{x}_{i}\right)$</p><h2 id="Partial-Channel-Connections"><a href="#Partial-Channel-Connections" class="headerlink" title="Partial Channel Connections"></a><strong>Partial Channel Connections</strong></h2><p>DARTS的问题是需要大量内存，为了调节$|\theta|$个运算，需要把每个运算的结果存储起来，需要使用$|\theta|$倍的内存，为了能存下必须降低batchsize大小，这就降低了速度。而本文提出了partially-connectedDARTS方法，简称为PC-DARTS，方法如图1所示。该方法将网络提取的特征在通道维度上进行了1/K采样，只对采样后的通道进行处理，然后将这些特征与剩余的特征进行拼接(concat)。为了减少由采样带来的不确定性，作者又提出了边正则化，添加了边级别的超参数$\beta$</p><p><img src="https://i.loli.net/2020/05/29/3RxMkinqbHE7IAs.png" alt="PC-DARTS方法图示"></p><p>PC-DARTS的运算加权公式为:$f_{i, j}^{\mathrm{PC}}\left(\mathbf{x}<em>{i} ; \mathbf{S}</em>{i, j}\right)=\sum_{o \in \mathcal{O}} \frac{\exp \left{\alpha_{i, j}^{o}\right}}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left{\alpha_{i, j}^{o^{\prime}}\right}} \cdot o\left(\mathbf{S}<em>{i, j} * \mathbf{x}</em>{i}\right)+\left(1-\mathbf{S}<em>{i, j}\right) * \mathbf{x}</em>{i}$</p><p>其中$S_{i,j}$为通道采样mask，标为1的通道直接作为输出。我们随机采样1/K个通道，其中这个K我们作为超参数用来平衡速度和准确率。挑选出通道的1/K可以减少K倍的计算量，还可以有更多的样本来采样，这对于网络搜索尤为重要。</p><p><strong>Edge Normalization</strong></p><p>上一节中对通道进行随机采样的好处是能减少所选操作的偏置，即对于边(i,j)，给定$x_i$,使用两组超参数${\alpha^{o}<em>{i,j}}$和$${\alpha^{o’}</em>{i,j}}$$的差距就减小了。但是它削弱了无权重运算（如跳层连接，最大池化）的优势。在早期，搜索算法更喜欢无权重的运算，因为这些运算没有参数，能够得到输出一致的结果。但是对于有权重的运算，优化过程中会出现不一致的情况。这样无权重的运算会占据很大的比重，后续即使有权重的运算优化的很好，也无法超过它们。这种现象在代理输入比较困难的时候尤其严重，这也导致DARTS在ImageNet上效果不好。</p><p>为此提出了边正则化来抑制该现象。边正则化为基本单元中的第i层网络的每个输入分配了一个$\beta$参数，公式表示如下：$\mathbf{x}<em>{j}^{\mathrm{PC}}=\sum</em>{i&lt;j} \frac{\exp \left{\beta_{i, j}\right}}{\sum_{i^{\prime}&lt;j} \exp \left{\beta_{i^{\prime}, j}\right}} \cdot f_{i, j}\left(\mathbf{x}_{i}\right)$</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>作者在两个常用的公共数据集CIFAR-10和ImageNet进行了实验。</p><p><strong>CIFAR-10实验结果</strong></p><p>CIFAR-10上的实验结果如图2所示。在搜索过程中，网络由8个cell堆叠组成（包含6个normalcells和2个reductioncells），并且每个cell由6个节点构成，normalcell和reductioncell结果如图2所示。</p><p>CIFAR-10上，选用K=4，即只有1/4的通道被采样，因此搜索期间的batchsize增加到256。训练时，SuperNet首先预热15个epochs（即固定架构超参数，只更新网络参数），使用带动量的SGD进行网络参数的优化，使用Adam优化器来对超参数${\alpha^{o}<em>{i,j}}$和$\beta</em>{i,j}$进行更新。</p><p>从表中可以看到，使用PC-DARTS方法，仅需0.1GPU天，错误率就可以达到2.57%，搜索时间和准确率都超过了baseline方法DARTS。在比较的方法中，PC-DARTS是错误率小于3%的方法中速度最快的。</p><p><img src="https://i.loli.net/2020/05/29/z71ISxRg5D2Tic4.png" alt="表1：在CIFAR-10上的实验结果"><img src="https://i.loli.net/2020/05/29/ogBx9ckQMwhSpPJ.png" alt="图2：在CIFAR-10上搜索出的cell结构"></p><h2 id="ImgaeNet上的实验结果"><a href="#ImgaeNet上的实验结果" class="headerlink" title="ImgaeNet上的实验结果"></a><strong>ImgaeNet上的实验结果</strong></h2><p>在ImageNet上的实验结果以及和SOTA方法的比较如表2所示。作者对用于CIFAR-10上的网络结构进行了小修改以适用于ImageNet。为了减小搜索时间，作者分别从ImageNet上随机采样了两个子集，采样率分别为10%和2.5%，前者用于训练网络参数权重，后者用于更新架构超参数。</p><p>由于在ImageNet进行搜索比CIFAR-10更难，为了保留更多的信息，选取K=2，即通道采样率为1/2，是CIFAR-10的两倍。仍然训练50个epochs，但是前35个epochs固定架构超参数，其他训练设置基本和CIFAR-10上的一致。</p><p>在ImageNet上的结果如表2所示，在ImageNet上的实验结果，Top-1和Top-5准确率可以达到24.2%和7.3%，是比较的方法中效果最好的，也证明了本方法在减少内存消耗上是有效的。</p><p><img src="https://i.loli.net/2020/05/29/mylQnNPtcx75V86.png" alt="表2：在ImageNet上的实验结果"></p><p>在ImageNet上搜索得到的normalcell和reductioncell如下图所示。</p><p><img src="https://i.loli.net/2020/05/30/Gr3N6YATl8Ev9XC.png" alt="图3：在ImageNet上搜索得到的cell结果"></p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a><strong>消融实验</strong></h2><h2 id="1-不同采样率的结果"><a href="#1-不同采样率的结果" class="headerlink" title="1. 不同采样率的结果"></a><strong>1. 不同采样率的结果</strong></h2><p>K是用来控制通道的采样率的一个超参数，在讨论K对实验结果的影响之前，要先明确这么一个信息：增加采样率（即使用一个更小的K值）能使得更精确的信息被传播；而对通道的更小一部分进行采样，会造成更大的正则化，可能会引起过拟合。为了研究K的影响，作者在CIFAR-10上实验了4种K值对性能的影响，分别为1/1,1/2,1/4和1/8，实验结果如图4所示。从图中可以看出来采样率在1/4时的效果最好，准确率最高，搜索速度最快。使用1/8的采样率，尽管会进一步减少搜索时间，但是会产生严重的性能下降。<img src="https://i.loli.net/2020/05/30/QTBl1tOjRS2g9i5.png" alt="图4：不同K值（即采样率）对实验结果的影响"></p><h2 id="2-PC-DARTS不同组件的作用"><a href="#2-PC-DARTS不同组件的作用" class="headerlink" title="2. PC-DARTS不同组件的作用"></a><strong>2. PC-DARTS不同组件的作用</strong></h2><p>作者又探讨PC-DARTS中的partialchannel connection（表中简称为PC）和edgenormalization（表中简称为EN）的作用，结果如表3所示。从表中可以很明显看到，EN及时在通道是全部连接的情况下，也能带来正则化的效果。同时，edgenormalization和partialchannel connection一起使用，可以提供更进一步的改进效果。而不使用edgenormalization，则网络参数的数量和精确度都会受到影响。</p><p><img src="https://i.loli.net/2020/05/30/3tqhK8Vr7bjQpXP.png" alt="表3：在CIFAR-10和ImageNet上的消融实验"></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>本篇论文提出了一个简单但却有效的PC-DARTS(partially-connecteddifferentiable architecturesearch)方法，<strong>它的核心思想是随机采样一部分通道用于运算搜索，这样能更有效的利用内存，可以使用更大的batchsize获得更高的稳定性。另一个贡献是提出了边标准化(edgenormalization)来稳定搜索的过程，这是个轻量化的模块，基本不需要太多的计算量。此方法在CIFAR-10上完整搜索只需要0.1GPU天，在Imagenet上搜索需要3.8GPU天。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;DARTS是可微分网络架构搜索，而本文将要解读的PC-DARTS是DARTS的扩展。DARTS方法速度快，但是由于它需要训练一个超网来寻找最优结构，需要消耗大量的内存和计算资源。因此论文作者提出了Partially-ConnectedDARTS，即部分通道连接的DARTS方法，通过对super-net进行一小部分的采样，能够减少网络搜索过程中计算的内存占用。但是由于通过在通道子集上执行运算搜索，而其他部分不变，这可能导致挑选超大网络边时出现不一致，为了解决这个问题，作者提出了边正则化，在搜索中添加边级别的超参数集合，来减少搜索的不确定性。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="AutoML" scheme="http://yuanquanquan.top/tags/AutoML/"/>
    
  </entry>
  
  <entry>
    <title>Typecho</title>
    <link href="http://yuanquanquan.top/2020/202005251/"/>
    <id>http://yuanquanquan.top/2020/202005251/</id>
    <published>2020-05-25T06:17:17.000Z</published>
    <updated>2020-12-16T08:10:48.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近因为疫情<a href="https://developer.aliyun.com/adc/student/?pid=mm_25282911_3455987_122436732" target="_blank" rel="noopener">阿里云可以白嫖半年的服务器</a>,刚好搭一个博客</p></blockquote><a id="more"></a>  <p><strong>一、安装宝塔面板</strong></p><ol><li><p>打开控制命令行:win+r，输入cmd</p></li><li><p>输入命令：ssh root@服务器IP地址，输入密码就能进入服务器.登陆成功的话如图所示!<img src="https://i.loli.net/2020/06/10/T9G27VM5Kl8e6EN.png" alt></p></li><li><p>在shell环境下输入安装宝塔的命令，安装命令宝塔官方上有，直接按照自己的系统版本选择，我这里选择ubuntn的安装命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y wget &amp;&amp; wget -O install.sh http://download.bt.cn/install/install_6.0.sh &amp;&amp; sh install.sh</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li>最后得到一个宝塔面板的链接网址和账户和登录密码。<img src="https://i.loli.net/2020/05/25/rh4Ucl7QoJmA1gs.png" alt></li></ol><p><strong>二、在面板上搭建typecho博客</strong></p><p>用刚才的账号和密码登录到宝塔面板，新建一个站点，然后输入域名（没有的话可以从阿里云买一个），勾选mysql数据库，注意记住创建之后的数据库名和密码，后面用的到。<img src="https://i.loli.net/2020/05/25/9fWHMQyv3AixCEd.png" alt></p><p>注意这里的端口号，默认是80，如果80端口号被占用，可以改成81或者82等。记住这里的端口号，之后将需要这个端口号设置typecho博客。<strong>注意阿里云要开放80端口<img src="https://i.loli.net/2020/05/25/uWCEPyclmRx2pFo.png" alt></strong></p><ul><li>从typecho官方网站下载1.1正式版：[typecho官方下载<img src="https://i.loli.net/2020/05/25/69BTLoYXgMeku5Z.png" alt></li></ul><ul><li><p>将下载之后的文件解压放到创建的网站文件夹下<br><img src="https://i.loli.net/2020/05/25/g9YlzInd3Q65LXF.png" alt></p></li><li><p>访问刚刚设置的网站域名端口号，一般80是被占用的，需要自己改一个端口号，我这里改成了81，但是此时直接访问81的端口号会404 NOT Found,这个时候需要去宝塔面板里放行端口号<img src="C:%5CUsers%5CLenovo%5CDesktop%5Cyolov1%5Ctypecho-7.png" alt></p></li></ul><ul><li><p>接下来就循规蹈矩的进行安装了,输入刚刚的数据库名、用户名和密码，密码忘记了可以去宝塔面板里自己改一个简单点的。<img src="https://i.loli.net/2020/05/25/YP2in6kyGRJhvXf.png" alt></p></li><li><p>这里会提示没有config.inc.php配置文件，这个时候需要自己去创建一个文件，将其中的代码复制进去，放到blog文件夹下</p></li></ul><p><strong>三、选择typecho主题，并可以随时切换</strong><br>typecho模板主题</p><p>将下载的主题，放到当前目录下，默认会有一个系统主题</p><p>登录到自己的博客地址，然后进行博客的相关设置。主题修改</p><p><strong>设置外观</strong></p><p>控制台 -&gt; 外观，“设置外观” 选项。里面是和主题相对应的定制选项，比如 网站 logo、功能开关、站点描述、缩略图设置等，主要是针对该主题的一些个性化、功能性设置。</p><p><img src="https://i.loli.net/2020/05/25/ZwTid3hyXcoY2b7.jpg" alt></p><p><strong>自定义修改</strong></p><p>“编辑当前外观” 自定义修改主题样式。这里列出的模板文件和主题有关，如果没有需要修改的那个文件，可以去主题文件夹里查找直接修改。</p><p><img src="https://i.loli.net/2020/05/25/vnzSfArp7Msgty1.jpg" alt></p><p><strong>Tips</strong></p><ul><li>有的主题需要更改主题文件夹为指定名称才有效，注意查看主题说明；</li><li>自定义修改主题样式后，如果刷新网站没有变化，尝试刷新 CDN 缓存或者浏览器本地缓存。</li></ul><hr><h2 id="主题推荐"><a href="#主题推荐" class="headerlink" title="主题推荐"></a>主题推荐</h2><p>这里分享的全部是免费主题。</p><p><strong>Pinghsu</strong></p><p>简介：卡片式设计，简洁美观。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//www.linpx.com/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/chakhsu/pinghsu" target="_blank" rel="noopener">https://github.com/chakhsu/pinghsu</a></li></ul><p><img src="https://i.loli.net/2020/05/25/jFxhHZwvpJztmGC.jpg" alt></p><p><strong>Material</strong></p><p>简介：Material Design theme for typecho. 扁平化设计主题。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//blog.lim-light.com/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/idawnlight/typecho-theme-material" target="_blank" rel="noopener">https://github.com/idawnlight/typecho-theme-material</a></li></ul><p><img src="https://i.loli.net/2020/05/25/Lak4Az31VHusSUt.jpg" alt></p><p><strong>NexT.Pisces</strong></p><p>简介：Hexo 主题 NexT.Pisces 的 Typecho 移植版，基于 zgq354 的 NexT.Mist 修改制作。</p><ul><li><a href="https://link.zhihu.com/?target=http%3A//notes.iissnan.com/" target="_blank" rel="noopener">Demo</a>（这个是 Hexo 的，效果差不多）</li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/newraina/typecho-theme-NexTPisces" target="_blank" rel="noopener">https://github.com/newraina/typecho-theme-NexTPisces</a></li></ul><p><img src="https://i.loli.net/2020/05/25/qNb1SeIEDVps8Td.jpg" alt></p><p><strong>Maupassant</strong></p><p>简介：极简响应式主题。</p><ul><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/pagecho/maupassant" target="_blank" rel="noopener">https://github.com/pagecho/maupassant</a></li></ul><p><img src="https://i.loli.net/2020/05/25/aXY7gzDoeGEvPBl.jpg" alt></p><p><strong>Optica</strong></p><p>简介：单栏小清新主题 Optica。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//1000yun.cn/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/iduzui/optica" target="_blank" rel="noopener">https://github.com/iduzui/optica</a></li></ul><p><img src="https://pic1.zhimg.com/80/v2-1deecd43d3f4c790780feb33462c88a4_720w.jpg" alt></p><p><strong>ArmX</strong></p><p>简介：响应式纯净前端结构，不依赖第三方前端框架；自带音乐播放器，全站pjax；支持第三方登录；支持cdn加速、生成缩略图等，功能完善。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//codeup.me/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/natcube/ArmX" target="_blank" rel="noopener">https://github.com/natcube/ArmX</a></li></ul><p><img src="https://i.loli.net/2020/05/25/kbZ2VihLnOjB8SI.jpg" alt></p><p><strong>Affinity</strong></p><p>简介：三列卡片式布局。移植自 ghost，原作地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/Showfom/Affinity" target="_blank" rel="noopener">https://github.com/Showfom/Affinity</a>。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//affinity.419.at/" target="_blank" rel="noopener">Demo</a></li><li>地址： <a href="https://link.zhihu.com/?target=https%3A//affinity.419.at/2017/07/28/affinity.html" target="_blank" rel="noopener">https://affinity.419.at/2017/07/28/affinity.html</a></li></ul><p><img src="https://i.loli.net/2020/05/25/7UpfkVOj3raQ1K8.jpg" alt></p><p><strong>Junichi</strong></p><p>简介：轻量级，无前端框架；响应式设计。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//uefeng.com/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/siseboy/junichi" target="_blank" rel="noopener">https://github.com/siseboy/junichi</a></li></ul><p><img src="https://i.loli.net/2020/05/25/jBua1S2DPHIpyiW.jpg" alt></p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Typecho 主题还是很丰富的，而且很多是博主们写着自己用，然后分享给网友的。如果喜欢可以选择购买付费主题或者赞助支持主题作者。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近因为疫情&lt;a href=&quot;https://developer.aliyun.com/adc/student/?pid=mm_25282911_3455987_122436732&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿里云可以白嫖半年的服务器&lt;/a&gt;,刚好搭一个博客&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="博客搭建" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="blog" scheme="http://yuanquanquan.top/tags/blog/"/>
    
      <category term="教程" scheme="http://yuanquanquan.top/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="typecho" scheme="http://yuanquanquan.top/tags/typecho/"/>
    
  </entry>
  
  <entry>
    <title>胃与真相</title>
    <link href="http://yuanquanquan.top/2020/20200524/"/>
    <id>http://yuanquanquan.top/2020/20200524/</id>
    <published>2020-05-24T15:25:45.000Z</published>
    <updated>2020-06-09T21:03:48.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>疫情期间常常听Ludovico的作品，蛰居陋室，埋头科研，不问世事，倒也颇映衬了他作品中空旷辽远的寂静和哲学思索。</p><p>在经过了盲目自信、怀疑论、疲乏应对、指责中国之后，新冠疫情在美国的爆发，终究还是成为了社会的主要议题。搬空的超市只是一个缩影，标志着不管你是否愿意，都必须接受这样一个事实 - 生活不会再像以前那样安宁祥和。而现在，被疫情考验之后的美国社会，更多的人开始意识到，再也回不到疫情之前的样子。纽约州长Cuomo在一次例行的发布会上说：人们都想要回到生活常态（back to normal），但事实是，回不去了，我们只会进入到一个新的常态（a new normal）。</p></blockquote><a id="more"></a>  <p>之前在Twitter上看到一个老美说，新冠疫情就是美国政府的一个谎言，目的是用来操纵民意，为大选服务。那时大多数美国人对于新冠是比较乐观的，而当时我只是觉得这人真傻，她不知道中国人民抗疫付出的代价有多大，得到的教训有多深刻，她只要稍微看看国际新闻，就会知道这个疾病的严重性，怎会是捏造之实。半个月之后，奥斯汀市政府宣布城市限行，民众居家禁止外出。一时间，所有人都被拽进一个陌生的现实，它就在那儿，至今仍然在那儿。</p><p>但我后来意识到，她的那种不屑，或者是无可奈何，大概是对的。那不代表她不看国际新闻，事实是，获得真相对改变现状的帮助太小，而听信谎言，或者是厌倦与逃离，也不一定是愚蠢的表现。不管形势如何发展，普通民众，除了恐慌性地抢购生活物资，在这种宏大层面的公共安全事件面前，又能做些什么呢。今天可以是新冠疫情，明天可以是任何突发事件。但生活总要继续，总要果腹，病死和饿死，又有什么区别。就算病死和饿死有区别，在最终报给总统的伤亡人数里，也都只是数字，背后的故事谁又会去讲述。</p><p>纪录片《华氏911》里说，他们把恐怖指数调到高，民众开始疯狂抢购，囤积自卫；然后又调到低，民众又像往常一样推婴儿车在公园散步和遛狗。不断地往复着模棱两可的语境，正如整个疫情当中，川普政府对于病毒危险性的评估不断反复闪烁其词，模棱两可，而真正应该领导疫情防控的Fauci博士却被拒绝参与众议院的新冠听证会。事实上，模糊的语境只会让民众不知所措，当民众足够恐慌，他们便像绵羊一样温顺，可以随意地剪他们的毛。就像训练一条狗，你叫它坐下，然后叫它打滚，刚开始要滚你又叫它站起来，狗不知道要怎么做才好。</p><p>人们希望真相，但谎言和真相，没有本质的区别。它们相互转化，而又被权威所利用，唯一的区别是，做谎言的帮凶，浑水摸鱼；还是做真相的寻找者，逆流而上。不幸的是，人类所建立的这个系统的复杂度和信息的非对称决定了，没有绝对的真相，也没有绝对的谎言。大概更永恒的，只有我们的这个胃了罢：平民的胃是早中晚三餐，是早九晚五有班可以上；既得利益者的胃是道琼斯指数，是一年四季有资本可以攫取。毕竟，胃才是任何文明，任何语境的最终动力，因为那些没法填饱自己胃的物种都灭绝了；毕竟，既得利益者们所做的一切，大概也是为了他们的后代的胃可以不用空着。</p><p>对饥饿的恐惧，是刻在人类骨子里基因里的。而疾病，同样如此。所有人都在玩着同样的生存游戏。</p><p>1976年版的爱因斯坦文集中译本第一卷开篇中“自述”的第二段写到：</p><p>“当我还是一个相对早熟的少年的时候，我就已经深切地意识到，大多数人终生无休止地追逐的那些希望和努力是毫无价值的。而且，我不久就发现了这种追逐的残酷，这在当年较之今天是更加精心地用伪善和漂亮的字句掩饰着的。每个人只是因为有个胃，就注定要参与这种追逐。而且，由于参与这种追逐，他的胃是有可能得到满足的；但是，一个有思想，有感情的人却不能由此而得到满足。”</p><p>我大概是非常同意这段话的，也惊叹于少年时期的爱因斯坦便已有这样深刻的见解。而我也对自己写下这些文字感到一些隐藏的担忧，我们这些研究物理学的人，本是没有足够的经验和资历来对社会现象做过多的评述。但我们仍然发现，社会和自然至少在基本的逻辑上，是按照相似的法则来运转的。比如说生态系统中的物质能量流动的结构就和社会中的物资供应链很相似。著名的马太效应也能找到自然中的起源，比如热力学中的Ostwald Ripening 效应就说，在同一个压力环境里面，大气泡的增长都是以小气泡的消亡为代价；又比如食物链顶端的物种数量可能只占一个生态系统中物种数量的不到百分之一，但却统治着系统中大部分的资源。所以大概自然界中的“穷者愈穷，富者愈富”比人类社会更严重，毕竟作为人类的我们还发明了公平和道德来对冲自然法则。</p><p>爱因斯坦的思路其实还是物理的思路，或许过于简化，但他揭示了一个模型，那就是社会就像自然一样，构建于一些简单的法则，比如胃的满足；但社会的复杂度，则来自于法则衍生出来的结构，比如掩饰，比如修辞，比如文明。</p><p>真相，或者谎言，都是修辞。而胃，才是真正在运作的东西。</p><p>前段时间Twitter上有一则赞数很高的帖子如是说：不是新冠疫情导致了社会的撕裂（divide），而是新冠疫情揭示（reveal）了一个撕裂着的社会。邻居们开始不赞同彼此的观点；民粹开始系统性地对抗精英，对抗建制（establishment），对抗生产力的进步；大量的人开始失业，并将失业的原因归咎于从墨西哥偷渡过来的移民；每个人都和他人物理隔离，却在互联网的世界里继续争吵。而在这种争吵中，谁也不会被谁说服。</p><p>这一切，其实早在新冠以前就已经开始。</p><p>在新冠之前，社会不就已经在往着嘈杂的网络，疏离的物理联系，越来越多的人类丢掉工作的方向发展了么？新冠只是加剧了这种转型的阵痛。人类面临的，是一个生产力模式的变革，我们将被迫更深刻地去审视人类在自然中的位置，审视个体在社会中的位置，正如人类积累的知识中所探讨的那样。人类的发展模式将越来越不依赖于人际联系，而是依赖于一个个共同的理性的大脑们以及它们所构建的生产力爆炸式增长。讽刺的是，如果说工业革命以前或者说早期的工业革命中，大部分人口为生产力与资本贡献了宝贵的劳动力，那么正在进行的智能化革命则剥夺了越来越多人口的工作。这部分人，这一大部分人，除了加入民粹和反智的潮流，还有别的选择么？这种转型是一种系统性的对低效率人类劳动的价值否定，它不仅是经济上的，也是文化上的对大众的边缘化。如果大众越来越无法参与到生产力的进步中和基于生产力的文化构建中，那社会的撕裂是必然的。也有很多人在说，智能化革命虽然替代了很多人类劳动力，但它也带来了新的工作机会，比如算法工程师，物联网开发者等等。这是一种乐观的估计，事实是，逐利的资本会愿意帮助大众具备获得这些工作机会的技能吗？如果20%的失业率是因为工作效率是普通人类10倍的机器替代了他们，那么你可以构建一种模式使得这20%的人以更高的效率去替代这些机器吗？要知道，这些机器的效率也会越来越高，而创造这些机器，可能都不需要哪怕是2%的人。</p><p>人类从蛮荒走向文明，是因为胃的驱动，我们打败了野兽，获得了食物，是因为人类可以思考，可以想象，可以定义真相，抑或是谎言。而文明也在塑造着我们的胃。这种不可思议的能力塑造着人类和自己的关系，和自然的关系。我们真正应该思考的是，胃将把人类从文明带向何方，文明会是我们存在的最终形式吗？也许会，也许不会。但确凿的理解是，文明正在以一种我们不充分理解的方式改变着修辞的版图，改变着胃的属性，而于此间，真正危险的事情是，除了接受这种改变，我们似乎没有第二种选择，因为每个人只有一个胃。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;疫情期间常常听Ludovico的作品，蛰居陋室，埋头科研，不问世事，倒也颇映衬了他作品中空旷辽远的寂静和哲学思索。&lt;/p&gt;
&lt;p&gt;在经过了盲目自信、怀疑论、疲乏应对、指责中国之后，新冠疫情在美国的爆发，终究还是成为了社会的主要议题。搬空的超市只是一个缩影，标志着不管你是否愿意，都必须接受这样一个事实 - 生活不会再像以前那样安宁祥和。而现在，被疫情考验之后的美国社会，更多的人开始意识到，再也回不到疫情之前的样子。纽约州长Cuomo在一次例行的发布会上说：人们都想要回到生活常态（back to normal），但事实是，回不去了，我们只会进入到一个新的常态（a new normal）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Conditional Random Fields</title>
    <link href="http://yuanquanquan.top/2020/20200511/"/>
    <id>http://yuanquanquan.top/2020/20200511/</id>
    <published>2020-05-11T05:30:00.000Z</published>
    <updated>2020-06-14T08:01:05.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>条件随机场(conditional random fields，简称 $CRF$，或$CRFs$)，是一种判别式概率模型，常用于标注或分析序列资料，如自然语言文字或是生物序列。</p><p>条件随机场是条件概率分布模型P(Y|X)，表示的是给定一组输入随机变量X的条件下另一组输出随机变量Y的马尔可夫随机场，也就是说$CRF$的特点是假设输出随机变量构成马尔可夫随机场。</p></blockquote><a id="more"></a>  <h2 id="知识框架"><a href="#知识框架" class="headerlink" title="知识框架"></a>知识框架<img src="https://i.loli.net/2020/05/14/Xodiy9fMFzDgebp.png" alt></h2><h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><p>定义：假设一个随机过程中， $t_n$时刻的状态$x_n$的条件发布，只与其前一状态$x_{n-1}$相关，即：<br>$$<br>P\left(x_{n} | x_{1}, x_{2}, \ldots, x_{n-1}\right)=P\left(x_{n} | x_{n-1}\right)<br>$$<br>则将其称为马尔可夫过程。</p><p><img src="https://i.loli.net/2020/05/14/h3UIwjL2z8pMOma.png" alt></p><h2 id="隐马尔可夫算法-HMM"><a href="#隐马尔可夫算法-HMM" class="headerlink" title="隐马尔可夫算法(HMM)"></a>隐马尔可夫算法(HMM)</h2><p><strong>1、定义</strong></p><p>隐马尔可夫算法是对含有未知参数（隐状态）的马尔可夫链进行建模的生成模型，如下图所示：</p><p><img src="https://i.loli.net/2020/05/14/EMGRTL3sxB7l25m.png" alt="CRF-3"></p><p>在隐马尔科夫模型中，包含隐状态和观察状态，隐状态$x_i$对于观察者而言是不可见的，而观察状态$y_i$对于观察者而言是可见的。隐状态间存在转移概率，隐状态$x_i$到对应的观察状态$y_i$间存在输出概率。</p><p><strong>2、假设</strong></p><p>假设隐状态$x_i$的状态满足马尔可夫过程，$i$时刻的状态$x_i$的条件分布，仅与其前一个状态$x_{i-1}$相关，即：<br>$$<br>P\left(x_{i} | x_{1}, x_{2}, \ldots, x_{i-1}\right)=P\left(x_{i} | x_{i-1}\right)<br>$$<br>假设观测序列中各个状态仅取决于它所对应的隐状态，即：<br>$$<br>P\left(y_{i} | x_{1}, x_{2}, \ldots, x_{i-1}, y_{1}, y_{2}, \ldots, y_{i-1}, y_{i+1}, \ldots\right)=P\left(y_{i} | x_{i}\right)<br>$$<br><strong>3、存在问题</strong></p><p>在序列标注问题中，隐状态（标注）不仅和单个观测状态相关，还和观察序列的长度、上下文等信息相关。例如词性标注问题中，一个词被标注为动词还是名词，不仅与它本身以及它前一个词的标注有关，还依赖于上下文中的其他词</p><h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><p>以线性链条件随机场为例</p><p><strong>1、定义</strong></p><p>给定$X=(x_1,x_2····,x_n)$,$Y=(y_1,y_2····,y_n)$均为线性链表示的随机变量序列，若在给随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔可夫性：<br>$$<br>P\left(y_{i} | x_{1}, x_{2}, \ldots, x_{i-1}, y_{1}, y_{2}, \ldots, y_{i-1}, y_{i+1}\right)=P\left(y_{i} | x, y_{i-1}, y_{i+1}\right)<br>$$<br>则称$P(Y|X)$为线性链条件随机场。</p><p>通过去除了隐马尔科夫算法中的观测状态相互独立假设，使算法在计算当前隐状态$x_i$时，会考虑整个观测序列，从而获得更高的表达能力，并进行全局归一化解决标注偏置问题。</p><p><strong>1）参数化形式</strong><br>$$<br>p(y | x)=\frac{1}{Z(x)} \prod_{i=1}^{n} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)<br>$$<br>其中：$Z(x)$为归一化因子，是在全局范围进行归一化，枚举了整个隐状态序列$x_{1…n}$的全部可能，从而解决了局部归一化带来的标注偏置问题。</p><p>$t_k$为定义在边上的特征函数，转移特征，依赖于前一个和当前位置$s_1$为定义在节点上的特征函数，状态特征，依赖于当前位置</p><p><strong>2）简化形式</strong></p><p>因为条件随机场中同一特征在各个位置都有定义，所以可以对同一个特征在各个位置求和，将局部特征函数转化为一个全局特征函数，这样就可以将条件随机场写成权值向量和特征向量的内积形式，即条件随机场的简化形式。</p><ul><li><strong>step 1</strong> 将转移特征和状态特征及其权值用统一的符号表示，设有$k_1$个转移特征，$k_2$个状态特征，,记</li></ul><p>$$<br>f_{k}\left(y_{i-1}, y_{i}, x, i\right)=\left{\begin{array}{l}<br>t_{k}\left(y_{i-1}, y_{i}, x, i\right), \quad k=1,2,3, \ldots, K_{1} \<br>s_{l}\left(y_{i}, x, i\right), \quad k=k_{1}+l ; l=1,2, \ldots, K_{2}<br>\end{array}\right.<br>$$</p><ul><li><strong>step 2</strong> 对转移与状态特征在各个位置求$i$和，记作</li></ul><p>$$<br>f_{k}(y, x)=\sum_{i=1}^{n} f_{k}\left(y_{i-1}, y_{i}, x, i\right), k=1,2, \ldots, K<br>$$</p><ul><li><strong>step 3</strong> 将 和 用统一的权重表示，记作</li></ul><p>$$<br>w_{k}=\left{\begin{array}{ll}<br>\lambda_{k}, &amp; k=1,2, \ldots, K_{1} \<br>\mu_{l}, &amp; k=K_{1}+l ; l=1,2, \ldots, K_{2}<br>\end{array}\right.<br>$$</p><ul><li><p><strong>step 4</strong> 转化后的条件随机场可表示为：<br>$$<br>\begin{aligned}<br>P(y | x) &amp;=\frac{1}{Z(x)} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \<br>Z(x) &amp;=\sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x)<br>\end{aligned}<br>$$</p><ul><li><strong>step 5</strong> 若 表示权重向量：$w=(w_1,w_2,…,w_k)^T$以$F(y,x)$表示特征向量，即<br>$$<br>F(y, x)=\left(f_{1}(y | x), f_{2}(y | x), \ldots, f_{K}(y | x)\right)^{T}<br>$$<br>   则，条件随机场写成内积形式为：<br>$$<br>\begin{array}{l}<br>P_{w}(y | x)=\frac{\exp (w \cdot F(y, x)}{Z_{w}(x)} \<br>Z_{w}(x)=\sum_{y} \exp (w \cdot F(y, x))<br>\end{array}<br>$$</li></ul></li></ul><p><strong>3）矩阵形式</strong><br>$$<br>P_{w}(y | x)=\frac{1}{Z_{w}(x)} \prod_{i=1}^{n+1} M_{i}\left(y_{i-1}, y_{i} | x\right)<br>$$<br><strong>2、基本问题</strong></p><p>条件随机场包含概率计算问题、学习问题和预测问题三个问题。</p><ul><li>概率计算问题：已知模型的所有参数，计算观测序列Y出现的概率，常用方法：前向和后向算法；</li><li>学习问题：已知观测序列Y,求解使得该观测序列概率最大的模型参数，包括隐状态序列、隐状态间的转移概率分布和从隐状态到观测状态的概率分布，常用方法：Baum-Wehch算法；</li><li>预测问题：一直模型所有参数和观测序列Y，计算最可能的隐状态序列X,常用算法：维特比算法。</li></ul><p><strong>案例：利用维特比算法计算给定输入序列$x$对应的最优输出序列$y^*$：</strong><br>$$<br>\max \sum_{i=1}^{3} w \cdot F_{i}\left(y_{i-1}, y_{i}, x\right)<br>$$<br>1.初始化<br>$$<br>\delta_{1}(j)=w \cdot F_{1}\left(y_{0}=\operatorname{start}, y_{1}=j, x\right), j=1,2 i=1, \delta_{1}(1)=1, \delta_{1}(2)=0.5<br>$$<br>2.递推，对$i=2,3,…,n$<br>$$<br>\begin{array}{c}<br>i=2, \delta_{2}(l)=\max <em>{j}\left{\delta</em>{1}(j)+w \cdot F_{2}(j, l, x)\right} \<br>\delta_{2}(1)=\max \left{1+\lambda_{2} t_{2}, 0.5+\lambda_{4} t_{4}\right}=1.6, \Psi_{2}(1)=1 \<br>\delta_{2}(2)=\max \left{1+\lambda_{1} t_{1}+\mu_{2} s_{2}, 0.5+\mu_{2} s_{2}\right}=2.5, \Psi_{2}(2)=1 \<br>i=3, \delta_{3}(l)=\max <em>{j}\left{\delta</em>{2}(j)+w \cdot F_{3}(j, l, x)\right} \<br>\delta_{3}(1)=\max \left{1.6+\mu_{5} s_{5}, 2.5+\lambda_{3} t_{3}+\mu_{3} s_{3}\right}=4.3, \Psi_{3}(1)=2 \<br>\delta_{3}(2)=\max \left{1.6+\lambda_{1} t_{1}+\mu_{4} s_{4}, 2.5+\lambda_{5} t_{5}+\mu_{4} s_{4}\right}=4.3, \Psi_{3}(2)=1<br>\end{array}<br>$$<br>3.终止<br>$$<br>\max <em>{y}(w \cdot F(y, x))=\max \delta</em>{3}(l)=\delta_{3}(1)=4.3 y_{3}^{<em>}=\operatorname{argmax}<em>{1} \delta</em>{3}(l)=1<br>$$<br>4.返回路径<br>$$<br>\begin{aligned}<br>y_{2}^{</em>}=&amp; \Psi_{3}\left(y_{3}^{<em>}=\Psi_{3}(1)=2 y_{1}^{</em>}=\Psi_{2}\left(y_{2}^{<em>}\right)=\Psi_{2}(2)=1\right.\<br>求得最优路径y^{</em>}=\left(y_{1}^{<em>}, y_{2}^{</em>}, \ldots, y_{n}^{*}\right)=(1,2,1)<br>\end{aligned}<br>$$<br>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''实现条件随机场预测问题的维特比算法</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, V, VW, E, EW)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param V:是定义在节点上的特征函数，称为状态特征</span></span><br><span class="line"><span class="string">        :param VW:是V对应的权值</span></span><br><span class="line"><span class="string">        :param E:是定义在边上的特征函数，称为转移特征</span></span><br><span class="line"><span class="string">        :param EW:是E对应的权值</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.V  = V  <span class="comment">#点分布表</span></span><br><span class="line">        self.VW = VW <span class="comment">#点权值表</span></span><br><span class="line">        self.E  = E  <span class="comment">#边分布表</span></span><br><span class="line">        self.EW = EW <span class="comment">#边权值表</span></span><br><span class="line">        self.D  = [] <span class="comment">#Delta表，最大非规范化概率的局部状态路径概率</span></span><br><span class="line">        self.P  = [] <span class="comment">#Psi表，当前状态和最优前导状态的索引表s</span></span><br><span class="line">        self.BP = [] <span class="comment">#BestPath，最优路径</span></span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Viterbi</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        条件随机场预测问题的维特比算法，此算法一定要结合CRF参数化形式对应的状态路径图来理解，更容易理解.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.D = np.full(shape=(np.shape(self.V)), fill_value=<span class="number">.0</span>)</span><br><span class="line">        self.P = np.full(shape=(np.shape(self.V)), fill_value=<span class="number">.0</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment">#初始化</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> == i:</span><br><span class="line">                self.D[i] = np.multiply(self.V[i], self.VW[i])</span><br><span class="line">                self.P[i] = np.array([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">                print(<span class="string">'self.V[%d]='</span>%i, self.V[i], <span class="string">'self.VW[%d]='</span>%i, self.VW[i], <span class="string">'self.D[%d]='</span>%i, self.D[i])</span><br><span class="line">                print(<span class="string">'self.P:'</span>, self.P)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="comment">#递推求解布局最优状态路径</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> y <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">1</span>]): <span class="comment">#delta[i][y=1,2...]</span></span><br><span class="line">                    <span class="keyword">for</span> l <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">1</span>]): <span class="comment">#V[i-1][l=1,2...]</span></span><br><span class="line">                        delta = <span class="number">0.0</span></span><br><span class="line">                        delta += self.D[i<span class="number">-1</span>, l]                      <span class="comment">#前导状态的最优状态路径的概率</span></span><br><span class="line">                        delta += self.E[i<span class="number">-1</span>][l,y]*self.EW[i<span class="number">-1</span>][l,y]  <span class="comment">#前导状态到当前状体的转移概率</span></span><br><span class="line">                        delta += self.V[i,y]*self.VW[i,y]            <span class="comment">#当前状态的概率</span></span><br><span class="line">                        print(<span class="string">'(x%d,y=%d)--&gt;(x%d,y=%d):%.2f + %.2f + %.2f='</span>%(i<span class="number">-1</span>, l, i, y, \</span><br><span class="line">                              self.D[i<span class="number">-1</span>, l], \</span><br><span class="line">                              self.E[i<span class="number">-1</span>][l,y]*self.EW[i<span class="number">-1</span>][l,y], \</span><br><span class="line">                              self.V[i,y]*self.VW[i,y]), delta)</span><br><span class="line">                        <span class="keyword">if</span> <span class="number">0</span> == l <span class="keyword">or</span> delta &gt; self.D[i, y]:</span><br><span class="line">                            self.D[i, y] = delta</span><br><span class="line">                            self.P[i, y] = l</span><br><span class="line">                    print(<span class="string">'self.D[x%d,y=%d]=%.2f\n'</span>%(i, y, self.D[i,y]))</span><br><span class="line">        print(<span class="string">'self.Delta:\n'</span>, self.D)</span><br><span class="line">        print(<span class="string">'self.Psi:\n'</span>, self.P)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#返回，得到所有的最优前导状态</span></span><br><span class="line">        N = np.shape(self.V)[<span class="number">0</span>]</span><br><span class="line">        self.BP = np.full(shape=(N,), fill_value=<span class="number">0.0</span>)</span><br><span class="line">        t_range = <span class="number">-1</span> * np.array(sorted(<span class="number">-1</span>*np.arange(N)))</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> t_range:</span><br><span class="line">            <span class="keyword">if</span> N<span class="number">-1</span> == t:<span class="comment">#得到最优状态</span></span><br><span class="line">                self.BP[t] = np.argmax(self.D[<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment">#得到最优前导状态</span></span><br><span class="line">                self.BP[t] = self.P[t+<span class="number">1</span>, int(self.BP[t+<span class="number">1</span>])]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#最优状态路径表现在存储的是状态的下标，我们执行存储值+1转换成示例中的状态值</span></span><br><span class="line">        <span class="comment">#也可以不用转换，只要你能理解，self.BP中存储的0是状态1就可以~~~~</span></span><br><span class="line">        self.BP += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'最优状态路径为：'</span>, self.BP)</span><br><span class="line">        <span class="keyword">return</span> self.BP</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CRF_manual</span><span class="params">()</span>:</span>   </span><br><span class="line">    S = np.array([[<span class="number">1</span>,<span class="number">1</span>],   <span class="comment">#X1:S(Y1=1), S(Y1=2)</span></span><br><span class="line">                  [<span class="number">1</span>,<span class="number">1</span>],   <span class="comment">#X2:S(Y2=1), S(Y2=2)</span></span><br><span class="line">                  [<span class="number">1</span>,<span class="number">1</span>]])  <span class="comment">#X3:S(Y3=1), S(Y3=1)</span></span><br><span class="line">    SW = np.array([[<span class="number">1.0</span>, <span class="number">0.5</span>], <span class="comment">#X1:SW(Y1=1), SW(Y1=2)</span></span><br><span class="line">                   [<span class="number">0.8</span>, <span class="number">0.5</span>], <span class="comment">#X2:SW(Y2=1), SW(Y2=2)</span></span><br><span class="line">                   [<span class="number">0.8</span>, <span class="number">0.5</span>]])<span class="comment">#X3:SW(Y3=1), SW(Y3=1)</span></span><br><span class="line">    E = np.array([[[<span class="number">1</span>, <span class="number">1</span>],  <span class="comment">#Edge:Y1=1---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>]], <span class="comment">#Edge:Y1=2---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                  [[<span class="number">0</span>, <span class="number">1</span>],  <span class="comment">#Edge:Y2=1---&gt;(Y3=1, Y3=2) </span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">1</span>]]])<span class="comment">#Edge:Y2=2---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">    EW= np.array([[[<span class="number">0.6</span>, <span class="number">1</span>],  <span class="comment">#EdgeW:Y1=1---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0.0</span>]], <span class="comment">#EdgeW:Y1=2---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                  [[<span class="number">0.0</span>, <span class="number">1</span>],  <span class="comment">#EdgeW:Y2=1---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0.2</span>]]])<span class="comment">#EdgeW:Y2=2---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">    </span><br><span class="line">    crf = CRF(S, SW, E, EW)</span><br><span class="line">    ret = crf.Viterbi()</span><br><span class="line">    print(<span class="string">'最优状态路径为:'</span>, ret)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    CRF_manual()</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><p><img src="https://i.loli.net/2020/05/14/4ToXzPEuKs5vqCf.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;条件随机场(conditional random fields，简称 $CRF$，或$CRFs$)，是一种判别式概率模型，常用于标注或分析序列资料，如自然语言文字或是生物序列。&lt;/p&gt;
&lt;p&gt;条件随机场是条件概率分布模型P(Y|X)，表示的是给定一组输入随机变量X的条件下另一组输出随机变量Y的马尔可夫随机场，也就是说$CRF$的特点是假设输出随机变量构成马尔可夫随机场。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="http://yuanquanquan.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Neural Architecture Search</title>
    <link href="http://yuanquanquan.top/2020/20200425/"/>
    <id>http://yuanquanquan.top/2020/20200425/</id>
    <published>2020-04-25T02:57:05.000Z</published>
    <updated>2020-06-09T21:05:39.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>整理了一些最近的object detection算法<br>忽然发现。。。。<br>这是要开启NAS时代的神仙打架了嘛/facepalm</p><p>不过讲真…Google这些nas论文，计算量贼大，要卡要数据的啊……复现起来不是一点两点困难</p><p>“我是FAIR时代的残党！新时代没有能载我的模型！”（说白了就是没卡。。。。）</p></blockquote><a id="more"></a>  <p><img src="https://i.loli.net/2020/06/10/CLdEjz2VFSqgoXN.png" alt></p><p>为了紧跟时代潮流，了解一下AutoML去看了一下<strong>《Neural Architecture Search: A Survey》</strong>这篇综述</p><h1 id="序"><a href="#序" class="headerlink" title="序"></a><em>序</em></h1><p>​      深度学习模型在很多任务上都取得了不错的效果，但调参对于深度模型来说是一项非常苦难的事情，众多的超参数和网络结构参数会产生爆炸性的组合，常规的 random search 和 grid search 效率非常低，为此人们想出了自动搜索神经网络架构（Neural Architecture Search）。一方面，自动神经网络架构搜索可以遍历架构找到性能最优的架构，另一方面自动神经网络架构搜索还可以打破人类思维的局限性找到人类所想不到的架构组织方式。</p><p><img src="https://i.loli.net/2020/05/03/ogn1dueI9DtS6PL.png" alt></p><p>本文从网络架构搜索的三个方面进行了分类综述，包括：</p><ul><li><p><strong>搜索空间</strong></p></li><li><p><strong>搜索策略</strong></p></li><li><p><strong>评价预估</strong></p></li></ul><h3 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h3><p>搜索空间定义了NAS方法原则上可能发现的神经体系结构即优化问题的变量。深度学习模型的性能是由参数来控制和决定的，所以只需要对复杂模型的架构参数和对应的超参数进行优化即可。</p><p><img src="https://i.loli.net/2020/05/03/Dd9WIgzb4SacLw2.png" alt></p><p>上图是两种不同的架构空间，图片中每个节点表示神经网络中的一层，例如卷积层，池化层，不同的层由不同的颜色标注。箭头描述了数据的流向。</p><p>左侧的图像是一个链式结构空间的组块，这种结构相当于一个 N 层的序列，每一层有几种可选的算子，比如卷积、池化等，每种算子包括一些超参数，比如卷积尺寸、卷积步长等。</p><p>右侧的图像是一个拥有多分支和跳远链接的搜索空间的组块。</p><p>链式结构神经网络通过层数N，层间操作，卷积核大小步幅等超参对搜索空间进行参数化，注意：搜索空间的参数不是固定长度的，而是条件空间。</p><p>分支结构用$g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)$来表示第$i$层的输入，则分支结构的特殊情况分为：</p><ul><li>链结构网络</li></ul><p>$$<br>g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)=L_{i-1}^{o u t}<br>$$</p><ul><li>残差网络</li></ul><p>$$<br>\left(g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)=L_{i-1}^{o u t}+L_{j}^{o u t}, j&lt;i-1\right.<br>$$</p><ul><li>Densenet<br>$$<br>g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)=\operatorname{concat}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)<br>$$<br>通过以上方式分别搜索这样的组块，这样的组块又被优化为保留输入维度的normal cells和减小空间维度的reduction cell（通过使stride=2）。最后通过预定义的方式堆叠这些组块构建最终的体系结构。</li></ul><h3 id="搜索策略"><a href="#搜索策略" class="headerlink" title="搜索策略"></a><strong>搜索策略</strong></h3><p>搜索策略详细说明了如何探索搜索空间，它一方面希望快速找到性能良好的架构，另一方面，也要避免过早收敛到次优架构的区域。搜索策略包括随机搜索（RS），贝叶斯优化（BO），进化方法，强化学习（RL）和基于梯度的方法。</p><p>强化学习方法，将神经体系结构的产生视为代理动作，将搜索空间视为动作空间，将体系结构的性能评估视为奖励，不同的RL方法在表示代理策略和如何优化策略方面有所不同。Neural architecture search with reinforcement learning使用递归神经网络（RNN）策略顺序采样字符串，进而对神经体系架构进行编码，他们最初使用REINFORCE policy gradient 算法训练了该网络，但是后来的工作中（Learning transferable architecturs for scalable image recognition / Proximal policy optimization algorithms）使用了Proximal Policy Optimization；Designing neural network architectures using reinforcement learning中使用Q-learning来训练一种可依次选择层的类型和相应超参的策略。</p><p>这些方法的一个替代观点是顺序决策过程，策略采样动作顺序生成架构，环境的状态包括目前采样的动作，并且只有在最后一个动作完成后后才能获得奖励。但是，过程中没有与环境发生互动（没有观察到外部状态，也没有中间奖励），因此将体系结构采样过程解释为单个动作的顺序生成更为直观，Efficient architecture search by network transformation提出了一种相关的方法，在他们的方法中，状态是当前（经过部分训练的）架构，奖励是对该架构性能的评估，而动作对应于保留功能的突变应用，随后是网络的训练阶段，为了处理可变长度的网络体系结构，他们使用双向LSTM将体系结构编码为固定长度的表示形式，基于此编码标识，动作网络决定采样的动作，这两个组成部分的组合构成了策略，该策略使用REINFORCE policy gradient 算法进行了端到端的训练。</p><p>进化方法用进化算法优化神经架构，第一个用此方法的可以追溯到30年前用遗传算法提出架构，用反向传播优化权重。自那以后，很多神经进化算法用遗传算法同时优化神经架构和和权重，然而，当扩展到具有数百万权重的当代神经体系结构以进行监督学习任务时，基于SGD的权重优化方法胜过进化的方法。所以后来人们使用基于梯度的方法优化权重，仅仅使用进化算法优化神经架构。进化算法进化出大量模型，即一组（可能训练有素的）网络；在每个进化步骤中，至少要采样种群中的一个模型，并作为父代通过对其应用突变来生成后代。在NAS的上下文中，变异是本地操作，例如添加或删除层，更改层的超参数，添加跳远连接以及更改训练超参数。在训练后代之后，评估它们的适应度（例如，在验证集中的表现）并将其添加到种群中。</p><p>神经进化方法在采样父母，更新种群和产生后代的方式上有所不同。采样父母：锦标赛选择、使用反密度从多目标Pareto前沿对父母进行采样。更新种群：去除最差的个人、去除最老的个体、不移除个人。产生后代：随机初始化子网络、Lamarckian inheritance：知识（以学习的权重的形式）通过使用网络态射从父网络传递到子网络、让后代继承不受其突变影响的父代所有参数。</p><p>Aging Evolution for Image Classifier Architecture Search比较了RL,evolution和random search方法，RL和进化在最终测试准确性方面表现均相当好，进化在任何时候都有更好的性能，并且找到更小的模型。在他们的实验中，这两种方法始终比RS表现更好，但幅度很小。</p><p>贝叶斯优化方法是最流行的超参数优化方法之一，但由于典型的BO工具箱基于高斯过程和专注于低维连续优化问题。Kernels for bayesian optimization in conditional parameter space和Neural architecture search with bayesian optimisation and optimal transport派生了用于架构搜索空间的内核函数，以便使用基于GP的经典BO方法。另外、一些作品使用基于树的模型，以达到在各种问题上有效地搜索高维条件空间并实现最先进的性能，共同优化神经体系结构及其超参数。尽管缺乏全面的比较，但初步证据表明这些方法也可以胜过进化算法。</p><p>Automatically Designing and Training Deep Architectures和Finding Competitive Network Architectures Within a Day Using UCT利用其搜索空间的树结构并使用了蒙特卡洛树搜索。Simple And Efficient Architecture Search for Convolutional Neural Networks提出了一种简单但性能良好的爬山算法，该算法通过贪婪地朝性能更好的架构的方向移动而发现高质量的架构，而无需更复杂的探索机制。上述方法采用离散搜索空间。DARTS提出了continuous relaxation来直接基于梯度优化operation的权重。SNAS,ProxylessNAS没有优化可能操作的权重α,而是建议在可能的操作上优化参数化分布。Differentiable neural network architecture search以及Ahmed和Maskconnect: Connextivity learning by gradient descent还采用了基于梯度的神经体系结构优化，但是分别专注于优化层超参数或连接模式。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;整理了一些最近的object detection算法&lt;br&gt;忽然发现。。。。&lt;br&gt;这是要开启NAS时代的神仙打架了嘛/facepalm&lt;/p&gt;
&lt;p&gt;不过讲真…Google这些nas论文，计算量贼大，要卡要数据的啊……复现起来不是一点两点困难&lt;/p&gt;
&lt;p&gt;“我是FAIR时代的残党！新时代没有能载我的模型！”（说白了就是没卡。。。。）&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
</feed>
