<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yuanquanquan的个人博客 | 我愿做你光华中淡淡的一笔</title>
  
  <subtitle>我愿做你光华中淡淡的一笔</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanquanquan.top/"/>
  <updated>2020-07-21T06:38:39.854Z</updated>
  <id>http://yuanquanquan.top/</id>
  
  <author>
    <name>理科生写给世界的情书</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>APDS-9960 RGB and Gesture Sensor</title>
    <link href="http://yuanquanquan.top/2020/20200720/"/>
    <id>http://yuanquanquan.top/2020/20200720/</id>
    <published>2020-07-20T00:19:47.000Z</published>
    <updated>2020-07-21T06:38:39.854Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>通过APDS-9960手势识别芯片在Arduino上实现手势识别。</p></blockquote><a id="more"></a><p> <img src="https://i.loli.net/2020/07/20/V7QfZtiyl1X3kOj.png" alt></p><h1 id="1-HARDWARE"><a href="#1-HARDWARE" class="headerlink" title="1. HARDWARE"></a>1. HARDWARE</h1><p>· Osoyoo UNO Board (Fully compatible with Arduino UNO rev.3) x 1</p><p>· APDS-9960 RGB and Gesture Sensor x 1</p><p>· Jumpers</p><p>· USB Cable x 1</p><p>· PC x 1</p><h1 id="2-SOFTWARE"><a href="#2-SOFTWARE" class="headerlink" title="2. SOFTWARE"></a>2. SOFTWARE</h1><p>· Arduino IDE (version 1.6.4+)</p><p>· Arduino library: <a href="https://codeload.github.com/adafruit/Adafruit_APDS9960/zip/master" target="_blank" rel="noopener">APDS9960.h</a></p><h1 id="3-About-APDS-9960-RGB-and-Gesture-Sensor"><a href="#3-About-APDS-9960-RGB-and-Gesture-Sensor" class="headerlink" title="3. About APDS-9960 RGB and Gesture Sensor"></a>3. About APDS-9960 RGB and Gesture Sensor</h1><p><img src="https://i.loli.net/2020/07/20/6fex8zmILMlo5Ua.png" alt> </p><p>This is the  RGB and Gesture Sensor, a small breakout board with a built in APDS-9960 sensor that offers ambient light and color measuring, proximity detection, and touchless gesture sensing. With this RGB and Gesture Sensor you will be able to control a computer, microcontroller, robot, and more with a simple swipe of your hand! This is, in fact, the same sensor that the Samsung Galaxy S5 uses and is probably one of the best gesture sensors on the market for the price.</p><p> <img src="https://i.loli.net/2020/07/20/Qt2qSDv4sdORk7o.png" alt></p><p>The APDS-9960 is a serious little piece of hardware with built in UV and IR blocking filters, four separate diodes sensitive to different directions, and an I2C compatible interface. For your convenience we have broken out the following pins: VL (optional power to IR LED), GND (Ground), VCC (power to APDS-9960 sensor), SDA (I2C data), SCL (I2C clock), and INT (interrupt). Each APDS-9960 also has a detection range of 4 to 8 inches (10 to 20 cm).</p><p><img src="https://i.loli.net/2020/07/20/Rtq621F7UxkYDlH.png" alt> </p><h2 id="3-1-PIN-DESCRIPTIONS"><a href="#3-1-PIN-DESCRIPTIONS" class="headerlink" title="3.1 PIN DESCRIPTIONS"></a>3.1 PIN DESCRIPTIONS</h2><table><thead><tr><th><strong><em>\</em>Pin Label**</strong></th><th><strong><em>\</em>Description**</strong></th></tr></thead><tbody><tr><td>VL</td><td>Optional power to the IR LED if PS jumper is disconnected. Must be 3.0 - 4.5V</td></tr><tr><td>GND</td><td>Connect to ground.</td></tr><tr><td>VCC</td><td>Used to power the APDS-9960 sensor. Must be 2.4 - 3.6V</td></tr><tr><td>SDA</td><td>I2C data</td></tr><tr><td>SCL</td><td>I2C clock</td></tr><tr><td>INT</td><td>External interrupt pin. Active LOW on interrupt event</td></tr></tbody></table><h2 id="3-2-FEATURES"><a href="#3-2-FEATURES" class="headerlink" title="3.2 FEATURES:"></a>3.2 FEATURES:</h2><p>· Model: GY-APDS 9960-3.3</p><p>· Using chip: APDS-9960</p><p>· Operational Voltage: 3.3V</p><p>· Ambient Light &amp; RGB Color Sensing</p><p>· Proximity Sensing</p><p>· Gesture Detection</p><p>· Operating Range: 4-8in (10-20cm)</p><p>· I2C Interface (I2C Address: 0x39)</p><p>· Size: 20mm * 15.3mm</p><p><strong>Recommended Reading</strong></p><p>Before getting started with the APDS-9960, there are a few concepts that you should be familiar with. Consider <a href="http://osoyoo.com/2017/09/21/osoyoo-advanced-kit-for-arduino/" target="_blank" rel="noopener">reading some of these tutorials before continuing.</a></p><h2 id="3-3-SETTING-THE-JUMPERS"><a href="#3-3-SETTING-THE-JUMPERS" class="headerlink" title="3.3 SETTING THE JUMPERS"></a>3.3 SETTING THE JUMPERS</h2><p><img src="https://i.loli.net/2020/07/20/Qlw6Am5xdPqIeiW.jpg" alt> </p><p>On the front of the breakout board are 2 solder jumpers:</p><p><strong>·</strong> <strong><em>\</em>PS**</strong> – This jumper connects the power supplies of the sensor and IR LED (also located on the APDS-9960) together. When the jumper is closed (i.e. connected), you only need to supply power to the VCC pin to power both the sensor and the IR LED. If the jumper is open, you need to provide power to both the VCC (2.4 - 3.6V) and VL (3.0 - 4.5V) pins separately. This jumper is <strong><em>\</em>closed**</strong> by default.</p><p><strong>·</strong> <strong><em>\</em>I2C PU**</strong> – This is a 3-way solder jumper that is used to connect and disconnect the I2C pullup resistors. By default, this jumper is <strong><em>\</em>closed**</strong>, which means that both SDA and SCL lines have connected pullup resistors on the breakout board. Use some solder wick to open the jumper if you do not need the pullup resistors (e.g. you have pullup resistors that are located on the I2C bus somewhere else).</p><h2 id="3-4-HARDWARE-HOOKUP"><a href="#3-4-HARDWARE-HOOKUP" class="headerlink" title="3.4 HARDWARE HOOKUP"></a>3.4 HARDWARE HOOKUP</h2><h3 id="3-4-1-Add-Headers"><a href="#3-4-1-Add-Headers" class="headerlink" title="3.4.1 Add Headers"></a>3.4.1 Add Headers</h3><p>Solder a row of break away male headers to the 6 headers holes on the board.</p><p><img src="https://i.loli.net/2020/07/20/eVvx14DpdJnFG8M.jpg" alt> </p><h3 id="3-4-2-Connect-the-Breakout-Board"><a href="#3-4-2-Connect-the-Breakout-Board" class="headerlink" title="3.4.2 Connect the Breakout Board"></a>3.4.2 Connect the Breakout Board</h3><p>We will be using the Arduino Pro’s regulated 3.3V power and I2C bus with the APDS-9960. Note that we are leaving VL on the breakout board unconnected. IMPORTANT: You must use 3.3V! If you try to use a 5V power supply  you risk damaging the APDS-9960. Connect the breakout board to the following pins on the Arduino:</p><table><thead><tr><th><strong><em>\</em>APDS-9960 Breakout Board**</strong></th><th><strong><em>\</em>OSOYOO UNO**</strong></th></tr></thead><tbody><tr><td>GND</td><td>GND</td></tr><tr><td>VCC</td><td>3.3V</td></tr><tr><td>SDA</td><td>A4</td></tr><tr><td>SCL</td><td>A5</td></tr></tbody></table><p><strong><em>\</em>NOTE:**</strong></p><p>· Connect the <strong><em>\</em>SCL**</strong> pin to the I2C clock <strong><em>\</em>SCL**</strong> pin on your Arduino. On an UNO &amp; ‘328 based Arduino, this is also known as <strong><em>\</em>A5**</strong>, on a Mega it is also known as <strong><em>\</em>digital 21**</strong> and on a Leonardo/Micro, <strong><em>\</em>digital 3**</strong></p><p>· Connect the <strong><em>\</em>SDA**</strong> pin to the I2C data <strong><em>\</em>SDA**</strong> pin on your Arduino. On an UNO &amp; ‘328 based Arduino, this is also known as <strong><em>\</em>A4**</strong>, on a Mega it is also known as <strong><em>\</em>digital 20**</strong> and on a Leonardo/Micro, <strong><em>\</em>digital 2**</strong></p><p><img src="https://i.loli.net/2020/07/20/Gutm3UWAv95jQgN.png" alt></p><h2 id="3-5-ARDUINO-LIBRARY-INSTALLATION"><a href="#3-5-ARDUINO-LIBRARY-INSTALLATION" class="headerlink" title="3.5 ARDUINO LIBRARY INSTALLATION"></a>3.5 ARDUINO LIBRARY INSTALLATION</h2><p>To use the APDS-9960, you will need some supporting software. If you are using an Arduino, then you are in luck! We created an Arduino library that makes the APDS-9960 easy to use. Click the button below to download the latest version of the APDS-9960 breakout board project, which includes the Arduino library. <a href="https://codeload.github.com/adafruit/Adafruit_APDS9960/zip/master" target="_blank" rel="noopener">DOWNLOAD THE PROJECT FILES!</a> Follow <a href="http://osoyoo.com/2017/05/08/how-to-install-additional-arduino-libraries/" target="_blank" rel="noopener">this guide on installing Arduino libraries</a> to install the files within the APDS9960 directory as an Arduino library. </p><h1 id="Gesture-Sensing-Example"><a href="#Gesture-Sensing-Example" class="headerlink" title="\Gesture Sensing Example**"></a><strong><em>\</em>Gesture Sensing Example**</strong></h1><h1 id="4-UPLOAD-SKETCH"><a href="#4-UPLOAD-SKETCH" class="headerlink" title="4. UPLOAD SKETCH"></a>4. UPLOAD SKETCH</h1><p>After above operations are completed, connect the Arduino board to your computer using the USB cable. The green power LED (labelled <strong><em>\</em>PWR**</strong>) should go on. </p><h1 id="5-CODE-PROGRAM"><a href="#5-CODE-PROGRAM" class="headerlink" title="5.CODE PROGRAM"></a>5.CODE PROGRAM</h1><p>You can copy below code to your Arduino IDE window, then select corresponding board type and port type for your Arduino board. </p><p><img src="https://i.loli.net/2020/07/20/FEjCHOW8l5XB4Nb.png" alt></p><h1 id="6-Running-Result"><a href="#6-Running-Result" class="headerlink" title="6.Running Result"></a>6.Running Result</h1><p>Click the Upload button and wait for the program to finish uploading to the Arduino. Once uploaded to your Adruino, open up the serial monitor at 115200 baud speed.More info on the Serial Terminal can be found <a href="http://osoyoo.com/2017/07/06/arduino-lesson-the-serial-monitor/" target="_blank" rel="noopener">here</a>. You should see a messages noting that “Device initialized! ” Hover your hand 4 to 8 inches (10 to 20 cm) above the sensor but off to one side (i.e. not directly above the sensor). While maintaining the same height, swipe your hand over the sensor (into and then immediately out of range of the sensor). If you move too fast, the sensor will not recognize the gesture.</p><p><img src="https://i.loli.net/2020/07/20/hHpdYazO9xjXJ7w.jpg" alt> </p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;通过APDS-9960手势识别芯片在Arduino上实现手势识别。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="-手势识别" scheme="http://yuanquanquan.top/tags/%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Arcore-Android</title>
    <link href="http://yuanquanquan.top/2020/20200706/"/>
    <id>http://yuanquanquan.top/2020/20200706/</id>
    <published>2020-07-06T07:56:07.000Z</published>
    <updated>2020-07-06T08:50:03.794Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ARCore 是 Google 为开发者构建的增强现实平台，如何让虚拟物体和真实世界完美融合，这一直是 Google ARCore 技术所探讨的问题。众所周知，当虚拟物体附近有现实物体时，有可能会出现互相交融、重叠等效果，大大地影响了用户体验。这一直是 AR 技术的难点，也是 Google 不懈努力的方向。</p></blockquote><a id="more"></a>  <h1 id="1-Quickstart-for-Android"><a href="#1-Quickstart-for-Android" class="headerlink" title="1. Quickstart for Android"></a>1. Quickstart for Android</h1><p><img src="https://developers.google.cn/ar/develop/java/images/android-studio.png" alt></p><h2 id="1-2-Set-up-your-development-environment"><a href="#1-2-Set-up-your-development-environment" class="headerlink" title="1.2 Set up your development environment"></a>1.2 Set up your development environment</h2><ul><li>Install <a href="https://developer.android.google.cn/studio/index.html" target="_blank" rel="noopener">Android Studio</a> version 3.1 or higher with Android SDK Platform version 7.0 (API level 24) or higher.</li><li>You will need a basic understanding of Android development. If you are new to Android, see <a href="https://developer.android.google.cn/training/basics/firstapp/index.html" target="_blank" rel="noopener">Building your first Android app for beginners</a>.</li></ul><h2 id="1-3-Open-the-sample-project"><a href="#1-3-Open-the-sample-project" class="headerlink" title="1.3 Open the sample project"></a>1.3 Open the sample project</h2><p>This quickstart uses <a href="https://en.wikipedia.org/wiki/OpenGL" target="_blank" rel="noopener">OpenGL</a>, a programming interface for rendering 2D and 3D vector graphics. Review the <a href="https://developers.google.cn/ar/develop/java/enable-arcore" target="_blank" rel="noopener">Enable ARCore</a> documentation before getting started with the steps below.</p><p>Get the sample project by cloning the repository with the following command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/google-ar/arcore-android-sdk.git</span><br></pre></td></tr></table></figure><p>In Android Studio, open the <strong>HelloAR</strong> sample project, located in the <strong>samples</strong> subdirectory within the <code>arcore-android-sdk</code> directory.</p><h2 id="1-4-Prepare-your-device-or-emulator"><a href="#1-4-Prepare-your-device-or-emulator" class="headerlink" title="1.4 Prepare your device or emulator"></a>1.4 Prepare your device or emulator</h2><p>You can run AR apps on a <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">supported device</a> or in the Android Emulator:</p><ul><li>In the emulator, you must sign into the Google Play Store or <a href="https://developers.google.cn/ar/develop/java/emulator#update-arcore" target="_blank" rel="noopener">update Google Play Services for AR</a> manually.</li></ul><h2 id="1-5-Run-the-sample"><a href="#1-5-Run-the-sample" class="headerlink" title="1.5 Run the sample"></a>1.5 Run the sample</h2><p>Make sure your Android device is connected to the development machine and click <strong>Run</strong> <img src="https://developers.google.cn/ar/develop/java/images/toolbar-run.png" alt="img"> in Android Studio. Then, choose your device as the deployment target and click <strong>OK</strong>.</p><p><img src="https://developers.google.cn/ar/develop/java/images/deployment-target.png" alt></p><p>Android Studio builds your project into a debuggable APK, installs the APK, and then runs the app on your device. For more information, see <a href="https://developer.android.google.cn/studio/run/index.html" target="_blank" rel="noopener">Build and Run Your App</a>.</p><p>You may be prompted to install or update <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> if it is missing or out of date. Select <strong>CONTINUE</strong> to install it from Google Play Store.</p><p>The <strong>HelloAR</strong> app lets you place and manipulate Android figurines on detected AR plane surfaces. It is implemented with <a href="https://developer.android.google.cn/reference/android/opengl/GLSurfaceView" target="_blank" rel="noopener">Android GL SurfaceView</a>, which is used to render the camera preview and basic AR objects such as Planes and Anchors. <strong>HelloAR</strong>‘s render can be found <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples/hello_ar_java/app/src/main/java/com/google/ar/core/examples/java/common/rendering" target="_blank" rel="noopener">here</a>.</p><p><img src="https://developers.google.cn/ar/develop/java/images/helloar-demo.jpg" alt></p><p><strong>Note:</strong> The lifecycle methods in <strong>HelloAR</strong> are different than those normally found in OpenGL applications. To ensure the correct AR setup for your own applications, follow the lifecycle management logic in <strong>HelloAR</strong>.</p><h2 id="1-6-Next-steps"><a href="#1-6-Next-steps" class="headerlink" title="1.6 Next steps"></a>1.6 Next steps</h2><ul><li>Try building and running other <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples" target="_blank" rel="noopener">sample projects</a> in the ARCore SDK.</li><li>Learn how to <a href="https://developers.google.cn/ar/develop/java/enable-arcore" target="_blank" rel="noopener">Enable ARCore</a> in your app.</li><li>Use <a href="https://developers.google.cn/ar/develop/java/augmented-images" target="_blank" rel="noopener">Augmented Images</a> to build apps that can respond to 2D images, such as posters or logos, in the user’s environment.</li><li>Use <a href="https://developers.google.cn/ar/develop/java/cloud-anchors/cloud-anchors-overview-android" target="_blank" rel="noopener">Cloud Anchors</a> to create shared AR experiences across iOS and Android users.</li><li>Review the <a href="https://developers.google.cn/ar/develop/developer-guides/runtime-considerations" target="_blank" rel="noopener">Runtime Considerations</a>.</li><li>Review the <a href="https://developers.google.cn/ar/develop/developer-guides/design-guidelines" target="_blank" rel="noopener">Design Guidelines</a>.</li></ul><h1 id="2-Enable-ARCore"><a href="#2-Enable-ARCore" class="headerlink" title="2. Enable ARCore"></a>2. Enable ARCore</h1><p>This page describes how to enable ARCore functionality in your Android Studio projects. To do this, you need to:</p><ol><li><a href="https://developers.google.cn/ar/develop/java/enable-arcore#manifest" target="_blank" rel="noopener">Add AR Required or AR Optional entries to the manifest</a></li><li><a href="https://developers.google.cn/ar/develop/java/enable-arcore#dependencies" target="_blank" rel="noopener">Add build dependencies</a> to your project</li><li><a href="https://developers.google.cn/ar/develop/java/enable-arcore#runtime" target="_blank" rel="noopener">Perform runtime checks</a> to ensure the device is <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore-supported</a>, that Google Play Services for AR is installed on it, and that camera permission has been granted</li><li>Make sure your app complies with ARCore’s <a href="https://developers.google.cn/ar/distribute/privacy-requirements" target="_blank" rel="noopener">User Privacy Requirements</a></li></ol><h2 id="2-1-Using-Google-Play-Services-for-AR-to-enable-ARCore-functionality"><a href="#2-1-Using-Google-Play-Services-for-AR-to-enable-ARCore-functionality" class="headerlink" title="2.1 Using Google Play Services for AR to enable ARCore functionality"></a>2.1 Using Google Play Services for AR to enable ARCore functionality</h2><p>ARCore SDKs make AR features available on <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore supported devices</a> that have <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> installed. Users can install and update Google Play Services for AR from the Google Play Store.</p><h2 id="2-2-Add-AR-Required-or-AR-Optional-entries-to-the-manifest"><a href="#2-2-Add-AR-Required-or-AR-Optional-entries-to-the-manifest" class="headerlink" title="2.2 Add AR Required or AR Optional entries to the manifest"></a>2.2 Add AR Required or AR Optional entries to the manifest</h2><p>An app that supports AR features can be configured in two ways: <strong>AR Required</strong> and <strong>AR Optional</strong>.</p><h3 id="2-2-1-AR-Required-apps"><a href="#2-2-1-AR-Required-apps" class="headerlink" title="2.2.1 AR Required apps"></a>2.2.1 AR Required apps</h3><p>To be usable, an <em>AR Required</em> app requires an <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore Supported Device</a> that has <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> installed on it.</p><ul><li>The Google Play Store makes AR Required apps available only on <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">devices that support ARCore</a>.</li><li>When users install an AR Required app, the Google Play Store automatically installs <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a>. However, your app must still perform additional <a href="https://developers.google.cn/ar/develop/java/enable-arcore#runtime" target="_blank" rel="noopener">runtime checks</a> in case Google Play Services for AR must be updated or has been manually uninstalled.</li></ul><p>For more information, see <a href="https://developers.google.cn/ar/distribute" target="_blank" rel="noopener">Publishing AR Apps in the Google Play Store</a>.</p><p>To declare your app to be <em>AR Required</em>, modify your <code>AndroidManifest.xml</code> to include the following entries:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;uses-permission android:name=&quot;android.permission.CAMERA&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Limits app visibility in the Google Play Store to ARCore supported devices</span><br><span class="line">     (https://developers.google.com/ar/discover/supported-devices). --&gt;</span><br><span class="line">&lt;uses-feature android:name=&quot;android.hardware.camera.ar&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;application …&gt;</span><br><span class="line">    …</span><br><span class="line"></span><br><span class="line">  &lt;!-- &quot;AR Required&quot; app, requires &quot;Google Play Services for AR&quot; (ARCore)</span><br><span class="line">       to be installed, as the app does not include any non-AR features. --&gt;</span><br><span class="line">    &lt;meta-data android:name=&quot;com.google.ar.core&quot; android:value=&quot;required&quot; /&gt;</span><br><span class="line">&lt;/application&gt;</span><br></pre></td></tr></table></figure><p>Then, modify your app’s <code>build.gradle</code> to specify a <code>minSdkVersion</code> of at least 24:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">android &#123;  defaultConfig &#123;    …    minSdkVersion 24  &#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-2-AR-Optional-apps"><a href="#2-2-2-AR-Optional-apps" class="headerlink" title="2.2.2 AR Optional apps"></a>2.2.2 AR Optional apps</h3><p>An <em>AR Optional</em> app has optional AR features, which are activated only on <a href="https://developers.google.cn/ar/discover/supported-devices" target="_blank" rel="noopener">ARCore supported devices</a> that have <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> installed.</p><ul><li>AR Optional apps can be installed and run on devices that don’t support ARCore.</li><li>When users install an AR Optional app, the Google Play Store will <em>not</em> automatically install <a href="https://play.google.com/store/apps/details?id=com.google.ar.core" target="_blank" rel="noopener">Google Play Services for AR</a> with the app.</li></ul><p>To declare your app to be <em>AR Optional</em>, modify your <code>AndroidManifest.xml</code> to include the following entries:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;uses-permission android:name=&quot;android.permission.CAMERA&quot; /&gt;</span><br><span class="line"></span><br><span class="line">&lt;application …&gt;</span><br><span class="line">    …</span><br><span class="line"></span><br><span class="line">    &lt;!-- &quot;AR Optional&quot; app, contains non-AR features that can be used when</span><br><span class="line">         &quot;Google Play Services for AR&quot; (ARCore) is not available. --&gt;</span><br><span class="line">    &lt;meta-data android:name=&quot;com.google.ar.core&quot; android:value=&quot;optional&quot; /&gt;</span><br><span class="line">&lt;/application&gt;</span><br></pre></td></tr></table></figure><p>Then, modify your app’s <code>build.gradle</code> to specify a <code>minSdkVersion</code> of at least 14:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">android &#123;</span><br><span class="line">    defaultConfig &#123;</span><br><span class="line">        …</span><br><span class="line">        minSdkVersion 14</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-3-Add-build-dependencies"><a href="#2-3-Add-build-dependencies" class="headerlink" title="2.3 Add build dependencies"></a>2.3 Add build dependencies</h2><p>To add ARCore to your Android Studio project, perform these steps:</p><ul><li><p>Make sure your <strong>project’s</strong> <code>build.gradle</code> file includes Google’s Maven repository:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">allprojects &#123;</span><br><span class="line">    repositories &#123;</span><br><span class="line">        google()</span><br><span class="line">        …</span><br></pre></td></tr></table></figure></li><li><p>Add the latest ARCore library as a dependency in your <strong>app’s</strong> <code>build.gradle</code> file:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dependencies &#123;</span><br><span class="line">    …</span><br><span class="line">    implementation &apos;com.google.ar:core:1.18.0&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-4-Perform-runtime-checks"><a href="#2-4-Perform-runtime-checks" class="headerlink" title="2.4 Perform runtime checks"></a>2.4 Perform runtime checks</h2><h3 id="2-4-1-Check-whether-ARCore-is-supported-AR-Optional-apps-only"><a href="#2-4-1-Check-whether-ARCore-is-supported-AR-Optional-apps-only" class="headerlink" title="2.4.1 Check whether ARCore is supported (AR Optional apps only)"></a>2.4.1 Check whether ARCore is supported (<em>AR Optional</em> apps only)</h3><p><em>AR Optional</em> apps can use <code>ArCoreApk.checkAvailability()</code> to determine if the current device supports ARCore. On devices that do not support ARCore, AR Optional apps should disable AR related functionality and hide associated UI elements:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">protected void onCreate(Bundle savedInstanceState) &#123;</span><br><span class="line">  super.onCreate(savedInstanceState);</span><br><span class="line"></span><br><span class="line">  // Enable AR related functionality on ARCore supported devices only.</span><br><span class="line">  maybeEnableArButton();</span><br><span class="line">  …</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void maybeEnableArButton() &#123;</span><br><span class="line">  ArCoreApk.Availability availability = ArCoreApk.getInstance().checkAvailability(this);</span><br><span class="line">  if (availability.isTransient()) &#123;</span><br><span class="line">    // Re-query at 5Hz while compatibility is checked in the background.</span><br><span class="line">    new Handler().postDelayed(new Runnable() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      public void run() &#123;</span><br><span class="line">        maybeEnableArButton();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, 200);</span><br><span class="line">  &#125;</span><br><span class="line">  if (availability.isSupported()) &#123;</span><br><span class="line">    mArButton.setVisibility(View.VISIBLE);</span><br><span class="line">    mArButton.setEnabled(true);</span><br><span class="line">    // indicator on the button.</span><br><span class="line">  &#125; else &#123; // Unsupported or unknown.</span><br><span class="line">    mArButton.setVisibility(View.INVISIBLE);</span><br><span class="line">    mArButton.setEnabled(false);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Note, <code>checkAvailability()</code> may need to query network resources to determine whether the device supports ARCore. During this time, it will return <code>UNKNOWN_CHECKING</code>. To reduce the perceived latency and pop-in, apps should call <code>checkAvailability()</code> once early in it’s life cycle to initiate the query, ignoring the returned value. This way a cached result will be available immediately when <code>maybeEnableArButton()</code> is called.</p><p>This flowchart illustrates the logic in the preceding code sample:</p><p><img src="https://developers.google.cn/ar/images/check-availability-flowchart.png" alt></p><h3 id="2-4-2-Request-camera-permission-AR-Optional-and-AR-Required-apps"><a href="#2-4-2-Request-camera-permission-AR-Optional-and-AR-Required-apps" class="headerlink" title="2.4.2 Request camera permission (AR Optional and AR Required apps)"></a>2.4.2 Request camera permission (<em>AR Optional</em> and <em>AR Required</em> apps)</h3><p>Both <em>AR Optional</em> and <em>AR Required</em> apps must ensure that the camera permission has been granted before creating an AR Session. The <strong>hello_ar_java</strong> sample includes a <a href="https://github.com/google-ar/arcore-android-sdk/search?q=CameraPermissionHelper.java" target="_blank" rel="noopener"><code>CameraPermissionHelper</code></a> class which can be copied into your project and should be called from your AR activity’s <code>onResume()</code> method:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">protected void onResume() &#123;</span><br><span class="line">  super.onResume();</span><br><span class="line"></span><br><span class="line">  // ARCore requires camera permission to operate.</span><br><span class="line">  if (!CameraPermissionHelper.hasCameraPermission(this)) &#123;</span><br><span class="line">    CameraPermissionHelper.requestCameraPermission(this);</span><br><span class="line">    return;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Your AR activity must also implement <code>onRequestPermissionsResult(…)</code>, as seen in <a href="https://github.com/google-ar/arcore-android-sdk/search?q=HelloArActivity.java" target="_blank" rel="noopener"><code>HelloArActivity</code></a>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void onRequestPermissionsResult(int requestCode, String[] permissions, int[] results) &#123;</span><br><span class="line">  if (!CameraPermissionHelper.hasCameraPermission(this)) &#123;</span><br><span class="line">    Toast.makeText(this, &quot;Camera permission is needed to run this application&quot;, Toast.LENGTH_LONG)</span><br><span class="line">        .show();</span><br><span class="line">    if (!CameraPermissionHelper.shouldShowRequestPermissionRationale(this)) &#123;</span><br><span class="line">      // Permission denied with checking &quot;Do not ask again&quot;.</span><br><span class="line">      CameraPermissionHelper.launchPermissionSettings(this);</span><br><span class="line">    &#125;</span><br><span class="line">    finish();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-3-Check-whether-Google-Play-Services-for-AR-is-installed-AR-Optional-and-AR-Required-apps"><a href="#2-4-3-Check-whether-Google-Play-Services-for-AR-is-installed-AR-Optional-and-AR-Required-apps" class="headerlink" title="2.4.3 Check whether Google Play Services for AR is installed (AR Optional and AR Required apps)"></a>2.4.3 Check whether Google Play Services for AR is installed (<em>AR Optional</em> and <em>AR Required</em> apps)</h3><p>To check whether a compatible version of Google Play Services for AR is installed, apps must also call <code>ArCoreApk.requestInstall()</code> before creating an ARCore session. This prompts the user to install or update the service if needed.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">// Set to true ensures requestInstall() triggers installation if necessary.</span><br><span class="line">private boolean mUserRequestedInstall = true;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">protected void onResume() &#123;</span><br><span class="line">  super.onResume();</span><br><span class="line"></span><br><span class="line">  // Check camera permission.</span><br><span class="line">  …</span><br><span class="line"></span><br><span class="line">  // Make sure Google Play Services for AR is installed and up to date.</span><br><span class="line">  try &#123;</span><br><span class="line">    if (mSession == null) &#123;</span><br><span class="line">      switch (ArCoreApk.getInstance().requestInstall(this, mUserRequestedInstall)) &#123;</span><br><span class="line">        case INSTALLED:</span><br><span class="line">          // Success, create the AR session.</span><br><span class="line">          mSession = new Session(this);</span><br><span class="line">          break;</span><br><span class="line">        case INSTALL_REQUESTED:</span><br><span class="line">          // Ensures next invocation of requestInstall() will either return</span><br><span class="line">          // INSTALLED or throw an exception.</span><br><span class="line">          mUserRequestedInstall = false;</span><br><span class="line">          return;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch (UnavailableUserDeclinedInstallationException e) &#123;</span><br><span class="line">    // Display an appropriate message to the user and return gracefully.</span><br><span class="line">    Toast.makeText(this, &quot;TODO: handle exception &quot; + e, Toast.LENGTH_LONG)</span><br><span class="line">        .show();</span><br><span class="line">    return;</span><br><span class="line">  &#125; catch (…) &#123;  // Current catch statements.</span><br><span class="line">    …</span><br><span class="line">    return;  // mSession is still null.</span><br><span class="line">  &#125;</span><br><span class="line">  …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This flowchart illustrates the logic in the preceding code sample:</p><p><img src="upload/image-20200706161631230.png" alt></p><p>If <code>requestInstall()</code> returns <code>INSTALL_REQUESTED</code>, the current activity pauses and the user is prompted to install or update Google Play Services for AR:</p><p><img src="https://developers.google.cn/ar/images/request-install-prompt.png" alt></p><p>The activity’s <code>onResume()</code> executes again once the user returns to the activity.</p><h2 id="2-5-Compliance-with-User-Privacy-Requirements"><a href="#2-5-Compliance-with-User-Privacy-Requirements" class="headerlink" title="2.5 Compliance with User Privacy Requirements"></a>2.5 Compliance with User Privacy Requirements</h2><p>Make sure your app complies with ARCore’s <a href="https://developers.google.cn/ar/distribute/privacy-requirements" target="_blank" rel="noopener">User Privacy Requirements</a>.</p><h2 id="2-6-Next-steps"><a href="#2-6-Next-steps" class="headerlink" title="2.6 Next steps"></a>2.6 Next steps</h2><ul><li>Read the code and comments in the <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples/hello_ar_java" target="_blank" rel="noopener">hello_ar_java</a> sample</li><li>Review the <a href="https://developers.google.cn/ar/reference/java" target="_blank" rel="noopener">Java API Reference</a></li></ul><h1 id="3-Run-AR-Apps-in-Android-Emulator"><a href="#3-Run-AR-Apps-in-Android-Emulator" class="headerlink" title="3. Run AR Apps in Android Emulator"></a>3. Run AR Apps in Android Emulator</h1><p>Use the <a href="https://developer.android.google.cn/studio/run/emulator.html" target="_blank" rel="noopener">Android Emulator</a> to test AR scenarios without a physical device. The Android Emulator lets you run ARCore apps in a virtual environment with an emulated device that you control.</p><p><strong>Warning:</strong> The Android Emulator does not support ARCore APIs for Depth, Augmented Faces, or Augmented Images. When any of these features are enabled, the camera preview image does not render correctly: the GPU camera texture is entirely black, although UI elements drawn on top of the preview image still render correctly.</p><h2 id="3-1-Set-up-your-development-environment"><a href="#3-1-Set-up-your-development-environment" class="headerlink" title="3.1 Set up your development environment"></a>3.1 Set up your development environment</h2><p>Software requirements:</p><ul><li><a href="https://developer.android.google.cn/studio/" target="_blank" rel="noopener">Android Studio</a> <strong>3.1</strong> or later.</li><li><a href="https://developer.android.google.cn/studio/run/emulator.html#Requirements" target="_blank" rel="noopener">Android Emulator</a> <strong>27.2.9</strong> or later.</li></ul><h2 id="3-2-Get-Android-Studio-and-SDK-tools-for-ARCore"><a href="#3-2-Get-Android-Studio-and-SDK-tools-for-ARCore" class="headerlink" title="3.2 Get Android Studio and SDK tools for ARCore"></a>3.2 Get Android Studio and SDK tools for ARCore</h2><ol><li><p>Install <a href="https://developer.android.google.cn/studio/" target="_blank" rel="noopener">Android Studio</a> 3.1 or later.</p></li><li><p>In Android Studio, go to <strong>Preferences &gt; Appearance and Behavior &gt; System Settings &gt; Android SDK</strong>.</p></li><li><p>Select the <strong>SDK Platforms</strong> tab and check <strong>Show Package Details</strong>.</p><p>Under <strong>Android 8.1 (Oreo)</strong>, select:<br><strong>Google APIs Intel x86 Atom System Image</strong> API Level 27, version 4 or later.</p></li><li><p>Select the <strong>SDK Tools</strong> tab and add <strong>Android Emulator</strong> 27.2.9 or later.</p></li><li><p>Click <strong>OK</strong> to install the selected packages and tools.</p></li><li><p>Click <strong>OK</strong> again to confirm changes.</p></li><li><p>Accept the license agreement for the Component Installer.</p></li><li><p>Click <strong>Finish</strong>.</p></li></ol><h2 id="3-3-Create-a-virtual-device-with-AR-support"><a href="#3-3-Create-a-virtual-device-with-AR-support" class="headerlink" title="3.3 Create a virtual device with AR support"></a>3.3 Create a virtual device with AR support</h2><p>For more information, see the Android Studio instructions to <a href="https://developer.android.google.cn/studio/run/managing-avds.html#createavd" target="_blank" rel="noopener">Create a Virtual Device</a>.</p><h3 id="3-3-1-Create-a-new-Android-Virtual-Device-AVD"><a href="#3-3-1-Create-a-new-Android-Virtual-Device-AVD" class="headerlink" title="3.3.1 Create a new Android Virtual Device (AVD)"></a>3.3.1 Create a new Android Virtual Device (AVD)</h3><ol><li>In Android Studio open the <em>AVD Manager</em> by clicking <strong>Tools &gt; AVD Manager</strong>.</li><li>Click <strong>Create Virtual Device</strong>, at the bottom of the <em>AVD Manager</em> dialog.</li><li>Select or create your desired <em>Phone</em> hardware profile and select <strong>Next</strong>.</li><li>Select an <code>x86</code> or <code>x86_64</code> system image running <strong>API Level 27 or later</strong> and select <strong>Next</strong>.<ul><li>While physical ARCore devices are supported on API Level 24 or later, Android Emulator support requires API Level 27 or later.</li><li>Only x86-based Android Emulator architectures are supported. Other architectures such as <code>arm64-v8a</code>, <code>armeabi-v7</code>, are not currently supported.</li><li><strong>macOS only with ARCore SDK 1.16.0 or later:</strong> Due to a <a href="https://issuetracker.google.com/141500087" target="_blank" rel="noopener">known issue</a>, Android Emulator <code>x86_64</code> system images are not supported on macOS with ARCore SDK 1.16.0 or later. As a workaround, use an <code>x86</code> system image.</li></ul></li><li>Verify that your virtual device is configured correctly:<ul><li>Click <strong>Show Advanced Settings</strong>.</li><li>Make sure that <strong>Camera Back</strong> is set to <strong>VirtualScene</strong>.</li></ul></li><li>Click <strong>Finish</strong> to create your AVD.</li></ol><h2 id="3-4-Run-your-app"><a href="#3-4-Run-your-app" class="headerlink" title="3.4 Run your app"></a>3.4 Run your app</h2><p>Test an ARCore app on an AR-supported virtual device in the emulator. To do this, you can follow the Android Studio instructions to <a href="https://developer.android.google.cn/studio/run/emulator.html#runningapp" target="_blank" rel="noopener">Run an app in the Android Emulator</a>.</p><p><strong>Note:</strong> To run apps with NDK components in the Android Emulator, your app must be built with <a href="https://developer.android.google.cn/ndk/guides/abis.html" target="_blank" rel="noopener"><strong>x86 ABIs</strong></a>. For an example, see the <a href="https://github.com/google-ar/arcore-android-sdk/tree/master/samples/hello_ar_c" target="_blank" rel="noopener"><strong>ARCore HelloAR C sample app</strong></a>.</p><h3 id="3-4-1-Update-Google-Play-Services-for-AR"><a href="#3-4-1-Update-Google-Play-Services-for-AR" class="headerlink" title="3.4.1 Update Google Play Services for AR"></a>3.4.1 Update Google Play Services for AR</h3><p>The version of Google Play Services for AR on the emulator is likely out of date. Follow these instructions to update it:</p><ol><li><p>Download the latest <strong>Google_Play_Services_for_AR_1.18.0_x86_for_emulator.apk</strong> from the GitHub <a href="https://github.com/google-ar/arcore-android-sdk/releases" target="_blank" rel="noopener">releases</a> page.</p></li><li><p>Install the downloaded APK into each AVD you’d like to use:</p><p>Start the desired AVD, then drag the downloaded APK onto the running emulator, or install it using <code>adb</code> while the virtual device is running:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adb install -r Google_Play_Services_for_AR_1.18.0_x86_for_emulator.apk</span><br></pre></td></tr></table></figure></li></ol><p>Repeat these steps process for any additional AVDs you’d like to use.</p><h3 id="3-4-2-Control-the-virtual-scene"><a href="#3-4-2-Control-the-virtual-scene" class="headerlink" title="3.4.2 Control the virtual scene"></a>3.4.2 Control the virtual scene</h3><p>When your app connects to ARCore, you’ll see an overlay describing how to control the camera and a status bar below the emulator window.</p><p><img src="https://developers.google.cn/ar/images/ar_emulator_overlay.png" alt></p><h4 id="Move-the-virtual-camera"><a href="#Move-the-virtual-camera" class="headerlink" title="Move the virtual camera"></a>Move the virtual camera</h4><p>Press and hold <strong>Option</strong> (macOS) or <strong>Alt</strong> (Linux or Windows) to access camera movement controls. Use the following controls to move the camera:</p><table><thead><tr><th style="text-align:left">Platform</th><th style="text-align:left">Action</th><th style="text-align:left">What to do</th></tr></thead><tbody><tr><td style="text-align:left"><strong>macOS</strong></td><td style="text-align:left">Move left or right</td><td style="text-align:left">Hold <strong>Option</strong> + press <strong>A</strong> or <strong>D</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move down or up</td><td style="text-align:left">Hold <strong>Option</strong> + press <strong>Q</strong> or <strong>E</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move forward or back</td><td style="text-align:left">Hold <strong>Option</strong> + press <strong>W</strong> or <strong>S</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Change device orientation</td><td style="text-align:left">Hold <strong>Option</strong> + move mouse</td></tr><tr><td style="text-align:left"><strong>Linux</strong> or <strong>Windows</strong></td><td style="text-align:left">Move left or right</td><td style="text-align:left">Hold <strong>Alt</strong> + press <strong>A</strong> or <strong>D</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move down or up</td><td style="text-align:left">Hold <strong>Alt</strong> + press <strong>Q</strong> or <strong>E</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Move forward or back</td><td style="text-align:left">Hold <strong>Alt</strong> + press <strong>W</strong> or <strong>S</strong></td></tr><tr><td style="text-align:left"></td><td style="text-align:left">Change device orientation</td><td style="text-align:left">Hold <strong>Alt</strong> + move mouse</td></tr></tbody></table><p>Release <strong>Option</strong> or <strong>Alt</strong> to return to interactive mode in the emulator.</p><p>Use the <strong>Virtual Sensors</strong> tab in <a href="https://developer.android.google.cn/studio/run/emulator.html#extended" target="_blank" rel="noopener">Extended controls</a> for more precise device positioning.</p><h3 id="3-4-3-Add-Augmented-Images-to-the-scene"><a href="#3-4-3-Add-Augmented-Images-to-the-scene" class="headerlink" title="3.4.3 Add Augmented Images to the scene"></a>3.4.3 Add Augmented Images to the scene</h3><p>Load images into the emulator’s simulated environment to test <a href="https://developers.google.cn/ar/develop/java/augmented-images" target="_blank" rel="noopener">Augmented Images</a>.</p><p><img src="https://developers.google.cn/ar/images/augmented-images-emulator.png" alt>Use the <strong>Camera</strong> tab in Extended controls to add or modify <strong>Scene images</strong>. There are two image locations, one on the wall and one on the table.</p><p><img src="https://developers.google.cn/ar/images/augmented-images-emulator-settings.png" alt></p><p>To view these image locations in the scene, launch your emulator, then move the camera to the dining room area through the door behind the camera’s starting position.</p><h3 id="3-4-4-Troubleshooting-tips"><a href="#3-4-4-Troubleshooting-tips" class="headerlink" title="3.4.4 Troubleshooting tips"></a>3.4.4 Troubleshooting tips</h3><ul><li>If your ARCore app launches and you see an “AR Core not supported” message, check the revision on your system image. Make sure you are using <strong>API Level 27 Revision 4</strong>.</li><li>If your ARCore app fails to open the camera when it launches, make sure that <strong>Camera Back</strong> is set to <strong>VirtualScene</strong>, as described in the <a href="https://developers.google.cn/ar/develop/java/emulator#configure_the_virtual_device" target="_blank" rel="noopener">configuration steps above</a>.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ARCore 是 Google 为开发者构建的增强现实平台，如何让虚拟物体和真实世界完美融合，这一直是 Google ARCore 技术所探讨的问题。众所周知，当虚拟物体附近有现实物体时，有可能会出现互相交融、重叠等效果，大大地影响了用户体验。这一直是 AR 技术的难点，也是 Google 不懈努力的方向。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="AR" scheme="http://yuanquanquan.top/tags/AR/"/>
    
  </entry>
  
  <entry>
    <title>PC-DARTS:Partial Channel Connections for Memory-Efficient DifferentiableArchitecture Search</title>
    <link href="http://yuanquanquan.top/2020/20200529/"/>
    <id>http://yuanquanquan.top/2020/20200529/</id>
    <published>2020-05-29T14:34:41.000Z</published>
    <updated>2020-06-09T21:01:40.285Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>DARTS是可微分网络架构搜索，而本文将要解读的PC-DARTS是DARTS的扩展。DARTS方法速度快，但是由于它需要训练一个超网来寻找最优结构，需要消耗大量的内存和计算资源。因此论文作者提出了Partially-ConnectedDARTS，即部分通道连接的DARTS方法，通过对super-net进行一小部分的采样，能够减少网络搜索过程中计算的内存占用。但是由于通过在通道子集上执行运算搜索，而其他部分不变，这可能导致挑选超大网络边时出现不一致，为了解决这个问题，作者提出了边正则化，在搜索中添加边级别的超参数集合，来减少搜索的不确定性。</p></blockquote><a id="more"></a><p><img src="https://i.loli.net/2020/05/30/aWSh36meoy2DkIL.png" alt="PC-DARTS"></p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>DARTS把运算操作进行连续松弛，可以让网络的超参数搜索可微，进而达到端对端的网络搜索。但是由于在一个很大的超网上进行搜索，导致它的搜索空间有大量的冗余，使得计算量和内存占用很大。作者为了减少内存和计算量，采用这样的思路：不用把全部通道都送入运算选择中，而是对通道子集进行随机采样进行运算，其他的通道直接通过。但是这种思路带来了一种问题：由于采样的随机性，网络连接的选择可能是不稳定的。由此作者又引入了边正则化（edgenormalization）进行稳定，添加一个额外的边选择超参数集合。同时得益于部分通道连接的策略，选择1/K的通道可以减少K倍内存，那么就可以增大K倍的batchsize，不仅可以加速K倍，还可以稳定搜索。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>DARTS把网络的搜索拆分为L个cell，每个cell为N个节点的有向无环边，每个节点是一个网络层。Cell的类型有两种：ReductionCell和NormalCell，在整个超网中共享。</p><p>一般网络的每一层代表着一种操作，这个操作可能是卷积、池化、激活等函数，但在超网络SuperNet中，每一层网络是由多种运算组合起来的，每一种运算对应一个系数$\alpha$,，混合运算的加权公式如下：$f_{i, j}\left(\mathbf{x}<em>{i}\right)=\sum</em>{o \in \mathcal{O}} \frac{\exp \left{\alpha_{i, j}^{o}\right}}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left{\alpha_{i, j}^{o^{\prime}}\right}} \cdot o\left(\mathbf{x}_{i}\right)$</p><h2 id="Partial-Channel-Connections"><a href="#Partial-Channel-Connections" class="headerlink" title="Partial Channel Connections"></a><strong>Partial Channel Connections</strong></h2><p>DARTS的问题是需要大量内存，为了调节$|\theta|$个运算，需要把每个运算的结果存储起来，需要使用$|\theta|$倍的内存，为了能存下必须降低batchsize大小，这就降低了速度。而本文提出了partially-connectedDARTS方法，简称为PC-DARTS，方法如图1所示。该方法将网络提取的特征在通道维度上进行了1/K采样，只对采样后的通道进行处理，然后将这些特征与剩余的特征进行拼接(concat)。为了减少由采样带来的不确定性，作者又提出了边正则化，添加了边级别的超参数$\beta$</p><p><img src="https://i.loli.net/2020/05/29/3RxMkinqbHE7IAs.png" alt="PC-DARTS方法图示"></p><p>PC-DARTS的运算加权公式为:$f_{i, j}^{\mathrm{PC}}\left(\mathbf{x}<em>{i} ; \mathbf{S}</em>{i, j}\right)=\sum_{o \in \mathcal{O}} \frac{\exp \left{\alpha_{i, j}^{o}\right}}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left{\alpha_{i, j}^{o^{\prime}}\right}} \cdot o\left(\mathbf{S}<em>{i, j} * \mathbf{x}</em>{i}\right)+\left(1-\mathbf{S}<em>{i, j}\right) * \mathbf{x}</em>{i}$</p><p>其中$S_{i,j}$为通道采样mask，标为1的通道直接作为输出。我们随机采样1/K个通道，其中这个K我们作为超参数用来平衡速度和准确率。挑选出通道的1/K可以减少K倍的计算量，还可以有更多的样本来采样，这对于网络搜索尤为重要。</p><p><strong>Edge Normalization</strong></p><p>上一节中对通道进行随机采样的好处是能减少所选操作的偏置，即对于边(i,j)，给定$x_i$,使用两组超参数${\alpha^{o}<em>{i,j}}$和$${\alpha^{o’}</em>{i,j}}$$的差距就减小了。但是它削弱了无权重运算（如跳层连接，最大池化）的优势。在早期，搜索算法更喜欢无权重的运算，因为这些运算没有参数，能够得到输出一致的结果。但是对于有权重的运算，优化过程中会出现不一致的情况。这样无权重的运算会占据很大的比重，后续即使有权重的运算优化的很好，也无法超过它们。这种现象在代理输入比较困难的时候尤其严重，这也导致DARTS在ImageNet上效果不好。</p><p>为此提出了边正则化来抑制该现象。边正则化为基本单元中的第i层网络的每个输入分配了一个$\beta$参数，公式表示如下：$\mathbf{x}<em>{j}^{\mathrm{PC}}=\sum</em>{i&lt;j} \frac{\exp \left{\beta_{i, j}\right}}{\sum_{i^{\prime}&lt;j} \exp \left{\beta_{i^{\prime}, j}\right}} \cdot f_{i, j}\left(\mathbf{x}_{i}\right)$</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>作者在两个常用的公共数据集CIFAR-10和ImageNet进行了实验。</p><p><strong>CIFAR-10实验结果</strong></p><p>CIFAR-10上的实验结果如图2所示。在搜索过程中，网络由8个cell堆叠组成（包含6个normalcells和2个reductioncells），并且每个cell由6个节点构成，normalcell和reductioncell结果如图2所示。</p><p>CIFAR-10上，选用K=4，即只有1/4的通道被采样，因此搜索期间的batchsize增加到256。训练时，SuperNet首先预热15个epochs（即固定架构超参数，只更新网络参数），使用带动量的SGD进行网络参数的优化，使用Adam优化器来对超参数${\alpha^{o}<em>{i,j}}$和$\beta</em>{i,j}$进行更新。</p><p>从表中可以看到，使用PC-DARTS方法，仅需0.1GPU天，错误率就可以达到2.57%，搜索时间和准确率都超过了baseline方法DARTS。在比较的方法中，PC-DARTS是错误率小于3%的方法中速度最快的。</p><p><img src="https://i.loli.net/2020/05/29/z71ISxRg5D2Tic4.png" alt="表1：在CIFAR-10上的实验结果"><img src="https://i.loli.net/2020/05/29/ogBx9ckQMwhSpPJ.png" alt="图2：在CIFAR-10上搜索出的cell结构"></p><h2 id="ImgaeNet上的实验结果"><a href="#ImgaeNet上的实验结果" class="headerlink" title="ImgaeNet上的实验结果"></a><strong>ImgaeNet上的实验结果</strong></h2><p>在ImageNet上的实验结果以及和SOTA方法的比较如表2所示。作者对用于CIFAR-10上的网络结构进行了小修改以适用于ImageNet。为了减小搜索时间，作者分别从ImageNet上随机采样了两个子集，采样率分别为10%和2.5%，前者用于训练网络参数权重，后者用于更新架构超参数。</p><p>由于在ImageNet进行搜索比CIFAR-10更难，为了保留更多的信息，选取K=2，即通道采样率为1/2，是CIFAR-10的两倍。仍然训练50个epochs，但是前35个epochs固定架构超参数，其他训练设置基本和CIFAR-10上的一致。</p><p>在ImageNet上的结果如表2所示，在ImageNet上的实验结果，Top-1和Top-5准确率可以达到24.2%和7.3%，是比较的方法中效果最好的，也证明了本方法在减少内存消耗上是有效的。</p><p><img src="https://i.loli.net/2020/05/29/mylQnNPtcx75V86.png" alt="表2：在ImageNet上的实验结果"></p><p>在ImageNet上搜索得到的normalcell和reductioncell如下图所示。</p><p><img src="https://i.loli.net/2020/05/30/Gr3N6YATl8Ev9XC.png" alt="图3：在ImageNet上搜索得到的cell结果"></p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a><strong>消融实验</strong></h2><h2 id="1-不同采样率的结果"><a href="#1-不同采样率的结果" class="headerlink" title="1. 不同采样率的结果"></a><strong>1. 不同采样率的结果</strong></h2><p>K是用来控制通道的采样率的一个超参数，在讨论K对实验结果的影响之前，要先明确这么一个信息：增加采样率（即使用一个更小的K值）能使得更精确的信息被传播；而对通道的更小一部分进行采样，会造成更大的正则化，可能会引起过拟合。为了研究K的影响，作者在CIFAR-10上实验了4种K值对性能的影响，分别为1/1,1/2,1/4和1/8，实验结果如图4所示。从图中可以看出来采样率在1/4时的效果最好，准确率最高，搜索速度最快。使用1/8的采样率，尽管会进一步减少搜索时间，但是会产生严重的性能下降。<img src="https://i.loli.net/2020/05/30/QTBl1tOjRS2g9i5.png" alt="图4：不同K值（即采样率）对实验结果的影响"></p><h2 id="2-PC-DARTS不同组件的作用"><a href="#2-PC-DARTS不同组件的作用" class="headerlink" title="2. PC-DARTS不同组件的作用"></a><strong>2. PC-DARTS不同组件的作用</strong></h2><p>作者又探讨PC-DARTS中的partialchannel connection（表中简称为PC）和edgenormalization（表中简称为EN）的作用，结果如表3所示。从表中可以很明显看到，EN及时在通道是全部连接的情况下，也能带来正则化的效果。同时，edgenormalization和partialchannel connection一起使用，可以提供更进一步的改进效果。而不使用edgenormalization，则网络参数的数量和精确度都会受到影响。</p><p><img src="https://i.loli.net/2020/05/30/3tqhK8Vr7bjQpXP.png" alt="表3：在CIFAR-10和ImageNet上的消融实验"></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>本篇论文提出了一个简单但却有效的PC-DARTS(partially-connecteddifferentiable architecturesearch)方法，<strong>它的核心思想是随机采样一部分通道用于运算搜索，这样能更有效的利用内存，可以使用更大的batchsize获得更高的稳定性。另一个贡献是提出了边标准化(edgenormalization)来稳定搜索的过程，这是个轻量化的模块，基本不需要太多的计算量。此方法在CIFAR-10上完整搜索只需要0.1GPU天，在Imagenet上搜索需要3.8GPU天。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;DARTS是可微分网络架构搜索，而本文将要解读的PC-DARTS是DARTS的扩展。DARTS方法速度快，但是由于它需要训练一个超网来寻找最优结构，需要消耗大量的内存和计算资源。因此论文作者提出了Partially-ConnectedDARTS，即部分通道连接的DARTS方法，通过对super-net进行一小部分的采样，能够减少网络搜索过程中计算的内存占用。但是由于通过在通道子集上执行运算搜索，而其他部分不变，这可能导致挑选超大网络边时出现不一致，为了解决这个问题，作者提出了边正则化，在搜索中添加边级别的超参数集合，来减少搜索的不确定性。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="AutoML" scheme="http://yuanquanquan.top/tags/AutoML/"/>
    
  </entry>
  
  <entry>
    <title>Typecho</title>
    <link href="http://yuanquanquan.top/2020/202005251/"/>
    <id>http://yuanquanquan.top/2020/202005251/</id>
    <published>2020-05-25T06:17:17.000Z</published>
    <updated>2020-06-09T21:03:09.112Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近因为疫情<a href="https://developer.aliyun.com/adc/student/?pid=mm_25282911_3455987_122436732" target="_blank" rel="noopener">阿里云可以白嫖半年的服务器</a>,刚好5.20快到了用服务器给妹妹搭一个情侣博客</p></blockquote><a id="more"></a>  <p><strong>一、安装宝塔面板</strong></p><ol><li><p>打开控制命令行:win+r，输入cmd</p></li><li><p>输入命令：ssh root@服务器IP地址，输入密码就能进入服务器.登陆成功的话如图所示!<img src="https://i.loli.net/2020/06/10/T9G27VM5Kl8e6EN.png" alt></p></li><li><p>在shell环境下输入安装宝塔的命令，安装命令宝塔官方上有，直接按照自己的系统版本选择，我这里选择ubuntn的安装命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y wget &amp;&amp; wget -O install.sh http://download.bt.cn/install/install_6.0.sh &amp;&amp; sh install.sh</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li>最后得到一个宝塔面板的链接网址和账户和登录密码。<img src="https://i.loli.net/2020/05/25/rh4Ucl7QoJmA1gs.png" alt></li></ol><p><strong>二、在面板上搭建typecho博客</strong></p><p>用刚才的账号和密码登录到宝塔面板，新建一个站点，然后输入域名（没有的话可以从阿里云买一个），勾选mysql数据库，注意记住创建之后的数据库名和密码，后面用的到。<img src="https://i.loli.net/2020/05/25/9fWHMQyv3AixCEd.png" alt></p><p>注意这里的端口号，默认是80，如果80端口号被占用，可以改成81或者82等。记住这里的端口号，之后将需要这个端口号设置typecho博客。<strong>注意阿里云要开放80端口<img src="https://i.loli.net/2020/05/25/uWCEPyclmRx2pFo.png" alt></strong></p><ul><li>从typecho官方网站下载1.1正式版：[typecho官方下载<img src="https://i.loli.net/2020/05/25/69BTLoYXgMeku5Z.png" alt></li></ul><ul><li><p>将下载之后的文件解压放到创建的网站文件夹下<br><img src="https://i.loli.net/2020/05/25/g9YlzInd3Q65LXF.png" alt></p></li><li><p>访问刚刚设置的网站域名端口号，一般80是被占用的，需要自己改一个端口号，我这里改成了81，但是此时直接访问81的端口号会404 NOT Found,这个时候需要去宝塔面板里放行端口号<img src="C:%5CUsers%5CLenovo%5CDesktop%5Cyolov1%5Ctypecho-7.png" alt></p></li></ul><ul><li><p>接下来就循规蹈矩的进行安装了,输入刚刚的数据库名、用户名和密码，密码忘记了可以去宝塔面板里自己改一个简单点的。<img src="https://i.loli.net/2020/05/25/YP2in6kyGRJhvXf.png" alt></p></li><li><p>这里会提示没有config.inc.php配置文件，这个时候需要自己去创建一个文件，将其中的代码复制进去，放到blog文件夹下</p></li></ul><p><strong>三、选择typecho主题，并可以随时切换</strong><br>typecho模板主题</p><p>将下载的主题，放到当前目录下，默认会有一个系统主题</p><p>登录到自己的博客地址，然后进行博客的相关设置。主题修改</p><p><strong>设置外观</strong></p><p>控制台 -&gt; 外观，“设置外观” 选项。里面是和主题相对应的定制选项，比如 网站 logo、功能开关、站点描述、缩略图设置等，主要是针对该主题的一些个性化、功能性设置。</p><p><img src="https://i.loli.net/2020/05/25/ZwTid3hyXcoY2b7.jpg" alt></p><p><strong>自定义修改</strong></p><p>“编辑当前外观” 自定义修改主题样式。这里列出的模板文件和主题有关，如果没有需要修改的那个文件，可以去主题文件夹里查找直接修改。</p><p><img src="https://i.loli.net/2020/05/25/vnzSfArp7Msgty1.jpg" alt></p><p><strong>Tips</strong></p><ul><li>有的主题需要更改主题文件夹为指定名称才有效，注意查看主题说明；</li><li>自定义修改主题样式后，如果刷新网站没有变化，尝试刷新 CDN 缓存或者浏览器本地缓存。</li></ul><hr><h2 id="主题推荐"><a href="#主题推荐" class="headerlink" title="主题推荐"></a>主题推荐</h2><p>这里分享的全部是免费主题。</p><p><strong>Pinghsu</strong></p><p>简介：卡片式设计，简洁美观。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//www.linpx.com/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/chakhsu/pinghsu" target="_blank" rel="noopener">https://github.com/chakhsu/pinghsu</a></li></ul><p><img src="https://i.loli.net/2020/05/25/jFxhHZwvpJztmGC.jpg" alt></p><p><strong>Material</strong></p><p>简介：Material Design theme for typecho. 扁平化设计主题。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//blog.lim-light.com/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/idawnlight/typecho-theme-material" target="_blank" rel="noopener">https://github.com/idawnlight/typecho-theme-material</a></li></ul><p><img src="https://i.loli.net/2020/05/25/Lak4Az31VHusSUt.jpg" alt></p><p><strong>NexT.Pisces</strong></p><p>简介：Hexo 主题 NexT.Pisces 的 Typecho 移植版，基于 zgq354 的 NexT.Mist 修改制作。</p><ul><li><a href="https://link.zhihu.com/?target=http%3A//notes.iissnan.com/" target="_blank" rel="noopener">Demo</a>（这个是 Hexo 的，效果差不多）</li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/newraina/typecho-theme-NexTPisces" target="_blank" rel="noopener">https://github.com/newraina/typecho-theme-NexTPisces</a></li></ul><p><img src="https://i.loli.net/2020/05/25/qNb1SeIEDVps8Td.jpg" alt></p><p><strong>Maupassant</strong></p><p>简介：极简响应式主题。</p><ul><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/pagecho/maupassant" target="_blank" rel="noopener">https://github.com/pagecho/maupassant</a></li></ul><p><img src="https://i.loli.net/2020/05/25/aXY7gzDoeGEvPBl.jpg" alt></p><p><strong>Optica</strong></p><p>简介：单栏小清新主题 Optica。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//1000yun.cn/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/iduzui/optica" target="_blank" rel="noopener">https://github.com/iduzui/optica</a></li></ul><p><img src="https://pic1.zhimg.com/80/v2-1deecd43d3f4c790780feb33462c88a4_720w.jpg" alt></p><p><strong>ArmX</strong></p><p>简介：响应式纯净前端结构，不依赖第三方前端框架；自带音乐播放器，全站pjax；支持第三方登录；支持cdn加速、生成缩略图等，功能完善。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//codeup.me/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/natcube/ArmX" target="_blank" rel="noopener">https://github.com/natcube/ArmX</a></li></ul><p><img src="https://i.loli.net/2020/05/25/kbZ2VihLnOjB8SI.jpg" alt></p><p><strong>Affinity</strong></p><p>简介：三列卡片式布局。移植自 ghost，原作地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/Showfom/Affinity" target="_blank" rel="noopener">https://github.com/Showfom/Affinity</a>。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//affinity.419.at/" target="_blank" rel="noopener">Demo</a></li><li>地址： <a href="https://link.zhihu.com/?target=https%3A//affinity.419.at/2017/07/28/affinity.html" target="_blank" rel="noopener">https://affinity.419.at/2017/07/28/affinity.html</a></li></ul><p><img src="https://i.loli.net/2020/05/25/7UpfkVOj3raQ1K8.jpg" alt></p><p><strong>Junichi</strong></p><p>简介：轻量级，无前端框架；响应式设计。</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//uefeng.com/" target="_blank" rel="noopener">Demo</a></li><li>地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/siseboy/junichi" target="_blank" rel="noopener">https://github.com/siseboy/junichi</a></li></ul><p><img src="https://i.loli.net/2020/05/25/jBua1S2DPHIpyiW.jpg" alt></p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Typecho 主题还是很丰富的，而且很多是博主们写着自己用，然后分享给网友的。如果喜欢可以选择购买付费主题或者赞助支持主题作者。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;最近因为疫情&lt;a href=&quot;https://developer.aliyun.com/adc/student/?pid=mm_25282911_3455987_122436732&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿里云可以白嫖半年的服务器&lt;/a&gt;,刚好5.20快到了用服务器给妹妹搭一个情侣博客&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="博客搭建" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="blog" scheme="http://yuanquanquan.top/tags/blog/"/>
    
      <category term="教程" scheme="http://yuanquanquan.top/tags/%E6%95%99%E7%A8%8B/"/>
    
      <category term="typecho" scheme="http://yuanquanquan.top/tags/typecho/"/>
    
  </entry>
  
  <entry>
    <title>胃与真相</title>
    <link href="http://yuanquanquan.top/2020/20200524/"/>
    <id>http://yuanquanquan.top/2020/20200524/</id>
    <published>2020-05-24T15:25:45.000Z</published>
    <updated>2020-06-09T21:03:48.540Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>疫情期间常常听Ludovico的作品，蛰居陋室，埋头科研，不问世事，倒也颇映衬了他作品中空旷辽远的寂静和哲学思索。</p><p>在经过了盲目自信、怀疑论、疲乏应对、指责中国之后，新冠疫情在美国的爆发，终究还是成为了社会的主要议题。搬空的超市只是一个缩影，标志着不管你是否愿意，都必须接受这样一个事实 - 生活不会再像以前那样安宁祥和。而现在，被疫情考验之后的美国社会，更多的人开始意识到，再也回不到疫情之前的样子。纽约州长Cuomo在一次例行的发布会上说：人们都想要回到生活常态（back to normal），但事实是，回不去了，我们只会进入到一个新的常态（a new normal）。</p></blockquote><a id="more"></a>  <p>之前在Twitter上看到一个老美说，新冠疫情就是美国政府的一个谎言，目的是用来操纵民意，为大选服务。那时大多数美国人对于新冠是比较乐观的，而当时我只是觉得这人真傻，她不知道中国人民抗疫付出的代价有多大，得到的教训有多深刻，她只要稍微看看国际新闻，就会知道这个疾病的严重性，怎会是捏造之实。半个月之后，奥斯汀市政府宣布城市限行，民众居家禁止外出。一时间，所有人都被拽进一个陌生的现实，它就在那儿，至今仍然在那儿。</p><p>但我后来意识到，她的那种不屑，或者是无可奈何，大概是对的。那不代表她不看国际新闻，事实是，获得真相对改变现状的帮助太小，而听信谎言，或者是厌倦与逃离，也不一定是愚蠢的表现。不管形势如何发展，普通民众，除了恐慌性地抢购生活物资，在这种宏大层面的公共安全事件面前，又能做些什么呢。今天可以是新冠疫情，明天可以是任何突发事件。但生活总要继续，总要果腹，病死和饿死，又有什么区别。就算病死和饿死有区别，在最终报给总统的伤亡人数里，也都只是数字，背后的故事谁又会去讲述。</p><p>纪录片《华氏911》里说，他们把恐怖指数调到高，民众开始疯狂抢购，囤积自卫；然后又调到低，民众又像往常一样推婴儿车在公园散步和遛狗。不断地往复着模棱两可的语境，正如整个疫情当中，川普政府对于病毒危险性的评估不断反复闪烁其词，模棱两可，而真正应该领导疫情防控的Fauci博士却被拒绝参与众议院的新冠听证会。事实上，模糊的语境只会让民众不知所措，当民众足够恐慌，他们便像绵羊一样温顺，可以随意地剪他们的毛。就像训练一条狗，你叫它坐下，然后叫它打滚，刚开始要滚你又叫它站起来，狗不知道要怎么做才好。</p><p>人们希望真相，但谎言和真相，没有本质的区别。它们相互转化，而又被权威所利用，唯一的区别是，做谎言的帮凶，浑水摸鱼；还是做真相的寻找者，逆流而上。不幸的是，人类所建立的这个系统的复杂度和信息的非对称决定了，没有绝对的真相，也没有绝对的谎言。大概更永恒的，只有我们的这个胃了罢：平民的胃是早中晚三餐，是早九晚五有班可以上；既得利益者的胃是道琼斯指数，是一年四季有资本可以攫取。毕竟，胃才是任何文明，任何语境的最终动力，因为那些没法填饱自己胃的物种都灭绝了；毕竟，既得利益者们所做的一切，大概也是为了他们的后代的胃可以不用空着。</p><p>对饥饿的恐惧，是刻在人类骨子里基因里的。而疾病，同样如此。所有人都在玩着同样的生存游戏。</p><p>1976年版的爱因斯坦文集中译本第一卷开篇中“自述”的第二段写到：</p><p>“当我还是一个相对早熟的少年的时候，我就已经深切地意识到，大多数人终生无休止地追逐的那些希望和努力是毫无价值的。而且，我不久就发现了这种追逐的残酷，这在当年较之今天是更加精心地用伪善和漂亮的字句掩饰着的。每个人只是因为有个胃，就注定要参与这种追逐。而且，由于参与这种追逐，他的胃是有可能得到满足的；但是，一个有思想，有感情的人却不能由此而得到满足。”</p><p>我大概是非常同意这段话的，也惊叹于少年时期的爱因斯坦便已有这样深刻的见解。而我也对自己写下这些文字感到一些隐藏的担忧，我们这些研究物理学的人，本是没有足够的经验和资历来对社会现象做过多的评述。但我们仍然发现，社会和自然至少在基本的逻辑上，是按照相似的法则来运转的。比如说生态系统中的物质能量流动的结构就和社会中的物资供应链很相似。著名的马太效应也能找到自然中的起源，比如热力学中的Ostwald Ripening 效应就说，在同一个压力环境里面，大气泡的增长都是以小气泡的消亡为代价；又比如食物链顶端的物种数量可能只占一个生态系统中物种数量的不到百分之一，但却统治着系统中大部分的资源。所以大概自然界中的“穷者愈穷，富者愈富”比人类社会更严重，毕竟作为人类的我们还发明了公平和道德来对冲自然法则。</p><p>爱因斯坦的思路其实还是物理的思路，或许过于简化，但他揭示了一个模型，那就是社会就像自然一样，构建于一些简单的法则，比如胃的满足；但社会的复杂度，则来自于法则衍生出来的结构，比如掩饰，比如修辞，比如文明。</p><p>真相，或者谎言，都是修辞。而胃，才是真正在运作的东西。</p><p>前段时间Twitter上有一则赞数很高的帖子如是说：不是新冠疫情导致了社会的撕裂（divide），而是新冠疫情揭示（reveal）了一个撕裂着的社会。邻居们开始不赞同彼此的观点；民粹开始系统性地对抗精英，对抗建制（establishment），对抗生产力的进步；大量的人开始失业，并将失业的原因归咎于从墨西哥偷渡过来的移民；每个人都和他人物理隔离，却在互联网的世界里继续争吵。而在这种争吵中，谁也不会被谁说服。</p><p>这一切，其实早在新冠以前就已经开始。</p><p>在新冠之前，社会不就已经在往着嘈杂的网络，疏离的物理联系，越来越多的人类丢掉工作的方向发展了么？新冠只是加剧了这种转型的阵痛。人类面临的，是一个生产力模式的变革，我们将被迫更深刻地去审视人类在自然中的位置，审视个体在社会中的位置，正如人类积累的知识中所探讨的那样。人类的发展模式将越来越不依赖于人际联系，而是依赖于一个个共同的理性的大脑们以及它们所构建的生产力爆炸式增长。讽刺的是，如果说工业革命以前或者说早期的工业革命中，大部分人口为生产力与资本贡献了宝贵的劳动力，那么正在进行的智能化革命则剥夺了越来越多人口的工作。这部分人，这一大部分人，除了加入民粹和反智的潮流，还有别的选择么？这种转型是一种系统性的对低效率人类劳动的价值否定，它不仅是经济上的，也是文化上的对大众的边缘化。如果大众越来越无法参与到生产力的进步中和基于生产力的文化构建中，那社会的撕裂是必然的。也有很多人在说，智能化革命虽然替代了很多人类劳动力，但它也带来了新的工作机会，比如算法工程师，物联网开发者等等。这是一种乐观的估计，事实是，逐利的资本会愿意帮助大众具备获得这些工作机会的技能吗？如果20%的失业率是因为工作效率是普通人类10倍的机器替代了他们，那么你可以构建一种模式使得这20%的人以更高的效率去替代这些机器吗？要知道，这些机器的效率也会越来越高，而创造这些机器，可能都不需要哪怕是2%的人。</p><p>人类从蛮荒走向文明，是因为胃的驱动，我们打败了野兽，获得了食物，是因为人类可以思考，可以想象，可以定义真相，抑或是谎言。而文明也在塑造着我们的胃。这种不可思议的能力塑造着人类和自己的关系，和自然的关系。我们真正应该思考的是，胃将把人类从文明带向何方，文明会是我们存在的最终形式吗？也许会，也许不会。但确凿的理解是，文明正在以一种我们不充分理解的方式改变着修辞的版图，改变着胃的属性，而于此间，真正危险的事情是，除了接受这种改变，我们似乎没有第二种选择，因为每个人只有一个胃。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;疫情期间常常听Ludovico的作品，蛰居陋室，埋头科研，不问世事，倒也颇映衬了他作品中空旷辽远的寂静和哲学思索。&lt;/p&gt;
&lt;p&gt;在经过了盲目自信、怀疑论、疲乏应对、指责中国之后，新冠疫情在美国的爆发，终究还是成为了社会的主要议题。搬空的超市只是一个缩影，标志着不管你是否愿意，都必须接受这样一个事实 - 生活不会再像以前那样安宁祥和。而现在，被疫情考验之后的美国社会，更多的人开始意识到，再也回不到疫情之前的样子。纽约州长Cuomo在一次例行的发布会上说：人们都想要回到生活常态（back to normal），但事实是，回不去了，我们只会进入到一个新的常态（a new normal）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Conditional Random Fields</title>
    <link href="http://yuanquanquan.top/2020/20200511/"/>
    <id>http://yuanquanquan.top/2020/20200511/</id>
    <published>2020-05-11T05:30:00.000Z</published>
    <updated>2020-06-14T08:01:05.507Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>条件随机场(conditional random fields，简称 $CRF$，或$CRFs$)，是一种判别式概率模型，常用于标注或分析序列资料，如自然语言文字或是生物序列。</p><p>条件随机场是条件概率分布模型P(Y|X)，表示的是给定一组输入随机变量X的条件下另一组输出随机变量Y的马尔可夫随机场，也就是说$CRF$的特点是假设输出随机变量构成马尔可夫随机场。</p></blockquote><a id="more"></a>  <h2 id="知识框架"><a href="#知识框架" class="headerlink" title="知识框架"></a>知识框架<img src="https://i.loli.net/2020/05/14/Xodiy9fMFzDgebp.png" alt></h2><h2 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h2><p>定义：假设一个随机过程中， $t_n$时刻的状态$x_n$的条件发布，只与其前一状态$x_{n-1}$相关，即：<br>$$<br>P\left(x_{n} | x_{1}, x_{2}, \ldots, x_{n-1}\right)=P\left(x_{n} | x_{n-1}\right)<br>$$<br>则将其称为马尔可夫过程。</p><p><img src="https://i.loli.net/2020/05/14/h3UIwjL2z8pMOma.png" alt></p><h2 id="隐马尔可夫算法-HMM"><a href="#隐马尔可夫算法-HMM" class="headerlink" title="隐马尔可夫算法(HMM)"></a>隐马尔可夫算法(HMM)</h2><p><strong>1、定义</strong></p><p>隐马尔可夫算法是对含有未知参数（隐状态）的马尔可夫链进行建模的生成模型，如下图所示：</p><p><img src="https://i.loli.net/2020/05/14/EMGRTL3sxB7l25m.png" alt="CRF-3"></p><p>在隐马尔科夫模型中，包含隐状态和观察状态，隐状态$x_i$对于观察者而言是不可见的，而观察状态$y_i$对于观察者而言是可见的。隐状态间存在转移概率，隐状态$x_i$到对应的观察状态$y_i$间存在输出概率。</p><p><strong>2、假设</strong></p><p>假设隐状态$x_i$的状态满足马尔可夫过程，$i$时刻的状态$x_i$的条件分布，仅与其前一个状态$x_{i-1}$相关，即：<br>$$<br>P\left(x_{i} | x_{1}, x_{2}, \ldots, x_{i-1}\right)=P\left(x_{i} | x_{i-1}\right)<br>$$<br>假设观测序列中各个状态仅取决于它所对应的隐状态，即：<br>$$<br>P\left(y_{i} | x_{1}, x_{2}, \ldots, x_{i-1}, y_{1}, y_{2}, \ldots, y_{i-1}, y_{i+1}, \ldots\right)=P\left(y_{i} | x_{i}\right)<br>$$<br><strong>3、存在问题</strong></p><p>在序列标注问题中，隐状态（标注）不仅和单个观测状态相关，还和观察序列的长度、上下文等信息相关。例如词性标注问题中，一个词被标注为动词还是名词，不仅与它本身以及它前一个词的标注有关，还依赖于上下文中的其他词</p><h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><p>以线性链条件随机场为例</p><p><strong>1、定义</strong></p><p>给定$X=(x_1,x_2····,x_n)$,$Y=(y_1,y_2····,y_n)$均为线性链表示的随机变量序列，若在给随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔可夫性：<br>$$<br>P\left(y_{i} | x_{1}, x_{2}, \ldots, x_{i-1}, y_{1}, y_{2}, \ldots, y_{i-1}, y_{i+1}\right)=P\left(y_{i} | x, y_{i-1}, y_{i+1}\right)<br>$$<br>则称$P(Y|X)$为线性链条件随机场。</p><p>通过去除了隐马尔科夫算法中的观测状态相互独立假设，使算法在计算当前隐状态$x_i$时，会考虑整个观测序列，从而获得更高的表达能力，并进行全局归一化解决标注偏置问题。</p><p><strong>1）参数化形式</strong><br>$$<br>p(y | x)=\frac{1}{Z(x)} \prod_{i=1}^{n} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)<br>$$<br>其中：$Z(x)$为归一化因子，是在全局范围进行归一化，枚举了整个隐状态序列$x_{1…n}$的全部可能，从而解决了局部归一化带来的标注偏置问题。</p><p>$t_k$为定义在边上的特征函数，转移特征，依赖于前一个和当前位置$s_1$为定义在节点上的特征函数，状态特征，依赖于当前位置</p><p><strong>2）简化形式</strong></p><p>因为条件随机场中同一特征在各个位置都有定义，所以可以对同一个特征在各个位置求和，将局部特征函数转化为一个全局特征函数，这样就可以将条件随机场写成权值向量和特征向量的内积形式，即条件随机场的简化形式。</p><ul><li><strong>step 1</strong> 将转移特征和状态特征及其权值用统一的符号表示，设有$k_1$个转移特征，$k_2$个状态特征，,记</li></ul><p>$$<br>f_{k}\left(y_{i-1}, y_{i}, x, i\right)=\left{\begin{array}{l}<br>t_{k}\left(y_{i-1}, y_{i}, x, i\right), \quad k=1,2,3, \ldots, K_{1} \<br>s_{l}\left(y_{i}, x, i\right), \quad k=k_{1}+l ; l=1,2, \ldots, K_{2}<br>\end{array}\right.<br>$$</p><ul><li><strong>step 2</strong> 对转移与状态特征在各个位置求$i$和，记作</li></ul><p>$$<br>f_{k}(y, x)=\sum_{i=1}^{n} f_{k}\left(y_{i-1}, y_{i}, x, i\right), k=1,2, \ldots, K<br>$$</p><ul><li><strong>step 3</strong> 将 和 用统一的权重表示，记作</li></ul><p>$$<br>w_{k}=\left{\begin{array}{ll}<br>\lambda_{k}, &amp; k=1,2, \ldots, K_{1} \<br>\mu_{l}, &amp; k=K_{1}+l ; l=1,2, \ldots, K_{2}<br>\end{array}\right.<br>$$</p><ul><li><p><strong>step 4</strong> 转化后的条件随机场可表示为：<br>$$<br>\begin{aligned}<br>P(y | x) &amp;=\frac{1}{Z(x)} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \<br>Z(x) &amp;=\sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x)<br>\end{aligned}<br>$$</p><ul><li><strong>step 5</strong> 若 表示权重向量：$w=(w_1,w_2,…,w_k)^T$以$F(y,x)$表示特征向量，即<br>$$<br>F(y, x)=\left(f_{1}(y | x), f_{2}(y | x), \ldots, f_{K}(y | x)\right)^{T}<br>$$<br>   则，条件随机场写成内积形式为：<br>$$<br>\begin{array}{l}<br>P_{w}(y | x)=\frac{\exp (w \cdot F(y, x)}{Z_{w}(x)} \<br>Z_{w}(x)=\sum_{y} \exp (w \cdot F(y, x))<br>\end{array}<br>$$</li></ul></li></ul><p><strong>3）矩阵形式</strong><br>$$<br>P_{w}(y | x)=\frac{1}{Z_{w}(x)} \prod_{i=1}^{n+1} M_{i}\left(y_{i-1}, y_{i} | x\right)<br>$$<br><strong>2、基本问题</strong></p><p>条件随机场包含概率计算问题、学习问题和预测问题三个问题。</p><ul><li>概率计算问题：已知模型的所有参数，计算观测序列Y出现的概率，常用方法：前向和后向算法；</li><li>学习问题：已知观测序列Y,求解使得该观测序列概率最大的模型参数，包括隐状态序列、隐状态间的转移概率分布和从隐状态到观测状态的概率分布，常用方法：Baum-Wehch算法；</li><li>预测问题：一直模型所有参数和观测序列Y，计算最可能的隐状态序列X,常用算法：维特比算法。</li></ul><p><strong>案例：利用维特比算法计算给定输入序列$x$对应的最优输出序列$y^*$：</strong><br>$$<br>\max \sum_{i=1}^{3} w \cdot F_{i}\left(y_{i-1}, y_{i}, x\right)<br>$$<br>1.初始化<br>$$<br>\delta_{1}(j)=w \cdot F_{1}\left(y_{0}=\operatorname{start}, y_{1}=j, x\right), j=1,2 i=1, \delta_{1}(1)=1, \delta_{1}(2)=0.5<br>$$<br>2.递推，对$i=2,3,…,n$<br>$$<br>\begin{array}{c}<br>i=2, \delta_{2}(l)=\max <em>{j}\left{\delta</em>{1}(j)+w \cdot F_{2}(j, l, x)\right} \<br>\delta_{2}(1)=\max \left{1+\lambda_{2} t_{2}, 0.5+\lambda_{4} t_{4}\right}=1.6, \Psi_{2}(1)=1 \<br>\delta_{2}(2)=\max \left{1+\lambda_{1} t_{1}+\mu_{2} s_{2}, 0.5+\mu_{2} s_{2}\right}=2.5, \Psi_{2}(2)=1 \<br>i=3, \delta_{3}(l)=\max <em>{j}\left{\delta</em>{2}(j)+w \cdot F_{3}(j, l, x)\right} \<br>\delta_{3}(1)=\max \left{1.6+\mu_{5} s_{5}, 2.5+\lambda_{3} t_{3}+\mu_{3} s_{3}\right}=4.3, \Psi_{3}(1)=2 \<br>\delta_{3}(2)=\max \left{1.6+\lambda_{1} t_{1}+\mu_{4} s_{4}, 2.5+\lambda_{5} t_{5}+\mu_{4} s_{4}\right}=4.3, \Psi_{3}(2)=1<br>\end{array}<br>$$<br>3.终止<br>$$<br>\max <em>{y}(w \cdot F(y, x))=\max \delta</em>{3}(l)=\delta_{3}(1)=4.3 y_{3}^{<em>}=\operatorname{argmax}<em>{1} \delta</em>{3}(l)=1<br>$$<br>4.返回路径<br>$$<br>\begin{aligned}<br>y_{2}^{</em>}=&amp; \Psi_{3}\left(y_{3}^{<em>}=\Psi_{3}(1)=2 y_{1}^{</em>}=\Psi_{2}\left(y_{2}^{<em>}\right)=\Psi_{2}(2)=1\right.\<br>求得最优路径y^{</em>}=\left(y_{1}^{<em>}, y_{2}^{</em>}, \ldots, y_{n}^{*}\right)=(1,2,1)<br>\end{aligned}<br>$$<br>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''实现条件随机场预测问题的维特比算法</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, V, VW, E, EW)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param V:是定义在节点上的特征函数，称为状态特征</span></span><br><span class="line"><span class="string">        :param VW:是V对应的权值</span></span><br><span class="line"><span class="string">        :param E:是定义在边上的特征函数，称为转移特征</span></span><br><span class="line"><span class="string">        :param EW:是E对应的权值</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.V  = V  <span class="comment">#点分布表</span></span><br><span class="line">        self.VW = VW <span class="comment">#点权值表</span></span><br><span class="line">        self.E  = E  <span class="comment">#边分布表</span></span><br><span class="line">        self.EW = EW <span class="comment">#边权值表</span></span><br><span class="line">        self.D  = [] <span class="comment">#Delta表，最大非规范化概率的局部状态路径概率</span></span><br><span class="line">        self.P  = [] <span class="comment">#Psi表，当前状态和最优前导状态的索引表s</span></span><br><span class="line">        self.BP = [] <span class="comment">#BestPath，最优路径</span></span><br><span class="line">        <span class="keyword">return</span> </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Viterbi</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        条件随机场预测问题的维特比算法，此算法一定要结合CRF参数化形式对应的状态路径图来理解，更容易理解.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.D = np.full(shape=(np.shape(self.V)), fill_value=<span class="number">.0</span>)</span><br><span class="line">        self.P = np.full(shape=(np.shape(self.V)), fill_value=<span class="number">.0</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment">#初始化</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> == i:</span><br><span class="line">                self.D[i] = np.multiply(self.V[i], self.VW[i])</span><br><span class="line">                self.P[i] = np.array([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">                print(<span class="string">'self.V[%d]='</span>%i, self.V[i], <span class="string">'self.VW[%d]='</span>%i, self.VW[i], <span class="string">'self.D[%d]='</span>%i, self.D[i])</span><br><span class="line">                print(<span class="string">'self.P:'</span>, self.P)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="comment">#递推求解布局最优状态路径</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> y <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">1</span>]): <span class="comment">#delta[i][y=1,2...]</span></span><br><span class="line">                    <span class="keyword">for</span> l <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">1</span>]): <span class="comment">#V[i-1][l=1,2...]</span></span><br><span class="line">                        delta = <span class="number">0.0</span></span><br><span class="line">                        delta += self.D[i<span class="number">-1</span>, l]                      <span class="comment">#前导状态的最优状态路径的概率</span></span><br><span class="line">                        delta += self.E[i<span class="number">-1</span>][l,y]*self.EW[i<span class="number">-1</span>][l,y]  <span class="comment">#前导状态到当前状体的转移概率</span></span><br><span class="line">                        delta += self.V[i,y]*self.VW[i,y]            <span class="comment">#当前状态的概率</span></span><br><span class="line">                        print(<span class="string">'(x%d,y=%d)--&gt;(x%d,y=%d):%.2f + %.2f + %.2f='</span>%(i<span class="number">-1</span>, l, i, y, \</span><br><span class="line">                              self.D[i<span class="number">-1</span>, l], \</span><br><span class="line">                              self.E[i<span class="number">-1</span>][l,y]*self.EW[i<span class="number">-1</span>][l,y], \</span><br><span class="line">                              self.V[i,y]*self.VW[i,y]), delta)</span><br><span class="line">                        <span class="keyword">if</span> <span class="number">0</span> == l <span class="keyword">or</span> delta &gt; self.D[i, y]:</span><br><span class="line">                            self.D[i, y] = delta</span><br><span class="line">                            self.P[i, y] = l</span><br><span class="line">                    print(<span class="string">'self.D[x%d,y=%d]=%.2f\n'</span>%(i, y, self.D[i,y]))</span><br><span class="line">        print(<span class="string">'self.Delta:\n'</span>, self.D)</span><br><span class="line">        print(<span class="string">'self.Psi:\n'</span>, self.P)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#返回，得到所有的最优前导状态</span></span><br><span class="line">        N = np.shape(self.V)[<span class="number">0</span>]</span><br><span class="line">        self.BP = np.full(shape=(N,), fill_value=<span class="number">0.0</span>)</span><br><span class="line">        t_range = <span class="number">-1</span> * np.array(sorted(<span class="number">-1</span>*np.arange(N)))</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> t_range:</span><br><span class="line">            <span class="keyword">if</span> N<span class="number">-1</span> == t:<span class="comment">#得到最优状态</span></span><br><span class="line">                self.BP[t] = np.argmax(self.D[<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment">#得到最优前导状态</span></span><br><span class="line">                self.BP[t] = self.P[t+<span class="number">1</span>, int(self.BP[t+<span class="number">1</span>])]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#最优状态路径表现在存储的是状态的下标，我们执行存储值+1转换成示例中的状态值</span></span><br><span class="line">        <span class="comment">#也可以不用转换，只要你能理解，self.BP中存储的0是状态1就可以~~~~</span></span><br><span class="line">        self.BP += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'最优状态路径为：'</span>, self.BP)</span><br><span class="line">        <span class="keyword">return</span> self.BP</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CRF_manual</span><span class="params">()</span>:</span>   </span><br><span class="line">    S = np.array([[<span class="number">1</span>,<span class="number">1</span>],   <span class="comment">#X1:S(Y1=1), S(Y1=2)</span></span><br><span class="line">                  [<span class="number">1</span>,<span class="number">1</span>],   <span class="comment">#X2:S(Y2=1), S(Y2=2)</span></span><br><span class="line">                  [<span class="number">1</span>,<span class="number">1</span>]])  <span class="comment">#X3:S(Y3=1), S(Y3=1)</span></span><br><span class="line">    SW = np.array([[<span class="number">1.0</span>, <span class="number">0.5</span>], <span class="comment">#X1:SW(Y1=1), SW(Y1=2)</span></span><br><span class="line">                   [<span class="number">0.8</span>, <span class="number">0.5</span>], <span class="comment">#X2:SW(Y2=1), SW(Y2=2)</span></span><br><span class="line">                   [<span class="number">0.8</span>, <span class="number">0.5</span>]])<span class="comment">#X3:SW(Y3=1), SW(Y3=1)</span></span><br><span class="line">    E = np.array([[[<span class="number">1</span>, <span class="number">1</span>],  <span class="comment">#Edge:Y1=1---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>]], <span class="comment">#Edge:Y1=2---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                  [[<span class="number">0</span>, <span class="number">1</span>],  <span class="comment">#Edge:Y2=1---&gt;(Y3=1, Y3=2) </span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">1</span>]]])<span class="comment">#Edge:Y2=2---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">    EW= np.array([[[<span class="number">0.6</span>, <span class="number">1</span>],  <span class="comment">#EdgeW:Y1=1---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0.0</span>]], <span class="comment">#EdgeW:Y1=2---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                  [[<span class="number">0.0</span>, <span class="number">1</span>],  <span class="comment">#EdgeW:Y2=1---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0.2</span>]]])<span class="comment">#EdgeW:Y2=2---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">    </span><br><span class="line">    crf = CRF(S, SW, E, EW)</span><br><span class="line">    ret = crf.Viterbi()</span><br><span class="line">    print(<span class="string">'最优状态路径为:'</span>, ret)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    CRF_manual()</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><p><img src="https://i.loli.net/2020/05/14/4ToXzPEuKs5vqCf.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;条件随机场(conditional random fields，简称 $CRF$，或$CRFs$)，是一种判别式概率模型，常用于标注或分析序列资料，如自然语言文字或是生物序列。&lt;/p&gt;
&lt;p&gt;条件随机场是条件概率分布模型P(Y|X)，表示的是给定一组输入随机变量X的条件下另一组输出随机变量Y的马尔可夫随机场，也就是说$CRF$的特点是假设输出随机变量构成马尔可夫随机场。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="http://yuanquanquan.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Neural Architecture Search</title>
    <link href="http://yuanquanquan.top/2020/20200425/"/>
    <id>http://yuanquanquan.top/2020/20200425/</id>
    <published>2020-04-25T02:57:05.000Z</published>
    <updated>2020-06-09T21:05:39.128Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>整理了一些最近的object detection算法<br>忽然发现。。。。<br>这是要开启NAS时代的神仙打架了嘛/facepalm</p><p>不过讲真…Google这些nas论文，计算量贼大，要卡要数据的啊……复现起来不是一点两点困难</p><p>“我是FAIR时代的残党！新时代没有能载我的模型！”（说白了就是没卡。。。。）</p></blockquote><a id="more"></a>  <p><img src="https://i.loli.net/2020/06/10/CLdEjz2VFSqgoXN.png" alt></p><p>为了紧跟时代潮流，了解一下AutoML去看了一下<strong>《Neural Architecture Search: A Survey》</strong>这篇综述</p><h1 id="序"><a href="#序" class="headerlink" title="序"></a><em>序</em></h1><p>​      深度学习模型在很多任务上都取得了不错的效果，但调参对于深度模型来说是一项非常苦难的事情，众多的超参数和网络结构参数会产生爆炸性的组合，常规的 random search 和 grid search 效率非常低，为此人们想出了自动搜索神经网络架构（Neural Architecture Search）。一方面，自动神经网络架构搜索可以遍历架构找到性能最优的架构，另一方面自动神经网络架构搜索还可以打破人类思维的局限性找到人类所想不到的架构组织方式。</p><p><img src="https://i.loli.net/2020/05/03/ogn1dueI9DtS6PL.png" alt></p><p>本文从网络架构搜索的三个方面进行了分类综述，包括：</p><ul><li><p><strong>搜索空间</strong></p></li><li><p><strong>搜索策略</strong></p></li><li><p><strong>评价预估</strong></p></li></ul><h3 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h3><p>搜索空间定义了NAS方法原则上可能发现的神经体系结构即优化问题的变量。深度学习模型的性能是由参数来控制和决定的，所以只需要对复杂模型的架构参数和对应的超参数进行优化即可。</p><p><img src="https://i.loli.net/2020/05/03/Dd9WIgzb4SacLw2.png" alt></p><p>上图是两种不同的架构空间，图片中每个节点表示神经网络中的一层，例如卷积层，池化层，不同的层由不同的颜色标注。箭头描述了数据的流向。</p><p>左侧的图像是一个链式结构空间的组块，这种结构相当于一个 N 层的序列，每一层有几种可选的算子，比如卷积、池化等，每种算子包括一些超参数，比如卷积尺寸、卷积步长等。</p><p>右侧的图像是一个拥有多分支和跳远链接的搜索空间的组块。</p><p>链式结构神经网络通过层数N，层间操作，卷积核大小步幅等超参对搜索空间进行参数化，注意：搜索空间的参数不是固定长度的，而是条件空间。</p><p>分支结构用$g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)$来表示第$i$层的输入，则分支结构的特殊情况分为：</p><ul><li>链结构网络</li></ul><p>$$<br>g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)=L_{i-1}^{o u t}<br>$$</p><ul><li>残差网络</li></ul><p>$$<br>\left(g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)=L_{i-1}^{o u t}+L_{j}^{o u t}, j&lt;i-1\right.<br>$$</p><ul><li>Densenet<br>$$<br>g_{i}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)=\operatorname{concat}\left(L_{i-1}^{o u t}, \ldots, L_{0}^{o u t}\right)<br>$$<br>通过以上方式分别搜索这样的组块，这样的组块又被优化为保留输入维度的normal cells和减小空间维度的reduction cell（通过使stride=2）。最后通过预定义的方式堆叠这些组块构建最终的体系结构。</li></ul><h3 id="搜索策略"><a href="#搜索策略" class="headerlink" title="搜索策略"></a><strong>搜索策略</strong></h3><p>搜索策略详细说明了如何探索搜索空间，它一方面希望快速找到性能良好的架构，另一方面，也要避免过早收敛到次优架构的区域。搜索策略包括随机搜索（RS），贝叶斯优化（BO），进化方法，强化学习（RL）和基于梯度的方法。</p><p>强化学习方法，将神经体系结构的产生视为代理动作，将搜索空间视为动作空间，将体系结构的性能评估视为奖励，不同的RL方法在表示代理策略和如何优化策略方面有所不同。Neural architecture search with reinforcement learning使用递归神经网络（RNN）策略顺序采样字符串，进而对神经体系架构进行编码，他们最初使用REINFORCE policy gradient 算法训练了该网络，但是后来的工作中（Learning transferable architecturs for scalable image recognition / Proximal policy optimization algorithms）使用了Proximal Policy Optimization；Designing neural network architectures using reinforcement learning中使用Q-learning来训练一种可依次选择层的类型和相应超参的策略。</p><p>这些方法的一个替代观点是顺序决策过程，策略采样动作顺序生成架构，环境的状态包括目前采样的动作，并且只有在最后一个动作完成后后才能获得奖励。但是，过程中没有与环境发生互动（没有观察到外部状态，也没有中间奖励），因此将体系结构采样过程解释为单个动作的顺序生成更为直观，Efficient architecture search by network transformation提出了一种相关的方法，在他们的方法中，状态是当前（经过部分训练的）架构，奖励是对该架构性能的评估，而动作对应于保留功能的突变应用，随后是网络的训练阶段，为了处理可变长度的网络体系结构，他们使用双向LSTM将体系结构编码为固定长度的表示形式，基于此编码标识，动作网络决定采样的动作，这两个组成部分的组合构成了策略，该策略使用REINFORCE policy gradient 算法进行了端到端的训练。</p><p>进化方法用进化算法优化神经架构，第一个用此方法的可以追溯到30年前用遗传算法提出架构，用反向传播优化权重。自那以后，很多神经进化算法用遗传算法同时优化神经架构和和权重，然而，当扩展到具有数百万权重的当代神经体系结构以进行监督学习任务时，基于SGD的权重优化方法胜过进化的方法。所以后来人们使用基于梯度的方法优化权重，仅仅使用进化算法优化神经架构。进化算法进化出大量模型，即一组（可能训练有素的）网络；在每个进化步骤中，至少要采样种群中的一个模型，并作为父代通过对其应用突变来生成后代。在NAS的上下文中，变异是本地操作，例如添加或删除层，更改层的超参数，添加跳远连接以及更改训练超参数。在训练后代之后，评估它们的适应度（例如，在验证集中的表现）并将其添加到种群中。</p><p>神经进化方法在采样父母，更新种群和产生后代的方式上有所不同。采样父母：锦标赛选择、使用反密度从多目标Pareto前沿对父母进行采样。更新种群：去除最差的个人、去除最老的个体、不移除个人。产生后代：随机初始化子网络、Lamarckian inheritance：知识（以学习的权重的形式）通过使用网络态射从父网络传递到子网络、让后代继承不受其突变影响的父代所有参数。</p><p>Aging Evolution for Image Classifier Architecture Search比较了RL,evolution和random search方法，RL和进化在最终测试准确性方面表现均相当好，进化在任何时候都有更好的性能，并且找到更小的模型。在他们的实验中，这两种方法始终比RS表现更好，但幅度很小。</p><p>贝叶斯优化方法是最流行的超参数优化方法之一，但由于典型的BO工具箱基于高斯过程和专注于低维连续优化问题。Kernels for bayesian optimization in conditional parameter space和Neural architecture search with bayesian optimisation and optimal transport派生了用于架构搜索空间的内核函数，以便使用基于GP的经典BO方法。另外、一些作品使用基于树的模型，以达到在各种问题上有效地搜索高维条件空间并实现最先进的性能，共同优化神经体系结构及其超参数。尽管缺乏全面的比较，但初步证据表明这些方法也可以胜过进化算法。</p><p>Automatically Designing and Training Deep Architectures和Finding Competitive Network Architectures Within a Day Using UCT利用其搜索空间的树结构并使用了蒙特卡洛树搜索。Simple And Efficient Architecture Search for Convolutional Neural Networks提出了一种简单但性能良好的爬山算法，该算法通过贪婪地朝性能更好的架构的方向移动而发现高质量的架构，而无需更复杂的探索机制。上述方法采用离散搜索空间。DARTS提出了continuous relaxation来直接基于梯度优化operation的权重。SNAS,ProxylessNAS没有优化可能操作的权重α,而是建议在可能的操作上优化参数化分布。Differentiable neural network architecture search以及Ahmed和Maskconnect: Connextivity learning by gradient descent还采用了基于梯度的神经体系结构优化，但是分别专注于优化层超参数或连接模式。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;整理了一些最近的object detection算法&lt;br&gt;忽然发现。。。。&lt;br&gt;这是要开启NAS时代的神仙打架了嘛/facepalm&lt;/p&gt;
&lt;p&gt;不过讲真…Google这些nas论文，计算量贼大，要卡要数据的啊……复现起来不是一点两点困难&lt;/p&gt;
&lt;p&gt;“我是FAIR时代的残党！新时代没有能载我的模型！”（说白了就是没卡。。。。）&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>逆向工程：从Gerber到PCB</title>
    <link href="http://yuanquanquan.top/2020/20200417/"/>
    <id>http://yuanquanquan.top/2020/20200417/</id>
    <published>2020-04-17T06:40:40.000Z</published>
    <updated>2020-06-09T21:06:07.358Z</updated>
    
    <content type="html"><![CDATA[<h1 id="序"><a href="#序" class="headerlink" title="序"></a><em>序</em></h1><blockquote><p>​       最近将PCB图纸送去工厂打样，需要将Gerber转换成PCB文件，如何将Gerber转换成PCB的文章在网上可以找到很多，但大部分文章只是简单罗列了其中的几个步骤。对于初学者来说，只是”知其然而不知其所以然”，如果转换不成功、甚至于转换完成之后的PCB的逻辑连接发生了错误，也只能是一脸懵逼，不知道错在哪里。</p><p>​        所以这篇博客记录一下Gerber转换PCB过程中的每个步骤以及执行该步骤的原因、目的，能对这一”逆向工程“的过程有一个清晰的了解。</p></blockquote><a id="more"></a>  <p><strong>Gerber文件是什么？</strong></p><p><strong>Gerber</strong>，又称光绘文件，是一种标准的文件格式。PCB板厂可以通过Gerber直接生产出最终的PCB。目前常用的格式为RS-274X，大部分板厂也支持最新Gerber X2格式(多了板的层叠信息与属性)。</p><p>国内很多工程师喜欢把PCB设计文件直接发给板厂，其实这一做法是有风险的。因为板厂最终还是需要把PCB文件转换成Gerber才能进行生产，而从PCB到Gerber的这一过程存在很多潜在的问题：</p><ul><li>由于Gerber精度设置的问题，PCB上连通的网络未必在Gerber上连通</li><li>由于EDA工具版本的问题，可能板厂打开的PCB文件已经发生了变化</li><li>由于缺乏沟通，PCB上一些不需要显示的参数也遗留在成品上</li></ul><p>总而言之，PCB转成Gerber的这一过程未必是copy不走样，设计师应对这一过程负责。所以一般有经验的老工程师都会亲手把PCB转成Gerber，然后通过CAM350之类的软件，检查一下Gerber是否有问题，然后再送板厂加工。</p><p>另外，无论您使用Altium、Allegro或者是Expedition，只要转成了Gerber，板厂都可以正确识别。<img src="https://i.loli.net/2020/05/14/fWR5BlijzE73xvT.png" alt></p><p>还有一点，从Gerber反向生成PCB是比较困难的，出于安全性的考虑，企业也不愿意把设计源文件直接发给板厂。</p><p><strong>Gerber的组成</strong></p><p>Gerber由两个重要的组成部分，缺一不可：</p><ul><li>Gerber光绘文件：PCB的每个层都对应一个独立的Gerber文件，用于描述该层中的内容(有点类似于胶片)</li><li>NC Drill钻孔文件：垂直方向上的钻孔，决定了不同层中导线的连接</li></ul><p>从3D的角度，完整的Gerber文件包含了每个层的信息以及各个层中用于信号连接的钻孔(过孔)：</p><p>以AD为例，生成Gerber文件时，不是一个文件，而是一组文件，包含了PCB中每个层中的信息。下表是AD生成的Gerber文件后缀极其代表的意义。</p><table><thead><tr><th><strong>GERBER后缀</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>G1，G2，etc</td><td>Mid-layer 1, 2(中间信号层)</td></tr><tr><td>GBL</td><td>Bottom Layer(底层)</td></tr><tr><td>GBO</td><td>Bottom Overlay(底层丝印)</td></tr><tr><td>GBP</td><td>Bottom Paste Mask(底层助焊)</td></tr><tr><td>GBS</td><td>Bottom Solder Mask(底层阻焊)</td></tr><tr><td>GD1, GD2, etc.</td><td>Drill Drawing(钻孔图)</td></tr><tr><td>GG1, GG2, etc.</td><td>Drill Guide(钻孔向导)</td></tr><tr><td>GKO</td><td>Keep Out Layer</td></tr><tr><td>GM1, GM2, etc.</td><td>Mechanical Layer 1, 2(机械层)</td></tr><tr><td>GP1, GP2, etc.</td><td>Internal Plane Layer 1, 2(内电层)</td></tr><tr><td>GPB</td><td>Pad Master Bottom(底层焊盘)</td></tr><tr><td>GPT</td><td>Pad Master Top(顶层焊盘)</td></tr><tr><td>GTL</td><td>Top Layer(顶层)</td></tr><tr><td>GTO</td><td>Top Overlay(顶层丝印)</td></tr><tr><td>GTP</td><td>Top Paste Mask(底层助焊)</td></tr><tr><td>GTS</td><td>Top Solder Mask(顶层阻焊)</td></tr></tbody></table><p>如果是Allegro，那就是一堆的*.art文件(Artwork)。</p><p>当然，只有Gerber文件是不够的，因为Gerber并没有层与层之间导线连接的关系。如果一个信号流过了多个层，肯定需要由过孔进行过渡。NC Drill文件记录了PCB上所有需要钻孔的信息(包括位置、孔的大小等)，也是制造PCB不可或缺的关键信息。</p><p>以下是AD生成的NC Drill钻孔文件：</p><table><thead><tr><th>文件名</th><th>描述</th></tr></thead><tbody><tr><td>FileName.DRL(可选)</td><td>二进制格式的钻孔文件。对于一个存在盲、埋孔的多层PCB，每个Layer Pair(层对)都会生成一个单独后缀的TX*</td></tr><tr><td>FileName.DRR</td><td>钻孔报告 - 包括tool的详细信息，孔的尺寸、数量、Tool travel等</td></tr><tr><td>FileName.TXT</td><td>ASCII格式的钻孔文件。对于一个存在盲、埋孔的多层PCB，每个Layer Pair(层对)都会生成一个单独后缀的TX*</td></tr><tr><td>FileName-Plated.TXT(可选)</td><td>ASCII格式的钻孔文件. 选择后只文件中只记录PTH的钻孔。对于不通类型的钻孔(比如Slot、方形)，会生成单独的文件</td></tr><tr><td>FileName-NonPlated.TXT(可选)</td><td>ASCII格式的钻孔文件. 选择后只文件中只记录NPTH的钻孔。对于不通类型的钻孔(比如Slot、方形)，会生成单独的文件</td></tr><tr><td>FileName-BoardEdgeRout.TXT(可选)</td><td>ASCII格式的rout文件。记录了板形以及板子的cutout</td></tr><tr><td>FileName.LDP</td><td>ASCII格式的钻孔对报告。被CAM编辑器用来检测盲、埋孔</td></tr></tbody></table><p>下图是一个默认状态下AD生成的NC Drill示例：</p><p><img src="http://q8pl344z8.bkt.clouddn.com/Geber-2.webp" alt></p><p>示例中含有盲、埋孔，生成NC Drill时每种类型的盲、埋孔都会单独生成一个*.TX1/2/3的文件。不同类型的孔(Slot、方形)也会单独生成一个TXT文件。</p><p>说了那么多无非是要强调一点，无论正向还是逆向，这两个类型的文件是绝对不可以少的：</p><ol><li>一个叫Gerber</li><li>一个叫NC Drill</li></ol><p><strong>如何从Gerber生成PCB</strong></p><p>Altium Designer自带了一个类似CAM350的工具，叫Camtastic。虽然用起来不如CAM350那么顺手，但基本的功能都是没有问题的。从Gerber到PCB的逆向工程，就由Camtastic工具来完成。</p><p><strong>第一步：新建一个Cam文档</strong></p><p><img src="https://i.loli.net/2020/05/03/GSeLkUrlho5YvTu.png" alt></p><p><strong>第二步：导入Gerber及NC Drill</strong></p><p><img src="https://i.loli.net/2020/05/03/CzNMwb9ZYPWjQf1.jpg" alt></p><p>这是比较关键的一步，如果只导入Gerber，不导入NC Drill，会导致后续无法解压网表。如果有IPC Netlist，也可以一起导入，这样在解压网表时，网表的名称就会与源设计相同，而默认情况下系统会自定义网表的名称。</p><p>在Preference中，可以定义CAM识别的文件后缀。如果要导入Allegro或者Expedition的Gerber，可以在这里添加相应的后缀以便于快速识别。</p><p><img src="http://q8pl344z8.bkt.clouddn.com/Gerber-5.webp" alt></p><p>导入完成后，就可以在AD中查看或编辑Gerber文件了：<img src="https://i.loli.net/2020/05/03/3oBV6FxwCaLASzq.png" alt></p><p><strong>第三步：层的映射</strong></p><p>这也是比较关键的一步，点击<strong>Tables » Layers</strong>打开映射界面：</p><p><img src="https://i.loli.net/2020/05/03/SewmgryXJZt4vOh.jpg" alt></p><p>左侧的Gerber层必须和Type列中的PCB层正确映射。其中信号层的映射尤其重要：顶层/底层分别映射为Top/Bottom；中间信号层映射为Internal；内电层映射为Pos Plane/Neg Plane。</p><p>除此之外，NC Drill的TXT文件也需要正确映射到Drill Top/Bottom/Int。</p><p><strong>第四步：层的顺序(Layer Order)</strong></p><p>层映射完毕之后，会弹出Create / Update Layer Order对话框(也可以通过<strong>Layer » Order</strong>菜单访问)：</p><p><img src="https://i.loli.net/2020/05/03/KJmFWs8dljNb1R4.png" alt></p><p>在这里，需要定义所有的信号层(包括Top、Bottom、中间信号层及内电层)的物理顺序。这一步也相当关键，如果存在盲、埋孔，这里定义错误的话会直接导致PCB导出的错误。记住，Top层的Physical  Order始终是1，其它层的顺序按实际的板层结构依次类推。Physical Order中的数字是不可以重复的。</p><p><strong>第五步：解压网表(Extract Net)</strong></p><p>如果以上步骤都没有问题的话，就可以通过Gerber和NC Drill反推出PCB中的网表信息。点击<strong>Tools</strong> <strong>»</strong> <strong>Netlist</strong> <strong>»</strong> <strong>Extract</strong>菜单得到PCB的网表：</p><p><img src="https://i.loli.net/2020/05/03/XrwDmWQ2xqKpoLl.png" alt></p><p><strong>第六步：导出PCB</strong></p><p>一切就绪，最后一步是导出PCB。如果缺少Net List或其它信息，Export to PCB的菜单是灰色不能点击的。</p><p><img src="https://i.loli.net/2020/05/03/TLkXbyhQFxs81eo.jpg" alt></p><p><strong>第七步：清理PCB文件</strong></p><p>​      逆向工程导出的PCB只是Track和Pad的组合体，压根就没有器件(Component)的概念。您还需要通过无数次的Ctrl+C、Ctrl+V才能重新把这些零零碎碎的对象组合成正常的PCBLib。向工程只是复原PCB的样式和逻辑连接而已，并非是恢复到原始的PCB设计文件。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;&lt;em&gt;序&lt;/em&gt;&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;​       最近将PCB图纸送去工厂打样，需要将Gerber转换成PCB文件，如何将Gerber转换成PCB的文章在网上可以找到很多，但大部分文章只是简单罗列了其中的几个步骤。对于初学者来说，只是”知其然而不知其所以然”，如果转换不成功、甚至于转换完成之后的PCB的逻辑连接发生了错误，也只能是一脸懵逼，不知道错在哪里。&lt;/p&gt;
&lt;p&gt;​        所以这篇博客记录一下Gerber转换PCB过程中的每个步骤以及执行该步骤的原因、目的，能对这一”逆向工程“的过程有一个清晰的了解。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Altium" scheme="http://yuanquanquan.top/tags/Altium/"/>
    
  </entry>
  
  <entry>
    <title>EfficientDet:Scalable and Efficient Object Detection</title>
    <link href="http://yuanquanquan.top/2020/20200323/"/>
    <id>http://yuanquanquan.top/2020/20200323/</id>
    <published>2020-03-23T09:54:14.000Z</published>
    <updated>2020-06-09T21:06:53.226Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>  EfficientDet是Google Brain于去年11月份公布的目标检测算法族，涵盖轻量级到高精度的多个模型，提出了7种不同的网络结构</p></blockquote><a id="more"></a>  <p><img src="https://i.loli.net/2020/05/03/dnoJUfuKp3lTQOR.jpg" alt>根据其复杂度不同，可以适应不同计算能力的平台，COCO数据集上达到 50.9 mAP，在coco榜单上算前无古人了，可以说是直接把YOLOV3，Mask-R CNN按在地上摩擦……..</p><p>​      前两天，Google Brain终于官方开源了，开源地址：(<a href="https://github.com/google/automl/tree/master/efficientdet)，位于Google新开的automl项目内，看样子以后这个项目还会有其他自动机器学习的算法开源。EfficientDet原出于论文" target="_blank" rel="noopener">https://github.com/google/automl/tree/master/efficientdet)，位于Google新开的automl项目内，看样子以后这个项目还会有其他自动机器学习的算法开源。EfficientDet原出于论文</a> EfficientDet: Scalable and Efficient Object Detection，开源页面显示，这篇论文已经被CVPR 2020接收</p><p>EfficientDet与EfficientNet的第一作者是同一人，可以说Google Brain在EfficientNet的基础上提出了针对于物体检测的可扩展模型架构EfficientDet。EfficientDet主要包括两方面贡献：</p><ol><li><p>双向FPN <strong><em>BiFPN（Bi-directional feature pyramid network</em></strong>，在simplified PANet上引入了lateral shortcut）和weighted-BiFPN（在不同scale的特征进行融合时引入注意力机制对不同来源的feature进行权重调整（per-feature / per-channel / pei-pixel），由实验来看带来的性能提升相比BiFPN较小，看论文总结BiFPN提高了4个百分点）</p></li><li><p>仿照EfficientNet中的Compound Scaling方法，对检测网络中的各个部分进行Compound Scaling（输入图像大小，backbone的深度、宽度，BiFPN的深度（侧向级联层数），cls/reg head的深度）。个人觉得这是最大的亮点，提出了目标检测网络联合调整复杂度的策略。</p></li></ol><p>除此之外值得一提的是EfficienDet中使用的模型缩放，作者结合BiFPN和特征融合策略设计了与YOLOv3精度相仿的EfficientDet-D0，按照一定的优化规则，在网络的深度、宽度、输入图像的分辨率上进行模型缩放，可在统一架构下得到适合移动端和追求高精度的多个模型，根据其复杂度不同，可以适应不同计算能力的平台。</p><h1 id="EfficientDet结构"><a href="#EfficientDet结构" class="headerlink" title="EfficientDet结构"></a><strong><em>EfficientDet结构</em></strong></h1><p><img src="https://i.loli.net/2020/05/03/DeR8pidSbA14kPa.jpg" alt><br>EfficientDet组合了backbone（使用了EfficientNet）和BiFPN（特征网络）和Box prediction net，上图整个框架就是EfficientDet的基本模型</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;  EfficientDet是Google Brain于去年11月份公布的目标检测算法族，涵盖轻量级到高精度的多个模型，提出了7种不同的网络结构&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CV" scheme="http://yuanquanquan.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>红外循迹传感器</title>
    <link href="http://yuanquanquan.top/2020/20200321/"/>
    <id>http://yuanquanquan.top/2020/20200321/</id>
    <published>2020-03-21T12:37:40.000Z</published>
    <updated>2020-06-09T21:07:36.606Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>​      这次尝试制作了一个简单的红外循迹传感器，红外循迹传感器是专为轮式机器人设计的一款距离可调式避障传感器。其具有一对红外线发射与接收管，发射管发射出一定频率的红外线，当检测方向遇到障碍物（反射面）时，红外线反射回来被接收管接收，经过比较器电路处理之后，开关指示灯会亮起，同时信号输出接口输出数字信号（一个低电平信号），可通过电位器旋钮调节检测距离，有效距离范围2～30cm，工作电压为3.3V-5V，由于工作电压范围宽泛，在电源电压波动比较大的情况下仍能稳定工作，该传感器的探测距离可以通过电位器调节、具有干扰小、使用方便等特点，可以广泛应用于机器人避障、避障小车和黑白线循迹等众多场合。适合多种单片机、Arduino控制器、树莓派使用，安装到机器人上即可感测周围环境的变化。</p></blockquote><a id="more"></a>  <p><img src="https://i.loli.net/2020/05/03/yRGVOFHch7bxqdS.jpg" alt></p><p>电路工作说明</p><p>1、 当模块检测到前方障碍物信号时，开关指示灯点亮电平，同时OUT端口持续输出低电平信号,该模块检测距离2～30cm，检测距离可以通过电位器进行调节。</p><p>2、传感器模块输出端口OUT可直接与单片机IO口连接即可，也可以直接驱动一个5V继电器。</p><p>3、比较器采用LM393。</p><p>4、可采用3-5V直流电源对模块进行供电。当电源接通时，电源指示灯点亮。</p><p>接口说明</p><p>1 、VCC 外接3.3V-5V电压（可以直接与5v单片机和3.3v单片机相连）；</p><p>2 、GND 外接电源负极；</p><p>3 、OUT 为数字量输出接口（输出0或1高低电平）</p><p>Altium Designer画的原理图和PCB图如下：<img src="https://i.loli.net/2020/05/03/t1X73vC2gzdZwME.png" alt></p><p><img src="https://i.loli.net/2020/05/03/PYmXjkhwLyDSEZu.png" alt="红外-2"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;​      这次尝试制作了一个简单的红外循迹传感器，红外循迹传感器是专为轮式机器人设计的一款距离可调式避障传感器。其具有一对红外线发射与接收管，发射管发射出一定频率的红外线，当检测方向遇到障碍物（反射面）时，红外线反射回来被接收管接收，经过比较器电路处理之后，开关指示灯会亮起，同时信号输出接口输出数字信号（一个低电平信号），可通过电位器旋钮调节检测距离，有效距离范围2～30cm，工作电压为3.3V-5V，由于工作电压范围宽泛，在电源电压波动比较大的情况下仍能稳定工作，该传感器的探测距离可以通过电位器调节、具有干扰小、使用方便等特点，可以广泛应用于机器人避障、避障小车和黑白线循迹等众多场合。适合多种单片机、Arduino控制器、树莓派使用，安装到机器人上即可感测周围环境的变化。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="单片机" scheme="http://yuanquanquan.top/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>SqueezeNAS:Fast neural architecture search for faster semantic segmentation</title>
    <link href="http://yuanquanquan.top/2020/20200313/"/>
    <id>http://yuanquanquan.top/2020/20200313/</id>
    <published>2020-03-13T03:32:41.000Z</published>
    <updated>2020-06-09T21:08:33.318Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>用NAS做语义分割，1.不使用代理，MobileNetV3先搜索分类任务作为代理，SqueezeNAS直接搜索语义分割。 2.使用资源感知损失，直接优化目标平台上的延迟，而不是优化MAC指标，它对于运行速度没有必然关系</p></blockquote><a id="more"></a>  <h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>真实场景下，需要DNN在目标任务上准确率高，在目标计算平台上推断延迟低。NAS已经被用于低延迟的图像分类，但是其他任务还很少。这是第一篇用于密集语义分割的无代理硬件感知的搜索。在Cityscapes数据集上达到最好的性能。我们证明了利用NAS对任务和推断同时优化可以获得极大地性能提升。</p><h2 id="1-Introduction-and-Motivation"><a href="#1-Introduction-and-Motivation" class="headerlink" title="1. Introduction and Motivation"></a>1. Introduction and Motivation</h2><p>在做非图像分类任务（如语义分割或目标检测）时，流行的方法是结构迁移：从一个图像分类网络开始，在网络末尾添加一些针对特定任务的层。</p><p>这种结构迁移的主流主要是一些传统假设，我们罗列一些并证明为什么他们已经过时了。</p><ul><li><strong>假设1： 在ImageNet上准确率最高的网络也应该是目标任务上准确率最好的骨干网络</strong></li></ul><p>实际上，ImageNet的准确率与目标任务准确率关系不太大。SqueezeNet在ImageNet上准确率低于VGG，但是对上定位图像相似部分的任务上更好。正确的网络设计取决于目标任务。</p><ul><li><strong>假设2： NAS成本过高</strong></li></ul><p>实际上，有些方法的确需要数千个GPU天，但是最近的“supernetwork”方法例如DARTS和FBNet可以在10个GPU天上取得最优结果。</p><ul><li><strong>假设3： 低MAC（Fewer multiply-accumulate）运算在目标平台上有较低的延迟</strong></li></ul><p>实际上，有工作证明了相同的平台上，同样的MAC可以有10x的延迟差异。取决于处理器和内核实现，即使是相同的MAC也有不同的速度。</p><p>为了在目标计算平台和任务上获得更低的延迟，更高的准确率：</p><ol><li>直接对目标任务运行NAS，例如目标检测、语义分割，不要去优化代理任务，如图像分类。</li><li>使用现代的基于supernetwork的NAS，相信搜索可以快速收敛。</li><li>让NAS同时优化准确率和延迟。</li></ol><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h2><p>【略】</p><h2 id="3-Architecture-Search-Space"><a href="#3-Architecture-Search-Space" class="headerlink" title="3. Architecture Search Space"></a>3. Architecture Search Space</h2><p>我们探索了顺序反向残差模块的语义分割网络编码器的空间。这些块参数化如图2。每次结构搜索，我们约束宏观结构，对每个块寻找最优参数。搜索空间和FBNet，MobileNetV2，MobileNetV3相似，这样可以直接比较对分割优化的网络和他们对分类优化的网络。</p><p><img src="https://i.loli.net/2020/05/03/LgdW9ArGtIoKjh2.png" alt></p><p>我们网络的基本结构如图1，解码器使用编码器的输出和低层特征。</p><p><img src="https://i.loli.net/2020/05/03/Bs4LQoznhZ5drAg.png" alt></p><h3 id="3-1-Constrained-Macro-Architecture"><a href="#3-1-Constrained-Macro-Architecture" class="headerlink" title="3.1. Constrained Macro-Architecture"></a>3.1. Constrained Macro-Architecture</h3><p>搜索三个空间，Small, Large, XLarge。为了定义结构空间，先约束编码网络的宏观结构。宏观结构描述了编码器的模块数量N，解码器也是这么多。对于每个模块，固定输入和输出通道，$c_{in} $和$c_{out}$每个块的深度卷积层步长使用1或2。由于运行跳跃连接，所以最终的层数可能小于N。</p><p>在Small和Large的搜索空间，使用LR-ASPP解码器。在XLarge搜索空间，我们使用完全深度卷积的ASPP变种。</p><h3 id="3-2-Block-Search-Space"><a href="#3-2-Block-Search-Space" class="headerlink" title="3.2. Block Search Space"></a>3.2. Block Search Space</h3><p>在每个宏观搜索空间，NAS挑选每个块最优的超参数，或者替换为无运算的跳跃连接。如图2，超参数定义了 1*1 卷积是否分组，深度绝技是否膨胀2倍，深度卷积的核大小k，膨胀率e。如图7，我们挑选12个可能的配置。</p><h2 id="4-Neural-Architecture-Search-Algorithm"><a href="#4-Neural-Architecture-Search-Algorithm" class="headerlink" title="4. Neural Architecture Search Algorithm"></a>4. Neural Architecture Search Algorithm</h2><p>把结构搜索看做是复杂supernetwork的路径选择问题，这样确定的结构可以看做是supernetwork的某些路径。如图3，我们定义supernetwork为superblocks的序列，图4是一个例子。</p><p><img src="https://i.loli.net/2020/05/03/Buy2SrwDzPehUv3.png" alt></p><p>同时优化卷积权重<em>w</em>和结构参数<em>θ</em>, 优化损失函数为L<em>(</em>θ<em>,</em>w<em>)=</em>L<strong>P<em>(</em>θ<em>,</em>w<em>)+</em>α<em>∗</em>L</strong>E<em>(</em>θ*)</p><p>$L_p$表示特定问题的损失，$L_E$是资源感知损失项，超参数<em>α</em>控制两个平衡。由于本工作关注与语义分割，是$L_p$像素级别的交叉熵。对于，$L_E$我们同时实验了目标平台的推断延迟和Multiply-Accumulates的估计值。</p><p><code>表示特定问题的损失，</code><em>α</em><code>LP</code>是像素级别的交叉熵。对于</p><p><code></code>我们同时实验了目标平台的推断延迟和Multiply-Accumulates的估计值。</p><h3 id="4-1-Gumbel-Softmax"><a href="#4-1-Gumbel-Softmax" class="headerlink" title="4.1. Gumbel-Softmax"></a>4.1. Gumbel-Softmax</h3><p>为了让supernetwork的优化和计算可跟踪，每个superblock独立挑选一个候选块。这样，可以把这个选择看成是对独立类别分布的采样，把superblock i的候选块j概率记作<em>p</em>(<em>i</em>,<em>j</em>) , 用softmax定义：$p ( i , j | \theta ) = \frac { e ^ { \theta _ { i , j } } } { \sum _ { j } ^ { 13 } e ^ { \theta _ { i , j } } }$</p><p>类别分布很难高效优化，所以我们使用Gumbel-Softmax松弛。Gumbel-Softmax分布由稳定参数t控制，t趋近于0，Gumbel-Softmax分布等价于类别分布，稳定参数从5.0到1.0退火。</p><h2 id="4-2-Early-Stopping"><a href="#4-2-Early-Stopping" class="headerlink" title="4.2. Early Stopping"></a>4.2. Early Stopping</h2><p>我们的supernetwork方法中，优化过程需要计算每一个候选的block，无论学到的结构分布是什么。当最优的网络结构收敛的时候，性能差的候选块虽然选择的概率低，但是依然需要继续计算。所以当候选概率小于0.5%的时候，直接移除。虽然有可能低概率的候选块可能后面是最优的，但是实践中没有发现这种情况。该优化可以减少一半的搜索时间。</p><h2 id="4-3-Resource-Aware-Architecture-Search"><a href="#4-3-Resource-Aware-Architecture-Search" class="headerlink" title="4.3. Resource-Aware Architecture Search"></a>4.3. Resource-Aware Architecture Search</h2><p>定义资源感知损失为：</p><p>$L _ { E } ( \theta ) = \sum _ { j } ^ { N } \sum _ { i } ^ { 13 } p \left( i , j | \theta _ { i } \right) C ( i , j )$</p><p><em>C</em>(<em>i</em>, <em>j</em>)表示网络块i选择候选j的资源成本。对每个块独立建模资源成本。</p><h2 id="5-Experiments-and-Results"><a href="#5-Experiments-and-Results" class="headerlink" title="5. Experiments and Results"></a>5. Experiments and Results</h2><p>证明两个关键点：第一，NAS是可以产生搞准确率低延迟网络的工具。第二，优化与硬件无直接联系的指标如MAC不是个合适的代理，可能导致局部最优。</p><p>沿用Small, Large, XLarge三种搜索空间。先用NAS在每个空间上对MAC进行搜索，然后在嵌入式设备上查看这些低MAC的网络的延迟作为基线。最后再搜索优化硬件感知的延迟找到3个新的网络。</p><h3 id="5-1-Hardware-Agnostic-Search"><a href="#5-1-Hardware-Agnostic-Search" class="headerlink" title="5.1. Hardware-Agnostic Search"></a>5.1. Hardware-Agnostic Search</h3><p>对于与硬件无关的结构搜索，使用NAS对MAC进行优化，在MAC和mIOU上找到pareto最优，根据查找表计算每个候选块j的MAC，使得<em>C</em>(<em>i</em>,<em>j</em>)=<em>M<strong>A</strong>C<strong>S</strong>i</em>,<em>j</em></p><p>。对每个搜索空间搜索找到最优的SqueezeNAS-MAC网络，结果如下表。</p><p><img src="https://i.loli.net/2020/05/03/KARr3uocmWqndG4.png" alt></p><h3 id="5-2-Hardware-Aware-Search"><a href="#5-2-Hardware-Aware-Search" class="headerlink" title="5.2. Hardware-Aware Search"></a>5.2. Hardware-Aware Search</h3><p>使用相同的NAS算法和搜索空间，但是使用延迟优化目标：<em>C</em>(<em>i</em>,<em>j</em>)=<em>L<strong>a</strong>t<strong>e</strong>n<strong>c</strong>y**i</em>,<em>j</em></p><p>。为了计算每个模块j在候选j上的延迟，我们测量了目标平台上所有候选的推断时间，最后得到三个SqueezeNAS-LAT网络。结果见表1。</p><h3 id="5-3-Implementation"><a href="#5-3-Implementation" class="headerlink" title="5.3. Implementation"></a>5.3. Implementation</h3><h3 id="5-4-Results"><a href="#5-4-Results" class="headerlink" title="5.4. Results"></a>5.4. Results</h3><h2 id="6-Network-Analysis"><a href="#6-Network-Analysis" class="headerlink" title="6. Network Analysis"></a>6. Network Analysis</h2><p>比较三者的模块选择：优化延迟的网络（Latency-aware），优化MAC的网络（MAC-aware），MobileNetV3。由于这三个都使用反向残差块，可以把MobileNetV3的块放入13个候选中，如图7。这里没有考虑SE块。</p><p><img src="https://i.loli.net/2020/05/03/Cby6WpkJZ7RnAe9.png" alt></p><p>可视化网络如图8，【略】<img src="https://i.loli.net/2020/05/03/rY8g2HIBMncEmGk.png" alt></p><p> <img src="https://i.loli.net/2020/05/03/zZOSA7D9TFeaXWH.png" alt></p><p><img src="https://i.loli.net/2020/05/03/u6zvOZDUQ2RxGWb.png" alt></p><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>第一、做无代理的语义分割搜索，我们的NAS生成的SqueezeNAS系列模型，相比MobileNet V3达到优越的延迟-准确率平衡。这种优越（至少部分）来自于MobileNetV3是用NAS优化图像分类作为代理实现语义分割任务的。</p><p>第二、虽然MobileNetV3作者使用了几千个GPU天，但是我们的方法每次搜索只需要7-15个GPU天。也就是说，基于supernetwork的NAS在8个GPU上不需要一个周末就可以跑出来最好的结果。</p><p>第三、我们做了两类NAS实验，一个搜索低MAC模型，一个搜索目标平台上低延迟模型。第二个我们获得了非常快同时准确率高的模型。最后，考虑到芯片的速度提升和计算平台的省级，NAS可以继续获得更低的延迟<img src="https://i.loli.net/2020/05/14/KEUDQtho26YVjx9.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;用NAS做语义分割，1.不使用代理，MobileNetV3先搜索分类任务作为代理，SqueezeNAS直接搜索语义分割。 2.使用资源感知损失，直接优化目标平台上的延迟，而不是优化MAC指标，它对于运行速度没有必然关系&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CV" scheme="http://yuanquanquan.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>【Object Detection】-YOLOV1</title>
    <link href="http://yuanquanquan.top/2020/20200312/"/>
    <id>http://yuanquanquan.top/2020/20200312/</id>
    <published>2020-03-12T02:36:03.000Z</published>
    <updated>2020-06-09T21:09:03.648Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>YOLO算法简介</strong></p><p>YOLO十分简单，一个网络同时对多个物体进行分类和定位，没有proposal的概念，是<strong><em>one-stage</em></strong>实时检测网络的里程碑，标准版在TitanX达到45 fps，快速版达到150fps，但精度不及当时的SOTA网络</p><p>YOLO算法使用深度神经网络进行对象的位置检测以及分类，主要的特点是速度够快，而且准确率也很高，采用直接预测目标对象的边界框的方法，将候选区和对象识别这两个阶段合二为一。</p></blockquote><a id="more"></a>  <p>Yolo算法不再是窗口滑动了，而是直接将原始图片分割成互不重合的小方块，然后通过卷积最后生产这样大小的特征图，基于上面的分析，可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标，这就是Yolo算法的朴素思想，而YOLOv3算法再以往的结构上做出了改进，增加了多尺度检测，以及更深的网络结构darknet53,这是比较主要的改进，还有某些细节上的变动。</p><p><em>Unified Detection</em></p><p><img src="https://i.loli.net/2020/05/03/2RriysU9MONjk3x.png" alt></p><p>将输入分为S*S的格子，如果GT的中心点在格子中，则格子负责该GT的预测：</p><p>对于PASCAL VOC数据集来说，设定s=7如图所示，分为7*7个小格子，每个格子预测两个bounding box。</p><p>如果一个目标的中心落入一个网格单元中，该网格单元负责检测该目标。</p><p>对每一个切割的小单元格预测（置信度，边界框的位置），每个bounding  box需要4个数值来表示其位置，(Center_x,Center_y,width,height)，即bounding  box的中心点的x坐标，y坐标，bounding box的宽度，高度)</p><p>置信度定义为<strong>是否存在目标</strong>与<strong>iou值</strong>的乘积，置信度可以反应格子是否包含物体以及包含物体的概率，无物体则为0，有则为IOU</p><p>$\text { Confidence } = \operatorname { Pr } ( \text { Object } ) * \text { IOU } _ { \text {pred } } ^ { \text {truth } }$</p><p>还要得到分类的概率结果；20个分类每个类别的概率。在测试时，将单独的bbox概率乘以类的条件概率得到最终类别的概率，综合了类别和位置的准确率</p><p> <strong>Network Design</strong></p><p>YOLO采用单个的卷积神经网络进行预测，YOLO的整个结构就是输入图片经过神经网络的变换得到一个输出的张量 。   步骤如下：</p><p>（1）骨干网络前20层接average-pooling层和全连接层进行ImageNet预训练，检测网络训练将输入从224×224增加到448×448</p><p>（2）在图像 上运行单个卷积网络</p><p>（3）由模型的置信度对所得到的检测进行阈值处理</p><p>首先，YOLO速度非常快。由于我们将检测视为回归问题，所以不需要复杂的流程。测试时在一张新图像 上简单的运行我们的神经网络来预测检测</p><p>其次，YOLO在进行预测时，会对图像进行全面地推理。与基于滑动窗口和区域提出的技术不同，YOLO在训练期间和测试时会看到整个图像，所以它隐式地编码了</p><p>关于类的上下文信息以及它们的外观。快速R-CNN是一种顶级的检测方法，但是它看不到更大的上下文信息，所以在图像中会将背景块误检为目标。与快速R-CNN相比，YOLO的背景误检数量少了一半</p><p>然后，由于YOLO具有高度泛化能力，因此在应用于新领域或碰到意外的输入时不太可能出故障。</p><p> <img src="https://i.loli.net/2020/05/03/dof2WHi4aJyIDBt.png" alt></p><p>所使用的卷积结构如图所示：受到GoogLeNet图像分类模型的启发。网络有24个卷积层，后面是2个全连接层，最后输出层用线性函数做激活函数，其它层激活函数都是Leaky ReLU。</p><p>我们 只使用1×1降维层，后面是3×3卷积层，</p><p><strong>Training</strong></p><p>最后一层使用ReLU，其它层使用leaky ReLU</p><p>YOLO的损失函数定义如下：</p><p>$\begin{array} { c } \lambda _ { \text {coord } } \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } \mathbb { 1 } _ { i j } ^ { \text {obj } } \left[ \left( x _ { i } - \hat { x } _ { i } \right) ^ { 2 } + \left( y _ { i } - \hat { y } _ { i } \right) ^ { 2 } \right] \ \quad + \lambda _ { \text {coord } } \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } \mathbb { 1 } _ { i j } ^ { \text {obj } } \left[ ( \sqrt { w _ { i } } - \sqrt { \hat { w } _ { i } } ) ^ { 2 } + ( \sqrt { h _ { i } } - \sqrt { \hat { h } _ { i } } ) ^ { 2 } \right] \end{array}$</p><p>$\begin{array} { l } \quad + \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } 1 _ { i j } ^ { \mathrm { obj } } \left( C _ { i } - \hat { C } _ { i } \right) ^ { 2 } \ + \lambda _ { \mathrm { nobbj } } \sum _ { i = 0 } ^ { S ^ { 2 } } \sum _ { j = 0 } ^ { B } 1 _ { i j } ^ { \mathrm { noobj } } \left( C _ { i } - \hat { C } _ { i } \right) ^ { 2 } \end{array}$</p><p>$+ \sum _ { i = 0 } ^ { S ^ { 2 } } \mathbb { 1 } _ { i } ^ { \mathrm { obj } } \sum _ { c \in \text { classes } } \left( p _ { i } ( c ) - \hat { p } _ { i } ( c ) \right) ^ { 2 }$</p><p> 损失函数如上图，一个GT只对应一个bounding box。由于训练时非目标很多，定位的训练样本较少，所以使用权重$\begin{array} { c } \lambda _ { \text {coord } }\end{array} $和$\begin{array} { c } \lambda _ { \text {nobjd} }\end{array} $来加大定位的训练粒度，包含3个部分：</p><p><strong>第一部分</strong>为坐标回归，使用平方差损失，为了使得模型更关注小目标的小误差，而不是大目标的小误差，对宽高使用了平方根损失进行变相加权。这里$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$指代当前b box是否负责GT的预测，需要满足2个条件，首先GT的中心点在该b box对应的格子中，其次该b box要是对应的格子的个box中与GT的IOU最大</p><p><strong>第二部分</strong>为b box置信度的回归，$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$跟上述一样，$\mathbb { 1 } _ { i j } ^ { \mathrm { nooobj } }$为$$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$$的b box，由于负样本数量较多，所以给了个低权重。若有目标，$\begin{array} hat { C }\end{array}$实际为IOU，虽然很多实现直接取1.</p><p><strong>第三部</strong>分为分类置信度，相对于格子而言，$\mathbb { 1 } _ { i j } ^ { \mathrm { obj } }$指代GT中心是否在格子中</p><p><img src="https://i.loli.net/2020/05/03/Bf5LEXw6rZ4xakY.png" alt="如图所示"></p><p>YOLO在ImageNet分类任务上以一半的分辨率（224*224的输入图像）预训练卷积层，然后将分辨 率加倍来进行检测。</p><p>训练中采用了drop out和数据增强（data augmentation）来防止过拟合.</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>  开创性的one-stage detector，在卷积网络后面接两个全连接层进行定位和置信度的预测，并设计了一个新的轻量级主干网络，虽然准确率与SOTA有一定距离，但是模型的速度真的很快<br>  作者提到了YOLO的几点局限性：</p><ul><li>每个格子仅预测一个类别，两个框，对密集场景预测不好</li><li>对数据依赖强，不能泛化到不常见的宽高比物体中，下采样过多，导致特征过于粗糙</li><li>损失函数没有完成对大小物体进行区别对待，应该更关注小物体的误差，因为对IOU影响较大，定位错误是模型错误的主要来源</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;YOLO算法简介&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;YOLO十分简单，一个网络同时对多个物体进行分类和定位，没有proposal的概念，是&lt;strong&gt;&lt;em&gt;one-stage&lt;/em&gt;&lt;/strong&gt;实时检测网络的里程碑，标准版在TitanX达到45 fps，快速版达到150fps，但精度不及当时的SOTA网络&lt;/p&gt;
&lt;p&gt;YOLO算法使用深度神经网络进行对象的位置检测以及分类，主要的特点是速度够快，而且准确率也很高，采用直接预测目标对象的边界框的方法，将候选区和对象识别这两个阶段合二为一。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="复健计划" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%A4%8D%E5%81%A5%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="Object Detection" scheme="http://yuanquanquan.top/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>【Object Detection】-Faster R-CNN</title>
    <link href="http://yuanquanquan.top/2020/20200311403/"/>
    <id>http://yuanquanquan.top/2020/20200311403/</id>
    <published>2020-03-11T03:40:03.000Z</published>
    <updated>2020-06-14T07:43:35.920Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong><em>写在前面</em></strong>：之前两个月没有看过cv方向的东西…..所以打算开个系列来写目标检测，图像分割的爆款算法，算是回顾一遍吧，当然一些太经典鼻祖级的模型就懒得看了……于是这个系列就决定叫复健计划了….目标检测的第一篇就从Faster R-CNN开始吧，Faster R-CNN 是目标检测中的一个很经典的two stage算法，许多其他的目标检测算法都会运用到Faster R-CNN的部分结构或思想。而且了解Faster R-CNN对理解其他R-CNN系列网络都有一定的帮助，包括Mask R-CNN，Stereo R-CNN 等等。</p></blockquote><a id="more"></a>  <p><img src="https://i.loli.net/2020/05/03/WpzKakMxdHJZ4w2.png" alt></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Faster R-CNN由R-CNN，Fast R-NN改进演变而来，相对于前两者，Faster R-CNN具有训练速度快，消耗内存减少，精度与检测速度都有大幅提升的优点。</p><p>先来一张Faster R-CNN的网络基本结构图<img src="https://i.loli.net/2020/05/03/AVMlhnPjQmbqNiZ.png" alt></p><p>个人认为网络主要分为五个部分：</p><ul><li>提取feature map（特征图）的<strong>Conv layer</strong>。该部分使用VGG-net作为预处理网络，运用多个conv，relu，pooling层提取图像特征图，为后面的网络提供图像信息。</li><li><strong>anchor</strong>的生成，Faster R-CNN对图像生成一系列的anchor，作为目标检测的先验框，用于多尺度预测，并在后面使用bounding box regression对其位置进行修正。</li><li><strong>RPN</strong>（Region Proposal Network），在feature map上的每个点生成anchor，然后将其映射回原图，对原图中的anchor进行修正、筛选，提取该区域的图像(region proposal)，也就是所谓的ROI，送进ROI pooing层。</li><li><strong>ROI pooing</strong>，对featuer map中的ROI划分为Pool_h*Poo_w(ROI pooling后特征图的高和宽)）个网格，对每一个网格进行maxpooling。</li><li>使用<strong>full connection</strong>，<strong>softmax</strong>对ROI进行分类与bounding box回归，确定bounding box的位置。</li></ul><p>下面从这五个部分进行说明。先上一个网上总结的Faster R-CNN的结构图。<img src="https://i.loli.net/2020/05/03/tTaqJoe8GMj3n6O.png" alt></p><h2 id="Conv-layer"><a href="#Conv-layer" class="headerlink" title="Conv layer"></a>Conv layer</h2><p>Conv layers包含3种层，分别是conv层，pooling层，relu层。<strong>conv</strong>层的卷积核大小都是3 * 3 , 步长都为1，并且都做了扩边处理，也就是经过conv层后图像的大小没有改变，只是深度改变。<strong>pooling</strong> 层的kernel_size=2,步长也为2，也就是说每经过一次pooling层后，图像尺寸减小一半。</p><p>Faster RCNN在将图片传入网络之前，会将图片缩放为<strong>M * N</strong>(VGG 为 800 * 600），从原图到feature map一共经过4次pooling，也就是feature map的大小为（int(M/16)，int(N/16)）。</p><h2 id="anchor"><a href="#anchor" class="headerlink" title="anchor"></a>anchor</h2><p>anchor其实是在图像上的一个个先验框，用来对后面检测框的修正以及region proposal。这里说一下anchor的生成过程。</p><p>先来看看作者的图<img src="https://i.loli.net/2020/05/03/b5MvdDNU43rHf8e.png" alt></p><p>可以看到，anchor的长宽比有[0.5, 1, 2]三种比例，每种anchor有[8, 16, 32]三个尺度比例，所以anchors一共有3 * 3 = 9个anchor。</p><p>生成anchor的主要分为一下步骤：</p><ul><li>首先设置一个16 <em> 16的窗口（因为feature map尺寸为原图的1/16，所以一个feature map上的点对应原图上16 </em> 16 的区域），计算的到[x_ctr, y_ctr, w, h]，也就是anchor的中心点坐标以及长宽4个量。</li><li>然后计算anchor的面积size = w <em> h，将size分别除[0.5, 1, 2]3种比例，再分别对3个新的sizes开根号作为新的anchor的3个w，再将w </em> [0.5, 1, 2]得到h，这样就得到3个anchor的长宽。</li><li>将3个anchor长宽分别再乘3个尺度比例，这样就得到9个anchor，再将anchor的表示转换为左上角和右下角的坐标[x_l, y_l, x_r, y_r]。</li></ul><p>此时每个feature map上的点都有9个anchor，也就是一共有（800/16）<em>（600/16） </em> 9=17100个anchor。再将这些anchor通过原图与feature map的映射关系，将其anchor映射回原图。<img src="https://i.loli.net/2020/05/03/DzSuMRHPLgarVsZ.png" alt></p><h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><p>RPN的主要结构如图<img src="https://i.loli.net/2020/05/03/8A6ejLOcau3KhfX.png" alt></p><p>首先将feature map在经过一次conv卷积（这里设得到的网络层为conv5_3)，然后分开两条线进行：</p><ul><li>先将conv5_3使用1 <em> 1 </em> 18的卷积核对anchor进行softmax分类，将anchor分为postivte和negative（rpn_cls_prob)，二分类。（18 = 2（positiv and negative）* 9（9个anchor）。</li><li>用于预测bounding box的坐标偏移值。</li><li>在Proposal层综合im_info（主要用来计算proposal的坐标以及限制proposal的大小以免超出图像边框）、rpn_box_pred 和rpn_cls_prob选择和提取ROI。</li></ul><h3 id="softmax-分类"><a href="#softmax-分类" class="headerlink" title="softmax 分类"></a>softmax 分类</h3><p>conv5_3经过1 <em> 1 </em> 18 卷积后，维度变为[W, H, 18]。softmax就是要将每个点的9个anchor进行二分类（positive和negative）。softmax前后各有一次reshape，其实只是为了让分类更方便而已，这是训练的一些trick。</p><h3 id="bounding-box-predict"><a href="#bounding-box-predict" class="headerlink" title="bounding box predict"></a>bounding box predict</h3><p>conv5_3经过1 <em> 1 </em> 36 卷积变为[W, H, 36]，第三个维度为每个anchor的2个坐标的偏移量，用于后面的bounding box regression。</p><h3 id="bounding-box-regression"><a href="#bounding-box-regression" class="headerlink" title="bounding box regression"></a>bounding box regression</h3><p>在训练时需要对anchor进行转换才能贴合GT_bbox。怎样转换呢，最简单的做法就是平移加缩放。</p><p>下面是转换的关系式：<br>$$<br>\begin{aligned}<br>&amp;t_{x}=\left(x-x_{\mathrm{a}}\right) / w_{\mathrm{a}}, \quad t_{\mathrm{y}}=\left(y-y_{\mathrm{a}}\right) / h_{\mathrm{a}}\<br>&amp;t_{\mathrm{w}}=\log \left(w / w_{\mathrm{a}}\right), \quad t_{\mathrm{h}}=\log \left(h / h_{\mathrm{a}}\right)\<br>&amp;\begin{aligned}<br>t_{\mathrm{x}}^{<em>} &amp;=\left(x^{</em>}-x_{\mathrm{a}}\right) / w_{\mathrm{a}}, \quad t_{\mathrm{y}}^{<em>}=\left(y^{</em>}-y_{\mathrm{a}}\right) / h_{\mathrm{a}} \<br>t_{\mathrm{w}}^{<em>} &amp;=\log \left(w^{</em>} / w_{\mathrm{a}}\right), \quad t_{\mathrm{h}}^{<em>}=\log \left(h^{</em>} / h_{\mathrm{a}}\right)<br>\end{aligned}<br>\end{aligned}<br>$$<br>损失函数为：<br>$$<br>\begin{aligned}<br>L\left(\left{p_{i}\right},\left{t_{i}\right}\right) &amp;=\frac{1}{N_{c l s}} \sum_{i} L_{c l s}\left(p_{i}, p_{i}^{<em>}\right) \<br>&amp;+\lambda \frac{1}{N_{r e g}} \sum_{i} p_{i}^{</em>} L_{r e g}\left(t_{i}, t_{i}^{*}\right)<br>\end{aligned}<br>$$</p><h2 id="ROI-proposal"><a href="#ROI-proposal" class="headerlink" title="ROI proposal"></a>ROI proposal</h2><p>ROI prosposal负责综合所有的关于anchor的变换和对softmax的分类positive anchor，在feature map上计算出精确的ROI，将其送入后面的ROI Pooling层。</p><p>主要步骤为：</p><ul><li>对softmax后的anchor按score进行排序，提取前N个score的anchor。</li><li>对这些anchor进行修正。</li><li>修正大于图像边缘的anchor。</li><li>对w或h小于设定阈值的anchor剔除。</li></ul><p><img src="http://q719r0aui.bkt.clouddn.com/fast%20r-cnn%288%29.webp" alt></p><p>​         <em>最后传入的ROI类似这样</em></p><p>至此，RPN的任务到此完成。</p><h2 id="ROI-pooling"><a href="#ROI-pooling" class="headerlink" title="ROI pooling"></a>ROI pooling</h2><p>Faster RCNN最后的Classification和bounding box 的预测需要用到全连接层，所以在将图片传入全连接层时需要将其变为固定大小。但是一般输入的ROI大小都不固定，如果利用采样的方法进行变换为所需要的大小，会对图像的结构产生影响。</p><p><img src="https://i.loli.net/2020/05/03/kDHwuP7YQKpcoTl.jpg" alt></p><p>为了解决这个问题，Faster RCNN提出了ROI pooling的方法。</p><p>具体方法为：</p><ul><li><p>将ROI划分为pool_h和pool_w个网格。</p></li><li><p>每个网格的起始和结束坐标计算方法为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int hstart = static_cast&lt;int&gt;(floor(ph * bin_size_h));        int wstart = static_cast&lt;int&gt;(floor(pw * bin_size_w));        int hend = static_cast&lt;int&gt;(ceil((ph + 1) * bin_size_h));        int wend = static_cast&lt;int&gt;(ceil((pw + 1) * bin_size_w));         //其中pw，ph是每个网格的坐标值</span><br></pre></td></tr></table></figure></li><li><p>计算完之后的每个网格可能会有重叠</p></li><li><p>将每个网格进行max pooling操作，这样就得到固定大小的图了。</p><p><strong>假设输出为2 * 2 大小</strong></p></li></ul><p><img src="C:%5CUsers%5CLenovo%5CDesktop%5Cyolov1%5Cfast%20r-cnn(11" alt>.jpg)</p><p><img src="https://i.loli.net/2020/05/03/droM81DilEjPFgN.png" alt></p><h2 id="Classification-，bounding-box-predict"><a href="#Classification-，bounding-box-predict" class="headerlink" title="Classification ，bounding box predict"></a>Classification ，bounding box predict</h2><p>这个没什么好说的了，就是使用全连接层和softmax层进行分类和预测bounding box坐标值。</p><p><img src="https://i.loli.net/2020/05/03/lnpwKxfgDUSdHhO.png" alt></p><table><thead><tr><th>方法</th><th>创新</th><th>缺点</th><th>改进</th></tr></thead><tbody><tr><td>R-CNN (Region-based Convolutional Neural Networks)</td><td>1、SS提取RP； 2、CNN提取特征； 3、SVM分类； 4、BB盒回归。</td><td>1、 训练步骤繁琐（微调网络+训练SVM+训练bbox）； 2、 训练、测试均速度慢 ； 3、 训练占空间</td><td>1、 从DPM HSC的34.3%直接提升到了66%（mAP）； 2、 引入RP+CNN</td></tr><tr><td>Fast R-CNN (Fast Region-based Convolutional Neural Networks)</td><td>1、SS提取RP； 2、CNN提取特征； 3、softmax分类； 4、多任务损失函数边框回归。</td><td>1、 依旧用SS提取RP(耗时2-3s，特征提取耗时0.32s)； 2、 无法满足实时应用，没有真正实现端到端训练测试； 3、 利用了GPU，但是区域建议方法是在CPU上实现的。</td><td>1、 由66.9%提升到70%； 2、 每张图像耗时约为3s。</td></tr><tr><td>Faster R-CNN (Fast Region-based Convolutional Neural Networks)</td><td>1、RPN提取RP； 2、CNN提取特征； 3、softmax分类； 4、多任务损失函数边框回归</td><td>1、 还是无法达到实时检测目标； 2、 获取region proposal，再对每个proposal分类计算量还是比较大。</td><td>1、 提高了检测精度和速度； 2、 真正实现端到端的目标检测框架； 3、 生成建议框仅需约10ms。</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;写在前面&lt;/em&gt;&lt;/strong&gt;：之前两个月没有看过cv方向的东西…..所以打算开个系列来写目标检测，图像分割的爆款算法，算是回顾一遍吧，当然一些太经典鼻祖级的模型就懒得看了……于是这个系列就决定叫复健计划了….目标检测的第一篇就从Faster R-CNN开始吧，Faster R-CNN 是目标检测中的一个很经典的two stage算法，许多其他的目标检测算法都会运用到Faster R-CNN的部分结构或思想。而且了解Faster R-CNN对理解其他R-CNN系列网络都有一定的帮助，包括Mask R-CNN，Stereo R-CNN 等等。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="复健计划" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%A4%8D%E5%81%A5%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="Object Detection" scheme="http://yuanquanquan.top/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>开新坑-硬件学习</title>
    <link href="http://yuanquanquan.top/2020/20200310/"/>
    <id>http://yuanquanquan.top/2020/20200310/</id>
    <published>2020-03-10T15:15:24.000Z</published>
    <updated>2020-06-14T09:26:38.030Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>序：鸽了快两个月没写博客，其实也是因为这两月都没怎么学习（逃，不过确实接触了一些新东西，在B站和知乎看到了一个电科毕业的大佬，一个人能抵一个项目组……于是也燃起了学习单片机的兴趣，想在做程序员的同时做一个业余电子工程师，这段时间零零散散做了一些东西</p></blockquote><a id="more"></a>  <p>一张用来装逼的PCB名片(<a href="https://github.com/Bazingaliu/PCB-card" target="_blank" rel="noopener">https://github.com/Bazingaliu/PCB-card</a>)</p><p><img src="https://i.loli.net/2020/05/03/VkLRrz8OJCYufxd.png" alt></p><p><img src="https://i.loli.net/2020/05/03/yXpUrjdAf47HIie.png" alt>一个根据经典开源飞控pix2.4.6原理图，用ad设计了一个仿制版(<a href="https://github.com/Bazingaliu/PCB-px41" target="_blank" rel="noopener">https://github.com/Bazingaliu/PCB-px41</a>)</p><p><img src="https://i.loli.net/2020/05/03/o92g4KOiz6flTnG.png" alt>一个用EMMC芯片实现的U盘(<a href="https://github.com/Bazingaliu/PCB_EMMC" target="_blank" rel="noopener">https://github.com/Bazingaliu/PCB_EMMC</a>)</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;序：鸽了快两个月没写博客，其实也是因为这两月都没怎么学习（逃，不过确实接触了一些新东西，在B站和知乎看到了一个电科毕业的大佬，一个人能抵一个项目组……于是也燃起了学习单片机的兴趣，想在做程序员的同时做一个业余电子工程师，这段时间零零散散做了一些东西&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="单片机" scheme="http://yuanquanquan.top/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>2019年计算机视觉综述论文汇聚</title>
    <link href="http://yuanquanquan.top/2019/20191230/"/>
    <id>http://yuanquanquan.top/2019/20191230/</id>
    <published>2019-12-30T13:23:38.000Z</published>
    <updated>2020-06-14T07:50:46.069Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>​        本文整理了2019年计算机视觉方面的综述论文，包含<strong>目标检测</strong>、<strong>图像分割(含语义/实例分割)</strong>、<strong>目标跟踪</strong>、<strong>医学图像分割</strong>、<strong>显著性目标检测</strong>、<strong>行为识别</strong>、<strong>深度估计</strong>等。可以使读者对相关领域有一个系统的了解。很适合初学者以及相关领域的研究人员。</p></blockquote><a id="more"></a>  <p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=4102261544,420722754&amp;fm=26&amp;gp=0.jpg" alt></p><p><strong><em>object detection</em></strong></p><ol><li>Imbalance Problems in Object Detection: A Reviewintro: under review at TPAMI</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1909.00169" target="_blank" rel="noopener">https://arxiv.org/abs/1909.00169</a></p><ol start="2"><li>Recent Advances in Deep Learning for Object Detectionintro: From 2013 (OverFeat) to 2019 (DetNAS)</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1908.03673" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03673</a></p><ol start="3"><li>A Survey of Deep Learning-based Object Detectionintro：From Fast R-CNN to NAS-FPN</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1907.09408" target="_blank" rel="noopener">https://arxiv.org/abs/1907.09408</a></p><ol start="4"><li>Object Detection in 20 Years: A Surveyintro：This work has been submitted to the IEEE TPAMI for possible publication</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1905.05055" target="_blank" rel="noopener">https://arxiv.org/abs/1905.05055</a></p><p><strong><em>图像分割</em></strong></p><ol><li>Deep Semantic Segmentation of Natural and Medical Images: A Reviewintro</li></ol><p>从 FCN(2014) 到 Auto-DeepLab(2019)，本综述共含179篇语义分割和医学图像分割参考文献</p><p>arXiv: <a href="https://arxiv.org/abs/1910.07655" target="_blank" rel="noopener">https://arxiv.org/abs/1910.07655</a></p><ol start="2"><li>Understanding Deep Learning Techniques for Image Segmentationintro</li></ol><p>本综述介绍了从2013年到2019年，主流的30多种分割算法（含语义/实例分割），50多种数据集，共计224篇参考文献</p><p>arXiv: <a href="https://arxiv.org/abs/1907.06119" target="_blank" rel="noopener">https://arxiv.org/abs/1907.06119</a></p><p><strong><em>目标跟踪</em></strong></p><ol><li>A Review of Visual Trackers and Analysis of its Application to Mobile Robotintro</li></ol><p>本目标跟踪综述共含185篇参考文献！从传统方法到最新的深度学习网络</p><p>arXiv: <a href="https://arxiv.org/abs/1910.09761" target="_blank" rel="noopener">https://arxiv.org/abs/1910.09761</a></p><ol start="2"><li>Deep Learning in Video Multi-Object Tracking: A Surveyintro</li></ol><p>38页目标跟踪综述，含30多种主流算法，共计174篇参考文献</p><p>arXiv: <a href="https://arxiv.org/abs/1907.12740" target="_blank" rel="noopener">https://arxiv.org/abs/1907.12740</a></p><p><strong><em>超分辨率</em></strong></p><ol><li>A Deep Journey into Super-resolution: A survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1904.07523" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07523</a></p><ol start="2"><li>Deep Learning for Image Super-resolution: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1902.06068" target="_blank" rel="noopener">https://arxiv.org/abs/1902.06068</a></p><p><strong><em>医学图像分割</em></strong></p><ol><li>Deep learning for cardiac image segmentation: A reviewintro</li></ol><p>本医学图像分割综述从FCN(2014)到Dense U-net(2019)，超过250篇的参考文献（论文中光画图的工作量就超级大）</p><p>arXiv: <a href="https://arxiv.org/abs/1911.03723" target="_blank" rel="noopener">https://arxiv.org/abs/1911.03723</a></p><ol start="2"><li>Machine  Learning Techniques for Biomedical Image Segmentation: An Overview of  Technical Aspects and Introduction to State-of-Art Applications</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1911.02521" target="_blank" rel="noopener">https://arxiv.org/abs/1911.02521</a></p><p><strong><em>显著性目标检测</em></strong></p><ol><li>Salient Object Detection in the Deep Learning Era: An In-Depth Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1904.09146" target="_blank" rel="noopener">https://arxiv.org/abs/1904.09146</a></p><p>github: <a href="https://github.com/wenguanwang/SODsurvey" target="_blank" rel="noopener">https://github.com/wenguanwang/SODsurvey</a></p><p><strong><em>行为识别</em></strong></p><ol><li>Spatio-temporal Action Recognition: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1901.09403" target="_blank" rel="noopener">https://arxiv.org/abs/1901.09403</a></p><p><strong><em>深度估计</em></strong></p><ol><li>Monocular Depth Estimation: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1901.09402" target="_blank" rel="noopener">https://arxiv.org/abs/1901.09402</a></p><p>地址连接：<a href="https://github.com/FranxYao/Deep-Generative-Models-for-Natural-Language-Processing" target="_blank" rel="noopener">https://github.com/FranxYao/Deep-Generative-Models-for-Natural-Language-Processing</a></p><p><strong><em>AutoML + NAS</em></strong></p><ol><li>Neural Architecture Search: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/pdf/1808.05377" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.05377</a></p><p><strong><em>GAN</em></strong></p><ol><li>The Six Fronts of the Generative Adversarial Networks</li></ol><p>arXiv: <a href="https://arxiv.org/pdf/1910.13076" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.13076</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;​        本文整理了2019年计算机视觉方面的综述论文，包含&lt;strong&gt;目标检测&lt;/strong&gt;、&lt;strong&gt;图像分割(含语义/实例分割)&lt;/strong&gt;、&lt;strong&gt;目标跟踪&lt;/strong&gt;、&lt;strong&gt;医学图像分割&lt;/strong&gt;、&lt;strong&gt;显著性目标检测&lt;/strong&gt;、&lt;strong&gt;行为识别&lt;/strong&gt;、&lt;strong&gt;深度估计&lt;/strong&gt;等。可以使读者对相关领域有一个系统的了解。很适合初学者以及相关领域的研究人员。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CV" scheme="http://yuanquanquan.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>leetcode 483.最小好进制</title>
    <link href="http://yuanquanquan.top/2019/2019122011/"/>
    <id>http://yuanquanquan.top/2019/2019122011/</id>
    <published>2019-12-20T03:09:20.000Z</published>
    <updated>2020-06-14T08:09:05.546Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h3 id="最小好进制"><a href="#最小好进制" class="headerlink" title="最小好进制"></a>最小好进制</h3><h3 id="题目链接"><a href="#题目链接" class="headerlink" title="题目链接"></a>题目链接</h3><p>英文链接：<a href="https://leetcode.com/problems/smallest-good-base/" target="_blank" rel="noopener">https://leetcode.com/problems/smallest-good-base/</a></p><p>中文链接：<a href="https://leetcode-cn.com/problems/smallest-good-base/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/smallest-good-base/</a></p><h3 id="题目详述"><a href="#题目详述" class="headerlink" title="题目详述"></a>题目详述</h3><p>对于给定的整数 n, 如果n的k（k&gt;=2）进制数的所有数位全为1，则称 k（k&gt;=2）是 n 的一个<strong>好进制</strong>。</p><p>以字符串的形式给出 n, 以字符串的形式返回 n 的最小好进制。</p></blockquote><a id="more"></a>  <p><img src="https://static.leetcode-cn.com/cn-mono-assets/production/head/f2ece5fe978d056f5a425ad3387216ee.svg" alt></p><p>   示例 1：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;13&quot;输出：&quot;3&quot;解释：13 的 3 进制是 111。</span><br></pre></td></tr></table></figure><p>示例 2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;4681&quot;输出：&quot;8&quot;解释：4681 的 8 进制是 11111。</span><br></pre></td></tr></table></figure><p>示例 3：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;1000000000000000000&quot;输出：&quot;999999999999999999&quot;解释：1000000000000000000 的 999999999999999999 进制是 11。</span><br></pre></td></tr></table></figure><p>提示：</p><ol><li><p>n的取值范围是 [3, 10^18]。</p></li><li><p>输入总是有效且没有前导 0。、</p></li></ol><h1 id="题目详解"><a href="#题目详解" class="headerlink" title="题目详解"></a>题目详解</h1><p>   ​        本题是寻找一个数最小的good base。其定义是对于一个数y，其x进制表示为全1，则称x是y的good base。应该比较好理解，其实就是将y写成1+x+x^2 +…+x^(n-1)，就是一个等比数列求和，于是我们可以将其转化为y = (x^n - 1)/(x - 1)，其中x&gt;=2, 3&lt;y&lt;10^18,为了寻找最小的x，我们可以先来确定一下n的取值范围，很明显x越小n越大，所以当x=2时，n最大为log2(y+1)。从第三个例子可以看出来，当x=y-1时，n最小为2。所以有了n的取值范围我们就可以遍历所有可能的n，然后每次循环中y和n都是确定值，在对x使用二叉搜索确定其值即可。</p><p>   这里有两个变量，一个是进制，一个是对应进制下的长度。可以固定一个变量，然后判断是否满足条件即可。进制的取值范围为 <code>[2, n - 1]</code>，范围太大，所以应该固定长度，判断是否存在满足条件的进制。进制越小，长度越长；进制越大，长度越短。我们可以从大到小枚举长度，判断是否存在满足条件的进制，这一步可以运用二分查找。</p><p>枚举长度的时间复杂度为 <code>O(logn)</code>，二分查找的时间复杂度为 <code>O(logn)</code>，辅助计算的时间复杂度为 <code>O(logn)</code>，总的时间复杂度为 <code>O(logn ^ 3)</code>。</p><p>另外一个需要注意的问题就是，因为本题中的数都比较大，所以要注意溢出问题，之前也做过一到这种题，可以使用java内置的BigInteger类进行处理。代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.math.BigInteger;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">smallestGoodBase</span><span class="params">(String n)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//现将字符串解析成long型数据</span></span><br><span class="line">        <span class="keyword">long</span> s = Long.parseLong(n);</span><br><span class="line">        <span class="comment">//对所有可能的指数n进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> max_e = (<span class="keyword">int</span>) (Math.log(s) / Math.log(<span class="number">2</span>)) + <span class="number">1</span>; max_e &gt;= <span class="number">2</span>; max_e--) &#123;</span><br><span class="line">            <span class="keyword">long</span> low = <span class="number">2</span>, high = s, mid;</span><br><span class="line">            <span class="comment">//进行二叉搜索，寻找最小的good base。</span></span><br><span class="line">            <span class="keyword">while</span> (low &lt;= high) &#123;</span><br><span class="line">                mid = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">                <span class="comment">//一开始没有使用BigInteger，会报错</span></span><br><span class="line">                BigInteger left = BigInteger.valueOf(mid);</span><br><span class="line">                left = left.pow(max_e).subtract(BigInteger.ONE);</span><br><span class="line">                BigInteger right = BigInteger.valueOf(s).multiply(BigInteger.valueOf(mid).subtract(BigInteger.ONE));</span><br><span class="line">                <span class="keyword">int</span> cmr = left.compareTo(right);</span><br><span class="line">                <span class="keyword">if</span> (cmr == <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">return</span> String.valueOf(mid);</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (cmr &gt; <span class="number">0</span>)</span><br><span class="line">                    high = mid - <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    low = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> String.valueOf(s - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>不过看题解的时候看到一个大佬的神仙解法，如下</p><p>有点类似于梯度下降</p><p>假设有两个变量x, y，求解方程 f(x, y) = z<br>当f(x, y)相对于x和y都是单调的时候<br>则可以先固定x，求下一个满足条件的y边界，然后固定y求下一个满足条件的x边界，</p><p>这样不断逼近最终解</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> ll = <span class="keyword">long</span> <span class="keyword">long</span>;</span><br><span class="line">    <span class="function">ll <span class="title">nextK</span><span class="params">(ll k, ll m, ll N, <span class="keyword">bool</span>&amp; match)</span> </span>&#123;</span><br><span class="line">        match = <span class="literal">false</span>;</span><br><span class="line">        ll lo = k + <span class="number">1</span>;</span><br><span class="line">        ll hi = N - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (lo &lt;= hi) &#123;</span><br><span class="line">            ll md = lo + (hi - lo) / <span class="number">2</span>;</span><br><span class="line">            ll s = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (ll i = <span class="number">0</span>; i &lt;= m; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s &gt; (N - <span class="number">1</span>) / md) &#123;</span><br><span class="line">                    s = N + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                s = s * md + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s == N) &#123;</span><br><span class="line">                match = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">return</span> md;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s &gt; N) hi = md - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> lo = md + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> lo;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">ll <span class="title">nextM</span><span class="params">(ll k, ll m, ll N, <span class="keyword">bool</span>&amp; match)</span> </span>&#123;</span><br><span class="line">        match = <span class="literal">false</span>;</span><br><span class="line">        ll lo = <span class="number">1</span>;</span><br><span class="line">        ll hi = m - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (lo &lt;= hi) &#123;</span><br><span class="line">            ll md = lo + (hi - lo) / <span class="number">2</span>;</span><br><span class="line">            ll s = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (ll i = <span class="number">0</span>; i &lt;= md; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s &gt; (N - <span class="number">1</span>) / k) &#123;</span><br><span class="line">                    s = N + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                s = s * k + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s == N) &#123;</span><br><span class="line">                match = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">return</span> md;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s &gt; N) hi = md - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> lo = md + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> hi;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">smallestGoodBase</span><span class="params">(<span class="built_in">string</span> n)</span> </span>&#123;</span><br><span class="line">        ll N = stoll(n);</span><br><span class="line">        ll k = <span class="number">2</span>;</span><br><span class="line">        ll m = N; <span class="comment">// m为N表示为k进制的最高次幂</span></span><br><span class="line">        <span class="keyword">bool</span> match = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">while</span> (k &lt; N &amp;&amp; m &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            m = nextM(k, m, N, match);</span><br><span class="line">            <span class="keyword">if</span> (match) <span class="keyword">break</span>;</span><br><span class="line">            k = nextK(k, m, N, match);</span><br><span class="line">            <span class="keyword">if</span> (match) <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> to_string(k);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h3 id=&quot;最小好进制&quot;&gt;&lt;a href=&quot;#最小好进制&quot; class=&quot;headerlink&quot; title=&quot;最小好进制&quot;&gt;&lt;/a&gt;最小好进制&lt;/h3&gt;&lt;h3 id=&quot;题目链接&quot;&gt;&lt;a href=&quot;#题目链接&quot; class=&quot;headerlink&quot; title=&quot;题目链接&quot;&gt;&lt;/a&gt;题目链接&lt;/h3&gt;&lt;p&gt;英文链接：&lt;a href=&quot;https://leetcode.com/problems/smallest-good-base/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://leetcode.com/problems/smallest-good-base/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;中文链接：&lt;a href=&quot;https://leetcode-cn.com/problems/smallest-good-base/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://leetcode-cn.com/problems/smallest-good-base/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;题目详述&quot;&gt;&lt;a href=&quot;#题目详述&quot; class=&quot;headerlink&quot; title=&quot;题目详述&quot;&gt;&lt;/a&gt;题目详述&lt;/h3&gt;&lt;p&gt;对于给定的整数 n, 如果n的k（k&amp;gt;=2）进制数的所有数位全为1，则称 k（k&amp;gt;=2）是 n 的一个&lt;strong&gt;好进制&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;以字符串的形式给出 n, 以字符串的形式返回 n 的最小好进制。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="算法" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="LeetCode" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="http://yuanquanquan.top/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>CSRNet：Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes</title>
    <link href="http://yuanquanquan.top/2019/2019121899/"/>
    <id>http://yuanquanquan.top/2019/2019121899/</id>
    <published>2019-12-18T11:49:36.000Z</published>
    <updated>2020-06-14T08:57:51.443Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h3 id="主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域，-生成高质量的人群分布密度图。"><a href="#主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域，-生成高质量的人群分布密度图。" class="headerlink" title="主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1/8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域， 生成高质量的人群分布密度图。"></a>主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1/8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域， 生成高质量的人群分布密度图。</h3></blockquote><a id="more"></a>  <h5 id="1、MCNN的多列设计没有显著作用："><a href="#1、MCNN的多列设计没有显著作用：" class="headerlink" title="1、MCNN的多列设计没有显著作用："></a>1、MCNN的多列设计没有显著作用：</h5><p>​    以前的拥挤场景分析工作主要基于<code>multi-scale architectures</code>。它们在该领域取得了很高的性能，但是当网络变得更深时，它们使用的设计也带来了两个显着的缺点：大量的训练时间和无效的分支结构（例如，MCNN ）。我们设计了一个实验来证明MCNN与表1中更深入的常规网络相比表现不佳。</p><pre><code>如我们先前所知，MCNN的每列专用于某一级别的拥塞场景。但是，使用MCNN的有效性可能并不突出。我们在图2中展示了MCNN中三个独立列（代表大，中，小的感受野）所学习的特征，并用ShanghaiTech Part A [18]数据集进行评估。该图中的三条曲线与具有不同拥塞密度的50个测试案例共享非常相似的模式（估计的错误率），这意味着这种分支结构中的每个列学习几乎相同的特征。它违背了MCNN设计的初衷，用于学习每列的不同功能。</code></pre><h5 id="2、膨胀卷积优于反卷积"><a href="#2、膨胀卷积优于反卷积" class="headerlink" title="2、膨胀卷积优于反卷积"></a>2、膨胀卷积优于反卷积</h5><p>​    已经在分割任务中证明了膨胀卷积层，其精度得到显着提高，并且它是池化层的良好替代方案。 尽管池化层（例如，最大和平均池化）被广泛用于维持不变性和控制过度拟合，但是它们还显着地降低了空间分辨率，这意味着丢失了特征映射的空间信息。 反卷积层可以减轻信息的丢失，但额外的复杂性和执行延迟可能并不适合所有情况。 膨胀卷积是一个更好的选择，它使用稀疏内核（如图3所示）来交替汇集和卷积层。 该字符在不增加参数数量或计算量的情况下扩大了感受野（例如，添加更多卷积层可以产生更大的感受野但引入更多操作）。<br>​     为了保持特征图的分辨率，与使用<code>卷积+池化+反卷积</code>的方案相比，<code>膨胀卷积</code>显示出明显的优点。我们在图4中选择一个例子用于说明。输入是人群的图像，并且它分别通过两种方法处理以产生具有相同大小的输出。在第一种方法中，输入由具有因子2的最大池化层进行下采样，然后将其传递到具有3X3 Sobel内核的卷积层。由于生成的特征映射仅是原始输入的1/2，因此需要通过解卷积层（双线性插值）<code>bilinear interpolation</code>对其进行上采样。在另一种方法中，我们尝试扩张卷积并使相同的3X3 Sobel内核适应具有因子= 2步幅的扩张内核。输出与输入共享相同的维度。最重要的是，扩张卷积的输出包含更详细的信息。</p><h3 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h3><p>​    在本文中，我们设计了一个更深入的网络，称为<code>CSR-Net</code>，用于计算人群和生成高质量的密度图。我们的模型使用<code>纯卷积层</code>作为主干，以灵活的分辨率支持输入图像。为了限制网络的复杂性，我们在所有层中使用<code>小尺寸</code>的卷积滤波器（如3x3）。我们将VGG-16 [21]的前10层作为前端和<code>膨胀卷积层</code>作为后端部署，以扩大感受域并提取更深的特征而不会丢失分辨率（因为不使用池化层）。</p><h3 id="主要实现："><a href="#主要实现：" class="headerlink" title="主要实现："></a>主要实现：</h3><h5 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h5><p>​    此前端网络的输出大小是原始输入大小的<code>1/8</code>。如果我们继续堆叠更多卷积层和池化层（VGG-16中的基本组件），输出大小将进一步缩小，并且很难生成高质量的密度映射。 我们尝试将膨胀卷积层作为后端来提取更深层的显着性信息以及维持输出分辨率。<br>​     我们在表3中提出了四种CSRNet网络配置，它们具有相同的前端结构但后端的扩展速率不同。 关于前端，我们采用VGG-16网络（全连接层除外）并仅使用3X3内核。 根据VGG的论文，当使用相同大小的感受野时，使用具有小内核的更多卷积层比使用具有更大内核的更少层更有效。<br>​     通过移除完全连接的层，我们尝试确定需要从VGG-16使用的层数。 最关键的部分是在准确性和资源开销（包括训练时间，内存消耗和参数数量）之间进行权衡。 实验表明，在保持前十层VGG-16 [21]只有3个池化层而不是五层时，可以实现最佳权衡，以抑制由池化操作引起的对输出精度的不利影响。 由于CSRNet的输出（密度图）较小（输入尺寸的1/8），我们选择因子为8的双线性插值进行缩放，并确保<code>输出与输入图像具有相同的分辨率</code>。 使用相同的大小，CSRNet生成的结果与使用<code>PSNR</code>（峰值信噪比）和<code>SSIM</code>（图像中的结构相似性）的基础事实结果相当。</p><h5 id="数据增强："><a href="#数据增强：" class="headerlink" title="数据增强："></a>数据增强：</h5><p>​    我们从不同位置的每个图像裁剪9个patches，原始图像的大小为1/4。 前四个patches包含四分之三的图像而没有重叠，而其他五个patches则从输入图像中随机裁剪。 之后，我们镜像patches，以便我们将训练集加倍。</p><h5 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h5><p>​    使用简单的方法将CSRNet作为端到端结构进行培训。 前10个卷积层由训练有素的VGG-16进行微调[21]。 对于其他层，初始值来自具有0.01标准偏差的高斯初始化。 随机梯度下降（SGD）在训练期间以1e-6的固定学习率应用。 此外，我们选择欧氏距离来测量地面实况与我们生成的估计密度图之间的差异，这与其他工作类似[19,18,4]。 </p><pre><code>我们还使用和来评估ShanghaiTech Part A数据集上输出密度图的质量。 为了计算和，我们遵循[5]给出的预处理，其中包括密度图调整大小（与原始输入相同的大小），对地面实况和预测密度图进行插值和归一化。     除了人群计数之外，我们在TRANCOS数据集[44]上设置了一个实验，用于车辆计数，以证明我们的方法的稳健性和一般化。 TRANCOS是一个公共交通数据集，包含由监控摄像机捕获的1244个不同拥挤交通场景的图像，其中包含46796个带注释的车辆。 此外，提供感兴趣区域（ROI）用于评估。 图像的视角不固定，图像是从非常不同的场景中收集的。 网格平均值平均绝对误差（GAME）[44]用于此测试中的估值。 GAME定义如下：</code></pre><h5 id="代码：https-github-com-leeyeehoo-CSRNet-pytorch"><a href="#代码：https-github-com-leeyeehoo-CSRNet-pytorch" class="headerlink" title="代码：https://github.com/leeyeehoo/CSRNet-pytorch"></a>代码：<a href="https://github.com/leeyeehoo/CSRNet-pytorch" target="_blank" rel="noopener">https://github.com/leeyeehoo/CSRNet-pytorch</a></h5><p>我在根据作者的github（<a href="https://github.com/leeyeehoo/CSRNet-pytorch）" target="_blank" rel="noopener">https://github.com/leeyeehoo/CSRNet-pytorch）</a>, 构建环境时遇到了一些问题。我调试过，百度花了很长时间才解决。写这一章的目的是帮助大家学得更好，少走弯路。</p><hr><h2 id="step1-install"><a href="#step1-install" class="headerlink" title="step1. install"></a>step1. install</h2><p>For the specific installation process, you can refer to the author’s github. Here I simply show the command line of my operation.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda create -n CSRNet python=3.6</span><br><span class="line">source activate CSRNet</span><br><span class="line">unzip CSRNet-pytorch-master.zip</span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torch torchvision</span><br><span class="line">pip install decorator cloudpickle&gt;=0.2.1 dask[array]&gt;=1.0.0 matplotlib&gt;=2.0.0 networkx&gt;=1.8 scipy&gt;=0.17.0 bleach python-dateutil&gt;=2.1 decorator</span><br><span class="line">unzip ShanghaiTech_Crowd_Counting_Dataset.zip</span><br><span class="line">jupyter nbconvert --to script make_dataset.ipynb  #Convert .ipynb file to .py file</span><br></pre></td></tr></table></figure><h2 id="step2-make-dataset-py"><a href="#step2-make-dataset-py" class="headerlink" title="step2.  make_dataset.py"></a>step2.  make_dataset.py</h2><p>I just run the command to convert the <strong>make_dataset.ipynb</strong> file to a <strong>make_dataset.py</strong> file.Now you need to modify the contents of the <strong>make_dataset.py</strong> file.</p><p>Find the location where <strong>root</strong> is, add <strong>def main()</strong> in the above line</p><p><img src="https://i.loli.net/2020/06/14/enwoR46SIMsZv8l.png" alt></p><p>Add these two lines at the end of the <strong>make_dataset.py</strong>, adjust the format of the code</p><p><img src="https://i.loli.net/2020/06/14/pKdG1rO62jefQv8.png" alt></p><p>There is an error in the author’s source code, you need to change the code</p><p>Replace <strong>pts = np.array(zip(np.nonzero(gt)[1], np.nonzero(gt)[0]))</strong> with <strong>pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))</strong> </p><p><img src="https://i.loli.net/2020/06/14/mgDRkCdfPTN1WOJ.png" alt></p><p>Then run the <strong>make_dataset.py</strong> file</p><p><img src="https://i.loli.net/2020/06/14/Pg6v4LujYbHOsxf.png" alt></p><hr><p><strong>The above is just a general summary, then we will run and visualize the line-by-line code.</strong></p><p>I will use this image as an example.</p><p><img src="https://i.loli.net/2020/06/14/pdmkGTUvf6DZFwE.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import h5py</span><br><span class="line">import scipy.io as io</span><br><span class="line">import PIL.Image as Image</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from scipy.ndimage.filters import gaussian_filter </span><br><span class="line">import scipy</span><br><span class="line">import json</span><br><span class="line">from matplotlib import cm as CM</span><br><span class="line">from image import *</span><br><span class="line">from model import CSRNet</span><br><span class="line">import torch</span><br><span class="line">img_path=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\images\\IMG_21.jpg&apos;</span><br><span class="line">mat=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\ground_truth\\GT_IMG_21.mat&apos;</span><br><span class="line">mat = io.loadmat(mat)</span><br><span class="line">img= plt.imread(img_path)</span><br><span class="line">k = np.zeros((img.shape[0],img.shape[1]))</span><br></pre></td></tr></table></figure><p>The following is the information of <strong>k</strong></p><p><img src="https://i.loli.net/2020/06/14/tohJys8YT1OXNbd.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gt = mat[&quot;image_info&quot;][0,0][0,0][0]</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/CZNjq5xyzYuJ3Ic.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in range(0,len(gt)):</span><br><span class="line">    if int(gt[i][1])&lt;img.shape[0] and int(gt[i][0])&lt;img.shape[1]:</span><br><span class="line">        k[int(gt[i][1]),int(gt[i][0])]=1</span><br><span class="line">gt = k</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/KeWcb2EO5Dmjuka.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">density = np.zeros(gt.shape, dtype=np.float32)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/nhXEzgB2YT9iWFQ.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gt_count = np.count_nonzero(gt)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/LdpFMxQURclJiau.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/utpH7GJKLF2gN6B.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">leafsize = 2048</span><br><span class="line"># build kdtree</span><br><span class="line">tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/uYoVTODtg7EZq4N.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distances, locations = tree.query(pts, k=4)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/uYoVTODtg7EZq4N.png" alt></p><p><img src="https://i.loli.net/2020/06/14/lhQVo4bW1sqXp5L.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;generate density...&apos;)</span><br><span class="line">for i, pt in enumerate(pts):</span><br><span class="line">    pt2d = np.zeros(gt.shape, dtype=np.float32)</span><br><span class="line">    pt2d[pt[1],pt[0]] = 1.</span><br><span class="line">    if gt_count &gt; 1:</span><br><span class="line">        sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1</span><br><span class="line">    else:</span><br><span class="line">        sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point</span><br><span class="line">    density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode=&apos;constant&apos;)</span><br><span class="line">print(&apos;done.&apos;)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/9cj482YACKQGe7I.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k = density</span><br><span class="line">with h5py.File(img_path.replace(&apos;.jpg&apos;,&apos;.h5&apos;).replace(&apos;images&apos;,&apos;ground_truth&apos;), &apos;w&apos;) as hf:</span><br><span class="line">        hf[&apos;density&apos;] = k</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/g6rPTmICwBUWJqR.png" alt></p><p>So far, we have generated true values for the image.  At this point I will sort the above code as follows</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import h5py</span><br><span class="line">import scipy.io as io</span><br><span class="line">import PIL.Image as Image</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from scipy.ndimage.filters import gaussian_filter </span><br><span class="line">import scipy</span><br><span class="line">import json</span><br><span class="line">from matplotlib import cm as CM</span><br><span class="line">from image import *</span><br><span class="line">from model import CSRNet</span><br><span class="line">import torch</span><br><span class="line">def gaussian_filter_density(gt):</span><br><span class="line">    print(gt.shape)</span><br><span class="line">    density = np.zeros(gt.shape, dtype=np.float32)</span><br><span class="line">    gt_count = np.count_nonzero(gt)</span><br><span class="line">    if gt_count == 0:</span><br><span class="line">        return density</span><br><span class="line"></span><br><span class="line">    # pts = np.array(zip(np.nonzero(gt)[1], np.nonzero(gt)[0]))</span><br><span class="line">    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))</span><br><span class="line">    leafsize = 2048</span><br><span class="line">    # build kdtree</span><br><span class="line">    tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)</span><br><span class="line">    # query kdtree</span><br><span class="line">    distances, locations = tree.query(pts, k=4)</span><br><span class="line"></span><br><span class="line">    print(&apos;generate density...&apos;)</span><br><span class="line">    for i, pt in enumerate(pts):</span><br><span class="line">        pt2d = np.zeros(gt.shape, dtype=np.float32)</span><br><span class="line">        pt2d[pt[1],pt[0]] = 1.</span><br><span class="line">        if gt_count &gt; 1:</span><br><span class="line">            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1</span><br><span class="line">        else:</span><br><span class="line">            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point</span><br><span class="line">        density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode=&apos;constant&apos;)</span><br><span class="line">    print(&apos;done.&apos;)</span><br><span class="line">    return density</span><br><span class="line">img_path=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\images\\IMG_21.jpg&apos;</span><br><span class="line">mat=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\ground_truth\\GT_IMG_21.mat&apos;</span><br><span class="line">img_paths = []</span><br><span class="line">img_paths.append(img_path)</span><br><span class="line">    for img_path in img_paths:</span><br><span class="line">        print(img_path)</span><br><span class="line">        mat = io.loadmat(mat)</span><br><span class="line">        img= plt.imread(img_path)        </span><br><span class="line">        k = np.zeros((img.shape[0],img.shape[1]))</span><br><span class="line">        gt = mat[&quot;image_info&quot;][0,0][0,0][0]</span><br><span class="line">        for i in range(0,len(gt)):</span><br><span class="line">            if int(gt[i][1])&lt;img.shape[0] and int(gt[i][0])&lt;img.shape[1]:</span><br><span class="line">                k[int(gt[i][1]),int(gt[i][0])]=1</span><br><span class="line">        k = gaussian_filter_density(k)        </span><br><span class="line">        with h5py.File(img_path.replace(&apos;.jpg&apos;,&apos;.h5&apos;).replace(&apos;images&apos;,&apos;ground_truth&apos;), &apos;w&apos;) as hf:</span><br><span class="line">                hf[&apos;density&apos;] = k</span><br></pre></td></tr></table></figure><hr><p>And….then, let’s plot the true values of the image:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gt_file = h5py.File(img_paths[0].replace(&apos;.jpg&apos;,&apos;.h5&apos;).replace(&apos;images&apos;,&apos;ground_truth&apos;),&apos;r&apos;)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/K8SWQbPzkq4ei6a.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groundtruth = np.asarray(gt_file[&apos;density&apos;])</span><br></pre></td></tr></table></figure><p>plot the true values of the image</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(groundtruth,cmap=CM.jet)</span><br></pre></td></tr></table></figure><p>Calculate how many people are in this picture</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(groundtruth)</span><br></pre></td></tr></table></figure><hr><p><strong>Based on the same operation above, I generated true values for all images in the dataset. The following operations are performed on the gpu server.</strong></p><p>That is, run the command line <strong>python make_dataset.py</strong> on the server to get the true value of all the pictures.</p><h2 id="step3-Training"><a href="#step3-Training" class="headerlink" title="step3.  Training"></a>step3.  Training</h2><p><strong>Note</strong>: if you use the python3.x</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. In model.py, change the xrange in line 18 to range</span><br><span class="line">2. In model.py, change line 19 to: list(self.frontend.state_dict().items())[i][1].data[:] = list(mod.state_dict().items())[i][1].data[:]</span><br><span class="line">3. In image.py, change line 40 to: target = cv2.resize(target,(target.shape[1]//8,target.shape[0]//8),interpolation = cv2.INTER_CUBIC)*64</span><br></pre></td></tr></table></figure><ul><li>In  part_A_train.json:change the path of images</li><li>In  part_A_val.json: change the path of images</li></ul><p>run </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py part_A_train.json part_A_val.json 0 0</span><br></pre></td></tr></table></figure><h2 id="step4-Testing"><a href="#step4-Testing" class="headerlink" title="step4. Testing"></a>step4. Testing</h2><p>These are our test images. number：182</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter nbconvert --to script val.ipynb</span><br></pre></td></tr></table></figure><p>Finally, the performance of this model on invisible data is tested. We will use the val.py file to verify the results. Remember to change the path to pre-train weights and images.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python val.py</span><br></pre></td></tr></table></figure><p>The average absolute error value that can be obtained by running this val.py file code</p><p>total ：182</p><p><img src="https://i.loli.net/2020/06/14/IboNJTvDgh8VOa9.png" alt></p><p><img src="https://i.loli.net/2020/06/14/aMECFyiT6e4LqS7.png" alt></p><p>The average absolute error value obtained is 65.96636956602663, which is very good.</p><hr><p>Now let’s examine the predicted values on a single image:</p><p>run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_single-image.py</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/14/WGPcwDxmfYCjBKQ.png" alt><img src="https://i.loli.net/2020/06/14/XmbDW9JzY65CBTV.png" alt><img src="https://i.loli.net/2020/06/14/kIjcnEDtfaFwLy7.png" alt></p><p>another one</p><p><img src="https://i.loli.net/2020/06/14/1zHNh6qZI8iSmDT.png" alt><img src="https://i.loli.net/2020/06/14/HkBcDnfxN7YuJSV.png" alt><img src="https://i.loli.net/2020/06/14/UykOqSaWmBin1Dl.png" alt></p><p>The effect is not too good，maybe the model is not trained enough，i guess。</p><hr><h2 id="Reading-paper-https-arxiv-org-pdf-1802-10062-pdf"><a href="#Reading-paper-https-arxiv-org-pdf-1802-10062-pdf" class="headerlink" title="Reading paper   https://arxiv.org/pdf/1802.10062.pdf"></a>Reading paper   <a href="https://arxiv.org/pdf/1802.10062.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.10062.pdf</a></h2><p>先说数据集用的ShanghaiTech dataset，根据.jpg和.mat处理之后生成train_den文件夹下.csv文件和图片一一对应<img src="https://i.loli.net/2020/06/14/tjIKHS7Q29FC5rV.png" alt></p><p><img src="https://i.loli.net/2020/06/14/tjIKHS7Q29FC5rV.png" alt></p><p>数据集扩充处理再补充一下：使用的是高斯模糊作用于图像中的每个人的头部。所有图像都被裁剪成9块，每块的大小是图像原始大小的1/4。</p><hr><p>空洞卷积也有的博客翻译成膨胀卷积、扩张卷积啥的，anyway，使用扩张卷积是在不增加参数的情况下扩大内核。因此，如果扩张率为1就是中间的图在整个图像上进行卷积。将膨胀率增加到2最右边图它可以替代pooling层</p><p><img src="https://i.loli.net/2020/06/14/HCJvzeDprl64k7N.png" alt></p><p>接下来再说它的数学公式上怎么计算的，</p><p><img src="https://i.loli.net/2020/06/14/7kcQDAUlC5Z8a36.png" alt></p><p>由上公式得到这个([k + (k-1)<em>(r-1)] </em> [k + (k-1)*(r-1)])</p><p><img src="https://i.loli.net/2020/06/14/4WRXklYKHSq2Cpe.png" alt="1554628903758"></p><p><img src="https://i.loli.net/2020/06/14/P8GFQJHRwcnq3s1.png" alt="1554628933425"></p><p><img src="https://i.loli.net/2020/06/14/uUj6KkXVzb1Moli.png" alt="1554628946463"></p><p><img src="https://i.loli.net/2020/06/14/xvocP3frquXzMKZ.png" alt="1554628958176"></p><p>这个过程是：首先预测出给定图像的密度图。如果没有人，像素值pixel value设为0。如果该像素对应于人，则将分配某个预定义值。所以图像中的人数就是总共有的像素值total pixel values </p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h3 id=&quot;主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域，-生成高质量的人群分布密度图。&quot;&gt;&lt;a href=&quot;#主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域，-生成高质量的人群分布密度图。&quot; class=&quot;headerlink&quot; title=&quot;主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1/8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域， 生成高质量的人群分布密度图。&quot;&gt;&lt;/a&gt;主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1/8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域， 生成高质量的人群分布密度图。&lt;/h3&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Fast Online Object Tracking and Segmentation:A Unifying Approach</title>
    <link href="http://yuanquanquan.top/2019/20191213002/"/>
    <id>http://yuanquanquan.top/2019/20191213002/</id>
    <published>2019-12-12T23:41:41.000Z</published>
    <updated>2020-06-14T07:49:10.264Z</updated>
    
    <content type="html"><![CDATA[<p>用单个方法实时做到同时目标跟踪和半监督视频目标分割。SiamMask通过二值分割任务增强损失提升了全卷积Siamense目标跟踪方法的训练过程。训练完成后，SiamMask只依赖于单个边界框初始值，可以做到旋转的边界框分割未知类别目标，达到每秒55帧。</p><a id="more"></a>  <h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>展示了如何用单个方法实时做到同时目标跟踪和半监督视频目标分割。我们的方法SiamMask，通过二值分割任务增强损失提升了全卷积Siamense目标跟踪方法的训练过程。训练完成后，SiamMask只依赖于单个边界框初始值，可以做到旋转的边界框分割未知类别目标，达到每秒55帧。性能也有很大的优势【略】。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig1.png" alt></p><p>本文希望通过SiamMask拉近任意目标跟踪和视频目标分割（VOS）的关系。SiamMask是一个简单的多任务方法，可以同时解决上述两个问题。我们希望保持离线的训练能力，以及在线的速度，同时对目标的表示进行快速调节。</p><p>为了达到这个目标，我们同时在三个任务上训练一个Siamese网络，每个对应于不同的目标对象和候选区域的策略。在Bertinetto的全卷积方法中，一个任务是学习目标对象和滑动窗口中多个候选的相似性度量。它的输出是密度响应图，表示目标的未知，但是不包含空间信息。为了提炼这个信息，我们同时训练另外两个任务：用Region Proposal Network训练边界框回归，未知类别二值分割。需要注意的是：二值标签只需要在离线训练时给出，在线分割和跟踪的时候不需要。每个任务用单独的分支表示，最终的损失对三个输出求和。</p><p>在训练后，SiamMask只依赖于单个初始边界框，不需要更新即可在线运算，最后生成目标分割的mask和旋转的边界框，达到每秒55帧。在VOT-2018上达到最好的效果。另外，相同的方法在半监督VOS上也很有竞争力。</p><h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><p>为了能在线运算，速度更快，我们采用了全卷积Siamese网络。我们实验了SiamFC和SiamRPN。</p><h3 id="3-1-Fully-convolutional-Siamese-networks"><a href="#3-1-Fully-convolutional-Siamese-networks" class="headerlink" title="3.1. Fully-convolutional Siamese networks"></a>3.1. Fully-convolutional Siamese networks</h3><p><strong>SiamFC</strong> Bertinetto提出作为跟踪系统的基本块，离线训练的全卷积Siamese网络在（大的）搜索图像x中比较一个模板图像z，获得一个密集响应图。z和x分别是$w \times h$大小的中心剪切的目标对象，和更大的中心剪切的目标位置目标估计。两个输入通过相同的CNN$f_{\theta}$处理，返回两个交叉相关的特征图：</p><p>$$g_{\theta}(z, x)=f_{\theta}(z) \star f_{\theta}(x) \qquad (1)$$</p><p>我们把响应图（式1左）的每个空间元素看做候选窗的响应（ROW）。例如$g_{\theta}^{n}(z, x)$表示一个模板z和x第n个候选窗的相似性。对于SiamFC而言，目标是响应图最大值对应于在搜索区域x中的目标位置。为了让每个ROW编码更丰富的目标对象信息，我们把式1的交叉相关替换为深度方向的交叉相关，生成多通道响应图。SiamFC在几百万视频帧上用logistic损失（记作$\mathcal{L}_{s i m}$）离线训练。</p><p><strong>SiamRPN</strong> Li等通过区域提议网络（RPN提上SiamFC的性能，可以用一个不同比例的边界框预测目标位置。在SiamRPN中，每个ROW表示k个archor box提议的集合，以及对应的目标/背景分数。两个输出的分支用平滑$L_{1}$和交叉熵训练。记作$\mathcal{L}<em>{b o x}$和$\mathcal{L}</em>{\text { score }}$。</p><h3 id="3-2-SiamMask"><a href="#3-2-SiamMask" class="headerlink" title="3.2. SiamMask"></a>3.2. SiamMask</h3><p>不同于现在的依赖于低保真度目标表示的方法，我们认为生成先前帧的目标二值分割mask是很重要的。除了相似性分数和边界框坐标，ROW也包含了生成二值mask的信息。可以在现有的网络基础上增加新的分支和损失。</p><p>通过参数为$\phi$的两层神经网络$h_{\phi}$预测$w \times h$的二值mask。令$m_{n}$表示第n个ROW上预测的mask：</p><p>$$m_{n}=h_{\phi}\left(g_{\theta}^{n}(z, x)\right) \qquad (2)$$</p><p>从式2可以看出mask的预测是图像到分割x以及目标对象z的函数。这里z可以用于指导分割过程：给定一个不同的参考图像，网络会对x生成不同的分割mask。</p><p><strong>Loss function</strong> 训练的时候，每个ROW由真实二值标签$y_{n} \in{ \pm 1}$标记，同时还有$w \times h$大小的像素级真实mask$c_{n}$。令$c_{n}^{i j} \in{ \pm 1}$表示第n个候选ROW像素$(i, j)$上的标签。损失$\mathcal{L}_{\text { mask }}$是二元logistic回归损失：</p><p>$$\mathcal{L}<em>{\operatorname{mask}}(\theta, \phi)=\sum</em>{n}\left(\frac{1+y_{n}}{2 w h} \sum_{i j} \log \left(1+e^{-c_{n}^{i j} m_{n}^{i j}}\right)\right) \qquad (3)$$</p><p>因此，$h_{\phi}$的分类层包括$w \times h$个分类器，每个表示给定像素是否候选窗中的对象。$\mathcal{L}<em>{\text { mask }}$只计算正的ROW（$y</em>{n}=1$）。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig2.png" alt></p><p><strong>Mask representation</strong> </p><p>$f_{\theta}(z)$和$f_{\theta}(x)$生成的深度方向交叉相关的（$17 \times 17$）ROW之一为mask的表示。分割任务的网络$h_{\phi}$由两个$1 \times 1$卷积层表示，一个256通道，一个$63^{2}$通道。这可以让每个像素分类器利用整个ROW包含的信息，也可以对x中的候选窗有完整的视角，这对于区分目标对象是很关键的。</p><p><strong>Two variants</strong> 【略】</p><p><strong>Box generation</strong> 【略】</p><h3 id="3-3-Implementation-details"><a href="#3-3-Implementation-details" class="headerlink" title="3.3. Implementation details"></a>3.3. Implementation details</h3><p>【略】</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Evaluation-for-visual-object-tracking"><a href="#4-1-Evaluation-for-visual-object-tracking" class="headerlink" title="4.1. Evaluation for visual object tracking"></a>4.1. Evaluation for visual object tracking</h3><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig3.png" alt><br><img src="D:/BazingaliuBlog/Bazingaliu.github.io-master/blog/papers/2019/R/SiamMask-fig3.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab2.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab3-4.png" alt></p><h3 id="4-2-Evaluation-for-semi-supervised-VOS"><a href="#4-2-Evaluation-for-semi-supervised-VOS" class="headerlink" title="4.2. Evaluation for semi-supervised VOS"></a>4.2. Evaluation for semi-supervised VOS</h3><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab5-6.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig4.png" alt></p><h3 id="4-3-Further-analysis"><a href="#4-3-Further-analysis" class="headerlink" title="4.3. Further analysis"></a>4.3. Further analysis</h3><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab7.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig5.png" alt></p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>我们提出了SiamMask方法，让全卷积Siamese跟踪器生成任意类别二值分割mask。我们把它成功运用到了目标跟踪和半监督视频目标分割中，速度快，性能高。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用单个方法实时做到同时目标跟踪和半监督视频目标分割。SiamMask通过二值分割任务增强损失提升了全卷积Siamense目标跟踪方法的训练过程。训练完成后，SiamMask只依赖于单个边界框初始值，可以做到旋转的边界框分割未知类别目标，达到每秒55帧。&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>jupyter远程连接炼丹</title>
    <link href="http://yuanquanquan.top/2019/2019121111/"/>
    <id>http://yuanquanquan.top/2019/2019121111/</id>
    <published>2019-12-11T03:55:00.000Z</published>
    <updated>2020-06-09T17:53:36.740Z</updated>
    
    <content type="html"><![CDATA[<p><img src="data:image/jpeg;base64,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" alt></p><a id="more"></a>  <h1 id="一、前提："><a href="#一、前提：" class="headerlink" title="一、前提："></a>一、前提：</h1><ol><li>安装Python3</li><li>安装Anaconda</li><li>配置jupyter notebook 并启动（重点）</li></ol><h1 id="二、配置jupyter文件"><a href="#二、配置jupyter文件" class="headerlink" title="二、配置jupyter文件"></a>二、配置jupyter文件</h1><p>因为服务器已经安装好anaconda和jupyter，python版本为python3.6，以下主要讲如何配置jupyter文件</p><h2 id="1、设置jupyter-的登录密码"><a href="#1、设置jupyter-的登录密码" class="headerlink" title="1、设置jupyter 的登录密码"></a>1、设置jupyter 的登录密码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config  # 生成jupyter notebook配置文件</span><br></pre></td></tr></table></figure><p>会生成有默认配置文件 jupyter_notebook_config.py</p><h2 id="2、然后打开ipython"><a href="#2、然后打开ipython" class="headerlink" title="2、然后打开ipython"></a>2、然后打开ipython</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from notebook.auth import passwd</span><br><span class="line">passwd() #生成密码</span><br></pre></td></tr></table></figure><h2 id="3、配置文件"><a href="#3、配置文件" class="headerlink" title="3、配置文件"></a>3、配置文件</h2><p>然后会让你输入密码，确认密码，。（这里面的密码是后面在本地打开jupyter时需要输入的，要记住，如设置密码为123456）<br> 然后会输出一长串哈希密码”sha1:XXXXX”  复制这一段密码，。后面要用<br> 然后就开始配置刚才生成的jupyter_notebook_config.py文件。，<br> 使用vim打开：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><p>将以下文字复制进jupyter_notebook_config.py中，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;*&apos;</span><br><span class="line">c.NotebookApp.password = u&apos;把上面的文本粘贴到这里&apos;</span><br><span class="line">c.NotebookApp.open_browser = False</span><br><span class="line">c.NotebookApp.port =8888</span><br></pre></td></tr></table></figure><p>编辑好后按esc键，</p><p>输入:wq保存并退出。</p><h2 id="4、安装ipykernel使得jupyter能访问远程的虚拟环境。"><a href="#4、安装ipykernel使得jupyter能访问远程的虚拟环境。" class="headerlink" title="4、安装ipykernel使得jupyter能访问远程的虚拟环境。"></a>4、安装ipykernel使得jupyter能访问远程的虚拟环境。</h2><p>[1] 启动虚拟环境</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`source` `activate &lt;your virtualenv&gt;`</span><br></pre></td></tr></table></figure><p>[2] 在虚拟环境安装jupyter</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`pip ``install` `jupyter`</span><br></pre></td></tr></table></figure><p>[3] 在虚拟环境安装ipykernel</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`pip ``install` `ipykernel`</span><br></pre></td></tr></table></figure><p>[4] 配置ipykernel</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`python -m ipykernel ``install` `--user --name testenv --display-name ``&quot;Python2 (py2env)&quot;`</span><br></pre></td></tr></table></figure><p>其中，–name的参数和–display-name的参数根据配置更改。</p><p>上面就是配置服务端jupyter的以及激活虚拟环境的全过程，总结一下就是：</p><ol><li>安装jupyter，生成key，修改配置文件，按照ip:端口号登陆。</li><li>在激活的虚拟环境中安装ipykernel并配置。</li></ol><p><img src="https://ask.qcloudimg.com/http-save/4069933/qxikwzn1oj.png?imageView2/2/w/1620" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;data:image/jpeg;base64,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&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="jupyter" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/jupyter/"/>
    
    
  </entry>
  
  <entry>
    <title>THE LOTTERY TICKET HYPOTHESIS： FINDING SPARSE, TRAINABLE NEURAL NETWORKS</title>
    <link href="http://yuanquanquan.top/2019/2019120912/"/>
    <id>http://yuanquanquan.top/2019/2019120912/</id>
    <published>2019-12-09T06:02:23.000Z</published>
    <updated>2020-06-14T07:48:18.657Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>神经网络剪枝可以减少超过90%的参数量，同时准确率没有太大影响。但是剪枝后的结构很难从头开始训练，不然就能近似提高训练性能了。</p><p>有一些子网络，在初始情况下就可以高效的训练网络，但我们发现标准的剪枝技术天然的没有包括它们。基于这些结果，我们提出了彩票假说：密集的，随机初始化的前馈网络包括一些子网络（中奖者），这些子网络独立训练时可以在相近的迭代次数达到相近的测试准确率。这些中奖者赢得了初始彩票：它们初始权重就能特别高效的训练。</p><p>我们提出了一个算法来找这些中奖者，一系列的实验也支持了我们的假说和那些偶然初始化的重要性。我们不断的在MNIST和CIFAR10数据集上发现，中奖者的大小只有全连接网络和卷积网络的10%到20%。除了尺寸，我们发现这些中奖者训练更快，测试准确率更高。</p></blockquote><a id="more"></a>  <h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><p>有一些方法可以减少90%的参数但是没有准确率影响，但是为什么开始时不用这个更小的结构来训练呢？研究发现这样做的结果比较差。</p><p>在图1中，我们从MNIST的全连接网络和CIFAR10的卷积网络中随机采样训练一个子网络。随机采样建模了非结构化剪枝。在不同的稀疏程度上，虚线表示迭代中的最小验证损失和准确率。网络越稀疏，学的越慢，最后的准确率越低。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig1.png" alt></p><p>本文中，我们展示总存在是个更小的网络，在从头训练时至少和原始网络一样快，测试准确率相近。图1实线是我们找到的网络。</p><p><strong>The Lottery Ticket Hypothesis</strong> 随机初始化的，密集的网络包括一个子网络，这个网络的初始化使得它能在单独训练时达到相似的准确率，需要的迭代次数也相近。</p><p>严格来说，考虑一个密集的前馈网络$f(x ; \theta)$，初始参数$\theta=\theta_{0} \sim \mathcal{D}<em>{\theta}$。在训练集中用SGD优化，$f$在迭代$j$次后达到最小验证损失$l$和测试准确率$a$。另外，考虑带参数mask $m \in{0,1}^{|\theta|}$训练$f(x ; m \odot \theta)$，所以初始化参数为$m \odot \theta</em>{0}$。当用SGD优化时，$f$在迭代$j‘$次后达到最小验证损失$l‘$和测试准确率$a’$。彩票假说预测，对于$\exists m$使得$j^{\prime} \leq j$，$a^{\prime} \geq a$，$|m|_{0} \ll|\theta|$参数量更少。</p><p>我们发现标准化的剪枝方法自动的略过了这样的网络。我们称这样的一个子网络为中奖者。在参数随机初始化后，$f\left(x ; m \odot \theta_{0}^{\prime}\right)$ where $\theta_{0}^{\prime} \sim \mathcal{D}_{\theta}$，它们就无法中奖了，除非初始化得好。</p><p><strong>Identifying winning tickets</strong> 我们通过训练，减去梯度最小的权重来定为这个网络。剩下的，没有剪枝的就是我们的中奖者。每个未剪枝的连接值在重新训练时在次填回原来初始化的值。这就是我们的实验核心：</p><ol><li>随机初始化网络$f\left(x ; \theta_{0}\right)\left(\text { where } \theta_{0} \sim \mathcal{D}_{\theta}\right)$</li><li>迭代j次，得到参数$\theta_{j}$</li><li>从参数$\theta_{j}$中减去$p \%$参数，生成mask $m$。</li><li>把剩下的参数值设为原来的值$\theta_{0}$，生成中奖者$f\left(x ; m \odot \theta_{0}\right)$</li></ol><p>可以看到，这个剪枝过程是one-shot：网络训练一次，$p \%$的权重剪枝，剩下的重新设置值。但是本文关注于迭代剪枝，不断的进行训练，剪枝，重设。结果显示，迭代剪枝比one-shot剪枝找到的网络更小。</p><p>结果：我们在MNIST的全连接网络和CIFAR10的卷积网络中找到了中奖者。我们使用非结构化的剪枝方法，因此这些中奖者是稀疏的。在更深的网络中，我们的基于剪枝的方法对学习率敏感，高学习率需要warmup来找中奖者。中奖者是原始网络的10-20%大小，准确率更高，迭代次数相近。随机初始化后，效果就很差了，意味着单独说结构不能解释中奖者的成功。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig2.png" alt></p><p><strong>The Lottery Ticket Conjecture</strong> 密集的随机初始化的网络相对于稀疏网络更容易训练，因为有更多可能的子网络。</p><p><strong>Contributions</strong></p><ul><li>我们证明了可以剪枝得到一个子网络，在相近的迭代后可以达到相似的测试准确率</li><li>我们展示了中奖者相比于原来学的更快，可以达到更高的准确率，泛化性更好</li><li>我们提出了彩票假说可以用来解释这个发现</li></ul><p><strong>Implications</strong> 我们可以探索：</p><ul><li><em>Improve training performance</em> 由于中奖者可以独立从头训练，所以我们可以设计一个训练方法来寻找中奖者，更容易的进行剪枝。</li><li>Design better networks* 中奖者揭示了稀疏结构和初始化对学习尤其重要。我们可以试着设计新的网络结构和初始化方案。甚至可以迁移到其他任务上。</li><li><em>Improve our theoretical understanding of neural networks</em></li></ul><h2 id="2-WINNING-TICKETS-IN-FULLY-CONNECTED-NETWORKS"><a href="#2-WINNING-TICKETS-IN-FULLY-CONNECTED-NETWORKS" class="headerlink" title="2 WINNING TICKETS IN FULLY-CONNECTED NETWORKS"></a>2 WINNING TICKETS IN FULLY-CONNECTED NETWORKS</h2><p>我们试验了MNIST上的全连接网络。使用Lenet-300-100。在随机初始化和训练网络后，我们剪枝，然后重新设置参数。我们使用了简单的修建方法，移除最小梯度的权重。</p><p><strong>Notation</strong> $P_{m}=\frac{|m|<em>{0}}{|\theta|}$表示m的稀疏性，例如$P</em>{m}=25 \%$表示75%的权重减掉了。</p><p><strong>Iterative pruning</strong> 图3表示迭代剪枝的结果。第一测修建，网络学得更快，准确率更高（图3左）。51.3%权重有最好的测试准确率，比原始网络快，但是慢于21.1%。3.6%是达到原始网络一样的性能。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig3.png" alt></p><p>图4a是每次迭代剪枝20%的情况。左侧是早停与权重比例的情况。中间是测试准确率。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig4.png" alt></p><p>中奖者从$P-m$为100%到21%学得越来越快，最后早了38%。进一步的剪枝学习变慢，在3.6%的时候早停性能和原始网络一致。测试准确率在13.5%下提升了0.3个百分点，后续就降低了，在3.6%的时候返回原始网络的水平。</p><p><strong>Random reinitialization</strong> 为了验证中奖者初始化的重要性，我们保持了中奖者的结构，但重新初始化。在图5中对每个中奖者随机初始化3次，发现初始化是极其重要的。图3右展示了迭代剪枝的情况。</p><p>【略】</p><p><strong>One-shot pruning</strong> 图4中也展示了one-shot的实验。【略】</p><h2 id="3-WINNING-TICKETS-IN-CONVOLUTIONAL-NETWORKS"><a href="#3-WINNING-TICKETS-IN-CONVOLUTIONAL-NETWORKS" class="headerlink" title="3 WINNING TICKETS IN CONVOLUTIONAL NETWORKS"></a>3 WINNING TICKETS IN CONVOLUTIONAL NETWORKS</h2><p>【略】</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h2 id=&quot;ABSTRACT&quot;&gt;&lt;a href=&quot;#ABSTRACT&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT&quot;&gt;&lt;/a&gt;ABSTRACT&lt;/h2&gt;&lt;p&gt;神经网络剪枝可以减少超过90%的参数量，同时准确率没有太大影响。但是剪枝后的结构很难从头开始训练，不然就能近似提高训练性能了。&lt;/p&gt;
&lt;p&gt;有一些子网络，在初始情况下就可以高效的训练网络，但我们发现标准的剪枝技术天然的没有包括它们。基于这些结果，我们提出了彩票假说：密集的，随机初始化的前馈网络包括一些子网络（中奖者），这些子网络独立训练时可以在相近的迭代次数达到相近的测试准确率。这些中奖者赢得了初始彩票：它们初始权重就能特别高效的训练。&lt;/p&gt;
&lt;p&gt;我们提出了一个算法来找这些中奖者，一系列的实验也支持了我们的假说和那些偶然初始化的重要性。我们不断的在MNIST和CIFAR10数据集上发现，中奖者的大小只有全连接网络和卷积网络的10%到20%。除了尺寸，我们发现这些中奖者训练更快，测试准确率更高。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="ICLR" scheme="http://yuanquanquan.top/tags/ICLR/"/>
    
  </entry>
  
</feed>
