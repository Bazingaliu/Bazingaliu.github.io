<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yuanquanquan的个人博客 | 我愿做你光华中淡淡的一笔</title>
  
  <subtitle>我愿做你光华中淡淡的一笔</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanquanquan.top/"/>
  <updated>2022-01-13T08:22:51.858Z</updated>
  <id>http://yuanquanquan.top/</id>
  
  <author>
    <name>Yann</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Low-latency Speculative Inference On Distributed Multi-modal Data Streams</title>
    <link href="http://yuanquanquan.top/2022/20220113/"/>
    <id>http://yuanquanquan.top/2022/20220113/</id>
    <published>2022-01-13T08:06:23.000Z</published>
    <updated>2022-01-13T08:22:51.858Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文介绍MobiSys 2021年的文章《Low-latency Speculative Inference On Distributed Multi-modal Data Streams》。多模态数据流（multi-modal data streams）在在分布式传感任务中很常见，常用的场景有人物跟踪、行为识别以及音频和视频分析等。考虑到不同模态数据之间高度的异构性，不同模态的数据传输速度是不一致的，因而较慢的数据流会显著地降低整个系统的推理性能和准确率。为此，该篇文章提出推测式推理来自动调节和适应多模态数据流中传输速度不一致的问题。与已有工作阻塞式推理的有所不同，该文章会根据残缺不完整的数据，推测生成一个完整的数据，并根据此生成的数据进行推断。同时，该文章提供一个回滚机制，用以确定是否可以接受利用不完整数据得到的推理结果。当在不接受结果时，回滚机制会等到足够多的数据到达时在做推理，以保证结果的正确性。实验显示该篇文章提出的方法，与最新的六个基准工作相比，能够在保证准确率不变的情况下，7-128倍的降低推断延迟</p></blockquote><span id="more"></span>  <div class="row">    <embed src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding/Hyperspectral/3458864.3467884.pdf" width="100%" height="550" type="application/pdf"></div><p>​        目前物联网设备上配有大量低功耗但数据丰富的传感器（如相机、麦克风、激光雷达、高光谱成像仪和射频成像仪）。这些传感器可以提供多种大量连续的数据用以进行复杂环境下的推理。这类多模态数据流的推理可以显著地提升语音识别、健康检测、增强现实和自动驾驶等任务的准确率。尽管有这么多的好处，多模态推理面临着多个数据流之间不同步，甚至是数据缺失的问题，具体带来三个挑战：(1) 多模态数据往往是不同维度的，差别较大（比如：音频和视频），因而很难通过某个模态的数据流去恢复或创建其他缺失的数据流。(2) 需要设计一个简单高效的回滚机制，既要保证尽可能地少进行回滚，又要保证回滚本身的开销较小。(3) 多个模态数据流之间存在时间漂移问题，即很多传感器设备由于缺少时钟同步，进而造成时间戳不准的现象。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113160818753" alt="系统workflow"></p><p>​        为了解决上述问题，该文作者提出了推测式推理的方法予以解决。如图1所示，具体包含三个部分：数据对齐模块、数据填补模块和回滚模块。多模态数据首先经过数据对齐模块，将不同模态间的数据时间戳对齐，然后数据填补模块会根据某个模态的数据流生成缺失模态的数据，并将生成的数据和原始的数据一并输入到多模态学习模型中。如果得到的结果是可以相信的，那么直接输出，否则回滚模块进行处理，得到最终的正确数据。接下来本文将详细展开这三部分的详细实现。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113160841622" alt="多模态推理过程"></p><p><strong>数据填补模块</strong></p><p>该文发现，直接用某个模态的数据流去生成另外一个模态的数据是十分困难的，同时也需要大量的训练开销。为此该文调研了多模态数据进行推断的方法，如图2所示。不同模态的数据首先经过各自的特征向量提取层，得到各自的feature map，然后将这些feature map拼接在一起经过后续的处理层，得到最终的输出。基于调研结果，该文提出基于某个模态的数据流去生成另一个模态的feature map的方法，既能保证生成的feature map准确，又能保证训练和生成过程的开销比较小，具体如图3所示。该文使用生成对抗网络（GAN）进行训练，训练时设置生成器和辨别器两个部分，生成器用于生成缺失数据流的feature map，辨别器用于区分生成的feature map和真实数据生成的feature map。生成器和辨别器相互对抗最后得到一个能够生成缺失数据流feature map的模型。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113160907237" alt="数据填补模型训练图"></p><p><strong>回滚模块</strong></p><p>该文发现不同类别的数据对某个模态数据的缺失的反应是不同的，比如“under”这个词仅通过唇语就能识别，而“allegations”却很难。针对这一现象，该文对所有待识别的数据做了分类，获得了每一个类别的数据要想准确预测出正确结果所需要的最少模态数据信息。基于这个信息设置回滚模块，当推测时已有的数据大于最少模态数据信息时，就不回滚；否则反之。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113160935523" alt="数据对齐示例"></p><p><strong>数据对齐模块</strong></p><p>该文发现，当两个模态的不同时刻的数据得到的推理结果具有最小的“距离”，那么这两个来自不同模态的时刻，实际上是一致的。图4给出了4个例子，画红圈的地方表示最小距离的时刻，也就是两个模态数据流时间偏移的距离。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113162245436" alt="在模拟Trace上的准确率和延迟上的分析"></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113161043432" alt="在真实Trace上的准确率和延迟上的分析"></p><p>本文所提出的工作在一个带有Xeon E5-2620 v3 2.40 GHz CPU和两个GTX-1080Ti GPU的32 GB RAM边缘服务器上实现的。并与阻塞算法（Blocking mechanism）、自适应码率算法（ABS）、周期分析法（Periodic profiling）、一次分析方法（One-time profiling）和帧采样算法（frame sampling）在语音识别数据集（LRW）、事件检测数据集（AVE）和活动识别数据集（STISEN）进行了比较，对比结果如图5和6所示。在模拟Trace上显示，该文提出的方法能够降低2-128倍的延迟，并提高1%-25%的准确率。在真实Trace上显示，该文提出的方法能够在延迟上与自动码率和帧采样算法类似，但却能得到和分析与阻塞算法一样的准确率，达到了同时降低延迟，并提升准确率的目的。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文介绍MobiSys 2021年的文章《Low-latency Speculative Inference On Distributed Multi-modal Data Streams》。多模态数据流（multi-modal data streams）在在分布式传感任务中很常见，常用的场景有人物跟踪、行为识别以及音频和视频分析等。考虑到不同模态数据之间高度的异构性，不同模态的数据传输速度是不一致的，因而较慢的数据流会显著地降低整个系统的推理性能和准确率。为此，该篇文章提出推测式推理来自动调节和适应多模态数据流中传输速度不一致的问题。与已有工作阻塞式推理的有所不同，该文章会根据残缺不完整的数据，推测生成一个完整的数据，并根据此生成的数据进行推断。同时，该文章提供一个回滚机制，用以确定是否可以接受利用不完整数据得到的推理结果。当在不接受结果时，回滚机制会等到足够多的数据到达时在做推理，以保证结果的正确性。实验显示该篇文章提出的方法，与最新的六个基准工作相比，能够在保证准确率不变的情况下，7-128倍的降低推断延迟&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Altium Designer 21版本初体验</title>
    <link href="http://yuanquanquan.top/2022/2022112/"/>
    <id>http://yuanquanquan.top/2022/2022112/</id>
    <published>2022-01-12T15:18:31.000Z</published>
    <updated>2022-01-13T07:38:45.656Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>AD21截止到今天，一共做了9个较大的版本更新，几乎每个月都会有一次。当然可能是由于更新的太快了，Bug也比较多，很多更新不到两周就会出一个Hot Fix的版本，本文盘点下AD21到底增加了哪些新的功能。</p></blockquote><span id="more"></span>  <p><strong>新的线长调制模式</strong></p><p>引入了新的Trombone（长号）和Sawtooth（锯齿）调整模式，并对Accordion（手风琴）模式进行了改进。现在一共支持三种调制模式：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232058666" alt=""></p><p>Sawtooth（锯齿）模式：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232121054" alt=""></p><p>Trombone（长号）模式：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113153838290" alt=""></p><p>相比早期的版本，AD18之后的线长调制功能有了相当大的改进，使调等长更灵活、更方便。客观的讲，AD的调线长功能已经完全不输于Allegro或者Pads。但对于高速设计而言，AD仍有不少需要改进的地方，比如更完善的规则设定（目前AD不支持虚拟T点，在某些拓扑结构中，调整起来还是比较麻烦）；比如任意角度的差分对及圆弧走线，AD还不够灵活。</p><p><strong>原理图功能的改进</strong></p><p><em>原理图页自动标号</em></p><p>支持对同一工程下所有的原理图添加序号，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232240353" alt=""></p><p>具体如何操作呢？只要打开Project Options对话框，在Options页面的General栏中勾选“Automatic Sheet Numbering”。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232300263" alt=""></p><p>这个功能可以让图纸的逻辑结构更清晰，便于快速定位。结合之后的Cross Reference交叉探查功能，可以让Port跳转定位更清晰。</p><p><em>网络(Net)名称识别</em></p><p>当鼠标悬浮在某根导线上方时，会显示当前网络(Net)的逻辑名称和物理名称：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232325969" alt=""></p><p>所谓逻辑名称通常用于层级结构中，表示一个有逻辑意义的网络名称，通常会通过Net Label、Port或Sheet Entry的名称来表示；所谓物理名称，就是实际的网表中分配的名字，典型的物料名称定义为器件位号+管脚号，如NetC2_2。在非层次结构的项目中，两者大部分情况下是一致的。</p><p>这个功能也可以用来判断导线是否与器件管脚正常连接。</p><p><em>全局高亮指定网络</em></p><p>对某个网络点击Alt+鼠标左键，可以在整个项目的原理图中高亮该网络。</p><p>所谓逻辑名称通常用于层级结构中，表示一个有逻辑意义的网络名称，通常会通过Net Label、Port或Sheet Entry的名称来表示；所谓物理名称，就是实际的网表中分配的名字，典型的物料名称定义为器件位号+管脚号，如NetC2_2。在非层次结构的项目中，两者大部分情况下是一致的。</p><p>这个功能也可以用来判断导线是否与器件管脚正常连接。</p><p><em>全局高亮指定网络</em></p><p>对某个网络点击Alt+鼠标左键，可以在整个项目的原理图中高亮该网络。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20220112232438397.png" alt=""></p><p>这个和在Navigator面板里选择网络感觉差不多，多了一个快捷键，聊胜于无吧。</p><p><em>为导线(Wire)对象增加了属性</em></p><p>Wire不再是简单的图形对象，现在可以为Wire增加变量属性了：包括用户自定义变量、规则、网络类等都可以包含在Wire变量中。</p><p>下图是将信号完整性规则置于Wire中的一个实例：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232543706" alt=""></p><p>说实话不知道这功能有啥用，因为很少会用AD做仿真或者SI。倒是线束管理可能会用到一些自定义的属性。官方文档说之后还会在该功能基础上继续进行开发，让我们拭目以待吧</p><p><strong>新的PCB设计规则编辑器</strong></p><p>这应该说是AD21最大的一个变化之一。现在的规则编辑器，不仅可以用原来的对话框模式，还支持新的文档编辑模式，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232652189" alt=""></p><p>以上这个界面，是不是有点熟悉…? 和下面这个是不是有点像？</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232705715" alt=""></p><p>模式和Allegro的Constrain Manager几乎一样，当然使用方法和内容还是有很大区别的。两种工具都使用过的小伙伴都知道，AD的规则一直是由规则对象驱动的，即先确定规则，然后把需要满足规则的设计对象填充进去；而Allegro则不同，是设计对象驱动的，即先找到需要定义规则的设计对象，然后再设置约束。举个简单的例子，定义5V网络的Width宽度规则，AD的做法是先找到Width规则，然后用Query查询语句InNet(‘5V’)定位到5V网络，然后才能定义最小、最大线宽；而Allegro是需要先在CM的Net类目先找5V的网络，然后直接设定最小、最大线宽即可。</p><p>就设计而言，Allegro的CM的设定显然更加严谨。但这次AD的更新，明显模糊了两者的区别，让AD的用户也多了一种可能更为合理的选择。</p><p>使用这种基于设计对象的方式，可以非常方便的看到应用于某个设计对象的规则，而这在以前的AD规则编辑器中是完全做不到的。</p><p>下图为电源类Power应用的规则：包含热焊盘连接方式及过孔类型两类规则。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232816499" alt=""></p><p>下图为在同一界面中编辑差分对的线宽规则及过孔规则：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232840359" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220112232934182" alt=""></p><p>打开Properties属性面板，可以随时对某个设计对象添加额外的规则：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/npicj28A7dDMaMkpOpbBjl9IBwXG6BwyqNUwJccX7HibtRicraWC4zPw0adVpgiaewLMOhgCqPmpJtHribiaceVSnxSQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt=""></p><p>AD的图形化示意模式无疑让用户对设计规则的使用方法有更直观的了解，这点比Allegro更人性化。</p><p>更有意思的是，AD21支持传统模式和文档模式之间的切换，也就是说用户可以随意在两种模式下定义规则，切换时AD会自动进行规则格式的转换… 总觉得这种转换可能会存在逻辑上的问题… </p><p><strong>在电源层上使用多边形铺铜</strong></p><p>大家都知道，AD的内电层（Plane层）默认定义为负片，即创建的Plane层默认都是铜箔。如果在Plane层上放一个矩形对象，就相当于把这块铜皮镂空；如果用线条围成一个闭合的多边形，就相当于完成了平面的分割：可以为围住的区域定义一个不同的电源网络，而线宽就是这个两个不同电源之间的间隙。这么做的目的是为了使数据存储的效率更高（因为内电层大部分都是铜箔）。然而这么做也会带来一些问题：内电层上相同网络的部分被视为一个整体，只能修改平面对其他电气对象（如过孔）的间距和连接方式，却无法对其形状进行编辑，有可能在平面层上出现铜岛、窄颈或死铜的情况。</p><p>新版本的AD21允许将内电层平面做为多边形铺铜(Polygon)处理。切割平面的操作与之前完全一样，唯一的区别是内电层平面现在可以进行独立的Polygon Repour操作，这样就可以人为的进行设置，避免死铜的出现：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113151924979" alt=""></p><p>如何使用这一新功能呢？首先要在右上角打开小齿轮Preference，然后在System的General也点击Advance按钮，并在高级设置中勾选PCB.SplitPlanes.Routing。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113151947242" alt=""></p><p>该选项默认是不勾选的，即按原来的方式处理内电层。</p><p>使用该功能请注意以下事项：</p><ul><li>启用该选项后，查看每个平面层，用配置好的Polygon选项重新铺铜平面多边形，以适应您的设计需要。</li><li>平面层的连接和间隙由PlaneConnect和PlaneClearance设计规则定义。</li><li>当修改了一个平面（连接或间隙）设计规则后，在每个平面层上至少重新铺铜一个多边形，以更新该层的连接/间隙。</li><li>在一个平面层上进行的编辑，如修改分割线的位置，会导致该平面层上的多边形自动重新铺铜。</li></ul><p>​    又一个细节的改进，虽然用到的概率不大，但如果真的碰到要求较高的设计，也多了一种解决问题的方法。但是该方法仍无法解决内电层中走线的特殊需求。如果需要在内电层中走信号线，还请使用信号层（Signal Layer）。</p><p><strong>区域（Region）和铺铜（Polygon）的端点</strong></p><p>Region和Polygon对象的属性中列出了外形端点的坐标：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113152049867" alt=""></p><p>这些端点坐标可以复制到Excel中进行编辑，然后再粘贴回来。这一功能对于需要精确控制铺铜或区域外形的场合适用。</p><p>聊胜于无的功能。</p><p><strong>任意角度的拼版阵列</strong></p><p>支持任意角度的拼版阵列（Embedded Board Arrays）:</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113152122250" alt=""></p><p>聊胜于无的功能。怎么拼版一般丢给板厂</p><p><strong>在PCB上放置一个矩形</strong></p><p>这是一个非常简单但千呼万唤始出来的功能… 以前要画一个矩形的板框只能依次画4根Track；如何需要做圆角或者倒角那更是痛苦万分。现在，这一切都成了过去。</p><p>在PCB的Place菜单中，可以直接进行矩形图形的摆放：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113152212807" alt=""></p><p>双击矩形，可以对其转角模式进行编辑。</p><p>下图分别是一个圆角的矩形和带倒角的矩形，倒角和圆角的尺寸都可以设置：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113152252704" alt=""></p><p><strong>在PCB上摆放一个图片</strong></p><p>以前如果需要在丝印层上放置一个比较复杂的Logo或者图形，只能使用脚本。</p><p>AD21支持了一种更简单粗暴的方法，点击菜单<strong>Place » Graphics</strong> ,就可以直接将JPG、BMP、PNG或SVG格式的图形放置在你的PCB上。</p><p>下图是我刚在丝印层上放置的图片</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113152319677" alt=""></p><p><strong>在3D视图中包含非电气层及机械层</strong></p><p>AD21支持在3D模式中显示机械层以及丝印、阻焊等非电气层的信息。</p><p>操作方式：首先点击L，打开View Configuration。</p><p>将View Options栏中，将3D Setting设置为By Layer:</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113152341316" alt=""></p><p>然后就可以在Layer &amp; Colors栏中，用小眼睛控制每一层在3D中的可见性了：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113152354043" alt=""></p><p>从另一个维度欣赏您的PCB吧：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20220113152407260" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;AD21截止到今天，一共做了9个较大的版本更新，几乎每个月都会有一次。当然可能是由于更新的太快了，Bug也比较多，很多更新不到两周就会出一个Hot Fix的版本，本文盘点下AD21到底增加了哪些新的功能。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="硬件学习" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/%E7%A1%AC%E4%BB%B6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="单片机" scheme="http://yuanquanquan.top/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>vn.py-下载回测数据</title>
    <link href="http://yuanquanquan.top/2021/2021130/"/>
    <id>http://yuanquanquan.top/2021/2021130/</id>
    <published>2021-11-30T12:38:30.000Z</published>
    <updated>2021-12-16T18:09:38.007Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>数据库的配置</p><p>下载数据</p><ul><li><p>方式1：程序下载</p></li><li><p>方式2：直接导入</p></li></ul><p>对于写好的策略，使用历史数据回测何其重要。记录下怎么导入回测用的数据</p></blockquote><span id="more"></span><h2 id="数据库的配置"><a href="#数据库的配置" class="headerlink" title="数据库的配置"></a>数据库的配置</h2><p>至于<code>MongoDB</code>的安装，这里不多介绍，网上资料比较多。</p><ul><li>创建表</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.createUser(&#123; user: &quot;vnpy&quot;, pwd: &quot;123456&quot;, roles: [&#123; role: &quot;readWrite&quot;, db: &quot;vnpy&quot; &#125;] &#125;)</span><br></pre></td></tr></table></figure><ul><li>修改配置文件</li></ul><blockquote><p>Mac系统的配置文件路径 <code>~/.vntrader/vt_setting.json</code>,  和window系统不一样，注意一下。（账号，密码自己指定）</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;database.driver&quot;: &quot;mongodb&quot;,</span><br><span class="line">  &quot;database.database&quot;: &quot;vnpy&quot;,</span><br><span class="line">  &quot;database.host&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">  &quot;database.port&quot;: 27017,</span><br><span class="line">  &quot;database.user&quot;: &quot;root&quot;,</span><br><span class="line">  &quot;database.password&quot;: &quot;123456&quot;,</span><br><span class="line">  &quot;database.authentication_source&quot;:&quot;&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="下载数据"><a href="#下载数据" class="headerlink" title="下载数据"></a>下载数据</h2><h3 id="方式1：程序下载"><a href="#方式1：程序下载" class="headerlink" title="方式1：程序下载"></a>方式1：程序下载</h3><blockquote><p>当然也可以通过图像界面的方式下载</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 初始化事件引擎</span><br><span class="line">event_engine = EventEngine()</span><br><span class="line"># 初始化主引擎</span><br><span class="line">main_engine = MainEngine(event_engine)</span><br><span class="line"></span><br><span class="line"># 加载币安现货的网关, 现货代码(小写)：btcusdt</span><br><span class="line">main_engine.add_gateway(BinanceGateway)</span><br><span class="line">main_engine.connect(binance_settings, &quot;BINANCE&quot;)</span><br><span class="line"></span><br><span class="line"># 加载币安合约的网关</span><br><span class="line">main_engine.add_gateway(BinancesGateway)</span><br><span class="line">main_engine.connect(binances_settings, &quot;BINANCES&quot;)</span><br><span class="line"></span><br><span class="line">main_engine.init_engines()</span><br><span class="line">sleep(15)</span><br><span class="line">engine = ManagerEngine(main_engine, event_engine)</span><br><span class="line"></span><br><span class="line"># 合约代码(大写)：BTCUSDT， 周期(Interval.MINUTE,Interval.HOUR,Interval.DAILY)</span><br><span class="line"># engine.download_bar_data(&quot;BTCUSDT&quot;, Exchange.BINANCE, Interval.MINUTE, datetime(2016, 1, 1))</span><br><span class="line"></span><br><span class="line"># 现货代码(小写)：btcusdt, 周期(Interval.MINUTE,Interval.HOUR,Interval.DAILY)</span><br><span class="line">engine.download_bar_data(&quot;bnbusdt&quot;, Exchange.BINANCE, Interval.HOUR, datetime(2016, 1, 1))</span><br><span class="line">main_engine.close()</span><br></pre></td></tr></table></figure><h3 id="方式2：直接导入"><a href="#方式2：直接导入" class="headerlink" title="方式2：直接导入"></a>方式2：直接导入</h3><p>我把自己下载的数据，总数<strong>8022246</strong>条数据已经导出，通过<code>mongorestore</code>直接导入到<code>MongoDB</code>,省时省力。</p><blockquote><p>数据如下，还是比较全的，小写的是现货，大写的是合约</p></blockquote><table><thead><tr><th style="text-align:left">Symbol</th><th style="text-align:left">Interval</th><th style="text-align:left">time区间</th><th style="text-align:left">数据量</th></tr></thead><tbody><tr><td style="text-align:left">btcusdt</td><td style="text-align:left">1m</td><td style="text-align:left">2017-08-17 ~2021-07-13</td><td style="text-align:left">2044721</td></tr><tr><td style="text-align:left">btcusdt</td><td style="text-align:left">1h</td><td style="text-align:left">2017-08-17 ~ 2021-07-15</td><td style="text-align:left">34154</td></tr><tr><td style="text-align:left">btcusdt</td><td style="text-align:left">d</td><td style="text-align:left">2017-08-17 ~ 2021-07-15</td><td style="text-align:left">1429</td></tr><tr><td style="text-align:left">ethusdt</td><td style="text-align:left">1m</td><td style="text-align:left">2017-08-17 ~ 2021-07-15</td><td style="text-align:left">2047718</td></tr><tr><td style="text-align:left">ethusdt</td><td style="text-align:left">1h</td><td style="text-align:left">2017-08-17 ~ 2021-07-15</td><td style="text-align:left">34154</td></tr><tr><td style="text-align:left">bnbusdt</td><td style="text-align:left">1m</td><td style="text-align:left">2017-11-06~2021-07-15</td><td style="text-align:left">1931504</td></tr><tr><td style="text-align:left">bnbusdt</td><td style="text-align:left">1h</td><td style="text-align:left">2017-11-06~2021-07-15</td><td style="text-align:left">32218</td></tr><tr><td style="text-align:left">bnbusdt</td><td style="text-align:left">d</td><td style="text-align:left">2017-11-06~2021-07-15</td><td style="text-align:left">1348</td></tr><tr><td style="text-align:left">ltcusdt</td><td style="text-align:left">1h</td><td style="text-align:left">2017-12-13~2021-07-15</td><td style="text-align:left">31330</td></tr><tr><td style="text-align:left">ltcusdt</td><td style="text-align:left">d</td><td style="text-align:left">2017-12-13~2021-07-15</td><td style="text-align:left">1311</td></tr><tr><td style="text-align:left">ETHUSDT</td><td style="text-align:left">1m</td><td style="text-align:left">2019-11-27 ~  2021-07-15</td><td style="text-align:left">857942</td></tr><tr><td style="text-align:left">ETHUSDT</td><td style="text-align:left">1h</td><td style="text-align:left">2019-11-27 ~ 2021-07-15</td><td style="text-align:left">14300</td></tr><tr><td style="text-align:left">ETHUSDT</td><td style="text-align:left">d</td><td style="text-align:left">2019-11-27 ~ 2021-07-15</td><td style="text-align:left">597</td></tr><tr><td style="text-align:left">BTCUSDT</td><td style="text-align:left">d</td><td style="text-align:left">2019-09-08 ~ 2021-07-15</td><td style="text-align:left">677</td></tr><tr><td style="text-align:left">BTCUSDT</td><td style="text-align:left">1h</td><td style="text-align:left">2019-09-09 ~ 2021-07-15</td><td style="text-align:left">16212</td></tr><tr><td style="text-align:left">BTCUSDT</td><td style="text-align:left">1m</td><td style="text-align:left">2019-09-09 ~ 2021-07-15</td><td style="text-align:left">972631</td></tr></tbody></table><blockquote><p>这里特别注意：如果自己原有的库<code>db_bar_data</code>,<code>db_bar_overview</code>中有重要数据,下面导入的方式就要注意，用的是<code>--drop</code>模式，有删库的风险</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mongorestore -h 127.0.0.1 --port 27017 -uvnpy -p 123456 -d vnpy --drop ./vnpy</span><br></pre></td></tr></table></figure><ul><li><code>-h</code> 地址</li><li><code>-u</code> 数据库账号名（没有可省略）</li><li><code>-p</code> 数据库账号密码（没有可省略）</li><li><code>-d</code> 数据库名</li><li><code>--drop</code>:  <strong>在导入之前删除每个集合</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2021-07-15T16:39:06.524+0800 [*#######################.] vnpy.db_bar_data 1.59GB/1.63GB (97.7%)*</span><br><span class="line">2021-07-15T16:39:09.525+0800 [*#######################.] vnpy.db_bar_data 1.60GB/1.63GB (98.4%)*</span><br><span class="line">2021-07-15T16:39:12.524+0800 [*#######################.] vnpy.db_bar_data 1.61GB/1.63GB (99.1%)*</span><br><span class="line">2021-07-15T16:39:15.525+0800 [*#######################.] vnpy.db_bar_data 1.63GB/1.63GB (99.9%)*</span><br><span class="line">2021-07-15T16:39:16.020+0800 [*########################] vnpy.db_bar_data 1.63GB/1.63GB (100.0%)*</span><br><span class="line">2021-07-15T16:39:16.020+0800 restoring indexes for collection vnpy.db_bar_data from metadata</span><br><span class="line">2021-07-15T16:39:43.859+0800 finished restoring vnpy.db_bar_data (8022246 documents, 0 failures)</span><br><span class="line">2021-07-15T16:39:43.860+0800 8022262 document(s) restored successfully. 0 document(s) failed to restore.</span><br></pre></td></tr></table></figure><p>系统已经有了，数据也有了，后面的文章主要介绍一些经典的量化策略。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/20211217020238.png" alt=""></p><h2 id="图像界面方式回测"><a href="#图像界面方式回测" class="headerlink" title="图像界面方式回测"></a>图像界面方式回测</h2><p>启动<code>run_window.py</code>文件，即可启动图像界面回测。启动成功后，整个界面如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run_window.py</span><br></pre></td></tr></table></figure><p><a href="https://camo.githubusercontent.com/959c3e43cf8cb155574d394ee960f3a97b73e2def82339b0615cea3cb3bb9e85/68747470733a2f2f692e6c6f6c692e6e65742f323032312f30352f32352f4e4a5966653858545663684d6e61642e706e67"><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/68747470733a2f2f692e6c6f6c692e6e65742f323032312f30352f32352f4e4a5966653858545663684d6e61642e706e67.png" alt=""></a></p><p>图像界面回测结果如下 <img src="https://camo.githubusercontent.com/f0fc6d4dfbd7d596f78d8d520a03b70d8e25357b0701bb925b854f12aa49c758/68747470733a2f2f692e6c6f6c692e6e65742f323032312f30352f32352f796d6733385a75466369595268786e2e706e67" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;数据库的配置&lt;/p&gt;
&lt;p&gt;下载数据&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;方式1：程序下载&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;方式2：直接导入&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于写好的策略，使用历史数据回测何其重要。记录下怎么导入回测用的数据&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="量化" scheme="http://yuanquanquan.top/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>白鹭量化面试题</title>
    <link href="http://yuanquanquan.top/2021/202111301/"/>
    <id>http://yuanquanquan.top/2021/202111301/</id>
    <published>2021-11-30T09:23:16.000Z</published>
    <updated>2021-11-30T09:57:27.197Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容."><div class="hbe-input-container"><input type="password" id="hbePass" placeholder="您好, 这里需要密码." /><label>您好, 这里需要密码.</label><div class="bottom-line"></div></div><script id="hbeData" type="hbeData" data-hmacdigest="d76237745e4126bcf2e2bf3da7ecc98f1ce2f3e96503320cc0c27043adaee90b">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</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      有东西被加密了, 请输入密码查看.
    
    </summary>
    
    
      <category term="量化" scheme="http://yuanquanquan.top/tags/%E9%87%8F%E5%8C%96/"/>
    
      <category term="面试" scheme="http://yuanquanquan.top/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>量化交易系统开发框架-vn.py</title>
    <link href="http://yuanquanquan.top/2021/202211125/"/>
    <id>http://yuanquanquan.top/2021/202211125/</id>
    <published>2021-11-25T03:27:39.000Z</published>
    <updated>2021-11-30T09:19:39.084Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>vn.py是一套基于Python的开源量化交易系统开发框架，于2015年1月正式发布，在开源社区6年持续不断的贡献下一步步成长为全功能量化交易平台，目前国内外金融机构用户已经超过600家，包括：私募基金、证券自营和资管、期货资管和子公司、高校研究机构、自营交易公司、交易所、Token Fund等。</p></blockquote><span id="more"></span><h1 id="1、VN-PY介绍"><a href="#1、VN-PY介绍" class="headerlink" title="1、VN.PY介绍"></a>1、VN.PY介绍</h1><blockquote><p>工欲善其事必先利其器，有一个好的回测框架，才能更好的验证自己的策略</p></blockquote><p>vn.py是一套基于Python的开源量化交易系统开发框架，于2015年1月正式发布，在开源社区6年持续不断的贡献下一步步成长为全功能量化交易平台，目前国内外金融机构用户已经超过600家，包括：私募基金、证券自营和资管、期货资管和子公司、高校研究机构、自营交易公司、交易所、Token Fund等。</p><h1 id="2、搭建回测环境"><a href="#2、搭建回测环境" class="headerlink" title="2、搭建回测环境"></a>2、搭建回测环境</h1><blockquote><p>再好的策略也得经过历史数据的回测，才敢放心的去实盘</p></blockquote><h2 id="2-1、环境"><a href="#2-1、环境" class="headerlink" title="2.1、环境"></a>2.1、环境</h2><ul><li>Python 3.8.x</li><li>vnpy 2.30，</li><li>MongoDB</li></ul><h2 id="2-2、安装"><a href="#2-2、安装" class="headerlink" title="2.2、安装"></a>2.2、安装</h2><ul><li><p>安装mongoDB</p><p>安装mongoD环境是为了存储数据，方便回测。</p></li></ul><p>安装教程，参考：<a href="https://docs.mongodb.com/manual/tutorial/install-mongodb-on-os-x/">https://docs.mongodb.com/manual/tutorial/install-mongodb-on-os-x/</a></p><p>建库建表的语句</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">use admin</span><br><span class="line">db.createUser(&#123;user:&#x27;admin&#x27;, pwd:&#x27;xxxx&#x27;, roles: [ &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ]&#125;)</span><br><span class="line"></span><br><span class="line">use vnpy</span><br><span class="line">db.createUser(&#123; user: &quot;root&quot;, pwd: &quot;xxxx&quot;, roles: [&#123; role: &quot;readWrite&quot;, db: &quot;vnpy&quot; &#125;] &#125;)</span><br></pre></td></tr></table></figure><ul><li>安装<code>vnpy</code></li></ul><blockquote><p>安装vnpy,别直接<code>pip install vnpy</code>， 因为<code>pypi.org</code>上的库已经好久没有更新了，所以用下面的安装方式</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install git+https://github.com/vnpy/vnpy.git</span><br></pre></td></tr></table></figure><ul><li><p>安装其他的包</p><p>成功安装<code>vnpy</code>后，还有部分包并没有成功安装，需要手动安装，如下：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install tzlocal</span><br><span class="line">pip install plotly</span><br><span class="line">pip install pymongo</span><br><span class="line">pip install mongoengine</span><br><span class="line">pip install quickfix</span><br></pre></td></tr></table></figure><h1 id="3、项目结构说明"><a href="#3、项目结构说明" class="headerlink" title="3、项目结构说明"></a>3、项目结构说明</h1><ul><li>代码结构</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">├── README.md</span><br><span class="line">├── .vntrader  # 这个文件自己创建，默认是~/.vntrader/,但是为了编码方便，我会在项目创建这个目录（注意主要信息的泄露）</span><br><span class="line">│   ├── cta_backtester_setting.json</span><br><span class="line">│   ├── cta_strategy_data.json</span><br><span class="line">│   ├── cta_strategy_setting.json</span><br><span class="line">│   ├── data_recorder_setting.json</span><br><span class="line">│   ├── database.db</span><br><span class="line">│   ├── log</span><br><span class="line">│   │   └── vt_20210522.log</span><br><span class="line">│   ├── risk_manager_setting.json</span><br><span class="line">│   └── vt_setting.json</span><br><span class="line">└── vnpy_strategy    </span><br><span class="line">    ├── backtest_fixed_time.py   # 运行回测</span><br><span class="line">    ├── run_window.py            # 通过图像界面操作</span><br><span class="line">    ├── strategies               # 自己写的量化策略,都在这个目录</span><br><span class="line">    │   └── simple_double_ma.py  </span><br><span class="line">    └── utils                    # 工具类代码，都会放在这个目录</span><br><span class="line">        └── download_data_binance.py</span><br></pre></td></tr></table></figure><ul><li><p>代码说明</p><p>​    使用VNPY创建自己的策略需要继承<code>CtaTemplate</code>, 下面也是官方一个简单均线策略，来对<code>vn.py</code>回测框架的熟悉。</p><p>​    双均线策略，指的是运用两条不同周期的移动平均线，即短周期移动平均线和长周期移动平均线的相对大小，研判买进与卖出时机的策略。由短周期均线自下向上穿越长周期均线，所形成的交点，称为金叉。当短周期均线自上而下穿越长周期均线，所形成的交点，称为死叉。</p><p>这样我们可以构建一个双均线策略：双均线金叉的时候，表明该币很强势，市场属于多头市场；反之，当出现死叉点时，市场属于空头市场。</p></li></ul><h1 id="4、回测"><a href="#4、回测" class="headerlink" title="4、回测"></a>4、回测</h1><blockquote><p>回测之前，需要下载历史数据；如果配置了MongoDB,数据下载完成后可以在MongoDB查看</p></blockquote><h2 id="4-1、命令方式回测"><a href="#4-1、命令方式回测" class="headerlink" title="4.1、命令方式回测"></a>4.1、命令方式回测</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run_backtest.py</span><br></pre></td></tr></table></figure><p>回测结果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">2021-05-24 23:18:06.187119    首个交易日：2017-09-05</span><br><span class="line">2021-05-24 23:18:06.187126    最后交易日：2018-05-20</span><br><span class="line">2021-05-24 23:18:06.187131    总交易日：258</span><br><span class="line">2021-05-24 23:18:06.187137    盈利交易日：139</span><br><span class="line">2021-05-24 23:18:06.187142    亏损交易日：117</span><br><span class="line">2021-05-24 23:18:06.187151    起始资金：300,000.00</span><br><span class="line">2021-05-24 23:18:06.187159    结束资金：300,969.58</span><br><span class="line">2021-05-24 23:18:06.187165    总收益率：0.32%</span><br><span class="line">2021-05-24 23:18:06.187170    年化收益：0.30%</span><br><span class="line">2021-05-24 23:18:06.187177    最大回撤:     -608.35</span><br><span class="line">2021-05-24 23:18:06.187183    百分比最大回撤: -0.20%</span><br><span class="line">2021-05-24 23:18:06.187188    最长回撤天数:     8</span><br><span class="line">2021-05-24 23:18:06.187194    总盈亏：969.58</span><br><span class="line">2021-05-24 23:18:06.187200    总手续费：101.55</span><br><span class="line">2021-05-24 23:18:06.187208    总滑点：0.00</span><br><span class="line">2021-05-24 23:18:06.187215    总成交金额：101,552.30</span><br><span class="line">2021-05-24 23:18:06.187220    总成交笔数：171</span><br><span class="line">2021-05-24 23:18:06.187226    日均盈亏：3.76</span><br><span class="line">2021-05-24 23:18:06.187231    日均手续费：0.39</span><br><span class="line">2021-05-24 23:18:06.187237    日均滑点：0.00</span><br><span class="line">2021-05-24 23:18:06.187243    日均成交金额：393.61</span><br><span class="line">2021-05-24 23:18:06.187250    日均成交笔数：0.6627906976744186</span><br><span class="line">2021-05-24 23:18:06.187256    日均收益率：0.00%</span><br><span class="line">2021-05-24 23:18:06.187262    收益标准差：0.02%</span><br><span class="line">2021-05-24 23:18:06.187267    Sharpe Ratio：1.11</span><br><span class="line">2021-05-24 23:18:06.187273    收益回撤比：1.60</span><br><span class="line">2021-05-24 23:18:06.187780    策略统计指标计算完成</span><br></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><h2 id="4-2、图像界面方式回测"><a href="#4-2、图像界面方式回测" class="headerlink" title="4.2、图像界面方式回测"></a>4.2、图像界面方式回测</h2><p>运行<code>run_window.py</code>文件，即可启动图像界面回测。启动成功后，整个界面如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run_window.py</span><br></pre></td></tr></table></figure><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><p>图像界面回测结果如下：</p><ul><li>回测之前，一定要先下载数据，有问题，可以多去看看官方的文档;</li></ul><h1 id="5-如何运行VNPY2-4-0"><a href="#5-如何运行VNPY2-4-0" class="headerlink" title="5 如何运行VNPY2.4.0"></a>5 如何运行VNPY2.4.0</h1><h3 id="5-1-环境配置"><a href="#5-1-环境配置" class="headerlink" title="5.1 环境配置"></a>5.1 环境配置</h3><ul><li><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDYxNDc3MQ==&amp;mid=2652390042&amp;idx=1&amp;sn=1b76748baa613014f9a0fe3d0922811d&amp;chksm=8491f006b3e6791072f6f76777e9b6f3ffd38912e0ad03b36b7a5ef81932b23b04bf57a9daa3&amp;token=2142790632&amp;lang=zh_CN&amp;scene=21#wechat_redirect">基本的环境配置，安装参考上一篇</a></li><li><strong>系统</strong>：Mac</li><li><strong>Python环境</strong>：3.8.8</li></ul><h3 id="5-2-拉取最新的代码"><a href="#5-2-拉取最新的代码" class="headerlink" title="5.2 拉取最新的代码"></a>5.2 拉取最新的代码</h3><ul><li>拉取代码</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 拉取代码</span><br><span class="line">git clone https://github.com/vnpy/vnpy.git</span><br><span class="line"># 切一个分支，如：local_dev</span><br><span class="line">git checkout -b local_dev</span><br></pre></td></tr></table></figure><ul><li>安装依赖</li></ul><blockquote><p>下面几个常用的模块已经拆出去作为一个独立的项目，如果要运行<code>VNPY</code>项目，下面几个包是需要安装的</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install vnpy-ctabacktester</span><br><span class="line">pip install vnpy-ctastrategy</span><br><span class="line">pip install vnpy-datamanager</span><br><span class="line">pip install vnpy-riskmanager</span><br></pre></td></tr></table></figure><h3 id="5-3-新建一个目录"><a href="#5-3-新建一个目录" class="headerlink" title="5.3 新建一个目录"></a>5.3 新建一个目录</h3><blockquote><p>为了原始项目干净，新建一个目录(名称随意)，在这个目录可以写自己的策略代码，工具类等</p></blockquote><ul><li><p>比如我自己新建了一个目录<code>workspace</code>,</p></li><li><p>在<code>workspace</code>目录下再新建两个目录：</p></li><li><ul><li><code>.vntrader</code>:  存放配置文件的地方(mac系统下默认存放路径是<code>~/.vntrader</code>, 不过编辑什么的不方便)</li><li><code>strategies</code>: 存放策略代码的地方</li></ul></li></ul><h3 id="5-4-实现代码"><a href="#5-4-实现代码" class="headerlink" title="5.4 实现代码"></a>5.4 实现代码</h3><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640" alt=""></p><ul><li>策略代码：<code>simple_double_ma</code></li><li>启动App(图像界面)：<code>run_app.py</code></li><li>回测(命令行)：<code>run_backtest.py</code></li></ul><h1 id="6-问题"><a href="#6-问题" class="headerlink" title="6. 问题"></a>6. 问题</h1><blockquote><p>系统的策略，还有自己写的策略，都加载不到</p></blockquote><p><strong>版本升级有风险，升级到2.4.0后，发现自己写的策略程序都无法加载，阅读源代码，才发现官方没有做到系统兼容，等待下一版修复。</strong></p><p>Mac，Linux用户要运行代码，肯定需要改下源码,测试就OK了：</p><ul><li>包<code>vnpy_ctastrategy</code> ，文件<code>engin</code> , 803行</li><li>包<code>vnpy_ctabacktester</code>， 文件<code>engin</code>, 95行</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 源代码 pathname: str = str(path) + f&quot;\\*.&#123;suffix&#125;&quot;</span><br><span class="line">pathname: str = str(path) + f&quot;/*.&#123;suffix&#125;&quot;</span><br></pre></td></tr></table></figure><h1 id="7-总结"><a href="#7-总结" class="headerlink" title="7.总结"></a>7.总结</h1><p>新的版本问题肯定是有的，下载源码，运行调试，不断探索。</p><h2 id="1-更新内容"><a href="#1-更新内容" class="headerlink" title="1. 更新内容"></a>1. 更新内容</h2><p>详细查看Github更新日志</p><p>主要看了下2.4.0版本做了很大的调整</p><ul><li>将Deribit接口剥离到vnpy_deribit项目中，并升级到2.0.1版本</li><li>剥离CTA策略模块下的穷举和遗传优化算法到vnpy.trader.optimize模块下</li><li>遗传算法优化完成后，输出所有回测过的参数对应结果（而不只是最优结果）</li><li>CTA策略引擎加载策略文件时，增加模块重载的操作，使得任何策略文件修改可以立即生效</li><li>CTA策略引擎扫描特定目录下的策略文件时，使用glob函数（替换原有的os.walk），避免对子目录中文件的错误加载</li><li>将CTA策略模块剥离到vnpy_ctastrategy项目中</li><li>将CTA回测模块剥离到vnpy_ctabacktester项目中</li><li>将XTP接口剥离到vnpy_xtp项目中，并升级到2.2.27.4版本</li><li>将事前风控模块剥离到vnpy_riskmanager项目中</li><li>将数据管理模块剥离到vnpy_datamanager项目中</li><li>将Deribit接口剥离到vnpy_bybit项目中，并升级到2021.6.21版本</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;vn.py是一套基于Python的开源量化交易系统开发框架，于2015年1月正式发布，在开源社区6年持续不断的贡献下一步步成长为全功能量化交易平台，目前国内外金融机构用户已经超过600家，包括：私募基金、证券自营和资管、期货资管和子公司、高校研究机构、自营交易公司、交易所、Token Fund等。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="量化" scheme="http://yuanquanquan.top/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>关于时间</title>
    <link href="http://yuanquanquan.top/2021/20211013/"/>
    <id>http://yuanquanquan.top/2021/20211013/</id>
    <published>2021-10-13T13:21:17.000Z</published>
    <updated>2021-10-13T14:36:46.378Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>关于2021</p></blockquote><span id="more"></span><p>人类对于时间的感知，似乎总是和色彩与光影纠缠。从泛黄的黑白照片，到河流波澜映射的斑驳光景；从若隐若现的点点星光，到绚烂隐密的似火残阳。大概是因为色彩与光影在记忆中的定格，正如后视镜里的故乡，越来越小，她定格在那里，定格在记忆中，然后我们才意识到，渐行渐远。</p><p>就像我仍然会想起那架把我带离故土的飞机，在凌晨起飞划破天际的泥沼，就像那座城市微光与夜空的点点星辰，我仍然会因此而潸然泪下。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20211013223629777.png" alt=""></p><p>我想我们悲伤的都是时间吧，这个世间最大的谜团。</p><p>这是一种微妙的错觉，我们不能存在于历史，也不能存在于未来，只能在现在，那么现在又存在何处呢？就像爱因斯坦的名言，“现实是一种错觉，只不过是一种持久的错觉。” 当我打下这几个字的时候，刚才的“现在”已经成了过去，而我又来到了未来，但是这个未来又在不可精细测量的微小时间里变成了“现在”然后又变成了历史。这么说来，“现在”其实是最缺乏定义的一个概念，它是那样快地衰变成过去，如果我们可以测量无穷小的时间间隔（目前所能测量的最短时间间隔大约是光通过一个氢原子直径距离的时间，~10^-15秒），那么“现在”这个概念就完全失去意义了，因为历史和未来的过渡只需要无穷短的时间。而我们对于变化的捕捉，只能依赖那些看起来不变的东西，就像里程碑，就像北极星，就像阳光穿过你手掌照耀到墙上，映出心的形状，又或者是一只鸽子。</p><p>我们的文化，以及科学，给予了变化在一切形式中的中心地位，没有变化，那就没有时间。比如说“热寂”这个概念，在无穷远的宇宙未来，当一切都，包括黑洞，完全蒸发为光子的时候，我们就失去时间了，因为宇宙已经无法让自己变得丝毫的不一样了。</p><p>人们因而在这长河中投下一块块路标，他们想要模仿宇宙永恒的秘密，除夕，春节，中秋，清明，谷雨，新年，，，这是我们共同的文化记忆，它们超越了个体的生命长度，就像是永恒的符号，我们来到一个节日，又来到另一个节日，因而我们有了共同的记录变化的方式，进而体验时间的方式。</p><p>或许人类真正的悲哀会在于，有一天可以测量足够小的时间，有一天我们已经找不到未来和过去的区别，有一天我们已经找不到真正永恒的路标，那么我们的时间还会存在吗？</p><p>2021的新年，我还记得我在朋友圈写了一段很长的感言，里面有句话我其实很喜欢：“走了足够远才发现，过去和未来一样神秘而未知，都笼罩着魔幻现实般扭曲的光。” 这是一种悲观的事实，就像时空里的光锥，我们只能在过去光锥和未来光锥相会的那个交点存在，它们存在着，但我们绝无任何可能可以接触到它们。对于我们无法在“现在”体验到的东西，它们还存在吗？我们说过去存在过， 那么它现在还存在吗？存在是独立于时间之外的实体吗？物理学是一种悲观的意识形态，它告诉我们自然所允许的与不允许发生的事，而注意到这样的事实，则是痛苦（suffering）的来源，因为注意到了，然后呢？然后戛然而止，只留无尽的沉默。</p><p>常常有人引用罗曼罗兰那句名言，“这世上只有一种英雄主义，那就是看清生活的真相之后依然热爱生活。” 其实从来就没有人真正看清过生活的真相，假使你真的看清了生活的真相，你也无法改变生活的样子，它就在那里，就像过去和未来光锥。而所谓热爱生活，不过是接受现实，融入那个古老如一的故事（the same old story），那个永远被重复讲述着的故事。</p><p>时间来到2021年的末尾，我想我还是会继续将这句话加在感言，或者说新年献词里面：</p><p>“走了足够远才发现，过去和未来一样神秘而未知，都笼罩着魔幻现实版扭曲的光。”</p><p>在时间的游戏中，我们唯一的参照物就是那些相对不变的东西。比如共同的文化记忆，比如希望，比如对真理的探寻，比如和他人的关联…… 当我们蜷缩在渺小的“现在”，这些路标便构成了与时间谈判的基石。</p><p>2021年即将过去，这一年的非凡的事件，这一年我们被改变的生活版图，终将成为历史，成为新的路标，去定义我们定义时间的方式。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;关于2021&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="生活" scheme="http://yuanquanquan.top/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>Moving Deep Learning into Web Browser:How Far Can We Go</title>
    <link href="http://yuanquanquan.top/2021/20210830/"/>
    <id>http://yuanquanquan.top/2021/20210830/</id>
    <published>2021-08-30T10:06:38.000Z</published>
    <updated>2021-08-30T11:11:32.061Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文是对发表于WWW 2019的论文《Moving Deep Learning into Web Browser: How Far Can We Go? 》的回顾。论文第一作者为中心的马郓助理教授，通信作者为中心的刘譞哲副教授，其余作者为中心本科毕业生向东伟、硕士生郑舒宇以及博士生田得雨。本文针对在浏览器中运行深度学习任务的新趋势，调研和测试了最热门的7个基于JavaScript的深度学习框架，以评估这些框架的功能和性能，并与传统的深度学习框架进行了性能比较。本文的发现能够帮助应用开发者、深度学习框架开发者、浏览器厂商对浏览器上的深度学习效率进行优化。</p></blockquote><span id="more"></span><div class="row">    <embed src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding/1901.09388.pdf" width="100%" height="550" type="application/pdf"></div><p>​    深度学习技术在过去十年极大地拓展了人工智能的边界，在图像处理、语音识别、自然语言处理等领域得到了广泛的应用。学术和产业界也提出和研发了多个深度学习框架如TensorFlow、Caffe等。然而，人工智能应用需要运行于Windows、iOS、Android等多种类型的平台上，将人工智能应用向多种平台移植极具挑战。Web应用具有较强的可移植性，因此基于Web的人工智能应用开发成为了研究热点。从2015年起，ConvNetJS 、TensorFlow.js 等一系列基于JavaScript的深度学习库和框架被提出。然而这些框架在功能和性能上能否很好地支持深度学习应用仍然存疑，本文所进行的实证研究回答了以下三个问题：现有框架提供了哪些功能和特性支持多种类型的深度学习任务？现有框架在不同的深度学习任务下性能表现如何？在浏览器中进行深度学习与在原生平台上运行相比有多大的性能差距？本文选择了7个基于JavaScript的深度学习框架进行研究，并将Tensorflow.js与基于Python的原生Tensorflow的性能进行了比较，得出了一系列结论并给出了建议。</p><h2 id="1、现有框架的功能与特性"><a href="#1、现有框架的功能与特性" class="headerlink" title="1、现有框架的功能与特性"></a>1、现有框架的功能与特性</h2><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210830184828621.png" alt="表1：基于JavaScript的深度学习框架的特性"></p><p>​        为了选择最新的浏览器支持的 DL 框架，作者在 GitHub 上搜索关键字“deep learning framework”，并用 JavaScript 语言过滤结果。然后<strong>选择了 GitHub 上星数超过 1000 的前 7 个框架[1]</strong>。对每个框架的具体介绍如下：</p><p><strong>TensorFlow.js[2]</strong> ：2018 年 3 月由 Google 发布，是一个 inbrowser 机器学习库，支持使用 JavaScript 在浏览器中定义、训练和运行模型。TensorFlow.js 由 WebGL 提供支持，并提供用于定义模型的高级 API。TensorFlow.js 支持所有 Keras 层（包括 Dense、CNN、LSTM 等）。因此，很容易将原生 TensorFlow 和 Keras 预先训练的模型导入到浏览器中并使用 Tensorflow.js。</p><p><strong>ConvNetJS[3]</strong> ：是一个 Javascript 库，最初由斯坦福大学的 Andrej Karpathy 编写。ConvNetJS 目前支持用于分类和回归的常用神经网络模型和代价函数。此外，它还支持卷积网络和强化学习。然而遗憾的是，尽管 ConvNetJS 可能是在 TensorFlow.js 之前最著名的框架，但其在 2016 年 11 月后已经不再维护了。</p><p><strong>Keras.js[4]</strong>：抽象出许多框架作为后端支撑，包括 TensorFlow、CNTK 等。它支持导入 Keras 预先训练的模型进行推理。在 GPU 模式下，Keras.js 的计算由 WebGL 执行。然而，这个项目也已经不再活跃。</p><p><strong>WebDNN[5]</strong>：由东京大学发布的 WebDNN 号称是浏览器中最快的 DNN 执行框架。它只支持推理（训练）任务。该框架支持 4 个执行后端：WebGPU、WebGL、WebAssembly 和 Fallback pure JavaScript 实现。WebDNN 通过压缩模型数据来优化 DNN 模型，以加快执行速度。</p><p><strong>brain.js[6]</strong>：是一个用于神经网络的 JavaScript 库，它取代了不推荐使用的 “brain” 库。它为训练任务提供 DNN、RNN、LSTM 和 GRU。该库支持将训练好的 DL 模型的状态用 JSON 序列化和加载。</p><p><strong>synaptic[7]</strong>：这是一个不依赖于 JavaScript 架构的神经网络库，基本上支持任何类型的一阶甚至二阶 RNN。该库还包括一些内置的 DL 架构，包括多层感知器、LSTM、液态机（Liquid state machines）和 Hopfield 网络。</p><p><strong>Mind[8]</strong>：这是一个灵活的神经网络库。核心框架只有 247 行代码，它使用矩阵实现来处理训练数据。它支持自定义网络拓扑和插件，以导入 mind 社区创建的预训练模型。然而，这个项目也已经不再活跃。</p><p>从提供的功能性来看，多数框架支持训练和推断两类任务；不同的框架支持的神经网络类型及操作有较大差异，仅有TensorFlow.js等3个框架支持了DNN、CNN、RNN三类神经网络；多数框架都支持以层为单位构建神经网络，TensorFlow.js相比其他框架支持了更多的层类别；TensorFlow.js也支持了更多种类的激活函数和优化器；7个框架中仅有TensorFlow.js支持使用GPU加速训练，TensorFlow.js等3个框架支持使用GPU加速推断。</p><p>从开发者支持来看，TensorFlow.js在开发文档、演示等多个方面优于其他框架，但因其支持较丰富的功能，其软件包也是较大的。</p><h2 id="2、现有框架的性能"><a href="#2、现有框架的性能" class="headerlink" title="2、现有框架的性能"></a>2、现有框架的性能</h2><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210830184912849.png" alt="图1：平均训练时间"></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210830184950411.png" alt="图2：平均推断时间"></p><p>测试结果显示，ConvNetJS在训练和推断上性能均为最佳；Tensorflow.js是唯一支持GPU加速的框架，且性能与ConvNetJS可比。ConvNetJS性能更佳的原因可能是其在实现上与其他框架存在较大区别。</p><h2 id="3、浏览器框架与原生框架性能对比"><a href="#3、浏览器框架与原生框架性能对比" class="headerlink" title="3、浏览器框架与原生框架性能对比"></a>3、浏览器框架与原生框架性能对比</h2><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210830185032610.png" alt="图3：Keras预训练模型的推断时间"></p><p>在推断任务的实验中，Tensorflow.js在nGPU上的表现弱于原生Tensorflow，但也仅慢1-2倍。Tensorflow.js在iGPU上的表现则优于原生Tensorflow在CPU上的表现。</p><p>在决策树的实验中，尽管在任何配置下Tensorflow.js几乎都慢于原生Tensorflow。但两个重要发现是，第一，较多使用CPU还是GPU对性能差异有极大的影响；第二，任务类型对性能差异有极大影响，在训练任务中Tensorflow.js平均慢33.9倍，而在推断任务中则只慢5.8倍。</p><h2 id="4、主要研究发现"><a href="#4、主要研究发现" class="headerlink" title="4、主要研究发现"></a>4、主要研究发现</h2><p>第一，浏览器端的深度学习框架仍在早期阶段，仅有Tensorflow.js提供了较全面的功能和支持了较多种类的深度学习任务。</p><p>第二，浏览器端对深度学习的支持主要仍然集中于推断任务，训练任务仍然受到较大局限。</p><p>第三，模型加载是推断任务中最耗时的部分，因此对于较小的模型，使用CPU的表现优于GPU。</p><p>第四，在没有独立显卡时，集成显卡对浏览器端深度学习框架性能有较大帮助。相比原生Tensorflow仅在CPU上运行，Tensorflow.js利用集成显卡表现出了更好的性能。</p><p>第五，系统资源的利用有待进一步优化。对于Tensorflow.js，CPU没有得到充分的利用；对于WebGL，分配的内存受限于浏览器，导致部分深度学习任务崩溃。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是对发表于WWW 2019的论文《Moving Deep Learning into Web Browser: How Far Can We Go? 》的回顾。论文第一作者为中心的马郓助理教授，通信作者为中心的刘譞哲副教授，其余作者为中心本科毕业生向东伟、硕士生郑舒宇以及博士生田得雨。本文针对在浏览器中运行深度学习任务的新趋势，调研和测试了最热门的7个基于JavaScript的深度学习框架，以评估这些框架的功能和性能，并与传统的深度学习框架进行了性能比较。本文的发现能够帮助应用开发者、深度学习框架开发者、浏览器厂商对浏览器上的深度学习效率进行优化。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>SETI Breakthrough Listen - E.T. Signal Search</title>
    <link href="http://yuanquanquan.top/2021/20210817/"/>
    <id>http://yuanquanquan.top/2021/20210817/</id>
    <published>2021-08-17T13:43:40.000Z</published>
    <updated>2021-08-17T15:29:55.013Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>比赛链接：<a href="https://www.kaggle.com/c/seti-breakthrough-listen">https://www.kaggle.com/c/seti-breakthrough-listen</a></p><p>比赛名称：SETI Breakthrough Listen - E.T. Signal Search</p><p>比赛内容：利用算法来识别异常的信号，CV类型赛题</p></blockquote><span id="more"></span> <hr><h3 id="比赛背景："><a href="#比赛背景：" class="headerlink" title="比赛背景："></a>比赛背景：</h3><p>“我们一个人在宇宙里吗？”这是最深刻且长期存在的人类问题之一。随着技术的进步，我们正在寻找新的和更强大的方法来寻求答案。加州大学伯克利大学使用世界上最强大的望远镜对数百万颗恒星进行技术扫描。现在希望Kaggle社区能够帮助解释他们收到的信号。</p><p><strong>加州大学伯克利分校的Breakthrough Listen 团队使用世界上最强大的望远镜扫描数百万颗恒星以寻找技术迹象。</strong></p><p><strong>现在它希望Kaggle社区帮助解释他们接收到的信号。Listen团队是外星智慧搜索(SETI) 的一部分，使用地球上最大的可操纵天线，即直径100 米的绿岸望远镜。</strong>与任何SETI搜索一样，交流的动机也是主要挑战。</p><p>人类已经建造了大量的无线电设备。很难在现代技术的巨大检测结果中寻找微弱的外星传播针。当前的方法使用两个过滤器来搜索大海捞针。</p><p><strong>首先，</strong>Listen团队将目标恒星的扫描与天空其他区域的扫描穿插在一起。两组扫描中出现的任何信号都可能不是来自目标恒星的方向。<strong>其次，</strong>管道会丢弃不会改变其频率的信号，因为这意味着它们可能在望远镜附近。</p><p><strong>运动中的源应该有一个暗示运动的信号，类似于路过的消防车警报器的音调变化。这两个过滤器非常有效，但我们知道它们可以改进。</strong></p><p>管道无疑会错过有趣的信号，尤其是那些具有复杂时间或频率结构的信号，以及那些在有大量干扰的频谱区域中的信号。<strong>在本次比赛中，利用您的数据科学技能帮助识别Breakthrough Listen 目标扫描中的异常信号。</strong></p><p>由于没有确认的用于训练机器学习算法的外星信号示例，<strong>该团队在来自望远镜的海量数据中加入了一些模拟信号（他们称之为“针”）。</strong>他们已经确定了一些隐藏的针，以便您可以训练您的模型以找到更多。</p><p><strong>数据由二维数组组成，因此可能存在有前景的计算机视觉方法，以及数字信号处理、异常检测等。</strong>成功识别最多针的算法将赢得现金奖励，但也有可能帮助回答科学中最大的问题之一。</p><h3 id="赛题目标"><a href="#赛题目标" class="headerlink" title="赛题目标"></a>赛题目标</h3><p>比赛的任务就是通过给定的频谱图预测对应的标签：0或者1在这场比赛中，利用算法来识别异常的信号。数据由二维数组组成，因此计算机视觉中可能会有一些方法，可能涉及的知识包括数字信号处理，异常检测等。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210817224014119.png" alt=""></p><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p>Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.</p><h3 id="赛题赛程"><a href="#赛题赛程" class="headerlink" title="赛题赛程"></a>赛题赛程</h3><ul><li>July 21, 2021 - Entry Deadline.</li><li>July 21, 2021 - Team Merger Deadline.</li><li>July 28, 2021 - Final Submission Deadline.</li></ul><h3 id="赛题数据"><a href="#赛题数据" class="headerlink" title="赛题数据"></a>赛题数据</h3><p>赛题数据与频谱图类似，但通常跨越几个GHz的无线电频谱。数据存储为滤波器组格式或HDF5格式文件。</p><p>通过交替观测来做到这一点：在恒星“A”上观察 5 分钟，然后在恒星“B”上观察 5 分钟，然后回到恒星“A”上 5 分钟，然后是“C” ”，然后回到“A”，然后在“D”星上用 5 分钟结束。一组六个观察值 (ABACAD) 被称为“节奏”。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210817224118204.png" alt="异常样本"></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210817232838744.png" alt="正常样本"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;比赛链接：&lt;a href=&quot;https://www.kaggle.com/c/seti-breakthrough-listen&quot;&gt;https://www.kaggle.com/c/seti-breakthrough-listen&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;比赛名称：SETI Breakthrough Listen - E.T. Signal Search&lt;/p&gt;
&lt;p&gt;比赛内容：利用算法来识别异常的信号，CV类型赛题&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="kaggle" scheme="http://yuanquanquan.top/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>MLB Player Digital Engagement Forecasting</title>
    <link href="http://yuanquanquan.top/2021/2202210803/"/>
    <id>http://yuanquanquan.top/2021/2202210803/</id>
    <published>2021-08-03T12:16:23.000Z</published>
    <updated>2021-08-17T15:22:35.036Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录一次kaggle比赛，这次比赛的场景主要通过MLB球员的历史表现数据、社交媒体数据以及市场规模等团队因素来预测在未来MLB 球员的数字内容互动趋势（社交媒体互动）。建立的模型将预测出MLB球员在未来的数字内容互动趋势指数（target1- target4）。</p></blockquote><span id="more"></span> <hr><p><img src="https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F603584%2Ff7c0669c09db26bd45f76ade61b2a91f%2FGoogleCloud_MLB_Lockup.jpg?generation=1623359034713516&amp;alt=media" alt=""></p><h1 id="MLB-Player-Digital-Engagement-Forecasting"><a href="#MLB-Player-Digital-Engagement-Forecasting" class="headerlink" title="MLB Player Digital Engagement Forecasting"></a>MLB Player Digital Engagement Forecasting</h1><h3 id="比赛背景与任务："><a href="#比赛背景与任务：" class="headerlink" title="比赛背景与任务："></a>比赛背景与任务：</h3><p>A player hits a walk-off home run. A pitcher throws a no-hitter. A team gets red hot going into the Postseason. We know some of the catalysts that increase baseball fan interest. Now Major League Baseball (MLB) and Google Cloud want the Kaggle community’s help to identify the many other factors which pique supporter engagement and create deeper relationships betweens players and fans.</p><p>The sport has a long history of being numbers-driven. Nearly every day from at least April through October, baseball fans watch, read, and search for information about players. Which individuals they seek can depend on player performance, team standings, popularity, among other, currently unknown factors—which could be better understood thanks to data science.</p><p>Since at least the early 1990s, MLB has led the sports world in the use of data, showing fans, players, coaches, and media what’s possible when you combine data with human performance. MLB continues its leadership using technology to engage fans and provide new fans innovative ways to experience America’s Favorite Pastime.</p><p>MLB has teamed up with Google Cloud to transform the fan experience through data. Google Cloud proudly supports this Kaggle contest to celebrate the launch of Vertex AI: Google Cloud’s new platform to unify your ML workflows.</p><p>In this competition, you’ll predict how fans engage with MLB players’ digital content on a daily basis for a future date range. You’ll have access to player performance data, social media data, and team factors like market size. Successful models will provide new insights into what signals most strongly correlate with and influence engagement.</p><p>Imagine if you could predict MLB All Stars all season long or when each of a team’s 25 players has his moment in the spotlight. These insights are possible when you dive deeper into the fandom of America’s pastime. Be part of the first method of its kind to try to understand digital engagement at the player level in this granular, day-to-day fashion. Simultaneously help MLB build innovation more easily using Google Cloud’s data analytics, Vertex AI and MLOps tools. You could play a part in shaping the future of MLB fan and player engagement.</p><p>该赛题为<strong>时间序列</strong>任务，通过MLB球员的历史表现数据、社交媒体数据以及市场规模等团队因素来预测在未来MLB 球员的数字内容互动趋势（社交媒体互动）。建立的模型将预测出MLB球员在未来的数字内容互动趋势指数（target1- target4）。旨在为MLB 球迷和球员的未来社交媒体互动参与度挖掘价值。</p><p> 至少从 1990 年代初期开始，美国职业棒球大联盟就在使用数据方面领先于体育界，向球迷、球员、教练和媒体展示了将数据与人类表现相结合的可能性。MLB使用创新技术吸引球迷，并为新球迷提供体验美国最受欢迎的消遣的创新方式。 </p><p><strong>评价指标</strong>：MCMAE 计算四个目标变量中的每一个的平均绝对误差，得分是这四个MAE值的平均值</p><h3 id="方案简述"><a href="#方案简述" class="headerlink" title="方案简述"></a>方案简述</h3><p>通过竞赛提供的在2021赛季活跃的2055位MLB球员的四种不同的数字内容参与度 ( target1- target4)和对应的球员团队、历史比赛、历史得分情况、所获奖项、比赛事件等累计7.9G的历史数据信息(2021年1-4月)来结合机器学习构建MLB球员未来（2021年5月）数字内容互动趋势指数预测模型。通过季节性EDA、MLB球员历史信息统计后进行特征工程，分别使用ANN（人工神经网络）和LightGBM、CatBoost（集成学习）进行模型融合并对各模型的超参数进行了网格优化后在排行榜取得了铜牌的成绩。</p><h3 id="方案流程："><a href="#方案流程：" class="headerlink" title="方案流程："></a>方案流程：</h3><ol><li>mlb-ann-training：ANN模型训练代码</li><li>mlb-lightgbm-training：LightGBM模型训练代码</li><li>mlb-catboost-training：：CatBoost模型训练代码</li><li>全流程推理代码（特征提取、超参数调优、模型融合）</li></ol><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><p>I used ~440 features. In addition to joins and asof merge of basic tables, the following features were used:</p><ul><li>lag features per player<ul><li>Average of the last 7/28/70/360/720 days</li><li>Average over the on-seasons</li><li>Average for the same period in the previous year</li><li>Average of days with/without a game</li></ul></li><li>number of events, pitch events, action events</li><li>days from last rosters, awards, transactions and box scores</li><li>sum of box scores in the last 7/30/90 days</li><li>number of games and events in the day</li><li>event-level meta feature<ul><li>aggregation of predictions of model trained on event table</li><li>group by (date, playerId), (date, teamId) and (date)</li></ul></li></ul><h3 id="Cumcount-Leakage"><a href="#Cumcount-Leakage" class="headerlink" title="Cumcount Leakage"></a>Cumcount Leakage</h3><p>There is a strange correlation between the cumcount of the dataframe retrieved from the Time-Series API and the target.</p><p>I noticed this problem 3 days before the competition ended. I did not post it in the discussion as it might confuse the participants, but contacted the host immediately.<br>Adding this cumcount to the features only improves the CV a little bit, so it’s probably some kind of artifact or something, but even if it doesn’t improve the CV much, it’s better to shuffle the test data since it’s nonsense that the order of the rows makes sense.</p><p>I did not end up using this leak for final submission.</p><h3 id="Implementation-Note"><a href="#Implementation-Note" class="headerlink" title="Implementation Note"></a>Implementation Note</h3><p>Building a complex data pipeline in Jupyter Notebook with the Time Series API can be a big pain. I’ll share some of my efforts.</p><ul><li>Maintain the source code on GitHub and paste the BASE64-encoded code into the jupyter notebook<ul><li>see: <a href="https://github.com/lopuhin/kaggle-imet-2019">https://github.com/lopuhin/kaggle-imet-2019</a></li></ul></li><li>The inference notebook is also maintained on GitHub and automatically uploaded as the Kaggle Kernel through GitHub Actions</li><li>Avoid the use of pandas and instead use a dictionary of numpy arrays to manage state updates</li><li>Use the same feature generation function for training data and inference<ul><li>Both training and test are treated as streaming data, and features were generated using for-loop.</li><li>This is the most important point to get a stable and bug-free data pipeline</li><li>see: <a href="https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/196942">https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/196942</a></li></ul></li><li>Debug code locally using the API emulator<ul><li>Test the robustness of my inference pipeline by “dropout” some of the data returned by the emulator (a kind of “Chaos Engineering”)</li></ul></li><li>Catch exceptions in various functions and convert them to appropriate “default” values</li></ul><iframe src="https://nbviewer.jupyter.org/github/Bazingaliu/MLB-Player-Digital-Engagement-Forecasting/blob/main/4.%E5%85%A8%E6%B5%81%E7%A8%8Binference.ipynb" width="570" height="2000"></iframe> ]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;记录一次kaggle比赛，这次比赛的场景主要通过MLB球员的历史表现数据、社交媒体数据以及市场规模等团队因素来预测在未来MLB 球员的数字内容互动趋势（社交媒体互动）。建立的模型将预测出MLB球员在未来的数字内容互动趋势指数（target1- target4）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="kaggle" scheme="http://yuanquanquan.top/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>网络单纯形法</title>
    <link href="http://yuanquanquan.top/2021/20210716/"/>
    <id>http://yuanquanquan.top/2021/20210716/</id>
    <published>2021-07-16T11:55:41.000Z</published>
    <updated>2021-08-17T15:31:13.185Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在实际应用中。迄今为止求解线性规划问题著名的仍是<strong>单纯形法</strong>。通过专业的实现，它具有卓越的性能：≈1000变量和≈1000约束的问题可以在0.1到0.5秒内得到解决。那么，是否可以尝试将该算法应用于图论中的问题呢？</p><p>​    事实上，最重要的网络优化问题都可以用线性规划来表示，比如确定最短路、最大流、最小费用流等。然而，直接应用通常的单纯形算法是没有意义的，因为所产生的程序将是<strong>笨拙(unwiedy)</strong>和<strong>高度退化的(highly degenerate)</strong>。这两个问题通过使用单纯形法的适当图论特殊化来避免：<strong>网络单纯形法</strong>。</p></blockquote><span id="more"></span><p>在此之前，我们先从<strong>最短路</strong>的故事说起。</p><p>   我们知道所有解决最短路问题的方法的基础是下面这样一个简单的想法：假设已知对每个v存在一条费用为π(v)的从r到v的有向路，并且我们找到一条满足π(v) + C(vw) &lt; π(w)的弧vw。由于把vw附加到从r到v的有向路可以得到一条到w的有向路，因此存在一条到w的费用为π(v) + C(vw)的更便宜的有向路。基于此，定义<strong>可行势（feasible potential）</strong>的概念是自然的：</p><ul><li><p><strong>Def1.</strong>如果π(v)是到v的有向路的最小费用，那么π满足π(v) + C(vw) ≥ π(w)，称π是一个可行势。</p><p> 可行势对最短路的费用给出了下界，我们有如下命题：</p></li><li><p><strong>Prop1.</strong>令π是可行势且P是从s到v的有向路，那么c(P)≥π(v).</p><p> 事实上，我们通过<strong>Ford算法</strong>在终止时得到了一个可行势和若干有向路，对它们来说上述命题中的等号成立。即有下面的定理陈述这个事实：</p></li><li><p><strong>Thm1.</strong> min{c(P):P是从s到t的有向路} = max{π(t):π可行势}</p><p> 我们可以通过这个陈述看到它与线性规划对偶性之间的联系。定理陈述中的最大化显然是一个线性规划问题：</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040907249.png" alt=""><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040923491.png" alt=""></p><p><strong>对偶最优值定理</strong>告诉我们(P)(D)中有一个最优值存在，则两个最优值都存在且相等。注意到任何r到s的有向路P为(D)提供了一个可行解，则(D)的目标函数值恰好是P的费用，因此，定理1意味着，当最短路存在时，(D)有一个最优解，它是一条简单有向路的特征向量。这个结果将等价于结论：</p><ul><li><strong>Prop2</strong>. (D)的可行解多面体的顶点是简单有向路的特征向量.</li></ul><p>​    因为我们已经可以用Ford算法解决线性规划问题(D)，那么这个算法和单纯形法之间的联系是什么呢？单纯形法保存了一个“基”弧（对应于(D)中的基变量）的集合T，(D)的一个可行解x及向量y∈R^{V}，它们满足</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040954255.png" alt=""></p><p>在每次迭代中，通过用集合外的一条弧替代集合中的一条，得到一个新的这样的集合。基弧的集合T必须对应于(D)的等式约束矩阵A的列极大线性无关组（列基）。事实上，这个矩阵就是图G的<strong>关联矩阵(incidence matrix)</strong>，它的列基可以用很好的方法来刻画。</p><ul><li><p><strong>Prop3.</strong>G:有向、连通，$A=\left{a_{e}: e \in E\right}$.集合$\left{a_{e}: e \in E\right}$是$A$的列基，当且仅当T是G的一颗生成树的弧集（<strong>ex.</strong>如果T不包含任何圈的弧集，那么它对应的列是线性无关的）</p><p>所以在这样的观察下，我们可以知道单纯形法是从生成树到生成树，每一步都可以看作是一序列的Ford算法步骤，她是一个被若干个不改变树的步骤所跟随着的通常步骤，直到y“追赶”为树。</p></li></ul><p>​    <strong>最大流问题</strong>同样是一个线性规划问题，所以通过线性规划对偶可以给出最大流的一个好的刻画。那么，我们的一个自然的问题就是这种刻画与我们通过割所给出的刻画有什么联系。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041025672.png" alt=""><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041043901.png" alt=""></p><p>同样由对偶最优定理，我们可以给出经典的<strong>最大流-最小割定理</strong>的如下刻画：<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041946529.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041954142.png" alt=""></p><p>现在故事来到了最小费用流问题（MCFP），它的一个标准陈述如下：<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041311274.png" alt=""></p><p>我们·可以自然的写出其对应的线性规划形式：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210724005804511.png" alt=""></p><p>作为一个重要的特例，我们想让其中每个u(e)都是无穷的，这样的问题称为<strong>转运问题(transshipment problem)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210724010218929.png" alt=""></p><p>设x是任意的b-流，即(P)的任意可行解，则由<strong>对偶最优定理</strong>，我们有如下结论：（互补松紧性条件给出了我们想要的最优性刻画）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210724010253027.png" alt=""></p><p>另外，下面两个观察是重要的：</p><ol><li>给定G中的一条路P。 定义P(关于c)的费用为∑(c(e) :e是P的正向弧)-∑ (c(e) :e是P的反向弧)。这样定义的原因是：假设我们在G中沿一条x-可扩路P发送ε单位的流，即xe在P的正向弧上提高了ε，在P的反向弧上降低了ε。那么cTx增加了∑(εce :e是P的正向弧)-∑ (εce :e是P的反向弧) ，即增加了ε倍的P的费用。特别的，如果x是(P)的可行解，那么某个负费用的x-可扩圈将给出一个费用更低的解。</li></ol><ol start="2"><li>对满足0≤x≤u的向量x，如同处理最大流时一样定义一个辅助有向图Gx，则G中每条x-可扩路对应于Gx中具有相同费用的一条有向路，特别的，G中一个负费用的x-可扩圈对应于Gx中一个负费用有向圈。</li></ol><p>于是我们就可以用最短路方法来确定Gx是否包含负费用有向圈。这便引出了下面对(P)的最优性的刻画：</p><ul><li><strong>Thm3</strong>.(P)的可行解x是最优的当且仅当不存在具有负费用c的x-可扩圈。</li><li><strong>(Klein 1967)</strong>(G,u,b,c). A b-flow f is of minimum cost iff there is no f-augmenting cycle with negative total weight iff there exists a feasible potential for (Gf , c).</li></ul><p>这是一条最重要的<strong>最优性准则</strong>。以上基本讲清了线性规划与最小费用流问题的联系。下面在我们正式将单纯形法应用于此之前，我们首先陈述这部分内容所需要的一些基本想法：</p><ul><li>假设问题定义在连通有向图G上（否则，限制在G的每个连通分支上处理即可）</li><li>可先对转运问题这一特殊情况进行分析（即假设对每条弧e有u(e)=∞）</li><li>定义转运问题的树解（支撑树解）</li><li>网络单纯形法保持可行的树解并且寻找一种特殊的负费用圈。(如果每个C(T,e)都具有非负费用，那么由T确定的树解x满足最优值定理的条件)</li><li>增加辅助边来找初始的树和流</li><li>避免循环：从一颗强可行树开始并使用离开弧规则(选择h是C(T,e)中第一条满足 x(h)=θ的反向弧)</li><li>网络单纯形法在有限步后会终止。</li><li>一般的，网络单纯形法将从树解移动到树解，只使用通过增加一条弧到T上形成的圈。</li></ul><p>我们首选对转运问题来定义树解，转运问题的支撑树解(spanning tree solution)是一个向量x∈R^{E}满足条件：对某棵树T，<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041834923.png" alt=""></p><p>树T和T的一条弧h=pq确定了将顶点氛围两部分R(T,h)和V\R(T,h)的一个划分如下：R(T,h)是在T中从r到v的简单路没有用到h的那些顶点v的集合。显然如图，r∈R(T,h)，h是T中唯一的一个端点在R(T,h)中，另一个端点不在R(T,h)中的弧。所以在与T相关的任何树解x中，流入R(T,h)的净流量一定是完全由h携带的。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725015500600.png" alt=""></p><p>这个观察将给我们下面的结果：</p><ul><li><p><strong>Prop4</strong>.一棵树T唯一地确定了它的树解.</p></li><li><p><strong>Prop5</strong>.(G:连通) 一个可行解是树解，当且仅当不存在每条弧都具有正流的圈。</p></li></ul><p>我们已经指出网络单纯形法是保持可行的树解并且寻找一种特殊的负费用圈，现在就给出这样的圈：<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725015539193.png" alt=""></p><p>那么这种特殊类型的圈有什么优点呢？</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725015603770.png" alt=""></p><p><strong>Prop6</strong>.如果树T确定了可行树解x且对每个e不属于T，C(T,e)都具有非负费用。那么x是最优的。</p><p>有了上面的陈述，我们已经可以叙述我们心目中算法的初步形式：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040144633.png" alt=""></p><p>这里立刻出现的一个问题是：<strong>如何找到初始的树和流。</strong>我们直接针对一般的最小费用流问题给出答案。</p><p>下面的命题演示了我们是如何从找到的圈中替换原来树上的一条弧从而实现“换基”操作的。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040244874.png" alt=""></p><ul><li><strong>Def2</strong>.如果一棵树T确定了一个满足T中至少有一条弧携带0流的流x，那么我们说x是退化的。</li></ul><p>这直接引发了一个重要的理论问题：</p><p><strong>Q</strong>. 在一系列退化的迭代(改变树而不改变流的迭代只可能出现在流是退化的情况下)后，该算法会不会返回同一棵树？（循环，导致算法不能终止/另一方面，如果循环不发生，由于不同的树的数目是有限的，那么算法一定在有限步内终止。）</p><p>(注：循环是可能发生的[Cunningham]，但Cook提到“尽管据我们所知，迄今为止它从未在实际问题的解决中发生)</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040310879.png" alt=""></p><p>令h=pq是T中的一条弧，我们说h在T中是远离(away from)r的，如果p∈R(T,h)；否则h在T中是朝向(toward)r的。上面由一棵树和一条弧导出的划分的图中弧h是远离r的。</p><ul><li><strong>Def3</strong>.一棵树T称为强可行的(strongly feasible)，如果它确定了一个可行流x，使得对T的每条满足xh=0的弧h，h在T中是远离r的。</li></ul><p>注意到如果流不是退化的，那么树平凡的满足这个条件，还注意到我们用来初始化算法的树是强可行的。现在假设我们从一棵强可行的树开始算法，并在算法的每一步，我们按如下规则选择弧h：</p><ul><li><strong>离开弧规则：</strong>选择h是C(T,e)中第一条满足x(h)=θ的反向弧。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725041729373.png" alt=""></p><ul><li><strong>Thm4.</strong> 如从一棵强可行树开始并使用离开弧规则的网络单纯形法在有限步后会终止。</li></ul><p>下面我们将转运问题的上述讨论直接扩展到解决一般的最小费用流问题的网络单纯形法，它也可以看作是对线性规划的有界变量单纯形法的一个解释。</p><ol><li><p>定义树解是一个向量x∈R^{E}，s.t.对某棵树T以及E\T的划分(L,U)我们有</p><p>fx(v)=b(v)，对所有v∈V；</p><p>x(e)=0，对所有e∈L；</p><p>x(e)=u(e)，对所有e∈U.</p></li><li><p>添加一条弧形成的圈C(T,L,U,e)满足以下性质：</p><p>C(T,L,U,e)的每条弧都是T∪{e}的元素；</p><p>如果e∈L，那么e是C(T,L,U,e)的正向弧，否则是反向弧；</p><p>C(T,L,U,e)的起点s是T中从v和w到r的简单路上的第一个公共顶点。</p></li><li><p>我们可以将之前得到的最优性定理拓展如下：</p></li></ol><ul><li><strong>Prop8</strong>.如果树(T,L,U)确定了可行树解x并且对每个e∉T,C(T,L,U,e)都有非负的费用，那么x是最优的。</li></ul><p>好，现在我们可以叙述本算法了。<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210725040336007.png" alt=""></p><p>参考文献： Bernhard Korte and Jens Vygen《Combinatorial Optimization Theory and Algorithms(6th,2018)》</p><p>​         Cook等《Combinatorial Optimization》</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在实际应用中。迄今为止求解线性规划问题著名的仍是&lt;strong&gt;单纯形法&lt;/strong&gt;。通过专业的实现，它具有卓越的性能：≈1000变量和≈1000约束的问题可以在0.1到0.5秒内得到解决。那么，是否可以尝试将该算法应用于图论中的问题呢？&lt;/p&gt;
&lt;p&gt;​    事实上，最重要的网络优化问题都可以用线性规划来表示，比如确定最短路、最大流、最小费用流等。然而，直接应用通常的单纯形算法是没有意义的，因为所产生的程序将是&lt;strong&gt;笨拙(unwiedy)&lt;/strong&gt;和&lt;strong&gt;高度退化的(highly degenerate)&lt;/strong&gt;。这两个问题通过使用单纯形法的适当图论特殊化来避免：&lt;strong&gt;网络单纯形法&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="组合优化" scheme="http://yuanquanquan.top/tags/%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>FFT的CUDA实现</title>
    <link href="http://yuanquanquan.top/2021/20210605/"/>
    <id>http://yuanquanquan.top/2021/20210605/</id>
    <published>2021-06-05T03:10:10.000Z</published>
    <updated>2021-06-05T06:15:35.196Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一维FFT算法在Maxwell架构上，归为访存密集算法。<br>即，在足够优化的情况下，可在一次memory copy的耗时内完成计算。</p><p>本文实现的FFT算法达到与官方库cuFFT一致的速度，通过整合kernel，可实现比调用CUFFT更快的算法整体执行速度。在处理65536*4以上大点数一维FFT+IFFT计算时（一个大核心共享内存放不下完整的一维FFT数据），组合算法可以实现比CUFFT少2个kernel调用的时间（减少两次显存数据交换），主要说明4096点FFT算法设计的思路及实现。大点数仅说明方法和测试结果。</p></blockquote><span id="more"></span><h2 id="算法原理及设计思路"><a href="#算法原理及设计思路" class="headerlink" title="算法原理及设计思路"></a>算法原理及设计思路</h2><p>本节说明快速傅里叶变换（Fast Fourier Transform）的原理和数值计算过程，重介绍能发挥GPU架构优势的算法类型。</p><h3 id="常规FFT实现（Cooley-Tukey）"><a href="#常规FFT实现（Cooley-Tukey）" class="headerlink" title="常规FFT实现（Cooley-Tukey）"></a>常规FFT实现（Cooley-Tukey）</h3><h4 id="公式推导与计算结构"><a href="#公式推导与计算结构" class="headerlink" title="公式推导与计算结构"></a>公式推导与计算结构</h4><h4 id=""><a href="#" class="headerlink" title=""></a><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210605141428000.png" alt=""></h4><p>旋转因子W具有对称、周期、可约的特点：<br>$$<br>\begin{array}{c}<br>W_{N}^{k+\frac{N}{2}}=-W_{N}^{k} \<br>W_{N}^{2 k r}=e^{-j \frac{2 \pi}{N} 2 k r}=e^{-j \frac{2 \pi}{N / 2} k r}=W_{N / 2}^{k r} \<br>W_{N}^{2}=e^{-j \frac{2 \pi}{N} 2}=e^{-j \frac{2 \pi}{N / 2}}=W_{N / 2}^{1}<br>\end{array}<br>$$<br>Cooley和Tukey利用旋转因子的特性，将DFT进行奇偶分解：<br>$$<br>\begin{aligned}<br>X(k) &amp;=D F T[x(n)]=\sum_{n=0}^{N-1} x(n) W_{N}^{n k}=\sum_{r=0}^{N / 2-1} x(2 r) W_{N}^{2 r k}+\sum_{r=0}^{N / 2-1} x(2 r+1) W_{N}^{(2 r+1) k} \<br>&amp;=\sum_{r=0}^{N / 2-1} x_{1}(r) W_{N}^{2 r k}+W_{N}^{k} \sum_{r=0}^{N / 2-1} x_{2}(r) W_{N}^{2 r k}=\sum_{r=0}^{N / 2-1} x_{1}(r) W_{N / 2}^{r k}+W_{N}^{k} \sum_{r=0}^{N / 2-1} x_{2}(r) W_{N / 2}^{r k} \<br>&amp;=X_{1}(k)+W_{N}^{k} X_{2}(k)<br>\end{aligned}<br>$$<br>式中，$X_1(k)$ 和$X_2(k)$ 分别是$X_1(r)$和$X_2(r)$的N/2点DFT。</p><p>分解之后只得到N/2点的序列，而$X(k)$有N点，还要计算另一半项数的结果，利用旋转因子的周期性：<br>$$<br>X_{1}\left(\frac{N}{2}+k\right)=\sum_{r=0}^{N / 2-1} x_{1}(r) W_{N / 2}^{r(N / 2+k)}=\sum_{r=0}^{N / 2-1} x_{1}(r) W_{N / 2}^{r k}=X_{1}(k)<br>$$</p><p>前半部分：<br>$$<br>X(k)=X_{1}(k)+W_{N}^{k} X_{2}(k) \quad, \quad k=0,1, \ldots, \frac{N}{2}-1<br>$$<br>后半部分：<br>$$<br>X\left(\frac{N}{2}+k\right)=X_{1}\left(\frac{N}{2}+k\right)+W_{N}^{(k+N / 2)} X_{2}\left(\frac{N}{2}+k\right)=X_{1}(k)-W_{N}^{k} X_{2}(k) \quad, \quad k=0,1, \ldots, \frac{N}{2}-1<br>$$<br>因此，只要求出(0,N/2)区间内所有$X_1(k)$ 和$X_2(k)$ 值，即可求出(0,N-1)区间内所有$X(k)$值，<strong>计算量“减半”</strong>。</p><p>FFT算法有很多结构，但通常可归为两类，时域抽取法FFT（Decimation-In-Time FFT，简称DIT-FFT）和频域抽取法FFT（Decimation-In-Frequency FFT，简称DIF-FFT）。Cooley-Tukey方法的8点基2 FFT计算流程图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbip3hfrba1d9h11uu13da1hmq2n-20210605141027100.png" alt=""><br>图1. 8点基2 DIT-FFT计算流程图</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbip3o5v1lb91ml77n6pnsc6734-20210605141030680.png" alt=""><br>图2. 8点基2 DIF-FFT计算流程图</p><p>从图1和图2可以清晰地看出Cooley-Tukey方法的特点：</p><ul><li>从输入输出来看，DIT-FFT需要先把顺序数据倒序（Bit-reverse），再进行计算，可以得到顺序的计算结果。DIF-FFT直接用顺序数据计算，但是得到的结果是倒序的，如果需要顺序结果，还要进行处理。</li><li>从计算结构来看，DIT-FFT和DIF-FFT具有相反的计算结构，DIT-FFT的计算跨度（stride）是2L ，即1,2,4…2L-1 ，DIF-FFT的计算跨度相反。</li><li>从计算过程来看，Cooley-Tukey方法具有原址（In-place）计算的特点，一个蝶形单元的计算结果还是放到原来的位置，不需要分配暂存空间。</li></ul><p>DFT与基-2 Cooley-Tukey DIT-FFT的程序如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">void DFTImag(Complex *Input, Complex *Output, int Amount)</span><br><span class="line">&#123;</span><br><span class="line">    int k, n;</span><br><span class="line">    Complex tmp;</span><br><span class="line">    float SumRe, SumIm, ReFactor, ImFactor, TAOPTK;</span><br><span class="line">    float TAOP = 2 * my_Pi / Amount;</span><br><span class="line">    for (k = 0; k &lt; Amount; k++)</span><br><span class="line">    &#123;</span><br><span class="line">        SumRe = 0; SumIm = 0;</span><br><span class="line">        TAOPTK = TAOP*k;</span><br><span class="line">        for (n = 0; n &lt; Amount; n++)</span><br><span class="line">        &#123;</span><br><span class="line">            ReFactor = cos(TAOPTK * n);</span><br><span class="line">            ImFactor = -sin(TAOPTK * n);</span><br><span class="line">            tmp = Input[n];</span><br><span class="line">            SumRe += tmp.x * ReFactor - tmp.y * ImFactor;   </span><br><span class="line">            SumIm += tmp.x * ImFactor + tmp.y * ReFactor;</span><br><span class="line">        &#125;</span><br><span class="line">        tmp.x = SumRe; tmp.y = SumIm;</span><br><span class="line">        Output[k] = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">void CTFFT_R2(Complex *array, Complex *array_out, unsigned int Power)</span><br><span class="line">&#123;</span><br><span class="line">    //---------------------init------------------</span><br><span class="line">    int len = pow((double)2, (int)Power);</span><br><span class="line">    bit_reverse(array, array_out, Power);            //倒序，数据放到array_out</span><br><span class="line">    //--------------------calcu------------------</span><br><span class="line">    for (int m = 2; m &lt;= len; m &lt;&lt;= 1)               //第一层计算单元：2  第二层：4   ...  第L层：2^L </span><br><span class="line">    &#123;</span><br><span class="line">        int mh = m &gt;&gt; 1;                             //单元计算跨度 1 2 4 </span><br><span class="line">        for (int i = 0; i &lt; mh; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            Complex wi = exp_calcu(-i*(my_Pi / mh)); //本来(2*pi/m) &gt;&gt; pi/(mh)  </span><br><span class="line">            for (int j = i; j &lt; len; j += m)         //j  第一层0 2 4 6   第二层0 4 (8&gt;=len 跳出)</span><br><span class="line">            &#123;                                                </span><br><span class="line">                Complex u = array_out[j];</span><br><span class="line">                Complex t = complx_mul(wi, array_out[j + mh]);</span><br><span class="line">                array_out[j] = complx_add(u, t);</span><br><span class="line">                array_out[j + mh] = complx_sub(u, t);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>本机CPU端计算4096点结果：<br>DFT的时间在2000ms左右，而FFT是4ms （Debug模式）。<br>DFT的时间在800ms左右，而FFT在1ms以内 （Release模式）。</p><p><strong>考虑到GPU的硬件架构，并行化Cooley-Tukey FFT算法存在以下问题：</strong></p><h4 id="倒序计算"><a href="#倒序计算" class="headerlink" title="倒序计算"></a><strong>倒序计算</strong></h4><p>​    数据倒序是对序列进行奇偶抽取的结果，倒序（bit reverse）是按位进行的，把数据的MSB和LSB反向，一个简单例子是0010 0000 =&gt; 0000 0100。下面分别是8bit倒序的较快算法，本文实现的根据数据位宽倒序的算法和CUDA MATH API中的32bit数据倒序程序调用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">unsigned char reverse_8bit(unsigned char x)</span><br><span class="line">&#123;</span><br><span class="line">    x = (((x &amp; 0xaa) &gt;&gt; 1) | ((x &amp; 0x55) &lt;&lt; 1));  // 交换每两位</span><br><span class="line">    x = (((x &amp; 0xcc) &gt;&gt; 2) | ((x &amp; 0x33) &lt;&lt; 2));  // 交换每四位中的前两位和后两位</span><br><span class="line">    return((x &gt;&gt; 4) | (x &lt;&lt; 4));                  // 交换前后两个4bit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">unsigned int </span><br><span class="line">index_rv(unsigned int index, unsigned int Power)</span><br><span class="line">&#123;</span><br><span class="line">    unsigned int tmp = 0;</span><br><span class="line">    unsigned int bit_value = 0;</span><br><span class="line">    //power*6的计算量</span><br><span class="line">    for (int i = 0; i &lt; Power; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        bit_value = index &amp; 1;</span><br><span class="line">        tmp += (bit_value &lt;&lt; ((Power - 1) - i));</span><br><span class="line">        index = index &gt;&gt; 1;</span><br><span class="line">    &#125;</span><br><span class="line">    return tmp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    可以看出，通常情况下要对数据进行几十次操作才能得出倒序结果。测试中使用了CUDA的库函数进行倒序，并没有比reverse_8bit快。另一个问题是，FFT kernel占用较多硬件资源，一个大核心只能保证2到3个block占用。<br>大量倒序计算在一些情况下会使访存密度降低（<strong>这里的意思是，在做到足够优化的情况下，算法是访存密集的，如果计算量再增加，就是计算密集的了，相同数据量的算法执行时间会增加</strong>），测试结果如下，左边表格是全带宽占用结果，红色箭头处带宽占用不满：<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiphbb91p9g1qskdoj77q1nki3h.png" alt=""><br>图3. 倒序与否kernel带宽占用测试对比</p><p>​    本文想到的解决办法是，使用查找表，通过纹理缓冲从显存中读取倒序索引（不占用共享内存），24KB的L1 cache基本满足小点数查找表需求，大点数会由于片上存不下，需要多次访问显存。</p><p>但倒序的计算还不是影响算法速度的关键因素。</p><h4 id="倒序存储"><a href="#倒序存储" class="headerlink" title="倒序存储"></a><strong>倒序存储</strong></h4><p>《Maxwell硬件架构与编程方法》中详细说明了GPU存储结构和使用注意事项，从算法局部性考虑，FFT算法的输入数据需要从显存顺序读到片上，在共享内存中进行倒序。而共享内存分为32个32bit的存储体（bank），在这一级倒序读取或存储都会导致存储体冲突（bank conflict）。根据倒序特点和单精度浮点的复数数据结构（两个float），实际地共享内存带宽占用仅有1/16。实测结果与理论分析一致：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiqa74thds13skf671bo11jsj3u.png" alt=""><br>图4. 无冲突共享内存访问测试</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiqaft7pq7fub1t4cg51nhv4b.png" alt=""><br>图5. 倒序冲突共享内存访问测试</p><p>​    注意，图4和图5中的数据量不同，表格第二列是传输数据量，在无冲突访存时，加载数据量=存储数据量=实际数据量。在有冲突时，数据需要排队，而每次传输数据都是32<em>32bit，所以浪费了很多带宽。图9中，加载数据量=实际数据量，倒序存储量=16</em>实际数据量，即有效带宽占用仅为全带宽的1/16。</p><p>​    本文还做了一个无冲突共享内存访问基准测试。Maxwell架构下，数据从显存读入片上，在保证完整占用显存带宽的情况下，可以在片上进行12次无冲突的数据交换，即24次存储或读取。而一次数据倒序存储就使用了16/24的时间，那么留给计算的时间就远远不够了。<br>所以，要高效地利用GPU计算和存储资源，就应尽量避免倒序。</p><h4 id="非向量化的数据抽取方式"><a href="#非向量化的数据抽取方式" class="headerlink" title="非向量化的数据抽取方式"></a><strong>非向量化的数据抽取方式</strong></h4><p>​    使用CUDA并行编程，数据是按1个warp，32个线程为基本单位读取的，再加上GPU存储器特点，并行化的算法数据存取结构尽可能实现连续无间隔地访存。先不考虑倒序，以32点DIT-FFT为例：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiqfa8ulvg1psnas1dp51mut4o-20210605135223962.png" alt=""><br>图6. 32点DIT-FFT 数据抽取流程示意</p><p>​    因为是浮点复数，占用64bit，相当于有16个64bit的bank。假设用16个线程来计算，那么第一级数据的第一次读取是0-15号线程读取数据0 , 2 , … , 30。其中0,2,4,6,8,10,12,14与16,18,20,22,24,26,28,30存在bank conflict，第二次读取同理。即第一次读取有一半的bank存在冲突，共享内存带宽利用率只有一半，第一次存储同理。第二级数据可以也类似，访问索引0,4,8,12,1,5,9,13的数据被合并到一起访问，与16,20,24,28,17,21,25,29冲突……直到最后一级（32点基-2运算共5级，图中值标了前4级，最后一级类似图4的stage3）才能分别连续访问0-15与16-31，利用完整带宽。前4级的数据交换共8次，实际消耗了16次无冲突数据交换的时间，浪费了1/2的带宽。</p><p>采用基-4结构的话，只有前两级有存储冲突，但每级带宽利用率是1/4，与基-2是等效的。基-8，基-16的计算结构同理，好处是后续计算级数更少。</p><p><strong>理论和基准测试结果表明，采用传统的FFT计算结构不利于在GPU架构上高效地进行计算。</strong></p><h3 id="向量化的DFT分解方法（Stockham-Autosort-Framework）"><a href="#向量化的DFT分解方法（Stockham-Autosort-Framework）" class="headerlink" title="向量化的DFT分解方法（Stockham Autosort Framework）"></a>向量化的DFT分解方法（Stockham Autosort Framework）</h3><h4 id="计算分解与结构"><a href="#计算分解与结构" class="headerlink" title="计算分解与结构"></a><strong>计算分解与结构</strong></h4><p>​    离散傅里叶变换可以非常多形式的分解，Cooley-Tukey FFT很好的减少了算法的空间占用（原址计算），但也存在数据倒序的问题。本小节说明Stockham结构的FFT实现，Stockham 方法以空间占用为代价避免了Cooley-Tukey过程的倒序计算。它的自动倒序结构背后的思想是把数据倒序和蝶形计算结合起来。</p><p>​    上一小节提到过DFT的矩阵形式, 可以简记为 $F(N)=W^{k n} f(k)$ 和 $f(k)=\frac{1}{N} W^{-k n} F(N)$ 。从矩阵分解的角度看, Cooley-Tukey FFT做了以下处理：倒序步骤用矩阵表示是一个置换矩阵 $P_{n}^{T} ;$ 包含旋转因子大矩阵 $W^{k n}$ 被分解为许多级<br>Tukey FFT可用矩阵表示为 $X(N)=F_{n} x(k)=A_{t} \cdots A_{1} P_{n}^{T} x(k)$ 。即数据经倒序和t级基-2运算，最终得到与原始 DFT计算方法相同的傅里叶变换结果。<br>​    用同样的方式说明Stockham的方法。与Cooley-Tukey显式的置换 $F_{n}=A_{t} \cdots A_{1} P_{n}^{T}$ 不同，Stockham通过置换矩陈 $\Gamma_{0} \cdots \Gamma_{t-1}$ 使得 $F_{n}=A_{t} \Gamma_{t-1} \cdots A_{2} \Gamma_{1} A_{1} \Gamma_{0}$, 即 $X(N)=F_{n} x(k)=A_{t} \Gamma_{t-1} \cdots A_{2} \Gamma_{1} A_{1} \Gamma_{0} x(k)$</p><p>对比两者的计算过程：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirag192o5dihvum10748sh5i.png" alt=""></p><p>Cooley-Tukey 方法首先把输入序列x完整地倒序，再进行各级蝶形计算。Stockham 方法把倒序结构和蝶形计算结合起来，在进行蝶形计算之前将序列x进行部分倒序，经过多级倒序和计算之后，得到与和Cooley-Tukey 方法相同的计算结果。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirbbse2h31t7a10e2lpamhd5v.png" alt=""><br>图7. 8点基-2 Stockham FFT计算流程图（DIT）<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirbgnt12t71c3g15ka1oi31ul16c.png" alt=""><br>图8. 8点基-2 Stockham FFT计算流程图（DIF）</p><p>图7和图8画出了DIT 和DIF两种Stockham FFT的计算流程，可以总结出以下特点：</p><ul><li>输入输出都是顺序的，没有显式的倒序步骤。</li><li>（级间有倒序处理）计算是非原址的（Out-place），蝶形单元的计算结果可能会覆盖还没计算的数据，这就要求分配和序列长度相等的空间用来存储。</li><li>Stockham DIT-FFT的数据读取索引是不变的，蝶形单元的数据存储跨度（stride）根据所在级数（stage）由1，2，一直到2L-1；DIF-FFT的数据存储索引是不变的，蝶形单元的数据读取跨度（stride）根据所在级数（stage）由1，2，一直到2L-1。这里，DIF的计算结构基本上与DIT相反，但计算结果一致。</li></ul><h4 id="算法实现与数据抽取"><a href="#算法实现与数据抽取" class="headerlink" title="算法实现与数据抽取"></a><strong>算法实现与数据抽取</strong></h4><p>把《Computational Frameworks for the Fast Fourier Transform》书中1.7节的Stockham 基-2算法（书P57，PDF的第75页）用C语言实现如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbire059msc1o931bl61sth14k76p.png" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">void stockham_fft_R2(Complex* x, Complex* y, int len, int power)</span><br><span class="line">&#123;</span><br><span class="line">    Complex wi, tau;</span><br><span class="line">    int L, L_d2, stride, stride_m2;</span><br><span class="line">    for (int q = 1; q &lt;= power; q++)</span><br><span class="line">    &#123;</span><br><span class="line">                                             //L*r=len</span><br><span class="line">        L         = pow((double)2, q);       //L      : 2 4 8 ... 计算组大小</span><br><span class="line">        stride    = len / L;                 //stride ：...4 2 1 计算组内的跨度</span><br><span class="line">        L_d2      = L / 2;</span><br><span class="line">        stride_m2 = len / L_d2;</span><br><span class="line">        memcpy(y, x, len * sizeof(Complex));</span><br><span class="line">        for (int j = 0; j &lt; L_d2; j++)       //j只有L的一半（R-2蝶形计算 两个一组）</span><br><span class="line">        &#123;</span><br><span class="line">            wi = exp_calcu(-j*(2 * my_Pi / L));</span><br><span class="line">            for (int k = 0; k &lt; stride; k++)</span><br><span class="line">            &#123;</span><br><span class="line">                tau = complx_mul(wi, y[j*stride_m2 + k + stride]);</span><br><span class="line">                x[j*stride + k] = complx_add(y[j*stride_m2 + k], tau);</span><br><span class="line">                x[(j + L_d2)*stride + k] = complx_sub(y[j*stride_m2 + k], tau);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以32点Stockham 基-2 DIT-FFT计算过程的数据抽取为例。由图9可知：前4级都可实现向量化数据读取，向量化长度分别为16、8、4、2，最后一级跨度为2，不是连续的访问；所有级的数据存储都是长度为16的向量化访问。数据读取的向量化长度是和计算点数有关的，比如256点的计算，向量化长度就会是128、64、32、16、8、4、2，以此类推。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiriqhu151m1f042juaeu1d2676.png" alt=""><br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirj163rvm1o3rv0lj681k0i7j.png" alt=""><br>图9. 32点Stockham 基-2 DIT-FFT算法的数据抽取示意（共5级）</p><p>结合GPU硬件，共享内存有16个64bit的bank，基-2计算在向量化访问长度为8、4、2、1（为表述方便，把最后一级也加进来，后同）的计算级，带宽利用率都是1/2。如果用基-4作为基本蝶形单元的话，256点计算级数为4级，访存的向量化长度是64、16、4、1，最后两级的带宽利用率为1/4，与基-2计算的4级1/2带宽利用率等效，但计算级数与基-2相比减少一半。</p><p>相比Cooley-Tukey方法Stockham FFT在GPU上实现，可以避免倒序计算和倒序存储两个重要问题。而且CUDA的线程以32个为最小单位同时执行，是向量化的处理机制，Stockham FFT中各级数据的向量化访问，非常适合用GPU来实现。虽然也有存储冲突问题，但在当前架构下基-4计算的(8-2)/24的冲突是可以接受的，剩下的时间可以完成FFT计算。GPU拥有寄存器、共享内存、片外显存的存储层次，Stockham计算的非原址问题并没有影响。在两级计算过程中，线程有足够的寄存器资源把共享内存中的数据全部读出，并行计算，再放回共享内存。</p><p>所以本文在GPU上的FFT算法实现，采用Stockham方法。下一节将会叙述Stockham FFT的CUDA实现。</p><hr><h2 id="4096点STFFT基本实现"><a href="#4096点STFFT基本实现" class="headerlink" title="4096点STFFT基本实现"></a>4096点STFFT基本实现</h2><h3 id="计算框架与工程方法"><a href="#计算框架与工程方法" class="headerlink" title="计算框架与工程方法"></a><strong>计算框架与工程方法</strong></h3><p>《Maxwell硬件架构与编程方法》中介绍过Maxwell架构下，每个大核心（SMM）内部有128个小核心（Core）、65536个32bit的寄存器文件（Register File）和96KB的共享内存空间（Shared Memory）。在软件层面，每个线程（thread）最多使用256个寄存器，每个线程块（blcok）最多分配1024个线程，最多占用65536个寄存器和49152bytes的共享内存。</p><p>49152bytes共享内存可存储12288个单精度浮点数据（FP32，4bytes），6144个复数（两个FP32）。由于FFT有多级数据交换，为了保证算法的运行时间，数据要放到片上计算，所以考虑到存储能力，理论上一次可以计算4096点。故从共享内存看，每个block需要32768bytes空间，一个大核心可以保证3个block的占用。</p><p>寄存器方面，为了4096点计算同时进行，需要8192个32bit寄存器用于数据存储，数值计算过程也会占用寄存器资源，后续还会说到寄存器占用情况和优化方法。实际上寄存器还有盈余，所以8192点的计算也可以勉强在片上一次性完成计算。</p><p>由上述架构特点和前一节的理论分析，采用如下计算框架：</p><ul><li>基-4作为基本蝶形计算单元，6级基-4完成4096点的Stockham FFT计算。</li><li>每个block分配4096*8bytes = 32768bytes的共享内存空间用于级间数据交换。</li><li>每个block分配256个线程，为同时计算4096点数据，每个线程一次读取16点数据进行处理。</li></ul><p>《Computational Frameworks for the Fast Fourier Transform》书中2.4.4节有Stockham 的基-4算法（书P105，PDF的第123页）：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbirv979a311sr1qpnnkn33180.png" alt=""></p><p>//201710回忆<br>//L*=L/4，应该是一个R4在L的组内抽取跨度<br>//r=n/L，是大组数<br>//W是按组内算的，所以中循环j</p><p>用C语言实现的代码较长，本文简介方法。</p><p>在C语言验证工程中，首先用递增数给序列赋值，再新建一个txt文件，调用stockham_fft_R4子程序完成计算后，<br>把结果放到txt文件中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> for (int i = 0; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        array[i].x = i;</span><br><span class="line">        array[i].y = i;</span><br><span class="line">    &#125;</span><br><span class="line">    char txtdataFileName[1024];</span><br><span class="line">    FILE * TxtWriter1;</span><br><span class="line">    sprintf(txtdataFileName, &quot;StockhamFFTdata.txt&quot;);</span><br><span class="line">    TxtWriter1 = fopen(txtdataFileName, &quot;wb&quot;);</span><br><span class="line">    stockham_fft_R4(array, FFTarray, len, power4, -1);</span><br><span class="line">    for (int i = 0; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        fprintf(TxtWriter1, &quot;%f \r\n&quot;, array[i].x);</span><br><span class="line">        fprintf(TxtWriter1, &quot;%f \r\n&quot;, array[i].y);</span><br><span class="line">    &#125;</span><br><span class="line">//---------------------------------------------------------------------------</span><br><span class="line">    for (int i = 0; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        array[i].x = i;</span><br><span class="line">        array[i].y = i;</span><br><span class="line">    &#125;</span><br><span class="line">    sprintf(txtdataFileName, &quot;StockhamFFTindex.txt&quot;);</span><br><span class="line">    TxtWriter1 = fopen(txtdataFileName, &quot;wb&quot;);</span><br><span class="line">    stockham_fft_R4_index(array, FFTarray, len, power4, -1, TxtWriter1);</span><br></pre></td></tr></table></figure><p>在MATLAB中用同样的方法调用库函数fft()得到结果，把C语言工程的结果导入MATLAB，对比两个序列的相关系数、标准差和序列的插值，进行计算结果验证。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">clear all;</span><br><span class="line">n=<span class="number">0</span>:<span class="number">4095</span>;</span><br><span class="line">x=n+n*<span class="built_in">i</span>;</span><br><span class="line">Y=fft(x,<span class="number">4096</span>);</span><br><span class="line">Y=Y.&#x27;;       <span class="comment">%转置用了 共轭转置  应该用.&#x27;</span></span><br><span class="line">data_index=[<span class="string">&#x27;E:/StockhamFFTdata.txt&#x27;</span>];</span><br><span class="line">FFT4096 =(importdata(data_index));</span><br><span class="line">FFT4096_c=FFT4096(<span class="number">1</span>:<span class="number">2</span>:<span class="number">8192</span>)+<span class="built_in">j</span>*FFT4096(<span class="number">2</span>:<span class="number">2</span>:<span class="number">8192</span>);</span><br><span class="line">corrcoef(<span class="built_in">real</span>(FFT4096_c),<span class="built_in">real</span>(Y))</span><br><span class="line">corrcoef(<span class="built_in">imag</span>(FFT4096_c),<span class="built_in">imag</span>(Y))</span><br><span class="line">std(FFT4096_c)</span><br><span class="line">std(Y)</span><br><span class="line">err=Y-FFT4096_c;</span><br><span class="line"><span class="built_in">max</span>(err)</span><br></pre></td></tr></table></figure><p>本文在子程序stockham_fft_R4()的基础上设计了一个把各级FFT计算中的数据抽取索引，旋转因子计算索引打印到txt文档中的子程序stockham_fft_R4_index()。这样可以直观快速地查看数值计算过程，本文的后续实现也大量使用这种方法。</p><h3 id="CUDA实现"><a href="#CUDA实现" class="headerlink" title="CUDA实现"></a><strong>CUDA实现</strong></h3><h4 id="C语言验证"><a href="#C语言验证" class="headerlink" title="C语言验证"></a>C语言验证</h4><p>GPU端分配一个256线程的block进行计算，每个线程每次负责16个数据的计算，占用32768bytes的共享内存空间。每一级计算的数据抽取方式都不同。为尽快尝试和验证算法，本文首先在CPU端用的C语言模拟GPU的多线程并行情况，得到正确结果后再移植到CUDA C工程中。</p><p>输入序列首地址data_in，相同大小的暂存空间首地址data_out和计算方向direction（正向FFT计算，-1）。首先声明自动变量，index用于保存访存下标索引，wi_c和wi保存旋转因子的浮点和复数数据，16个Complex变量用于保存每个线程读取的16个数据。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">StockhamR4_FFT_4096</span><span class="params">(Complex *data_in, Complex *data_tmp, <span class="keyword">int</span> direction)</span></span></span><br><span class="line"><span class="function"></span>&#123;   </span><br><span class="line">    <span class="comment">//---------------------init-------------------------</span></span><br><span class="line">    <span class="keyword">int</span>  index = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> wi_c = <span class="number">0</span>;</span><br><span class="line">    Complex wi;</span><br><span class="line">    Complex r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12, r13, r14, r15, r16;</span><br></pre></td></tr></table></figure><p>下面是4096点FFT的第1层计算代码，一共6级计算，基本保持这样的形式。</p><p>通过256次for循环模拟256个线程的并行计算，变量tid代表线程号。</p><p>从序列data_in中抽取数据，由于非原址的结构，计算完成后为避免覆盖其他位置的数据，将结果存到data_tmp中，下一次再从data_tmp取数，计算后存到data_in……以此往复，直到计算完成。</p><p>寄存器r1,r2,r3,r4中保存的是一个基-4蝶形单元的数据，在第一级，4096点按基-4抽取，跨度4096/4=1024。所以同一个tid的r1,r2,r3,r4地址偏移按1024递增。第一组的4个数据抽取，在并行情况下256线程实际抽取了4096点数据的4个1024数据段中的前256个数据。所以之后的r5,r6,r7,r8在读取数据时，索引要加上256，以此类推。</p><p>读取数据后，每个线程计算4组基-4单元。这里调用了基础模块StockhamR4_block()，传入4个数据和1个旋转因子的地址，传入计算旋转因子需要的两个常数wi_c和index（根据线程不同，index不同；根据计算层级不同，wi_c同），图19中第一层旋转因子的基本常数wi_c是 -2π/4 ，而索引index都为0。</p><p>计算层级基本保持这一形式，由于数据存储方式不变，之后的示例仅截取for循环的前半部分进行说明。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////Level in 4////////////////////////////////////////////////////////////////</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> tid = <span class="number">0</span>; tid &lt; <span class="number">256</span>; tid++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//本层 in out index 相同</span></span><br><span class="line">    r1  = data_in[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">    r2  = data_in[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">    r3  = data_in[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">    r4  = data_in[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">    r5  = data_in[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">    r6  = data_in[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">    r7  = data_in[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">    r8  = data_in[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">    r9  = data_in[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">    r10 = data_in[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">    r11 = data_in[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">    r12 = data_in[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">    r13 = data_in[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">    r14 = data_in[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">    r15 = data_in[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">    r16 = data_in[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">    wi_c = <span class="number">2</span> * my_Pi / <span class="number">4</span> * direction;</span><br><span class="line">    StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">0</span>] = r1;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">0</span>] = r2;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">0</span>] = r3;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">0</span>] = r4;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">1</span>] = r5;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">1</span>] = r6;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">1</span>] = r7;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">1</span>] = r8;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">2</span>] = r9;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">2</span>] = r10;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">2</span>] = r11;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">2</span>] = r12;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">3</span>] = r13;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">3</span>] = r14;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">3</span>] = r15;</span><br><span class="line">    data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">3</span>] = r16;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>按理论分析，第二层级以1024点为计算单元，还是基-4抽取，1024除以4，跨度是256。同理，第三层级计算单元为256点，内部跨度64；第四层级计算单元为64点，内部跨度16；第五层级计算单元为16点，内部跨度4；第六层级计算单元为4点，内部跨度1。</p><p>本文的设计是256线程并行，所以下面第二层级的数据抽取恰好是256*4个变量存一个1024计算单元，变量1-4、5-8、9-12、13-16分别是第1、2、3、4个1024计算单元内的基-4一组蝶形运算，对应旋转因子计算索引0、1、2、3。层级的旋转因子常数是-2π/16。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">r1  = data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">r2  = data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">r3  = data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">r4  = data_tmp[tid + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">r5  = data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">r6  = data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">r7  = data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">r8  = data_tmp[tid + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">r9  = data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">r10 = data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">r11 = data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">r12 = data_tmp[tid + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">r13 = data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">0</span>];</span><br><span class="line">r14 = data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">1</span>];</span><br><span class="line">r15 = data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">2</span>];</span><br><span class="line">r16 = data_tmp[tid + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">256</span> * <span class="number">3</span>];</span><br><span class="line">wi_c = <span class="number">2</span> * my_Pi / <span class="number">16</span> * direction;</span><br><span class="line">StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, <span class="number">0</span>);</span><br><span class="line">StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, <span class="number">1</span>);</span><br><span class="line">StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, <span class="number">2</span>);</span><br><span class="line">StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, <span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>第三层计算单元为256点，内部跨度为64点。并行的线程数是256，如果还是一次读取连续的256点，每个线程只能得到单元内的一个数据，不能计算，所以需要根据线程号来划分各自的计算区域。256点（计算单元长度）除以4（基-4计算变量个数）得到64，所以连续的64个线程负责一个256单元的计算。由于一共有16个变量，所以64个线程计算了4组256单元。整体情况是256个线程前4次读取了1024点数据，4组连续的64线程分别负责4块256单元的计算，后面3组4变量读取按照这种形式分别进行了1024点的计算。</p><p>如下面程序所示，与上两层计算不同，第三层首先计算了数据抽取索引index。线程号tid的范围0-255，tid&amp;(256 - 64)可以得到0、64、128、192四个数，即以64为单位将线程分为4组。每组线程负责256点的计算，所以 (tid&amp;(256 - 64)) <em> 4得到0、256、512、768。然后加上tid%64得到线程的组内序号。在读取数据时，以索引index为基地址，进行偏移即可，第三层级计算跨度为64，可以从图21看到读取方式符合这一形式。<br>第三层有16个256计算单元，所以基-4计算索引根据64线程组和4变量组得到：index = tid/64 + 4</em>(0 or 1 or 2 or 3)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//64threads 一组</span></span><br><span class="line">index = (tid&amp;(<span class="number">256</span> - <span class="number">64</span>)) * <span class="number">4</span> + tid % <span class="number">64</span>;</span><br><span class="line">r1 = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">64</span> * <span class="number">0</span>];</span><br><span class="line">r2 = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">64</span> * <span class="number">1</span>];</span><br><span class="line">r3 = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">64</span> * <span class="number">2</span>];</span><br><span class="line">r4 = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">64</span> * <span class="number">3</span>];</span><br><span class="line">r5 = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">64</span> * <span class="number">0</span>];</span><br><span class="line">r6 = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">64</span> * <span class="number">1</span>];</span><br><span class="line">r7 = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">64</span> * <span class="number">2</span>];</span><br><span class="line">r8 = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">64</span> * <span class="number">3</span>];</span><br><span class="line">r9 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">64</span> * <span class="number">0</span>];</span><br><span class="line">r10 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">64</span> * <span class="number">1</span>];</span><br><span class="line">r11 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">64</span> * <span class="number">2</span>];</span><br><span class="line">r12 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">64</span> * <span class="number">3</span>];</span><br><span class="line">r13 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">64</span> * <span class="number">0</span>];</span><br><span class="line">r14 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">64</span> * <span class="number">1</span>];</span><br><span class="line">r15 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">64</span> * <span class="number">2</span>];</span><br><span class="line">r16 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">64</span> * <span class="number">3</span>];</span><br><span class="line">wi_c  = <span class="number">2</span> * my_Pi / <span class="number">64</span> * direction;</span><br><span class="line">index = tid / <span class="number">64</span>;</span><br><span class="line">StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, index + <span class="number">4</span>*<span class="number">0</span>);</span><br><span class="line">StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, index + <span class="number">4</span>*<span class="number">1</span>);</span><br><span class="line">StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, index + <span class="number">4</span>*<span class="number">2</span>);</span><br><span class="line">StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, index + <span class="number">4</span>*<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>根据之前的分析和第三层级的规律，容易得到之后层级的计算结构。要注意的是，共享内存有32个32bit的bank，到第四层级计算单元长度是64，基-4的计算跨度是16（16个64bit数据），恰好还能满足全带宽访问。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">index = (tid&amp;(<span class="number">256</span> - <span class="number">16</span>)) * <span class="number">4</span> + tid % <span class="number">16</span>;</span><br><span class="line">r1  = data_tmp[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">16</span> * <span class="number">0</span>];</span><br><span class="line">r2  = data_tmp[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">16</span> * <span class="number">1</span>];</span><br><span class="line">r3  = data_tmp[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">16</span> * <span class="number">2</span>];</span><br><span class="line">r4  = data_tmp[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">16</span> * <span class="number">3</span>];</span><br><span class="line">r5  = data_tmp[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">16</span> * <span class="number">0</span>];</span><br><span class="line">r6  = data_tmp[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">16</span> * <span class="number">1</span>];</span><br><span class="line">r7  = data_tmp[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">16</span> * <span class="number">2</span>];</span><br><span class="line">r8  = data_tmp[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">16</span> * <span class="number">3</span>];</span><br><span class="line">r9  = data_tmp[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">16</span> * <span class="number">0</span>];</span><br><span class="line">r10 = data_tmp[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">16</span> * <span class="number">1</span>];</span><br><span class="line">r11 = data_tmp[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">16</span> * <span class="number">2</span>];</span><br><span class="line">r12 = data_tmp[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">16</span> * <span class="number">3</span>];</span><br><span class="line">r13 = data_tmp[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">16</span> * <span class="number">0</span>];</span><br><span class="line">r14 = data_tmp[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">16</span> * <span class="number">1</span>];</span><br><span class="line">r15 = data_tmp[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">16</span> * <span class="number">2</span>];</span><br><span class="line">r16 = data_tmp[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">16</span> * <span class="number">3</span>];</span><br><span class="line">wi_c  = <span class="number">2</span> * my_Pi / <span class="number">256</span> * direction;</span><br><span class="line">index = tid / <span class="number">16</span>;</span><br><span class="line">StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, index + <span class="number">16</span>*<span class="number">0</span>);</span><br><span class="line">StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, index + <span class="number">16</span>*<span class="number">1</span>);</span><br><span class="line">StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, index + <span class="number">16</span>*<span class="number">2</span>);</span><br><span class="line">StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, index + <span class="number">16</span>*<span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>第五、六层的16点计算单元、内部跨度4和4点计算单元、内部跨度1都只能占用1/4的带宽（3/4 bank conflict）。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////Level 1024////////////////////////////////////////////////////////////////</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> tid = <span class="number">0</span>; tid &lt; <span class="number">256</span>; tid++)</span><br><span class="line">&#123;</span><br><span class="line">    index = (tid&amp;(<span class="number">256</span> - <span class="number">4</span>)) * <span class="number">4</span> + tid % <span class="number">4</span>;</span><br><span class="line">    r1  = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">4</span> * <span class="number">0</span>];</span><br><span class="line">    r2  = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">4</span> * <span class="number">1</span>];</span><br><span class="line">    r3  = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">4</span> * <span class="number">2</span>];</span><br><span class="line">    r4  = data_in[index + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">4</span> * <span class="number">3</span>];</span><br><span class="line">    r5  = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">4</span> * <span class="number">0</span>];</span><br><span class="line">    r6  = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">4</span> * <span class="number">1</span>];</span><br><span class="line">    r7  = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">4</span> * <span class="number">2</span>];</span><br><span class="line">    r8  = data_in[index + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">4</span> * <span class="number">3</span>];</span><br><span class="line">    r9  = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">4</span> * <span class="number">0</span>];</span><br><span class="line">    r10 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">4</span> * <span class="number">1</span>];</span><br><span class="line">    r11 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">4</span> * <span class="number">2</span>];</span><br><span class="line">    r12 = data_in[index + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">4</span> * <span class="number">3</span>];</span><br><span class="line">    r13 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">4</span> * <span class="number">0</span>];</span><br><span class="line">    r14 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">4</span> * <span class="number">1</span>];</span><br><span class="line">    r15 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">4</span> * <span class="number">2</span>];</span><br><span class="line">    r16 = data_in[index + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">4</span> * <span class="number">3</span>];</span><br><span class="line">    wi_c  = <span class="number">2</span> * my_Pi / <span class="number">1024</span> * direction;</span><br><span class="line">    index = tid / <span class="number">4</span>;</span><br><span class="line">    StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, index + <span class="number">64</span> * <span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, index + <span class="number">64</span> * <span class="number">1</span>);</span><br><span class="line">    StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, index + <span class="number">64</span> * <span class="number">2</span>);</span><br><span class="line">    StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, index + <span class="number">64</span> * <span class="number">3</span>);</span><br></pre></td></tr></table></figure><p>还要注意，第六层的计算结构与前面几层不同，一个线程负责连续4点数据的计算，所以读取索引是tid<em>4+1024</em>(0 or 1 or 2 or 3)+(0 or 1 or 2 or 3)。其他部分还是延续规律。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">r1  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">0</span>];</span><br><span class="line">r2  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">1</span>];</span><br><span class="line">r3  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">2</span>];</span><br><span class="line">r4  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">0</span> + <span class="number">3</span>];</span><br><span class="line">r5  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">0</span>];</span><br><span class="line">r6  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">1</span>];</span><br><span class="line">r7  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">2</span>];</span><br><span class="line">r8  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">1</span> + <span class="number">3</span>];</span><br><span class="line">r9  = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">0</span>];</span><br><span class="line">r10 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">1</span>];</span><br><span class="line">r11 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">2</span>];</span><br><span class="line">r12 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">2</span> + <span class="number">3</span>];</span><br><span class="line">r13 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">0</span>];</span><br><span class="line">r14 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">1</span>];</span><br><span class="line">r15 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">2</span>];</span><br><span class="line">r16 = data_tmp[tid * <span class="number">4</span> + <span class="number">1024</span> * <span class="number">3</span> + <span class="number">3</span>];</span><br><span class="line">wi_c  = <span class="number">2</span> * my_Pi / <span class="number">4096</span> * direction;</span><br><span class="line">index = tid;</span><br><span class="line">StockhamR4_block(&amp;r1,  &amp;r2,  &amp;r3,  &amp;r4,  &amp;wi, wi_c, index + <span class="number">256</span>*<span class="number">0</span>);</span><br><span class="line">StockhamR4_block(&amp;r5,  &amp;r6,  &amp;r7,  &amp;r8,  &amp;wi, wi_c, index + <span class="number">256</span>*<span class="number">1</span>);</span><br><span class="line">StockhamR4_block(&amp;r9,  &amp;r10, &amp;r11, &amp;r12, &amp;wi, wi_c, index + <span class="number">256</span>*<span class="number">2</span>);</span><br><span class="line">StockhamR4_block(&amp;r13, &amp;r14, &amp;r15, &amp;r16, &amp;wi, wi_c, index + <span class="number">256</span>*<span class="number">3</span>);</span><br></pre></td></tr></table></figure><h4 id="CUDA-C实现"><a href="#CUDA-C实现" class="headerlink" title="CUDA C实现"></a>CUDA C实现</h4><p>根据C语言验证结果，把程序移植到GPU端，用CUDA实现。考虑到复用，将4096点的计算封装为一个inline模块，在编译时，模块会“嵌入” 到被调用的地方，而不是跳转调用。函数及参数声明如下，传递的是寄存器地址和共享内存首地址data_shared：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> __device__ <span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">Stockham_4096_block</span></span></span><br><span class="line"><span class="function"><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *r1,  Complex *r2,  Complex *r3,  Complex *r4,</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *r5,  Complex *r6,  Complex *r7,  Complex *r8,</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *r9,  Complex *r10, Complex *r11, Complex *r12,</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *r13, Complex *r14, Complex *r15, Complex *r16,</span></span></span><br><span class="line"><span class="params"><span class="function">    Complex *data_shared</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span></span><br></pre></td></tr></table></figure><p>总结第2、3、4、5层的数据抽取和计算规律，可以把这几层用一个for循环实现，如图26所示。要注意的是，数据的读取和存储都在共享内存上操作，所以在计算完成后需要调用线程同步API __syncthreads();（第30行），等所有线程都计算完成后，才一起把数据放回共享内存，以免覆盖了其他线程的计算数据。为保证for循环下次读取数据的正确性，在数据放回后后也需要进行线程同步。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////Level 16 64 256 1024///////////////////////////////////////////////////</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=<span class="number">64</span>;i=i*<span class="number">4</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> inexd_tmp = <span class="number">256</span>/i;</span><br><span class="line">    index = (tid&amp;(<span class="number">256</span> - inexd_tmp))*<span class="number">4</span> + tid%inexd_tmp;</span><br><span class="line">    *r1  = data_shared[index + <span class="number">1024</span>*<span class="number">0</span> + inexd_tmp*<span class="number">0</span>];</span><br><span class="line">    *r2  = data_shared[index + <span class="number">1024</span>*<span class="number">0</span> + inexd_tmp*<span class="number">1</span>];</span><br><span class="line">    *r3  = data_shared[index + <span class="number">1024</span>*<span class="number">0</span> + inexd_tmp*<span class="number">2</span>];</span><br><span class="line">    *r4  = data_shared[index + <span class="number">1024</span>*<span class="number">0</span> + inexd_tmp*<span class="number">3</span>];</span><br><span class="line">    *r5  = data_shared[index + <span class="number">1024</span>*<span class="number">1</span> + inexd_tmp*<span class="number">0</span>];</span><br><span class="line">    *r6  = data_shared[index + <span class="number">1024</span>*<span class="number">1</span> + inexd_tmp*<span class="number">1</span>];</span><br><span class="line">    *r7  = data_shared[index + <span class="number">1024</span>*<span class="number">1</span> + inexd_tmp*<span class="number">2</span>];</span><br><span class="line">    *r8  = data_shared[index + <span class="number">1024</span>*<span class="number">1</span> + inexd_tmp*<span class="number">3</span>];</span><br><span class="line">    *r9  = data_shared[index + <span class="number">1024</span>*<span class="number">2</span> + inexd_tmp*<span class="number">0</span>];</span><br><span class="line">    *r10 = data_shared[index + <span class="number">1024</span>*<span class="number">2</span> + inexd_tmp*<span class="number">1</span>];</span><br><span class="line">    *r11 = data_shared[index + <span class="number">1024</span>*<span class="number">2</span> + inexd_tmp*<span class="number">2</span>];</span><br><span class="line">    *r12 = data_shared[index + <span class="number">1024</span>*<span class="number">2</span> + inexd_tmp*<span class="number">3</span>];</span><br><span class="line">    *r13 = data_shared[index + <span class="number">1024</span>*<span class="number">3</span> + inexd_tmp*<span class="number">0</span>];</span><br><span class="line">    *r14 = data_shared[index + <span class="number">1024</span>*<span class="number">3</span> + inexd_tmp*<span class="number">1</span>];</span><br><span class="line">    *r15 = data_shared[index + <span class="number">1024</span>*<span class="number">3</span> + inexd_tmp*<span class="number">2</span>];</span><br><span class="line">    *r16 = data_shared[index + <span class="number">1024</span>*<span class="number">3</span> + inexd_tmp*<span class="number">3</span>];</span><br><span class="line">    wi_c = <span class="number">-2</span>*FFT_Pi/(<span class="number">16</span>*i);</span><br><span class="line">    index = tid / inexd_tmp;</span><br><span class="line">    StockhamR4_block(r1,  r2,  r3,  r4,  &amp;wi, wi_c, index + i*<span class="number">0</span>);</span><br><span class="line">    StockhamR4_block(r5,  r6,  r7,  r8,  &amp;wi, wi_c, index + i*<span class="number">1</span>);</span><br><span class="line">    StockhamR4_block(r9,  r10, r11, r12, &amp;wi, wi_c, index + i*<span class="number">2</span>);</span><br><span class="line">    StockhamR4_block(r13, r14, r15, r16, &amp;wi, wi_c, index + i*<span class="number">3</span>);</span><br><span class="line">    __syncthreads();</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">0</span> + <span class="number">256</span>*<span class="number">0</span>] = *r1;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">1</span> + <span class="number">256</span>*<span class="number">0</span>] = *r2;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">2</span> + <span class="number">256</span>*<span class="number">0</span>] = *r3;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">3</span> + <span class="number">256</span>*<span class="number">0</span>] = *r4;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">0</span> + <span class="number">256</span>*<span class="number">1</span>] = *r5;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">1</span> + <span class="number">256</span>*<span class="number">1</span>] = *r6;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">2</span> + <span class="number">256</span>*<span class="number">1</span>] = *r7;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">3</span> + <span class="number">256</span>*<span class="number">1</span>] = *r8;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">0</span> + <span class="number">256</span>*<span class="number">2</span>] = *r9;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">1</span> + <span class="number">256</span>*<span class="number">2</span>] = *r10;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">2</span> + <span class="number">256</span>*<span class="number">2</span>] = *r11;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">3</span> + <span class="number">256</span>*<span class="number">2</span>] = *r12;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">0</span> + <span class="number">256</span>*<span class="number">3</span>] = *r13;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">1</span> + <span class="number">256</span>*<span class="number">3</span>] = *r14;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">2</span> + <span class="number">256</span>*<span class="number">3</span>] = *r15;</span><br><span class="line">    data_shared[tid + <span class="number">1024</span>*<span class="number">3</span> + <span class="number">256</span>*<span class="number">3</span>] = *r16;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此外，将计算模块进行<strong>for循环优化</strong>，可以<strong>减少寄存器资源占用</strong>。从目前的实际情况来看，即使不存在依赖，代码编译后寄存器的占用和代码长度成正相关。所以应尽可能把层级计算整合到for循环里，以减少占用。 只要单个线程的寄存器占用在80左右，在本文的情况下，就可达到每个大核心3个block的最大占用。</p><p>优化前寄存器占用在100个左右，只能达到2个block的占用，优化后寄存器占用降到46个，提升明显。</p><p>最后，本文把大量使用的Stockham基-4蝶形计算单元封装为inline模块，也就是将stockham_fft_R4()中的核心循环：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; L_d4; j++)</span><br><span class="line">&#123;</span><br><span class="line">    wi1 = exp_calcu(direction* j*(<span class="number">2</span> * my_Pi / L));</span><br><span class="line">    wi2 = complx_mul(wi1, wi1);</span><br><span class="line">    wi3 = complx_mul(wi1, wi2);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; stride; k++)</span><br><span class="line">    &#123;</span><br><span class="line">        alpha = y[j*stride_m4 + k];</span><br><span class="line">        beta = complx_mul(wi1, y[j*stride_m4 + <span class="number">1</span> * stride + k]);</span><br><span class="line">        gamma = complx_mul(wi2, y[j*stride_m4 + <span class="number">2</span> * stride + k]);</span><br><span class="line">        delta = complx_mul(wi3, y[j*stride_m4 + <span class="number">3</span> * stride + k]);</span><br><span class="line">        tau[<span class="number">0</span>] = complx_add(alpha, gamma);</span><br><span class="line">        tau[<span class="number">1</span>] = complx_sub(alpha, gamma);</span><br><span class="line">        tau[<span class="number">2</span>] = complx_add(beta, delta);</span><br><span class="line">        tau[<span class="number">3</span>] = complx_sub(beta, delta);</span><br><span class="line">        <span class="comment">//tau[3]*wi(0,1)</span></span><br><span class="line">        tmp = tau[<span class="number">3</span>].x;</span><br><span class="line">        tau[<span class="number">3</span>].x = -tau[<span class="number">3</span>].y;</span><br><span class="line">        tau[<span class="number">3</span>].y = tmp;</span><br><span class="line">        x[(j + L_d4 * <span class="number">0</span>)*stride + k] = complx_add(tau[<span class="number">0</span>], tau[<span class="number">2</span>]);</span><br><span class="line">        x[(j + L_d4 * <span class="number">1</span>)*stride + k] = complx_sub(tau[<span class="number">1</span>], tau[<span class="number">3</span>]);</span><br><span class="line">        x[(j + L_d4 * <span class="number">2</span>)*stride + k] = complx_sub(tau[<span class="number">0</span>], tau[<span class="number">2</span>]);</span><br><span class="line">        x[(j + L_d4 * <span class="number">3</span>)*stride + k] = complx_add(tau[<span class="number">1</span>], tau[<span class="number">3</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>模块化为：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> __device__ <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">StockhamR4_block</span></span></span><br><span class="line"><span class="function"><span class="params">(Complex *a, Complex *b, Complex *c, Complex *d, Complex *wi, <span class="keyword">float</span> wi_c, <span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Complex tmp, alpha, beta, gamma, delta;</span><br><span class="line">    alpha = *a;                     *wi = exp_calcu(index * wi_c);  tmp = *wi;</span><br><span class="line">    beta  = ComplexMul(*wi, *b);    *wi = ComplexMul(*wi, *wi);</span><br><span class="line">    gamma = ComplexMul(*wi, *c);    *wi = ComplexMul(tmp, *wi);</span><br><span class="line">    delta = ComplexMul(*wi, *d);</span><br><span class="line">    tmp   = alpha;</span><br><span class="line">    alpha = ComplexAdd(alpha, gamma);</span><br><span class="line">    gamma = ComplexSub(tmp,   gamma);</span><br><span class="line">    tmp   = beta;</span><br><span class="line">    beta  = ComplexAdd(beta, delta);</span><br><span class="line">    delta = ComplexSub(tmp,  delta);</span><br><span class="line">    <span class="comment">//tau3*wi(0,1)</span></span><br><span class="line">    tmp.x   = delta.x;</span><br><span class="line">    delta.x = -delta.y;</span><br><span class="line">    delta.y = tmp.x;</span><br><span class="line">    *a = ComplexAdd(alpha,  beta);</span><br><span class="line">    *b = ComplexSub(gamma, delta);</span><br><span class="line">    *c = ComplexSub(alpha,  beta);</span><br><span class="line">    *d = ComplexAdd(gamma, delta);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基-4蝶形单元具有很好的计算特性，涉及的旋转因子复数乘法比基-8和基-16单元少很多，每个线程负责16个数据，也就是4个基-4单元的计算，实际测试结果表明，计算很好地掩盖了访问共享内存的延迟，本文实现的4096点FFT计算时间与cuFFT一致（这里是8192组4096点1维FFT计算时间测试）。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbiue2qu1u58fd69dt1thb1ma88d-20210605135115483.png" alt=""></p><p>cuFFT结果 ： 3.614ms<br>本文实现结果 ： 3.600ms</p><p>本文运行环境是WIN7 x64 + CUDA 7.5。</p><hr><h2 id="大点数FFT的计算方法"><a href="#大点数FFT的计算方法" class="headerlink" title="大点数FFT的计算方法"></a>大点数FFT的计算方法</h2><p>根据GPU硬件和FFT多级数据交换的特点，为了保证算法的运行时间，不能把FFT的数据交换放到显存里，尽量通过片上的共享内存进行。由于存储资源限制，理论上一次可以计算4096点（详见前述）。所以在实现大点数FFT时，首先想到的是把序列分解为多个4096点DFT，再用Stockham方法实现DFT的快速计算。</p><p>当序列长度n为复合数时，可以分解为一些因子的乘积，即混合基算法的基本原理。比如，128点的序列，用基-2算法需要7级计算，用基-2/4混合基的话需要3级基-4和1级基-2计算，减少了数据交换次数。在此，本文已经高效地实现了4096点FFT计算，所以可以将大点数序列分解成n=n0*4096的形式计算。</p><p>下面先说明混合基算法原理：</p><p>一维离散傅里叶变换被描述为: $\quad X_{k}=\sum_{j=0}^{n-1} x_{j} \omega_{n}^{j k}, 0 \leq k \leq n-1$<br>其中 $\omega_{n}=e^{-2 \pi i / n}, i=\sqrt{-1}, \mathrm{n}$ 是 $\mathrm{FFT}$ 的序列长度，且 $\mathrm{n}$ 具有因子 $n_{0}$ 和 $n_{1}\left(n=n_{0} \times n_{1}\right)$.</p><p>$\mathrm{j}$ 和 $\mathrm{k}$ 可以表示为:$j=j_{0} \times n_{1}+j_{1}, k=k_{1} \times n_{0}+k_{0}$<br>$x$ 和 $X$ 可用二维数组来描述:<br>$x_{j}=x\left(j_{0}, j_{1}\right), 0 \leq j_{0} \leq n_{0}-1,0 \leq j_{1} \leq n_{1}-1$<br>$X_{k}=X\left(k_{1}, k_{0}\right), 0 \leq k_{1} \leq n_{1}-1,0 \leq k_{0} \leq n_{0}-$<br>将上述式子带入离散傅里叶变换公式: $\quad x\left(k_{1}, k_{0}\right)=\sum_{j=0}^{n-1} \sum_{j_{0}=0}^{n_{0}-1} x\left(j_{0}, j_{1}\right) \omega_{n_{0}}^{j k_{0}} \omega_{n}^{j k_{0}} \omega_{n_{1}}^{j k_{1}}$<br>式(4-1)包含两步多重 FFT 计算。<br>第一步先计算 $n_{1}$ 次 $n_{0}$ 点 $\mathrm{FFT}$, 并将结果乘以一组旋转因子 $\omega_{n}^{j k_{0}}: \quad X_{1}\left(k_{0}, j_{1}\right)=\omega_{n}^{j / 1 / 0} \sum_{j_{0}=0}^{n_{0}-1} x\left(j_{0}, j_{1}\right) \omega_{n_{0}}^{j / 6_{0}}$<br>第二步计算 $n_{0}$ 次 $n_{1}$ 点 FFT: $\quad X_{2}\left(k_{0}, k_{1}\right)=\sum_{j=0}^{n-1} X_{1}\left(k_{0}, j_{1}\right) \omega_{n_{1}}^{j / 1 / 1}$<br>最终得到:</p><p>$$<br>X_{k}=X\left(k_{1}, k_{0}\right)=X_{2}\left(k_{0}, k_{1}\right)<br>$$</p><h3 id="以16384点DIF-FFT为例分析计算结构"><a href="#以16384点DIF-FFT为例分析计算结构" class="headerlink" title="以16384点DIF-FFT为例分析计算结构"></a>以16384点DIF-FFT为例分析计算结构</h3><p>16384=4*4096，n=16384，n0=4，n1=4096；j0=(0,3)，j1=(0,4095)；k0=(0,3)，k1=(0,4095)。<br>先按基-4抽取（每隔4096点抽一个数据）做4096组4点DIF-FFT，按所在序列乘以对应的，即的。<br>再计算4组4096点FFT得到结果。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bnfad4n95fa7c7cokkirn0m.png" alt=""></p><p>从数值计算的角度分析运算过程：</p><p>4096组4点FFT计算时，要注意，数据抽取是在16384点数据中以4096为跨度进行的（基-4抽取）。<br>由于是DIF-FFT计算结构，这里的基-4计算要按DIF的方式进行。<br>DIF-FFT的数据是顺序进，倒序出，所以在12行乘以旋转因子的时候是乘以倒序的旋转因子。<br>为了得到顺序的4块4096序列，计算结果的需要按照倒序放回（17行）。</p><p>由于在基-4计算中序列按MOD4抽取，所以在进行4组4096点FFT计算后，<br>要把4块数据进行合并才能得到数据的16384点FFT计算结果（25-29行）。</p><p>计算可以分为3个部分：4096组4点FFT计算，4组4096FFT计算和数据的抽取合并。用CUDA C实现的话，考虑到片上存储能力，每部分计算都要做成一个核函数（kernel），耗时是小点数的3倍，和官方库cuFFT两倍左右的耗时有较大的差距。</p><p>3个部分中包含大量计算的是中间的4组4096点FFT处理，最后一部分仅改变了数据位置。问题的关键在于，用DIF分解方式可以2步得到FFT计算结果，但这个结果的是分散在n0个数据块中的。如果要降低算法时间，一个可行的办法是结合第2和第3步，将第2步的数据跨越式地存存储。但用GPU实现时，跨越式地把数据存到显存中，会大大降低带宽利用率。要解决这个问题，就要构造出可以在与显存进行数据交换时可以连续访存的算法结构。经测试，使用单精度复数作为数据类型时，还需要保证数据的首地址对齐（128bytes的倍数）。<br>128bytes/8bytes = 16，即一个warp线程与显存数据交换的最小数量时16个单精度复数，那么底线就是32个线程（一个warp）中前16个线程和后16个线程访问的显存位置可以不一样，但在16个线程中，它们访问的数据必须是连续的，且首地址对齐128bytes。为达到这一要求，在FFT框架下即每次至少要计算16个数据块（因为，在DIF计算完成后，16个线程的数据是从不同数据块的对应位置抽取的），而当前GPU架构每次可以计算4096点数据，所以需要把大点数序列划分为以256为计算单元的多个数据块。比如16384点，就要划分为64组256点FFT，每次抽取16组到片上计算。</p><p>受到这些约束，16384点FFT计算的实际分解方式是：</p><p>16384=64*256，n=16384，n0=64，n1=256；j0=(0,63)，j1=(0,255)；k0=(0,63)，k1=(0,255)。<br>先按基-64抽取（每隔256点抽一个数据）做256组64点DIF-FFT，按所在序列乘以对应的，即的。<br>再计算64组256点FFT得到结果（在GPU上实现，有更细的划分）。</p><p>以上是理论说明。</p><h3 id="65536-4以上大点数的处理方法"><a href="#65536-4以上大点数的处理方法" class="headerlink" title="65536*4以上大点数的处理方法"></a>65536*4以上大点数的处理方法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">本节作为附加介绍</span><br></pre></td></tr></table></figure><p>为了实现更快速度，在FFT+点乘+IFFT。这样的频域算法计算中。<br>结合Cooley-Tukey和Stockham结构，中间结果不要求完全“顺序”：</p><p>顺序输入 &gt;&gt; Cooley-Tukey &gt;&gt; 倒序输出<br>65536*4/4096=64，即256K点数据可以分为64组4096点FFT计算。<br>先用DIF Cooley-Tukey 拆分4096点数据（见上一节理论和CTFFT计算结构示意图）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbip3o5v1lb91ml77n6pnsc6734.png" alt=""></p><p>可以看出DIF第一级计算以后，8点分治为两个4点计算（即后续的计算两个4点之间没有依赖）</p><p>按照4096点Stockham结构，计算64组数据，即可得到FFT计算结果。<br>注意，此时的数据是按64块倒序存放的（注意描述，按分块倒序，块内数据是“顺序”的）<br>因为，已经得到结果，为了节省时间，不再用一个kernel得到顺序结果。</p><p>点乘。完成频域计算。</p><p>进行64组4096点IFFT</p><p>利用DIT Cooley-Tukey结构处理64块4096点数据<br>（在stage0不用倒序，因为数据已经按块倒序了）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbip3hfrba1d9h11uu13da1hmq2n.png" alt=""></p><p>所以计算步奏如下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1、DIF Cooley-Tukey 拆分数据（顺序输入，按块倒序输出）</span><br><span class="line">2、计算64组4096点Stockham FFT</span><br><span class="line">3、频域点乘</span><br><span class="line">4、计算64组4096点IFFT</span><br><span class="line">5、DIT Cooley-Tukey 合并数据（按块倒序输入，顺序输出）</span><br></pre></td></tr></table></figure><p>利用Cooley-Tukey FFT的计算结构，拆分大点数计算。<br>利用Stockham FFT 向量化的计算结构，进行大部分计算。<br>计算步骤2、3、4可以合并为一个kernel。</p><p><strong>整体时间为4个访存密集kernel时间</strong>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">而调用cuFFT的话，从这个数据长度开始 FFT，IFFT都需要3个访存密集kernel时间</span><br><span class="line">再加上点乘调用，共需要7个kernel时间。</span><br></pre></td></tr></table></figure><p><strong>所以实际工程中，可以加速2倍左右。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这一方法在本文框架中，可以处理65536*4到65536*256的计算规模：</span><br><span class="line">65536*4=4096*16</span><br><span class="line">65536*256=4096*4096</span><br></pre></td></tr></table></figure><hr><h4 id="时间测试"><a href="#时间测试" class="headerlink" title="时间测试"></a>时间测试</h4><p>16384点<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bnfbvd6hq7t1kq527mluj1dsf13.png" alt=""></p><p>cuFFT结果 ： 3.642 + 3.665 = 7.307ms<br>本文实现结果 ： 4.215 + 3.740 = 7.955ms （稍慢，实际整合频域算法的话，快1/3的时间）</p><p>…</p><p>65536*4点<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bnfcb2h79vm1c4h1ldp1ojuhqu1g.png" alt=""></p><p>cuFFT结果 ： 3.65*6 + 3.65 = 25.55ms （3FFT 3IFFT 1频域计算）<br>本文实现结果 ： 3.85 + 6.54 + 3.85 = 14.24ms （注意，234步骤已整合到第二个kernel）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">补充一点，在这一维数据长度，调用cuFFT，库自己拆分为三个kernel进行计算，非用户操纵。</span><br></pre></td></tr></table></figure><p>补充公式</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210605141333528.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;一维FFT算法在Maxwell架构上，归为访存密集算法。&lt;br&gt;即，在足够优化的情况下，可在一次memory copy的耗时内完成计算。&lt;/p&gt;
&lt;p&gt;本文实现的FFT算法达到与官方库cuFFT一致的速度，通过整合kernel，可实现比调用CUFFT更快的算法整体执行速度。在处理65536*4以上大点数一维FFT+IFFT计算时（一个大核心共享内存放不下完整的一维FFT数据），组合算法可以实现比CUFFT少2个kernel调用的时间（减少两次显存数据交换），主要说明4096点FFT算法设计的思路及实现。大点数仅说明方法和测试结果。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CUDA" scheme="http://yuanquanquan.top/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>一维卷积的SASS实现</title>
    <link href="http://yuanquanquan.top/2021/20210604/"/>
    <id>http://yuanquanquan.top/2021/20210604/</id>
    <published>2021-06-03T20:51:23.000Z</published>
    <updated>2021-06-04T16:19:49.391Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本项目使用MaxAs提供的汇编器编写GPU一维卷积算法</p><p>有16点和1024点卷积核两种实现，SASS代码在目录下，两个VS2013工程在文件夹中</p><p>实验环境：WIN7 + VS2013 + CUDA 6.5 + MaxAs (SASS代码已注入cubin，运行不需要MaxAs)</p><hr><p>16点卷积和1024点卷积恰好可划分在访存密集和计算密集这两个类型中，是比较好的练手项目。</p><p>1024点卷积在Maxwell架构下达到硬件75%的峰值算力，还在找未达到90%以上的原因。</p></blockquote><span id="more"></span><h2 id="1、一维卷积计算过程"><a href="#1、一维卷积计算过程" class="headerlink" title="1、一维卷积计算过程"></a>1、一维卷积计算过程</h2><p>线性卷积: $y(n)=x(n) * h(n)$</p><p>设x(n)长度为N1，h(n)长度为N2 ， 则y(n)长为 N=N1+N2-1<br>为方便表示，在序列x(n)后添N2-1个0 ，使x(n)的长度变为 N</p><p>卷积公式 ： $y(n)=\sum_{i=0}^{N-1} x(i) \times h(n-i), \quad 0 \leq n \leq N-1$</p><p>可用矩阵表示为：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avo8jtg41p6e5egvii1vt9uec1t.png" alt=""></p><p>一个简单的计算过程示意：</p><p>x(n)=[4,3,2,1]<br>h(x)=[3,2,1]<br>y(n)=x(n)*h(n)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x  |        4  3  2  1        | </span><br><span class="line">   |                          | </span><br><span class="line">   y0 |  1  2  3                 | = 4*3              = 12</span><br><span class="line">   y1 |     1  2  3              | = 4*2 + 3*3        = 17</span><br><span class="line">   y2 |        1  2  3           | = 4*1 + 3*2 + 2*3  = 16</span><br><span class="line">   y3 |           1  2  3        | = 3*1 + 2*2 + 1*3  = 10</span><br><span class="line">   y4 |              1  2  3     | = 2*1 + 1*2        = 4</span><br><span class="line">   y5 |                 1  2  3  | = 1*1              = 1</span><br></pre></td></tr></table></figure><p>为保证局部性：</p><p>在实现时，卷积核H的数据先放到片上<br>（H点数较小时直接放到寄存器，点数稍大可放在共享内存，过大时分批从片外内存读取）</p><p>实际的原始数据X，点数较大，分块（blocking）处理。</p><p>Y的计算不按照上例中的一次性计算$y^2=4 <em>1+3</em>2+2*3=16$ 。<br>而是 多次乘加 得到结果 ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//Y初始化为0</span><br><span class="line">regX = shareadX[0];      //共享内存读取x</span><br><span class="line">Y0  += regX*H1;</span><br><span class="line">Y1  += regX*H2;</span><br><span class="line">Y2  += regX*H3;</span><br><span class="line">regX = shareadX[1];      </span><br><span class="line">Y1  += regX*H1;   </span><br><span class="line">Y2  += regX*H2;</span><br><span class="line">Y3  += regX*H3;</span><br><span class="line">...</span><br><span class="line">//计算完成后把Y 在共享内存reshape 然后放回片外</span><br></pre></td></tr></table></figure><p>上例每次读一个X。实现时，X批量读入，计算掩盖访存延迟。</p><h2 id="2、两个实现：16点卷积与1024点卷积"><a href="#2、两个实现：16点卷积与1024点卷积" class="headerlink" title="2、两个实现：16点卷积与1024点卷积"></a>2、两个实现：16点卷积与1024点卷积</h2><h3 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h3><table><thead><tr><th style="text-align:center">算法</th><th style="text-align:center">卷积</th></tr></thead><tbody><tr><td style="text-align:center">数据量</td><td style="text-align:center">N*2 (输入X / 输出Y / H较小不计入)</td></tr><tr><td style="text-align:center">计算量</td><td style="text-align:center">N*N2*2 (乘加按两个指令计算)</td></tr><tr><td style="text-align:center"><strong>硬件</strong></td><td style="text-align:center">GTX970</td></tr><tr><td style="text-align:center">峰值计算能力</td><td style="text-align:center">1.25GHz <em> 1664cores </em> 2 = 4160 GFLOPS（实测达到）</td></tr><tr><td style="text-align:center">访存带宽：</td><td style="text-align:center">224GB/S （实测连续访存140GB/s ，即35G/s的FP32数据）</td></tr></tbody></table><p>算法计算强度 $=\frac{\text { 计算量 }}{\text { 数据量 }}=\frac{2 <em> N </em> N 2}{2 N}=N 2$</p><p>(即一维卷积计算算法强度与卷积核长度直接相关： 16点卷积计算强度为16， 1024点卷积计算强度为1024)</p><p>硬件计算强度 $=\frac{\text { 浮点计算能力 }}{\text { 浮点带宽 }}=\frac{4160 \mathrm{GFLOPS}}{35 \mathrm{GFloatData}}=118.8$</p><p>所以16点卷积计算 ：受限于带宽 （16 &lt; 116.8 访存密集）<br>1024点卷积计算 ： 受限于核心计算能力（1024 &gt; 116.8 计算密集）</p><p>所有算法都可以从这一角度，划分为上述两个类型之一。<br>计算密集的算法，可以针对硬件架构进行优化。</p><h3 id="16点卷积"><a href="#16点卷积" class="headerlink" title="16点卷积"></a>16点卷积</h3><p>一维卷积核H：16 float （直接拷贝到寄存器）<br>输入数据量X：1024000 float<br>输出数据量Y：1024000 + 16 float （最后16个Y为了方便，没有进行计算）<br>线程结构 : 32个thread一个blcok，每个thread计算16点Y，一共需要2000个block。<br>片上存储 ： 分配544 float * 4 bytes = 2176 bytes 大小的共享内存空间用于存储。</p><h4 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a><strong>存储结构</strong></h4><p><strong>Shared存储</strong><br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avomqh2tgdqqvvct61ubn3u583.png" alt=""></p><p>每个block负责32*16=512点Y计算<br>32个threads 对显存合并访问 读取X<br>每个thread进行4次LDG.128访问 + 一次LDG访问（32bit，末尾的32个X，只用到1/2的数据）<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1be05en8318a1vtp1fqd1jetng5m.png" alt=""></p><p><strong>Shared访存</strong><br>整体来看，一个线程需要访问8次共享内存（128bit形式）<br>eg：<br>第一次，所有线程访问第一行，第二次第二行，第三次第三行，第四次第四行<br>第五次时，需要向左移一列访问，接下来再访问三行</p><p>（注：图看不清的话，可以点击放大）<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avolnfikeh316dp1crrcciqgt7m.png" alt=""></p><p>以thread 0为例 ，蓝色框是其访存范围，红色线条是访存顺序。<br>黑框是 thread 1 的访存范围，前后两个线程共享16个 float 数据<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avon1gnd1c38v0j1clh15kt78d8g.png" alt=""></p><p>这样的LD/ST方法，利用了完整的共享内存带宽。</p><hr><h4 id="算法结构"><a href="#算法结构" class="headerlink" title="算法结构"></a><strong>算法结构</strong></h4><p>计算示意（结合 1、一维卷积计算过程 理解）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avonaiq71mp1i8p11od4qnfka8t.png" alt=""></p><p>完全避免寄存器冲突的办法</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avonpv7ieokp3dis41r66f289a.png" alt=""></p><h3 id="1024点卷积"><a href="#1024点卷积" class="headerlink" title="1024点卷积"></a>1024点卷积</h3><p>计算的核心循环与16点卷积一致，32点X与16点H参与计算。<br>区别在于下一次循环还要使用32个当前计算X中的的后16个；下一次计算使用的H是新的16个。</p><p>理论分析：由于一个小循环算完16点Y，数据都在片上，所以再套上大循环，对X、Y做双缓冲不会有速度提升<br>实际处理：H加双缓冲无效，寄存器双缓冲预取共享内存中的X，计算掩盖延迟。</p><p>一维卷积核H：1024 float<br>输入数据量X：2097152 float<br>输出数据量Y：2097152+1024 float （最后1024个Y为了方便，没有进行计算）<br>线程结构：128个thread一个blcok，每个thread计算16点Y，一共需要1024个block。</p><h4 id="存储结构-1"><a href="#存储结构-1" class="headerlink" title="存储结构"></a><strong>存储结构</strong></h4><p>X数据存储与16点类似，一个block存128线程<em>(4float</em>6次加载)=3072个单精度浮点。<br>还需要1024<em>4byes存放H。<br>共分配4096</em>4bytes的共享内存空间<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avoqm9741tr17n6b7jd4f1eufbb.png" alt=""></p><h4 id="计算结构"><a href="#计算结构" class="headerlink" title="计算结构"></a><strong>计算结构</strong></h4><p>block内计算示意<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avop9ch21fqrovk1nrsk9k1821a4.png" alt=""></p><p>block间计算示意<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avopgujc51q1fhf1nhf1j7iu8sau.png" alt=""></p><blockquote><p>注:Y存回片外时通过共享内存reshape，有bank conflict，只用到了共享内存1/4的完整带宽。<br>但实测对kernel执行时间基本没有影响（占比太小）。</p></blockquote><p>SASS代码：<br><a href="https://github.com/Velaciela/1D-convolution-with-SASS/blob/master/conv1024_nobuffer.sass">conv1024_nobuffer.sass</a></p><hr><h2 id="3、实验结果"><a href="#3、实验结果" class="headerlink" title="3、实验结果"></a>3、实验结果</h2><h3 id="16点卷积-1"><a href="#16点卷积-1" class="headerlink" title="16点卷积"></a>16点卷积</h3><p>每个线程使用60个寄存器（优化后），恰好达到1个SM上32个Block的占用上限。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avoeeebb182op4fusfj8s1r053u.png" alt=""></p><p>实测，只要一个SM上达到6个Block的占用，就可占满带宽。<br>6占用kernel执行时间和32个占用的时间一致。<br>占用多了，带宽跟不上，还是需要等待。</p><p>数据量(32*16*2000)*2 = 2*1024000FP32 = 2*1024000*4bytes/1024/1024 = 7.8125MB</p><p>下图可知，kernel平均计算时间为56us<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avof1ebn1grl7jo7am9qmso54o.png" alt=""></p><p>带宽占用： 7.8125MB / 0.056ms = 139.51GB/s<br>与理论分析一致</p><hr><h3 id="1024点卷积-1"><a href="#1024点卷积-1" class="headerlink" title="1024点卷积"></a>1024点卷积</h3><p>Visual Profiler （nvvp）分析：</p><p>带宽未占满<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avofvn3l1ngb1l8a10r2ulobpl5v.png" alt=""></p><p>计算指令占比接近90%<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avofsmlrdqc1bq7415114d11o65i-20210604233547601.png" alt=""></p><p>SM占用6<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avog5jnc1g2tapc1iej1f7negm6p-20210604233513105.png" alt=""><br>实测占用 6、4、2个block的情况，kernel执行时间一致。<br>由于计算能力限制，占用多了也算不过来。</p><p>kernel执行时间1.4ms<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avog45dp1nhm14qc15ho14m9e86c.png" alt=""></p><table><thead><tr><th style="text-align:center">结果</th><th style="text-align:center">达到硬件峰值计算能力的74% （未达到90%）</th></tr></thead><tbody><tr><td style="text-align:center">数据量</td><td style="text-align:center">输入X+输出Y = 2<em>2097152 float = 2</em>2097152*4bytes/1024/1024 = 16MB</td></tr><tr><td style="text-align:center">kernel时间</td><td style="text-align:center">1.4ms</td></tr><tr><td style="text-align:center">带宽占用</td><td style="text-align:center">16MB / 1.4ms = 11.42GB/s</td></tr><tr><td style="text-align:center">kernel算力</td><td style="text-align:center">2097152_X输入<em>1024_H卷核</em>2_乘加 / 1.4ms = 4.295 GigaFloatOp / 0.0014s = 3067.86 GFLOPS</td></tr><tr><td style="text-align:center">硬件算力</td><td style="text-align:center">1.25GHz <em> 1664cores </em> 2 = 4160 GFLOPS</td></tr></tbody></table><hr><p><strong>数据生成及结果验证</strong></p><p>原始数据X、卷积核H和结果Y的长度分别为N、M、P。<br>卷积核与原始数据用伪随机数填充。<br>CPU计算的卷积结果放在数据块T中。<br>GPU计算的卷积结果拷贝到数据块Y中。</p><p>打印前1024个T与Y的误差<br>Y与T差的绝对值大于1则打印出来（计算精度所在位置随数据大小变化）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1avoclkfcqsai1f1uu71sua123u3h.png" alt=""></p><h2 id="4、小结"><a href="#4、小结" class="headerlink" title="4、小结"></a>4、小结</h2><p>常用的优化方法：<br>调整算法结构（匹配硬件特性），kernel间整合（减少访存），kernel优化（数值计算过程与汇编）</p><p>16点卷积和1024点卷积恰好可划分在访存密集和计算密集这两个类型中。<br>是比较好的练手项目。</p><p>访存密集型的算法不需要用汇编优化，保证基本的合并访存即可。<br>但一些算法结构复杂，需要仔细分析，利用好片上和片外内存的带宽。</p><p>1024点卷积kernel应该达到90%+的峰值算力，未找到受限原因。<br>理论上1024点卷积不需要嵌套循环，不用对X做双缓冲预取，因为当前计算的Y与下一大组X没有数据依赖关系。</p><p>按现在的理解，在SM上block占用低于一个阈值的情况下，双缓冲可以带来一定的计算能力提升。<br>在占用足够的情况下，线程级并行、线程间的切换，其实和双缓冲是等效的。<br>（2017补充）</p><p>实际工程中常用FFT在频域进行快速卷积。</p><h2 id="5、后续"><a href="#5、后续" class="headerlink" title="5、后续"></a>5、后续</h2><p>二维卷积<br>FFT</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本项目使用MaxAs提供的汇编器编写GPU一维卷积算法&lt;/p&gt;
&lt;p&gt;有16点和1024点卷积核两种实现，SASS代码在目录下，两个VS2013工程在文件夹中&lt;/p&gt;
&lt;p&gt;实验环境：WIN7 + VS2013 + CUDA 6.5 + MaxAs (SASS代码已注入cubin，运行不需要MaxAs)&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;16点卷积和1024点卷积恰好可划分在访存密集和计算密集这两个类型中，是比较好的练手项目。&lt;/p&gt;
&lt;p&gt;1024点卷积在Maxwell架构下达到硬件75%的峰值算力，还在找未达到90%以上的原因。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CUDA" scheme="http://yuanquanquan.top/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>SEGMM的CUDA Core实现</title>
    <link href="http://yuanquanquan.top/2021/20210602/"/>
    <id>http://yuanquanquan.top/2021/20210602/</id>
    <published>2021-06-02T15:20:08.000Z</published>
    <updated>2021-06-03T20:06:37.805Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>SGEMM：Single precision floatiing General Matrix Multiply</p><p>本文的SGEMM实现思路整理于MaxAs wiki和对MaxAs项目SGEMM汇编代码的理解。<br>MaxAs是一个开源的Maxwell架构GPU汇编器： <a href="https://github.com/NervanaSystems/maxas">Github链接</a><br>作者<a href="https://github.com/scott-gray">Scott Gray</a>提到两篇相关论文<a href="http://icl.cs.utk.edu/projectsfiles/magma/pubs/fermi_gemm.pdf">MAGMA paper</a>和<a href="https://hal.inria.fr/file/index/docid/789958/filename/112_Lai.pdf">Kepler sgemm paper</a></p></blockquote><span id="more"></span><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>​        $A=\left(a_{i j}\right) $ 是一个$ m \times s$ 矩阵, $B=\left(b_{i j}\right) $是一个 $s \times n $矩阵,规定矩阵A与矩阵B的乘积是一个$m \times n$矩阵$,C=\left(c_{i j}\right)$其中$c_{i j}=a_{i 1} b_{1 j}+a_{i 1} b_{1 j}+\cdots+a_{i s} b_{s j}=\sum_{k=1}^{s} a_{i k} b_{k j} \quad(i=1,2, \ldots, m ; \quad j=1,2, \ldots, n)$,把这个乘积记做$C=A B$。</p><p>​        由定义可知，计算一个元素$c_{i j}$的时间复杂度是$O(N)$，矩阵$C$中有$N^2$个数据，矩阵乘法的时间复杂度为$O(N^3)$。</p><h3 id="计算示例"><a href="#计算示例" class="headerlink" title="计算示例"></a>计算示例</h3><p>$A=\left(\begin{array}{cc}-2 &amp; 4 \ 1 &amp; -2\end{array}\right) \quad B=\left(\begin{array}{cc}2 &amp; 4 \ -3 &amp; -6\end{array}\right)$</p><p>$c_{11}=A$ 的第一行 $\cdot B$ 的第一列 $=a_{11} b_{11}+a_{12} b_{21}=-2 \times 2-3 \times 4=-16$</p><p>$c_{12}=A$ 的第一行 $\cdot B$ 的第二列 $=a_{11} b_{21}+a_{12} b_{22}=-2 \times 4-4 \times 6=-32$</p><p>$c_{21}=A$ 的第二行 $\cdot B$ 的第一列 $=a_{21} b_{11}+a_{22} b_{21}=1 \times 2+2 \times 3=8$</p><p>$c_{22}=A$ 的第二行・ $B$ 的第二列 $=a_{21} b_{21}+a_{22} b_{22}=1 \times 4+2 \times 6=16$</p><p>即 $C=\left(\begin{array}{cc}-2 &amp; 4 \ 1 &amp; -2\end{array}\right)\left(\begin{array}{cc}2 &amp; 4 \ -3 &amp; -6\end{array}\right)=\left(\begin{array}{cc}-16 &amp; -32 \ 8 &amp; 16\end{array}\right)$</p><h2 id="计算结构"><a href="#计算结构" class="headerlink" title="计算结构"></a>计算结构</h2><h3 id="朴素算法"><a href="#朴素算法" class="headerlink" title="朴素算法"></a>朴素算法</h3><p>解题时的书面计算习惯，通常是在A、B两矩阵中各取第i行、第j列，按照定义点乘累加，计算矩阵C中的一个元素$c_{i j}$<img src="http://static.zybuluo.com/Velaciela/72psazacnux5gx9lbdtnsd1h/image_1bbb66jog11h2p5d1uta9c1jh49.png" alt=""></p><p>设矩阵A、B和C都是NxN的矩阵，则有朴素算法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">    <span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(k=<span class="number">0</span>;k&lt;n;k++)</span><br><span class="line">            sum += A[i][k]*B[k][j];</span><br><span class="line">        C[i][j] += sum;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>​    由图示可知，使用朴素算法实现时，不连续地读取矩阵B的一列数据（内存空间数据跨度为N），空间局部性很差。<br>​    另外，与$c_{i j}$同行或同列的C元素，都由相同行或列的A、B数据计算得到，但朴素算法每个核心循环只计算一个元素$c_{i j}$，不能重复利用缓存中的矩阵数据（N较大的情况下，还没轮到下一个循环，cache空间已经被新数据占用了，不能命中，A的首次不命中概率为：1/cache字长，B的不命中概率为1），同行、列的数据利用率只有1/N，所以时间局部性也很差。</p><p>计算量分析：</p><p>​    设A、B和C都为N阶方阵，则计算一个元素$c_{i j}$的访存量为$2N+1$，计算量是$2N$（乘加按两次操作算）单位访存的计算量为：1</p><p>​    各类硬件的单位片外访存时间 远高于 单位片内计算操作的时间消耗，所以这个计算结构是访存密集的。算法的执行时间受访存带宽限制，大多数时间里，计算单元在等待数据加载。</p><h3 id="分块算法"><a href="#分块算法" class="headerlink" title="分块算法"></a>分块算法</h3><p>​    分块（blocking）技术可以提高内循环的局部性（locality）。分块的大致思想是将数据结构组织成的片（chunks）称为块（block）。（在这个上下文中，“块”指的是一个应用级的数据块，而不是高速缓存块。）这样构造程序，使得能够将一个片加载到高速缓存中，并在这个片中进行所需的所有读写，然后丢掉这个片，加载下一个片，依此类推。</p><p>​    不同于为提高空间局部性做简单的循环变换（朴素算法有ijk、jki、kij几类循环方式，内层循环数据加载对应AB、AC和BC，BC加载类型每次迭代的总不命中次数最少），分块虽然在一些系统上能获得很大的性能收益，但也使得代码更难阅读和理解，更适合优化编译器或者频繁执行的库函数。</p><p>​    根据硬件资源，划分A、B、C子块的大小，使它们都能放到片上存储。每取一列A数据和一行B数据，都能计算n*n次C数据，多次取数据点乘累加得到完整的$c_{i j}$数值。对于矩阵C（或矩阵C子块）中的一点，每次读取AB的2组n点数据后，核心计算为：$c_{i j}=a_{i k} b_{k j}+c_{i j}$<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbbq122dk9i1e2nn15boakgjm.png" alt=""></p><p>计算量分析：</p><p><strong>注：分析针对每个block的核心循环，故按分块，分析小矩阵。</strong></p><p>​    假设A子矩阵大小为$n \times K$，B子矩阵大小为$K \times n$，C子矩阵大小为$n \times n$，则分块矩阵乘法的访存量为$2Kn$（<strong>注：在核心循环里不访问C，计算完成后才放回，故不计入$n^2$</strong>)，计算量是$n \times n \times K \times 2=2 K n^{2}$，即K组对应的行列都有$2n^2$次计算，乘加计算视为两次操作。</p><p>​    单位访存的计算量为（即kernel的<strong>计算强度</strong>）：$\frac{2 K n^{2}}{2 K n}=n$,即n取值越大（子块越大），单位访存的计算量越大。n=128时，单位访存的计算量为128.</p><h3 id="算法与硬件性能"><a href="#算法与硬件性能" class="headerlink" title="算法与硬件性能"></a>算法与硬件性能</h3><table><thead><tr><th style="text-align:center">显卡</th><th style="text-align:center">GTX970</th></tr></thead><tbody><tr><td style="text-align:center">峰值计算能力</td><td style="text-align:center">4TFLOPS （乘加实测达到）</td></tr><tr><td style="text-align:center">访存带宽</td><td style="text-align:center">224GB/S （实测连续访存140GB/s ，即35G/s的FP32数据）</td></tr></tbody></table><p>硬件的单位访存计算能力 = 浮点计算能力/浮点带宽 = 4000GFLOPS/(35G FP32 DATA) = 114.3</p><p>1&lt;&lt;114.3&lt;128</p><p>所以，就GTX970来说，128分块矩阵的计算需求比硬件能力稍大，可以完整占用计算资源，是很好的实现方式。<br>属于计算密集算法，需要在算法结构和汇编细节上做优化，才能利用好硬件算力。</p><h2 id="MaxAs中的SGEMM实现"><a href="#MaxAs中的SGEMM实现" class="headerlink" title="MaxAs中的SGEMM实现"></a>MaxAs中的SGEMM实现</h2><p>按照上一小节的分块算法，构造计算结构，尽可能重复利用从各个存储层次获取的数据。在GPU上，也就是：</p><p><strong>显存&gt;&gt;L2缓存&gt;&gt;纹理缓存&gt;&gt;寄存器&gt;&gt;共享内存&gt;&gt;寄存器&gt;&gt;指令操作数缓存&gt;&gt;寄存器&gt;&gt;显存</strong></p><p>(其中，指令操作数缓存是Maxwell架构的新特性)<br>上述的每个数据路径都有延迟，要通过指令级并行和线程级并行（instruction and thread level parallelism ，ILP &amp; TLP）来掩盖。此外，还存在存储单元和合并访存的限制（banking and coalescing constraints）。作者的SGEMM代码可以在这些限制下，使算法的计算效率达到98%的GPU峰值计算性能。</p><p>这里，先说一下最终采用的基本计算模型，稍后会解释参数选择的原因。<br>把A、B矩阵划分为多个128x8的小块，如图所示，将矩阵A中所有横向数据块和矩阵B中所有纵向数据块带入计算，乘加，最后得到128x128的C矩阵子块的结果。一个时间段内，只需要拷贝A、B各一片对应数据块到片上共享内存，计算过程里，128x128点数据的中间结果保存在各线程的片上寄存器里，图中两组蓝色长条数据块都计算完成后，再把矩阵C的128x128子块计算结果放回显存。<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bb8hga1815hsfle1b0p1mfr1gcv9-20210604040513431.png" alt=""></p><h3 id="关键结构与参数"><a href="#关键结构与参数" class="headerlink" title="关键结构与参数"></a>关键结构与参数</h3><h4 id="双缓冲"><a href="#双缓冲" class="headerlink" title="双缓冲"></a><strong>双缓冲</strong></h4><p>使用大小为8的<strong>双缓冲寄存器块</strong>（double buffered）存放从显存加载的矩阵数据。<br>双缓冲能基本掩盖数据传输延迟：在计算一组数据的同时加载下一组数据。<br>寄存器块的寄存器数量设为8，可以配合长度为4的<strong>向量化内存加载指令</strong>（quad vector memory instructions），同时保证每个线程的寄存器使用数量在128个以下。超过128个寄存器的界限的话，SM的block占用会下降。</p><p>对共享内存也做双缓冲处理，这样可以移除一个本来需要的在主循环中使用的BAR.SYNCS：在存<br>储下一组数据时等待所有共享内存加载完成（按上下文应该是shared加载到reg）。<br>有了双缓冲以后，数据可以直接写入另一块的共享内存区域，各线程仍可以从之前的共享内存区域读取数据。这样做需要在主循环中增加三条XOR指令，但（时间）代价比BAR.SYNCS小。</p><p>每个block有256个线程，每个线程计算88=64点子矩阵C的数据，<br>每个block处理一块正方形的子矩阵C，方形的边长就是所有点数的平方根。<br>可以计算得到：$\sqrt(256*88)$ = $128 units wide$</p><h4 id="展开因子"><a href="#展开因子" class="headerlink" title="展开因子"></a><strong>展开因子</strong></h4><p>​    展开因子(unroll factor)对应每次从A和B中读入的行数（global&gt;&gt;shared），也是从共享内存中存储、加载以及计算数据的量。展开因子的选择有两个考虑：一方面希望用尽可能多的计算来掩盖纹理内存加载延迟，另一方面不希望循环大小超过指令缓冲区大小。否则，额外增加的取指延迟，又要想办法掩盖。作者测得Maxwell的<strong>指令缓冲区大小</strong>为8KB，所以不能让主循环大小超过1024个8byte指令，而且4个指令一组，其中的第4个指令是控制代码（control code），所以实际可用指令数限制为768个。另外还有指令对齐方面的考虑,所以指令数最好低于768。</p><p>​    综上考虑，把展开因子设为8，8*64=512个FFMA指令（浮点乘加）加上循环所需的访存指令和整数计算指令（大约40个），低于768。每个循环8行展开也能较好地配合纹理内存加载，512个FFMA计算足够掩盖200+时钟的纹理<strong>内存加载延迟</strong>。</p><h4 id="共享内存大小"><a href="#共享内存大小" class="headerlink" title="共享内存大小"></a><strong>共享内存大小</strong></h4><p>​    共享内存大小，由每个block的加载宽度乘以展开因子得到。需要256线程：每个循环处理8行，每个block加载宽度128，单精度浮点数据字长4bytes，矩阵A数据双缓冲，矩阵B数据双缓冲 &gt;&gt; 需要$8<em>128</em>4<em>2</em>2 = 16384$ bytes 的共享内存空间，Maxwell架构下：一个SM有65536个32bit的寄存器，98304 bytes的共享内存空间。<br>​    98304/16384 = 6<br>​    65536/(256线程*128寄存器) = 65536 / 32768 = 2,也就是说，共享内存大小可供6个block占用，但由于寄存器资源不够，只能得到2个blcok的占用。这里，共享内存的大小不是SM上block占用的主要因素，SGEMM的占用主要受寄存器资源的影响。（占用多不代表算得快，快慢主要受计算和访存能力影响。这里2个block的占用已经足够了）</p><h3 id="256线程算法实现"><a href="#256线程算法实现" class="headerlink" title="256线程算法实现"></a>256线程算法实现</h3><h4 id="加载A、B到共享内存"><a href="#加载A、B到共享内存" class="headerlink" title="加载A、B到共享内存"></a><strong>加载A、B到共享内存</strong></h4><p>​    作者把线程分为两部分，一半线程加载一个矩阵。256线程的话，就是4个warp（128线程）加载一个矩阵。条件加载在cuda中优化得不好，因为编译器不会判断加载是否以warp为单位而做优化。对于纹理内存加载来说（MaxAs中的SGEMM用了纹理，但相关的开源项目，后来不使用纹理，直接加载global），分开处理是必要的，因为<strong>每个warp的指令在一个时间只能处理一个纹理内存</strong>（分开加载更高效）。<strong>编译器会给纹理加载加入warp切换和分支</strong>（warp shuffles and branch），还有同步指令来强制执行。如果Nvidia给出以warp为单位的条件判断结构会很好（而不是仅仅有分支，ie：bra.uni）。</p><p>​    一个线程来只负责加载某一个矩阵，所以只需要一组索引寄存器（track registers）来存放纹理加载的索引。主循环里的整数加法指令因此减少一半，this is a big win 。在核心循环中，要抓住任何提升FFMA指令/非FFMA指令比例的机会。</p><p>​    另外，维持4个单独的索引变量以避免使用依赖栅栏（dependency barriers），每次纹理加载后给索引寄存器加上加一次的地址偏移。在架构上，<strong>访存指令发出后，并不会保存它调用的寄存器值</strong>，这样做可能是为了节省硬件资源。指令发出后，访存工作仍在执行(in flight)，而地址索引仍保存在一个寄存器中，这就需要用一个栅栏来保证不对相关寄存器进行写入。栅栏等待不一定就是坏事，线程级并行（TLP）可以掩盖延迟，但减少整体的延迟有助于提高性能，<strong>提高有可用warp来掩盖延迟的几率</strong>。</p><p>​    接下来就是从纹理单元加载。使用显式的纹理加载，而不用全局加载或不连续缓存<strong>（?）</strong>，有两个好处。一是代码更简单，不需要担心加载超出边界。二是，同样的kernel代码可以加载8 bit或16 bit浮点数据，大大减少带宽和存储的需求，对于一些不需要32 bit精度应用很有优势。</p><p>​    此外，对访存做4单位的<strong>向量化加载</strong>。cublas没有使用向量加载，因为输入数据有4字长对齐的限制，不能适用于普遍情况，cublas有自己固定的形式。使用4点向量化数据加载后索引偏置常数lda和ldb相应减小4倍，附带的好处是加载矩阵的索引可以增加到31bits，而普通纹理加载受限于27bits。另一个vec4加载的效果是，这一访存模式每次读取都占用整个缓冲区，缓存性能主要受L2限制（这点不太明白）。</p><h4 id="从共享内存加载数据到寄存器"><a href="#从共享内存加载数据到寄存器" class="headerlink" title="从共享内存加载数据到寄存器"></a><strong>从共享内存加载数据到寄存器</strong></h4><p>​    每个线程从共享内存的A、B区域读取数据，原则是避免bank conflict。根据文档，只要访存在32字长（128bytes）以内即可。warp内各线程进行vec4加载，某几个线程从相同地址加载，通过广播机制，可以在128 btyes的限制下读取数据。Maxwell的文档说明并不完整，<strong>在一些情况下，即使一个warp中的线程访问的数据在128 bytes以内也有触发存储冲突的情况</strong>（理论上，Maxwell架构的共享内存有32个32bit端口，同时读取的数据超过128 bytes带宽，就要排队等待，同时读取的数据占用相同端口，也要排队等待，即访存冲突。另外，实测warp内不连续或不同深度的访问，时间也会增加）。</p><p>​    128 bytes带宽可供加载8组16 bytes的4点向量化数据(vec4)。在这一限制下从A、B共享内存中加载。每次从共享内存块加载的宽度为4*64=256 bytes，所以把加载分为为两次，第二个加载指令跨度为64个数据。把这两个一维加载构造为二维，得到加载后线程寄存器空间与C子矩阵数据空间的对应关系，即C子矩阵数据在每个线程的64个寄存器中的存放位置：<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbe9jvnkbrnbfjfbp13511ik53u.png" alt=""></p><p>​    矩阵A中一行（这里作者按已经转置的A矩阵描述）的每个点与矩阵B中一行的每个点一一对应。</p><p>​    在warp内部，比较直接的方式是连续或有跨度地加载，但这样会发生不明原因的存储冲突。如果按照线程号用<strong>Z形加载就不会产生冲突</strong>（见上图warp0线程与数据关系，0、1、2、3……线程是Z字形映射）（自己没有验证过，平时只是合并访存，不知道是特定CUDA版本或架构有这个问题，还是都这样，作者没有对加载规格和模式做详尽地测试来检查哪个有效，哪个不行）。</p><h4 id="计算C子矩阵-寄存器组与重用"><a href="#计算C子矩阵-寄存器组与重用" class="headerlink" title="计算C子矩阵: 寄存器组与重用"></a><strong>计算C子矩阵: 寄存器组与重用</strong></h4><p>​    现在，每个线程里都有两个寄存器组，每组8个寄存器，保存矩阵A和B的数据，通过64次FFMA乘加计算得到C子矩阵的中间结果。为了实现全速低功耗地计算，需要考虑几个问题。最基本的就是<strong>寄存器组和操作数重用</strong>。</p><p>在Maxwell架构下，寄存器组（register banks）宽度为4（即4个32bit寄存器 ）。Kepler架构（宽度也是4）直接把序号和组关联起来。而在Maxwell架构下，关联由寄存器序号对4取余得到（即每 序号%4 相同的寄存器一组，占用同一个寄存器访问端口，类似共享内存）。在Kepler架构下，可以通过调整64个FFMA指令来消除所有存储冲突。在Maxwell架构下通过<strong>操作数重用缓存</strong>（operand reuse cache）解决这一问题，同时能减少寄存器传输次数。指令的每个<strong>源操作数槽位(source operand slot)</strong>有8 bytes的数据重用缓存。每次发射指令的时候，有一个标志位用来指定对应的操作数是否将被再次引用。设置标志位后，下一条指令在同一个操作数槽位引用同一个寄存器时，不需要再到寄存器组去取这个数据。可以利用这个特性，来避免寄存器冲突。</p><p>寄存器组概念参考下图：<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbec68ea1p26177ql57d7iq64b.png" alt=""></p><p>第一步是通过操作数重用来减少寄存器存储冲突。为实现目的，需要显式地选择要占用的寄存器。这是用MaxAs做汇编器的一个重要优点，ptxas在避免寄存器冲突方面做得还行，但不够完美，向量化存储操作做得不够好（在SGEMM实现中这点很重要）。实现如下：</p><ul><li>0-63作为矩阵C的寄存器</li><li>64-71和80-87作为矩阵A的双缓冲寄存器块</li><li>72-79和88-95作为矩阵B的双缓冲寄存器块</li></ul><p>按照下图来安排8*8矩阵的寄存器，用不同的颜色表示每个寄存器所在的组位。矩阵C中，选择颜色与对应的A、B不同的寄存器（不在同一端口）。通过这种方式可以消除C中所有的寄存器冲突。但还剩下A、B的16个寄存器冲突，这些冲突位置用黑框示意（作者图）：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbecjim8j781mdu1nppmj9gk4o.png" alt=""></p><p>如果不重用缓存，16个冲突每个都有1个时钟的延迟，理论上使得计算效率下降20%左右(在518时钟的循环上再加128个时钟)。实测时，执行没有重用标记的SGEMM汇编代码，发现性能只下降了200 Gflops左右。Nvidia的<a href="http://www.google.com/patents/US7834881">operand collectors</a>相关专利里的存储冲突（bank conflict）章节中描述了缓和存储冲突的一系列办法。不好说Maxwell架构是怎么处理的，可能利用了线程级并行（TLP）来掩盖存储冲突延迟。operand collectors能在一定程度上掩盖了存储冲突，但在大量冲突下可能不堪重负。持续地缓存能让硬件更好地避免存储冲突导致的延迟。MaxAs项目通过汇编器控制重用标记，预先判断哪些寄存器值得缓存，哪些寄存器在使用后就可以丢弃。</p><p>优化重用标记的工作已经由MaxAs实现了。我们要做的就是规划指令顺序，尽可能多地重用。最简单的排序是两个嵌套for循环一行一行地遍历矩阵，但这样只能用到8 bytes操作数缓存中的4 bytes，不能完全避免存储冲突。 通过往复遍历数据，可以避免所有的冲突并提高寄存器的重用率（39%）。不过作者最高效的实现方法是旋转遍历（重用率47%）。下面是FFMA指令按照矩阵C寄存器序号的执行顺序：</p><blockquote><p> 1, 0, 2, 3, 5, 4, 6, 7, 33, 32, 34, 35, 37, 36, 38, 39,<br>45, 44, 46, 47, 41, 40, 42, 43, 13, 12, 14, 15, 9, 8, 10, 11,<br>17, 16, 18, 19, 21, 20, 22, 23, 49, 48, 50, 51, 53, 52, 54, 55,<br>61, 60, 62, 63, 57, 56, 58, 59, 29, 28, 30, 31, 25, 24, 26, 27</p></blockquote><p>自己做的标记如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbedhbgn1b6o1dvhpc8ibp1gvc5i.png" alt=""></p><p>通过旋转，使得C寄存器组交错加载，避免冲突。此外，向量化数据加载除了高效，还可以减少访存指令数量，从而减少存储冲突的概率。</p><p>指令访问寄存器时可能发生存储冲突，导致延迟，所以应该为操作数选则不同组的寄存器。MaxAs可以控制寄存器的映射，SGEMM代码中为track0-3, tex, readAs, readBs和writeS选择了合适的寄存器组。作者提到在cublas中（当时的版本是CUDA 6.5），第一个FFMA指令选择的寄存器有存储冲突，这里冲突不能通过重用缓存避免，因为之前没有指令加载，也就没有寄存器值能放到操作数缓存。在GM204上，这个”bug”降低了cublas 28 Gflops的性能。</p><p>最后一个有关FFMA指令的的问题是，如何交错地执行FFMA和上述提到的存储操作。具体可以对照源码sgemm_pre_64.sass来看。为了掩盖延迟，共享内存的双缓冲加载越早越好，所以和第一个FFMA指令一起双发射执行。用两条FFMA指令隔开两条加载指令，因为<strong>存储单元</strong>好像在<strong>一半吞吐率</strong>的时候工作得<strong>最优</strong>（<strong>※不要连续发射存储指令※</strong>）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">01:-:-:-:0      FFMA cx02y00, j0Ax02, j0By00, cx02y00; // Wait Dep 1--:-:-:-:1      LDS.U.128 j1Ax00, [readAs + 4x&lt;1*64 + 00&gt;];--:-:-:-:1      FFMA cx02y01, j0Ax02, j0By01, cx02y01;--:-:-:-:0      FFMA cx00y01, j0Ax00, j0By01, cx00y01;--:-:-:-:1      LDS.U.128 j1By00, [readBs + 4x&lt;1*64 + 00&gt;];</span><br></pre></td></tr></table></figure><p>为了不让存储单元被指令淹没（overwhelm，没译准），纹理加载在两组共享内存加载中间执行，给纹理加载指令读取操作数的机会（in flight概念）。为下一个大循环加载的数据共享内存加载指令放在最后一个FFMA指令块中。之前还有一个加在第7和第8个FFMA指令块之间的BAR.SYNC指令（同步，作用是保证整个block tex加载的数据都通过STS保存到另一共享内存buffer后，再继续下一次加载计算）。</p><p>以上所有指令执行位置是经过大量测试取最优的，这样细粒度的指令位置控制在ptxas是中无法实现的。而且ptxas有优化掉共享内存双缓冲加载方法的倾向。在选择寄存器组、优化指令执行顺序以实现操作数重用和选择存储指令位置以后，之前只能跑到硬件计算能力70%的kernel现在可以跑到98%。</p><p>理论性能的计算，用主循环中的FFMA指令总数（518）除以所有指令需要的发射时间（双发射不算）即可得到。以下是256线程版本的具体分析：</p><table><thead><tr><th style="text-align:center">Op</th><th style="text-align:center">Count</th></tr></thead><tbody><tr><td style="text-align:center">FFMA</td><td style="text-align:center">512</td></tr><tr><td style="text-align:center">LDS</td><td style="text-align:center">32  dual issued</td></tr><tr><td style="text-align:center">STS</td><td style="text-align:center">2  dual issued</td></tr><tr><td style="text-align:center">TLD</td><td style="text-align:center">2  dual issued</td></tr><tr><td style="text-align:center">IADD</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">XOR</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">STEP</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">BAR</td><td style="text-align:center">1  dual issued</td></tr><tr><td style="text-align:center">BAR</td><td style="text-align:center">1  dual issued</td></tr></tbody></table><p>一共用了518个时钟来发送指令（双发射不计入时钟消耗），所以这个kernel的性能上限是512/518，即98.8%。大型矩阵计算的实际性能接近理论值。</p><p>根据主循环代码和指令的时钟消耗，可以粗略估计这个kernel的访存带宽上限。对于GM204架构，有：</p><ul><li>每个线程加载2组vec4的4byte数据（即32bytes）</li><li>每个loop需要518个clock（之后的计算部分有具体细节）</li><li>每个SM同一时刻时有128个线程执行</li><li>有13个时钟频率1.2GHz的SM</li><li>每GB相当于0.931GiB</li></ul><p>所以，对于GTX 970 有：<br>（这里可以和上一大节的分析对比，两种分析基本能对应上，充分利用了硬件算力）</p><hr><h4 id="线程数据交换与写回"><a href="#线程数据交换与写回" class="headerlink" title="线程数据交换与写回"></a>线程数据交换与写回</h4><p>再次用图：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bb8hga1815hsfle1b0p1mfr1gcv9.png" alt=""></p><p>在循环的末尾（end=N-8，标记判断），线程块中的C子矩阵数据已经计算完成，要把结果存回显存。由于从共享内存进行4点向量化加载，在写回全局内存的时候，对应的矩阵C的地址并没有作合并访存优化。虽然可以直接写回，但还有优化的余地。线程间通过共享内存交换矩阵C寄存器的数据，重新组织数据，构造合并访存的写入操作。注意，这里warp shuffle指令并不适用，因为需要在不同的线程之间交换不同寄存器的数据。</p><p>如下图示意，把要交换的数据分为8块，在存放C子矩阵的寄存器中按顺序沿纵向划分。这里还是以0号线程数据为例，红色线条是第1次处理的8个数据，注意相邻线程同一次加载的B方向数据，跨度为4（两蓝色线条示意）。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbgveibjnkl14n41lja133317s61t.png" alt=""></p><p>每个线程每次从8个寄存器读取数据到共享内存，借用作者配图表示：<br>此时的数据是按A方向存储的。绿色格子是0号线程的两组vec4数据。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbgvji6bnt01uf212unqsftvb2a.png" alt=""></p><p>然后马上读回数据，此时按B方向读取，由于数据不连续了，分成8次单独访问。<br>黄色格子是线程，绿色格子是0号线程读取的数据。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbgvt0u9l218ko1d3mget1r4r2n.png" alt=""></p><p>​    这里有一个细节是，<strong>上述共享内存读写不作同步</strong>，因为数据交换是在同一warp内进行的。（猜测，存储单元按指令先后处理，前面的存储工作完成后才进行后面的读取工作）最后C子矩阵计算结果从寄存器放回显存。<br>​    整体来看，256个线程分为8个warp，线程与数据空间映射关系如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbh01h71n6ldkn1m3b15bc1tvh34.png" alt=""></p><p>下图是64*64数据块的传输细节。<br>从并行的角度，各线程在A方向连续读取，因为B方向相邻4点存在同一个线程中，而且在数据空间有一行的跨度所以（为了合并访存）不能在一次传输中连续处理。因此，每次存放到共享内存空间的<strong>一行数据</strong>在B方向上有<strong>四行跨度</strong>（图中带颜色的数据方块就是1/8次传输的前一组vec4数据）。<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image_1bbh01ta5fcg1tce1bj01dprku03h.png" alt=""><br>至此，就完成了一个block的计算。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;SGEMM：Single precision floatiing General Matrix Multiply&lt;/p&gt;
&lt;p&gt;本文的SGEMM实现思路整理于MaxAs wiki和对MaxAs项目SGEMM汇编代码的理解。&lt;br&gt;MaxAs是一个开源的Maxwell架构GPU汇编器： &lt;a href=&quot;https://github.com/NervanaSystems/maxas&quot;&gt;Github链接&lt;/a&gt;&lt;br&gt;作者&lt;a href=&quot;https://github.com/scott-gray&quot;&gt;Scott Gray&lt;/a&gt;提到两篇相关论文&lt;a href=&quot;http://icl.cs.utk.edu/projectsfiles/magma/pubs/fermi_gemm.pdf&quot;&gt;MAGMA paper&lt;/a&gt;和&lt;a href=&quot;https://hal.inria.fr/file/index/docid/789958/filename/112_Lai.pdf&quot;&gt;Kepler sgemm paper&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CUDA" scheme="http://yuanquanquan.top/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>flask实现图像处理API</title>
    <link href="http://yuanquanquan.top/2021/20210515/"/>
    <id>http://yuanquanquan.top/2021/20210515/</id>
    <published>2021-05-15T06:30:36.000Z</published>
    <updated>2021-05-15T06:43:29.165Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Flask是一个轻量级的后台框架，学习成本低，维护简单，传统<a href="https://www.java.com/">java</a>、<a href="https://www.php.net/">Php</a>太过笨重。如果只是从简单这个角度出发，Flask是开发后端的佼佼者，最快只需要7行代码完成一个<a href="https://en.wikipedia.org/wiki/Web">Web</a>应用。</p><p>本文通过flask实现了一个图像处理的API接口，写了不少，hexo有字符无法转义，就转pdf放上来了</p></blockquote><span id="more"></span>  <div class="row">    <embed src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding/flask.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Flask是一个轻量级的后台框架，学习成本低，维护简单，传统&lt;a href=&quot;https://www.java.com/&quot;&gt;java&lt;/a&gt;、&lt;a href=&quot;https://www.php.net/&quot;&gt;Php&lt;/a&gt;太过笨重。如果只是从简单这个角度出发，Flask是开发后端的佼佼者，最快只需要7行代码完成一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Web&quot;&gt;Web&lt;/a&gt;应用。&lt;/p&gt;
&lt;p&gt;本文通过flask实现了一个图像处理的API接口，写了不少，hexo有字符无法转义，就转pdf放上来了&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="flask" scheme="http://yuanquanquan.top/tags/flask/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow分布式实现-Kubernetes</title>
    <link href="http://yuanquanquan.top/2021/20210415/"/>
    <id>http://yuanquanquan.top/2021/20210415/</id>
    <published>2021-04-15T00:23:00.000Z</published>
    <updated>2021-05-17T08:15:17.592Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>随着海量数据的出现和模型参数的增多，我们必然需要更大的集群来运行模型，这样最大的好处在于把原本可能需要周级别的训练时间缩短到天级别甚至小时级别。未来的模型训练面对的都是上亿数据和上亿参数，稳定的计算能力和管理便捷的集群环境至关重要。Kubernetes 是目前应用最广泛的容器集群管理工具之一，它可以为对分布式TensorFlow 的监控、调度等生命周期管理提供所需的保障。</p></blockquote><span id="more"></span> <h2 id="分布式TensorFlow-在Kubernetes-中的运行"><a href="#分布式TensorFlow-在Kubernetes-中的运行" class="headerlink" title="分布式TensorFlow 在Kubernetes 中的运行"></a>分布式TensorFlow 在Kubernetes 中的运行</h2><p>本节介绍在Kubernetes 中运行分布式TensorFlow 的方法。首先学习如何部署Kubernetes 环境，接着在搭建好的环境中运行分布式TensorFlow，并用MNIST 来训练。</p><h3 id="部署及运行"><a href="#部署及运行" class="headerlink" title="部署及运行"></a>部署及运行</h3><p>首先需要先安装Kubernetes。</p><p>用Minikube 来创建本地Kubernetes 集群。安装Minikube 需要预先安装VirtualBox 虚拟机，可以从<a href="https://www.virtualbox.org/">官网</a>上直接下载安装，注意选择对应的操作系统版本即可。<img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517145615182.png" alt=""></p><p>Minikube 用Go 语言编写，发布形式是一个独立的二进制文件，所以只需要下载下来，然后放在对应的位置即可。因此安装Minikube，只需要一条命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.14.0/</span><br><span class="line"></span><br><span class="line">minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/</span><br></pre></td></tr></table></figure><p>Kubernetes 提供了一个客户端kubectl，可直接通过kubectl 以命令行的方式与集群交互。</p><p>安装kubectl 的方法如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -Lo kubectl http://storage.googleapis.com/kubernetes-release/release/v1.5.1/bin/darwin/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/</span><br></pre></td></tr></table></figure><p>下面在Minikube 中启动 Kubernetes 集群，如图所示。<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517145843210.png" alt=""><br>可以观察到VirtualBox 中也启动了相应的虚拟机，如图所示。<br><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517145904869.png" alt=""><br>采用<a href="https://hub.docker.com/r/tensorflow/tensorflow/">Docker Hub</a>上的最新镜像tensorflow/tensorflow（基于TensorFlow 的1.0 版本）。<br>首先，配置参数服务器的部署（deployment）文件，命名为tf-ps-deployment.json。代码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;apiVersion&quot;</span>: <span class="string">&quot;extensions/v1beta1&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;kind&quot;</span>: <span class="string">&quot;Deployment&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line">  <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-ps2&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;replicas&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="attr">&quot;template&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;labels&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line">            <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-ps2&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;role&quot;</span>: <span class="string">&quot;ps&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;containers&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;ps&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;image&quot;</span>: <span class="string">&quot;tensorflow/tensorflow&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;ports&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;containerPort&quot;</span>: <span class="number">2222</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置参数服务器的服务（Service）文件，命名为tf-ps-service.json，代码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;apiVersion&quot;</span>: <span class="string">&quot;v1&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;kind&quot;</span>: <span class="string">&quot;Service&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;ports&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;port&quot;</span>: <span class="number">2222</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;targetPort&quot;</span>: <span class="number">2222</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">],</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;selector&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-ps2&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;labels&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;role&quot;</span>: <span class="string">&quot;service&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-ps2-service&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置计算服务器的部署文件，命名为tf-worker-deployment.json，代码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;apiVersion&quot;</span>: <span class="string">&quot;extensions/v1beta1&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;kind&quot;</span>: <span class="string">&quot;Deployment&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-worker2&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;replicas&quot;</span>: <span class="number">2</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;template&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;labels&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-worker2&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;role&quot;</span>: <span class="string">&quot;worker&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;containers&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;worker&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;image&quot;</span>: <span class="string">&quot;tensorflow/tensorflow&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;ports&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;containerPort&quot;</span>: <span class="number">2222</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置计算服务器的服务文件，命名为tf-worker-service.json，代码如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;apiVersion&quot;</span>: <span class="string">&quot;v1&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;kind&quot;</span>: <span class="string">&quot;Service&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;spec&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;ports&quot;</span>: [</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;port&quot;</span>: <span class="number">2222</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;targetPort&quot;</span>: <span class="number">2222</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">],</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;selector&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-worker2&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;metadata&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;labels&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-worker2&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;role&quot;</span>: <span class="string">&quot;service&quot;</span></span><br><span class="line"></span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: <span class="string">&quot;tensorflow-wk2-service&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f tf-ps-deployment.json</span><br><span class="line"></span><br><span class="line">kubectl create -f tf-ps-service.json</span><br><span class="line"></span><br><span class="line">kubectl create -f tf-worker-deployment.json</span><br><span class="line"></span><br><span class="line">kubectl create -f tf-worker-service.json</span><br></pre></td></tr></table></figure><p>分别输出以下结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">deployment &quot;tensorflow-ps2&quot; created</span><br><span class="line"></span><br><span class="line">service &quot; tensorflow-ps2-service&quot; created</span><br><span class="line"></span><br><span class="line">deployment &quot;tensorflow-worker2&quot; created</span><br><span class="line"></span><br><span class="line">service &quot;tensorflow-wk2-service&quot; created</span><br></pre></td></tr></table></figure><p>稍等片刻，运行kubectl get pod，可以看到参数服务器和计算服务器全部创建完成</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517150527645.png" alt=""></p><p>下面我们进入每个服务器（Pod）中，部署好需要运行的mnist_replica.py 文件。</p><p>首先查看以下2 台ps_host 的 IP 地址</p><p>然后查看2 台worker_host 的IP 地址</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517150600269.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210517150612314.png" alt=""></p><p>打开4 个终端，分别进入4 个Pod 当中，命令如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -ti tensorflow-ps2-3073558082-3b08h /bin/bash</span><br><span class="line"></span><br><span class="line">kubectl exec -ti tensorflow-ps2-3073558082-4x3j2 /bin/bash</span><br><span class="line"></span><br><span class="line">kubectl exec -ti tensorflow-worker2-3070479207-k6z8f /bin/bash</span><br><span class="line"></span><br><span class="line">kubectl exec -ti tensorflow-worker2-3070479207-6hvsk /bin/bash</span><br></pre></td></tr></table></figure><p>通过下面的方式将mnist_replica.py 分别部署到4 个Pod 中，如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl https://raw.githubusercontent.com/tensorflow/tensorflow/master/</span><br><span class="line"></span><br><span class="line">tensorflow/tools/dist_test/python/mnist_replica.py -o mnist_replica.py</span><br></pre></td></tr></table></figure><p>在参数服务器的两个容器中分别执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python mnist_replica.py --ps_hosts=172.17.0.16:2222,172.17.0.17:2222 --worker_</span><br><span class="line"></span><br><span class="line">hosts=172.17.0.3:2222,172.17.0.8:2222 --job_name=&quot;ps&quot; --task_index=0</span><br><span class="line"></span><br><span class="line">python mnist_replica.py --ps_hosts=172.17.0.16:2222,172.17.0.17:2222 --worker_</span><br><span class="line"></span><br><span class="line">hosts=172.17.0.3:2222,172.17.0.8:2222 --job_name=&quot;ps&quot; --task_index=1</span><br></pre></td></tr></table></figure><p>在计算服务器的两个容器中分别执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python mnist_replica.py --ps_hosts=172.17.0.16:2222,172.17.0.17:2222 --worker_</span><br><span class="line"></span><br><span class="line">hosts=172.17.0.3:2222,172.17.0.8:2222 --job_name=&quot;worker&quot; --task_index=0</span><br><span class="line"></span><br><span class="line">python mnist_replica.py --ps_hosts=172.17.0.16:2222,172.17.0.17:2222 --worker_</span><br><span class="line"></span><br><span class="line">hosts=172.17.0.3:2222,172.17.0.8:2222 --job_name=&quot;worker&quot; --task_index=1</span><br></pre></td></tr></table></figure><p>执行输出与14.6 节的输出类似。一共执行200 次迭代，工作节点1（172.17.0.3:2222）执行了144 次迭代，如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">job name = worker</span><br><span class="line"></span><br><span class="line">task index = 0</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize</span><br><span class="line"></span><br><span class="line">GrpcChannelCache for job ps -&gt; &#123;0 -&gt; localhost:2222&#125;</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannel</span><br><span class="line"></span><br><span class="line">Cache for job worker -&gt; &#123;0 -&gt; localhost:2223, 1 -&gt; localhost:2224&#125;</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:217] Started server</span><br><span class="line"></span><br><span class="line">with target: grpc://localhost:2223</span><br><span class="line"></span><br><span class="line">Worker 0: Initializing session...</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/master_session.cc:994] Start master session</span><br><span class="line"></span><br><span class="line">0d791a02977e5701 with config:</span><br><span class="line"></span><br><span class="line">device_filters: &quot;/job:ps&quot;</span><br><span class="line"></span><br><span class="line">device_filters: &quot;/job:worker/task:0&quot;</span><br><span class="line"></span><br><span class="line">allow_soft_placement: true</span><br><span class="line"></span><br><span class="line">Worker 0: Session initialization complete.</span><br><span class="line"></span><br><span class="line">Training begins @ 1483516057.489495</span><br><span class="line"></span><br><span class="line">1483516057.518419: Worker 0: training step 1 done (global step: 0)</span><br><span class="line"></span><br><span class="line">1483516057.541053: Worker 0: training step 2 done (global step: 1)</span><br><span class="line"></span><br><span class="line">1483516057.569677: Worker 0: training step 3 done (global step: 2)</span><br><span class="line"></span><br><span class="line">1483516057.584578: Worker 0: training step 4 done (global step: 3)</span><br><span class="line"></span><br><span class="line">1483516057.646970: Worker 0: training step 5 done (global step: 4)</span><br><span class="line"></span><br><span class="line">\# ……中间略去</span><br><span class="line"></span><br><span class="line">1483516059.286596: Worker 0: training step 141 done (global step: 197)</span><br><span class="line"></span><br><span class="line">1483516059.291600: Worker 0: training step 142 done (global step: 198)</span><br><span class="line"></span><br><span class="line">1483516059.297347: Worker 0: training step 143 done (global step: 199)</span><br><span class="line">1483516059.303738: Worker 0: training step 144 done (global step: 200)</span><br><span class="line"></span><br><span class="line">Training ends @ 1483516059.303808</span><br><span class="line"></span><br><span class="line">Training elapsed time: 1.614513 s</span><br><span class="line"></span><br><span class="line">After 200 training step(s), validation cross entropy = 1235.56</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>工作节点2（172.17.0.8:2222）执行了56 次迭代，输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">job name = worker</span><br><span class="line"></span><br><span class="line">task index = 1</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannel</span><br><span class="line"></span><br><span class="line">Cache for job ps -&gt; &#123;0 -&gt; localhost:2222&#125;</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannel</span><br><span class="line"></span><br><span class="line">Cache for job worker -&gt; &#123;0 -&gt; localhost:2223, 1 -&gt; localhost:2224&#125;</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:217] Started server</span><br><span class="line"></span><br><span class="line">with target: grpc://localhost:2224</span><br><span class="line"></span><br><span class="line">Worker 1: Waiting for session to be initialized...</span><br><span class="line"></span><br><span class="line">I tensorflow/core/distributed_runtime/master_session.cc:994] Start master session</span><br><span class="line"></span><br><span class="line">92e671f3dd1ffd05 with config:</span><br><span class="line"></span><br><span class="line">device_filters: &quot;/job:ps&quot;</span><br><span class="line"></span><br><span class="line">device_filters: &quot;/job:worker/task:1&quot;</span><br><span class="line"></span><br><span class="line">allow_soft_placement: true</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Worker 1: Session initialization complete.</span><br><span class="line"></span><br><span class="line">Training begins @ 1483516058.803010</span><br><span class="line"></span><br><span class="line">1483516058.832164: Worker 1: training step 1 done (global step: 121)</span><br><span class="line"></span><br><span class="line">1483516058.844464: Worker 1: training step 2 done (global step: 123)</span><br><span class="line"></span><br><span class="line">1483516058.860988: Worker 1: training step 3 done (global step: 126)</span><br><span class="line"></span><br><span class="line">1483516058.873543: Worker 1: training step 4 done (global step: 128)</span><br><span class="line"></span><br><span class="line">1483516058.884758: Worker 1: training step 5 done (global step: 130)</span><br><span class="line"></span><br><span class="line">\# ……中间略去</span><br><span class="line"></span><br><span class="line">1483516059.152332: Worker 1: training step 52 done (global step: 176)</span><br><span class="line"></span><br><span class="line">1483516059.167606: Worker 1: training step 53 done (global step: 178)</span><br><span class="line"></span><br><span class="line">1483516059.177215: Worker 1: training step 54 done (global step: 180)</span><br><span class="line"></span><br><span class="line">1483516059.301384: Worker 1: training step 55 done (global step: 182)</span><br><span class="line"></span><br><span class="line">1483516059.309557: Worker 1: training step 56 done (global step: 202)</span><br><span class="line"></span><br><span class="line">Training ends @ 1483516059.309638</span><br><span class="line"></span><br><span class="line">Training elapsed time: 0.536126 s</span><br><span class="line"></span><br><span class="line">After 200 training step(s), validation cross entropy = 1235.56</span><br></pre></td></tr></table></figure><p>在这个例子中，更好的方式是把需要执行的源代码以及训练数据和测试数据放在持久卷（persistent volume）中，在多个Pod 间实现共享，从而避免在每一个Pod 中分别部署。对应TensorFlow 的GPU 的Docker 集群部署，Nvidia 官方提供了nvidia-docker 的方式，原理主要是利用宿主机上的GPU 设备，将它映射到容器中。更多与部署相关的内容可以参考<a href="https://github.com/NVIDIA/nvidia-docker。">https://github.com/NVIDIA/nvidia-docker。</a></p><h2 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h2><p>训练好模型之后可以将它打包制作成环境独立的镜像，这样能够极大地方便测试人员部署一致的环境，也便于对不同版本的模型做标记、比较不同模型的准确率，从整体上降低测试、部署上线等的工作复杂性，具有很大的优势。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>将Kubernete 与TensorFlow 结合，借助Kubernetes 提供的稳定计算环境，对TensorFlow 集</p><p>群进行便捷的管理，降低了搭建大规模深度学习平台的难度，这也是社区非常推崇的部署方案。</p><p>本章主要讲述了用Kubernetes 管理TensorFlow 集群的方法，以及在Kubernetes 上部署分布式</p><p>TensorFlow 的方式，最后采用MNIST 的分布式例子进行了实践。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;随着海量数据的出现和模型参数的增多，我们必然需要更大的集群来运行模型，这样最大的好处在于把原本可能需要周级别的训练时间缩短到天级别甚至小时级别。未来的模型训练面对的都是上亿数据和上亿参数，稳定的计算能力和管理便捷的集群环境至关重要。Kubernetes 是目前应用最广泛的容器集群管理工具之一，它可以为对分布式TensorFlow 的监控、调度等生命周期管理提供所需的保障。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="分布式" scheme="http://yuanquanquan.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>《CUDA权威编程指南》笔记-1</title>
    <link href="http://yuanquanquan.top/2021/20210407/"/>
    <id>http://yuanquanquan.top/2021/20210407/</id>
    <published>2021-04-07T02:05:01.000Z</published>
    <updated>2021-06-04T15:44:20.681Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>CUDA是一种通用的并行计算平台和编程模型，是在C语言上扩展的。借助于CUDA，你可以像编写C语言程序一样实现并行算法。你可以在NIVDIA的GPU平台上用CUDA为多种系统编写应用程序，范围从嵌入式设备、平板电脑、笔记本电脑、台式机工作站到HPC集群。在CUDA编程平台中，GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此当我们在说GPU并行计算时，其实是指的基于CPU+GPU的异构计算架构。在异构计算架构中，GPU与CPU通过PCIe总线连接在一起来协同工作，CPU所在位置称为为主机端（host），而GPU所在位置称为设备端（device）。</p></blockquote><span id="more"></span><h2 id="第一章-基于CUDA的异构并行计算"><a href="#第一章-基于CUDA的异构并行计算" class="headerlink" title="第一章  基于CUDA的异构并行计算"></a>第一章  基于CUDA的异构并行计算</h2><h3 id="并行计算"><a href="#并行计算" class="headerlink" title="并行计算"></a>并行计算</h3><ul><li>一个大的问题可以被分解成多个小问题，然后在不同的计算资源上并行处 理这些小问题。</li><li>并行计算的软件和硬件层面是紧密联系的。</li><li>事实上，并行计算通常涉及两个不同的计算技术领域。</li><li>串行编程和并行编程</li><li>大多数现代处理器的哈佛体系结构（Harvard architecture）</li><li>计算机架构（硬件方面）</li><li>并行程序设计（软件方面）</li></ul><hr><h3 id="并行性"><a href="#并行性" class="headerlink" title="并行性"></a>并行性</h3><ul><li><p>多层次的并行性设计是架构设计的驱动力。在应用程序中有两种基本的并行类型。</p><blockquote><p>CUDA编程非常适合解决数据并行计算的问题。本书的重点便是如何使用CUDA编程 解决数据并行问题。</p></blockquote></li></ul><h4 id="数据并行程序设计"><a href="#数据并行程序设计" class="headerlink" title="数据并行程序设计"></a>数据并行程序设计</h4><ul><li><ul><li>块划分（block partitioning）</li><li>周期划分（cyclic partitioning）</li><li>一组连续的数据被分到一个块内</li><li>每个数据块以任 意次序被安排给一个线程</li><li>线程通常在同一时间只处理一个数据块</li><li>更少的数据被分到一个块内</li><li>相邻的线程处理相邻的数据块</li><li>每个线程可以处理多个数据块</li><li>为一个待处理的线程选择一个新的块，就意味着要跳过和现有线程一样多的数据块</li><li>第一步是把数据依据线程进行划分，使每个线程处理一部分数据</li><li>利用多核系统对数据进行分配</li><li>利用多核系统对任务进行分配</li><li>任务并行</li><li>数据并行</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210410133904604.png" alt=""></p><h4 id="计算机架构"><a href="#计算机架构" class="headerlink" title="计算机架构"></a>计算机架构</h4><ul><li><p>弗林分类法（Flynn’s Taxonomy）</p></li><li><ul><li>降低延迟</li><li>提高带宽</li><li>提高吞吐量</li><li>延迟是一个操作从开始到完成所需要的时间，常用微秒来表示</li><li>延迟用来衡量完成一次操作的时间</li><li>带宽是单位时间内可 处理的数据量，通常表示为MB/s或GB/s。</li><li>吞吐量是单位时间内成功处理的运算数量，通常表示为gflops（即每秒十亿次的浮点运算数量），特别是在重点使用浮点计算的科学计算领域经常用到</li><li>而吞吐量用来衡量在给定的单位时 间内处理的操作量</li><li>单指令单数据（SISD）(传统计算机，一种串行架构)</li><li>单指令多数据（SIMD） (按串行逻辑思考但对并行数据操作实现并行加速，而其他细节则由编译器来负责)</li><li>多指令单数据（MISD） (每个核心通过使用多个指令流处理同一个数据流)</li><li>多指令多数据（MIMD）(多个核心使用多个指令流来异步处理多个数据流，从而实现空间上的并行性)</li><li>根据指令和数据进入CPU的方式</li><li>实现以下目的，架构取得了许多进展</li></ul></li></ul><ul><li><p>计算机架构也能根据内存组织方式进行进一步划分</p></li><li><ul><li><p>要么是与同一个物理内存相关联</p></li><li><p>要么共用一个低延迟的链路（如PCIExpress或PCIe）</p></li><li><p>多核架构已经永久地取代了单核架构</p></li><li><p>多线程、 MIMD（多指令多数据）、SIMD（单指令多数据），以及指令级并行。</p><p>NVIDIA公司称这 种架构为SIMT（单指令多线程）</p></li><li><p>“众核”（many-core）通常是指有很多核心（几十或几百个）的多核架构</p></li><li><p>GPU代表了一种众核架构，几乎包括了前文描述的所有并行结构：</p></li><li><p>这种系统常被称作集群</p></li><li><p>分布式内存的多节点系统</p></li><li><p>共享内存的多处理器系统</p></li></ul></li><li></li><li><p>GPU核心和CPU核心</p></li><li><ul><li>较轻，用于优化具有简单控制逻辑的数据并行任务，注重并行程序的吞吐量</li><li>较重，用来处理非常复杂的控制逻辑，以优化串行程序执行</li><li>两种核心是完全不同的</li><li>CPU核心</li><li>GPU核心</li></ul></li></ul><hr><h3 id="异构计算"><a href="#异构计算" class="headerlink" title="异构计算"></a>异构计算</h3><blockquote><p>GPU指的是离散的设备从同构系统到异构系统的转变是高性能计算 史上的一个里程碑。</p></blockquote><ul><li><p>同构计算使用的是同一架构下的一个或多个处理器来执行一个应用。</p></li><li><p>而异构计算则使用一个处理器架构来执行一个应用，为任务选择适合它的架构，使其最终对性能有所改进。</p></li><li><ul><li>系统的有效利用 受限于增加应用程序设计的复杂性</li></ul></li></ul><h4 id="异构架构"><a href="#异构架构" class="headerlink" title="异构架构"></a>异构架构</h4><ul><li><p>一个典型的异构计算节点</p></li><li><ul><li>GPU不 是一个独立运行的平台而是CPU的协处理器</li><li>通过PCIe总线与基于CPU的 主机相连来进行操作</li><li>两个多核CPU插槽（主机端）</li><li>两个或更多个的众核GPU （设备端）</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210410134053193.png" alt=""></p><ul><li><p>一个异构应用包括两个部分</p></li><li><ul><li><p>在计算密集型应用中，往往有很多并行数据的程序段。</p><p>GPU就是用来提高这些并行数据的执行速度的。</p></li><li><p>在设备端加载计算密集型任务之前，CPU代码负责管理设备端的环境、代码和数据。</p></li><li><p>主机代码（CPU上运行）</p></li><li><p>设备代码（GPU上运行）</p></li></ul></li><li><p>NVIDIA公司的GPU计算平台</p></li><li><ul><li>Fermi是Tesla系列产品中的一种，用作GPU加速器，近来在高性能计算中获得了广泛应用</li><li>Fermi之后的新一代GPU 计算架构Kepler，于2012年秋季发布，其处理能力相比以往的GPU有很大提升，并且提供 了新的方法来优化和提高GPU并行工作的执行，有望将高性能计算提升到新的高度</li><li>Tegra系列产品是专为移动和嵌入式设备而设计的，如平板电脑和手机</li><li>GeForce面向图形用户</li><li>Quadro用于专业绘图设计</li><li>Tesla用于大规模的并行计算</li></ul></li><li><p>描述GPU容量的两个重要特征</p></li><li><ul><li>CUDA核心数量</li><li>内存大小</li></ul></li><li><p>两种不同的指标来评估GPU的性能</p></li><li><ul><li>内存带宽是从内存中读取或写入数据的比率。</li><li>内存带宽通常用 GB/s表示。</li><li>峰值计算性能是用来评估计算容量的一个指标，通常定义为每秒能处理的单精度或双 精度浮点运算的数量.</li><li>峰值性能通常用GFlops（每秒十亿次浮点运算）或TFlops（每秒万 亿次浮点运算）来表示。</li><li>峰值计算性能</li><li>内存带宽</li></ul></li><li><p>计算能力</p></li><li><ul><li>NVIDIA使用一个术语“计算能力”（compute capability）来描述整个Tesla系列的GPU加 速器的硬件版本。</li></ul></li></ul><h3 id="异构计算范例"><a href="#异构计算范例" class="headerlink" title="异构计算范例"></a>异构计算范例</h3><blockquote><p>GPU计算并不是要取代CPU计算</p></blockquote><blockquote><p>Tip：这一段我就不写了，本章节的主要内容用一张图即可概括</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210410134140428.png" alt=""></p><h3 id="CUDA：一种异构计算平台"><a href="#CUDA：一种异构计算平台" class="headerlink" title="CUDA：一种异构计算平台"></a>CUDA：一种异构计算平台</h3><h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><blockquote><p>CUDA提供了两层API来管理GPU设备和组织线程。</p><p>这两种API是相互排斥的，你必须使用两者之一，从两者中混合函数调用是不可能的。</p></blockquote><ul><li><p>CUDA驱动API</p></li><li><ul><li>驱动API是一种低级API，它相对来说较难编程</li><li>对于在GPU设备使用上提供了更多的控制</li></ul></li><li><p>CUDA运行时API</p></li><li><ul><li>运行时API是一个高级API</li><li>它在驱动API的上层实现</li><li>每个运行时API函数都被分解为更多传给驱动API的基本运算</li></ul></li></ul><blockquote><p>也就是说Runtime api 可以看作是由Driver api 封装而成的</p></blockquote><h4 id="CUDA程序"><a href="#CUDA程序" class="headerlink" title="CUDA程序"></a>CUDA程序</h4><ul><li><p>一个CUDA程序包含了以下两个部分的混合。</p></li><li><ul><li>在CPU上运行的主机代码</li><li>在GPU上运行的设备代码</li></ul></li></ul><ul><li><p>一个典型的CUDA编程结构包括5个主要步骤</p></li><li><ul><li>分配GPU内存</li><li>从CPU内存中拷贝数据到GPU内存</li><li>调用CUDA内核函数来完成程序指定的运算</li><li>将数据从GPU拷回CPU内存</li><li>释放GPU内存空间</li></ul></li></ul><h4 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h4><ul><li><p>数据局部性在并行编程中是一个非常重要的概念。</p></li><li><ul><li>时间局部性是指在相对较短的时间段内数据和/或资源的重用。</li><li>空间局部性是指在相对较接近的存储空间内数据元素的重用。</li><li>数据局部性指的是数据重用，以降低内存访问的延迟。</li><li>数据局部性有两种基本类型</li></ul></li><li><p>CUDA中有内存层次和线程层次的概念</p></li><li><ul><li><p>内存层次结构</p></li><li><p>线程层次结构</p></li><li><p>例如：</p><p>在CUDA编程模型中使用的共享内存（一个特殊的内存）。</p><p>共享内存可以视为 一个被软件管理的高速缓存，通过为主内存节省带宽来大幅度提高运行速度。</p><p>有了共享内存，你可以直接控制代码的数据局部性。</p></li></ul></li></ul><p><strong>编译环境</strong>：本代码将使用<code>nvcc</code>编译器来编译，你可以使用以下命令来检查CUDA是否正确安装:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ which nvcc</span><br><span class="line">/usr/local/cuda-8.0/bin/nvcc  # cuda-8.0 版本</span><br></pre></td></tr></table></figure><h3 id="用GPU输出-Hello-World"><a href="#用GPU输出-Hello-World" class="headerlink" title="用GPU输出 Hello World"></a>用GPU输出 Hello World</h3><p>不妨先写一个cuda C程序，命名为<code>helloFromGPU</code>，用它来输出字符串 “Hello World from GPU！”</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">========================== 代码清单 1-1 Hello World from GPU (hello.cu) ==========================</span><br><span class="line">// hello.cu</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">__global__ void helloFromGPU (void) </span><br><span class="line">&#123;</span><br><span class="line">    printf(&quot;Hello World from GPU!\n&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(void)</span><br><span class="line">&#123;</span><br><span class="line">    // hello from cpu</span><br><span class="line">    printf(&quot;Hello World from CPU!\n&quot;);</span><br><span class="line"></span><br><span class="line">    helloFromGPU &lt;&lt;&lt;1, 10&gt;&gt;&gt;();</span><br><span class="line">    cudaDeviceReset();</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在linux终端下使用以下命令进行编译<a href="https://github.com/YunYang1994/cuda-tutorial/blob/master/src/chapter01/hello.cu"><code>hello.cu</code></a>，然后执行程序得到</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ nvcc -arch sm_20 hello.cu -o hello</span><br><span class="line">$ ./hello</span><br><span class="line">Hello World from CPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br><span class="line">Hello World from GPU!</span><br></pre></td></tr></table></figure><p>在上面的代码中，<code>cudaDeviceReset</code>表示重置当前线程所关联过的当前设备的所有资源；修饰符<code>__global__</code>告诉编译器这是一个内核函数，它将从CPU中调用，然后在GPU上执行，在CPU上通过下面的代码启动内核函数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helloFromGPU &lt;&lt;&lt;1, 10&gt;&gt;&gt;();</span><br></pre></td></tr></table></figure><blockquote><p>三重尖号意味着从主线程到端代码的调用。1和10分别表示有1个块区域和10个线程，后续会作相关介绍。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;CUDA是一种通用的并行计算平台和编程模型，是在C语言上扩展的。借助于CUDA，你可以像编写C语言程序一样实现并行算法。你可以在NIVDIA的GPU平台上用CUDA为多种系统编写应用程序，范围从嵌入式设备、平板电脑、笔记本电脑、台式机工作站到HPC集群。在CUDA编程平台中，GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此当我们在说GPU并行计算时，其实是指的基于CPU+GPU的异构计算架构。在异构计算架构中，GPU与CPU通过PCIe总线连接在一起来协同工作，CPU所在位置称为为主机端（host），而GPU所在位置称为设备端（device）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CUDA" scheme="http://yuanquanquan.top/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>yolov5的openvino部署</title>
    <link href="http://yuanquanquan.top/2021/20210406/"/>
    <id>http://yuanquanquan.top/2021/20210406/</id>
    <published>2021-04-06T05:30:50.000Z</published>
    <updated>2021-04-06T11:43:03.005Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>OpenVINO是一个用于提升深度学习模型部署性能的跨平台的工具包，安装和使用都非常简单，下面 demo 视频是在 Mac 机器上运行的例子，安装过程在上面的文档链接中，安装主要是需要下载大量深度框架相关的库和模型，以及 Intel 已经封装好的一些脚本。</p><p>之前在<a href="https://yuanquanquan.top/2020/20201027/">神经计算棒</a>上部署过openvino来测试神经棒的速度。</p><p>Demo 大致的过程是下载一个预先训练好的模型，这个模型是用来检测图片车辆的一个模型，然后针对例子中的车的图片，并做 Inference 的过程。</p></blockquote><a id="more"></a> <h2 id="软硬件环境"><a href="#软硬件环境" class="headerlink" title="软硬件环境"></a>软硬件环境</h2><ul><li>ubuntu 18.04 64bit</li><li>openvino_2020.3.341</li><li>yolov5 4.0</li></ul><h2 id="openvino是什么"><a href="#openvino是什么" class="headerlink" title="openvino是什么"></a>openvino是什么</h2><p><code>openvino</code>是一个用于解决在<code>intel</code>硬件平台上进行深度学习部署的方案，支持<code>windows</code>、<code>linux</code>和<code>macOS</code>。</p><h2 id="openvino环境搭建"><a href="#openvino环境搭建" class="headerlink" title="openvino环境搭建"></a>openvino环境搭建</h2><p>下载地址：<a href="https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html" target="_blank" rel="noopener">https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html</a></p><p>下载后解压并进入目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar xvf l_openvino_toolkit_p_2020.3.341.tgz</span><br><span class="line">cd l_openvino_toolkit_p_2020.3.341</span><br></pre></td></tr></table></figure><p>执行脚本开始按提示安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 或者使用带GUI界面的安装脚本sudo ./install_GUI.sh</span><br><span class="line">sudo ./install.sh</span><br></pre></td></tr></table></figure><p>接下来测试下安装是否成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/intel/openvino/deployment_tools/demo</span><br><span class="line">./demo_security_barrier_camera.sh</span><br><span class="line">./demo_security_barrier_camera.sh -d GPU(测试gpu环境)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210406193810747.png" alt></p><p>这就说明，<code>openvino</code>的环境安装成功了。</p><p>这里需要注意下，由于，在安装过程中，脚本已经帮我们设置了相关的环境，所以我们去测试时无需做任何设置。但是如果下次开机，或者打开新的<code>terminal</code>，我们就需要重新来设置环境，执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /opt/intel/openvino/bin/setupvars.sh</span><br></pre></td></tr></table></figure><p>或者直接将上述命令写入<code>~/.bashrc</code>中，就不用每次手动敲了。</p><p>目录<code>/opt/intel</code>下有<code>openvino</code>和<code>openvino_2020.3.341</code>2个目录，其实它们是同一个东西，<code>openvino</code>是个软链接文件。</p><h2 id="pt转onnx"><a href="#pt转onnx" class="headerlink" title="pt转onnx"></a>pt转onnx</h2><p>首先准备依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install onnx coremltools networkx defusedxml</span><br></pre></td></tr></table></figure><p>由于目前<code>openvino</code>的版本对<code>onnx opset</code> 11 版本后的支持有问题，因此需要修改文件<code>model/export.py</code>，将原来<code>opset_version</code>由12改为10，如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.onnx.export(model, img, f, verbose=False, opset_version=10, input_names=[&apos;images&apos;],</span><br><span class="line">                    output_names=[&apos;classes&apos;, &apos;boxes&apos;] if y is None else [&apos;output&apos;])</span><br></pre></td></tr></table></figure><p>接下来就可以将<code>yolov5</code>的<code>pt</code>模型转换成<code>onnx</code>格式了，这里使用其自带的<code>yolov5s.pt</code>模型进行测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python models/export.py --weights weights/yolov5s.pt --img-size 640 --batch-size 1</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210406193513763.png" alt></p><p>转换结束后，就会在<code>weights</code>文件夹下生成<code>yolov5s.onnx</code></p><h2 id="onnx转ir"><a href="#onnx转ir" class="headerlink" title="onnx转ir"></a>onnx转ir</h2><p><code>ir</code>即<code>Intermediate Representation</code>，<code>openvino</code>的模型优化器(<code>Model Optimizer</code>)会将给定的模型转化为标准的<code>ir</code>格式，并对其进行优化。</p><p>使用<code>openvino</code>自带的脚本，就可以完成从<code>.onnx</code>到<code>.bin</code>和<code>.xml</code>的转换，命令如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mo.py --input_model weights/yolov5s.onnx</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210406193617693.png" alt></p><h2 id="openvino测试"><a href="#openvino测试" class="headerlink" title="openvino测试"></a>openvino测试</h2><p>这里使用<code>c++</code>语言编写测试程序，下载地址：<a href="https://github.com/fb029ed/yolov5_cpp_openvino，非常感谢作者`fb029ed`的分享" target="_blank" rel="noopener">https://github.com/fb029ed/yolov5_cpp_openvino，非常感谢作者`fb029ed`的分享</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd yolov5_cpp_openvino/demo</span><br><span class="line">mkdir build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">./detect_test</span><br></pre></td></tr></table></figure><p>需要将前面生成的<code>yolov5s.bin</code>、<code>yolov5s.xml</code>和一张测试图片拷贝到<code>res</code>目录下，图片重命名为<code>bus.jpg</code></p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210406193652959.png" alt></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html" target="_blank" rel="noopener">https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html</a></li><li><a href="https://github.com/violet17/yolov5_demo" target="_blank" rel="noopener">https://github.com/violet17/yolov5_demo</a></li><li><a href="https://github.com/fb029ed/yolov5_cpp_openvino" target="_blank" rel="noopener">https://github.com/fb029ed/yolov5_cpp_openvino</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;OpenVINO是一个用于提升深度学习模型部署性能的跨平台的工具包，安装和使用都非常简单，下面 demo 视频是在 Mac 机器上运行的例子，安装过程在上面的文档链接中，安装主要是需要下载大量深度框架相关的库和模型，以及 Intel 已经封装好的一些脚本。&lt;/p&gt;
&lt;p&gt;之前在&lt;a href=&quot;https://yuanquanquan.top/2020/20201027/&quot;&gt;神经计算棒&lt;/a&gt;上部署过openvino来测试神经棒的速度。&lt;/p&gt;
&lt;p&gt;Demo 大致的过程是下载一个预先训练好的模型，这个模型是用来检测图片车辆的一个模型，然后针对例子中的车的图片，并做 Inference 的过程。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="openvino" scheme="http://yuanquanquan.top/tags/openvino/"/>
    
  </entry>
  
  <entry>
    <title>模型训练完成后自动发送微信、邮件</title>
    <link href="http://yuanquanquan.top/2021/20210402/"/>
    <id>http://yuanquanquan.top/2021/20210402/</id>
    <published>2021-04-01T19:05:17.000Z</published>
    <updated>2021-04-01T19:58:48.308Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这是篇关于炼丹期间如何摸鱼的文章～～</p></blockquote><a id="more"></a><h2 id="微信设置"><a href="#微信设置" class="headerlink" title="微信设置"></a>微信设置</h2><p>本来可以通过itcahr，在模型训练完成后向文件传输助手发送消息来告知模型已经训练完毕。但是现在貌似网页版微信已经不可以使用了，所以基于ServerChan发送消息，分享给大家。</p><h3 id="Server-Chan"><a href="#Server-Chan" class="headerlink" title="Server Chan"></a>Server Chan</h3><p>首先需要在server酱官网：<a href="http://sc.ftqq.com/3.version注册账号，直接绑定github账号即可，会获得一个SCKEY" target="_blank" rel="noopener">http://sc.ftqq.com/3.version注册账号，直接绑定github账号即可，会获得一个SCKEY</a></p><p>发送消息非常简单，只需要向以下URL发一个GET或者POST请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://sc.ftqq.com/SCU167709T5fac1f4393bbd77463bb8b3d13ce33836066159859760.send</span><br></pre></td></tr></table></figure><p>接受两个参数：</p><ul><li>text：消息标题，最长为256，必填。</li><li>desp：消息内容，最长64Kb，可空，支持MarkDown。</li></ul><p>最简单的消息发送方式是通过浏览器，在地址栏输入以下URL，回车后即可发送：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://sc.ftqq.com/SCU167709T5fac1f4393bbd77463bb8b3d13ce33836066159859760.send?text=主人服务器又挂掉啦~</span><br></pre></td></tr></table></figure><hr><p>在PHP中，可以直接用file_get_contents来调用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file_get_contents(&apos;https://sc.ftqq.com/SCU167709T5fac1f4393bbd77463bb8b3d13ce33836066159859760.send?text=&apos;.urlencode(&apos;主人服务器又挂掉啦~&apos;));</span><br></pre></td></tr></table></figure><p>可以把它封装成一个函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">function sc_send(  $text , $desp = &apos;&apos; , $key = &apos;SCU167709T5fac1f4393bbd77463bb8b3d13ce33836066159859760&apos;  )</span><br><span class="line">&#123;</span><br><span class="line">$postdata = http_build_query(</span><br><span class="line">    array(</span><br><span class="line">        &apos;text&apos; =&gt; $text,</span><br><span class="line">        &apos;desp&apos; =&gt; $desp</span><br><span class="line">    )</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">$opts = array(&apos;http&apos; =&gt;</span><br><span class="line">    array(</span><br><span class="line">        &apos;method&apos;  =&gt; &apos;POST&apos;,</span><br><span class="line">        &apos;header&apos;  =&gt; &apos;Content-type: application/x-www-form-urlencoded&apos;,</span><br><span class="line">        &apos;content&apos; =&gt; $postdata</span><br><span class="line">    )</span><br><span class="line">);</span><br><span class="line">$context  = stream_context_create($opts);</span><br><span class="line">return $result = file_get_contents(&apos;https://sc.ftqq.com/&apos;.$key.&apos;.send&apos;, false, $context);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>直接用pip安装dinglingling插件或者使用上文的github链接中的代码根据教程自行安装后即可。<br>直接在需要监测的函数前加上装饰器wx_reminder即可。使用demo代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dinglingling <span class="keyword">import</span> wx_reminder</span><br><span class="line"></span><br><span class="line">SCKEY = <span class="string">""</span> <span class="comment"># the sckey you get from http://sc.ftqq.com/3.version</span></span><br><span class="line">proxy = <span class="string">""</span> <span class="comment"># if you needn't proxy, ignore it.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@wx_reminder(SCKEY=SCKEY, proxy=proxy, remind_started=True)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_correct_func</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"hello world"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@wx_reminder(SCKEY=SCKEY, proxy=proxy)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_error</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">raise</span> Exception(<span class="string">"test error"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    test_correct_func()</span><br><span class="line">    <span class="comment"># test_error()</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210402035305911.png" alt></p><h3 id="邮件"><a href="#邮件" class="headerlink" title="邮件"></a>邮件</h3><ul><li>设置好邮箱的smtp服务，（和outlook客户端收发邮件一个道理）</li><li>把需要发送的信息作为邮件发送出去</li></ul><p>核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import smtplib for the actual sending function</span></span><br><span class="line"><span class="keyword">import</span> smtplib</span><br><span class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</span><br><span class="line"><span class="keyword">from</span> email.header <span class="keyword">import</span> Header</span><br><span class="line"><span class="keyword">from</span> email.mime.image <span class="keyword">import</span> MIMEImage</span><br><span class="line"><span class="keyword">from</span> email.mime.multipart <span class="keyword">import</span> MIMEMultipart</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_email</span><span class="params">(subject=<span class="string">"No subject"</span>, content=<span class="string">"I am boring"</span>)</span>:</span></span><br><span class="line">    mail_host = <span class="string">"smtp.163.com"</span></span><br><span class="line">    mail_user = <span class="string">"yuetan@163.com"</span></span><br><span class="line">    mail_pw = <span class="string">"********"</span>  <span class="comment"># 授权码</span></span><br><span class="line">    sender = <span class="string">"yuetan@163.com"</span></span><br><span class="line">    receiver = <span class="string">"yuetan@lynk.com"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the container (outer) email message.</span></span><br><span class="line">    msg = MIMEText(content, <span class="string">"plain"</span>, <span class="string">"utf-8"</span>)</span><br><span class="line">    msg[<span class="string">'Subject'</span>] = subject</span><br><span class="line">    msg[<span class="string">'From'</span>] = sender</span><br><span class="line">    msg[<span class="string">'To'</span>] = receiver</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        smtp = smtplib.SMTP_SSL(mail_host, <span class="number">994</span>)  <span class="comment"># 实例化smtp服务器</span></span><br><span class="line">        smtp.login(mail_user, mail_pw)  <span class="comment"># 登录</span></span><br><span class="line">        smtp.sendmail(sender, receiver, msg.as_string())</span><br><span class="line">        print(<span class="string">"Email send successfully"</span>)</span><br><span class="line">    <span class="keyword">except</span> smtplib.SMTPException:</span><br><span class="line">        print(<span class="string">"Error: email send failed"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    send_email(subject=<span class="string">"Training finished"</span>, content=<span class="string">"I am boring"</span>)</span><br></pre></td></tr></table></figure><h2 id="邮箱设置"><a href="#邮箱设置" class="headerlink" title="邮箱设置"></a>邮箱设置</h2><p>程序中有个mail_pw是邮箱授权码，可以通过自己的邮箱获取。登录自己常用的邮箱，以163为例。打开设置，将SMPT服务开启：</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210402034224166.png" alt></p><p>开启后，通过发送短信得到授权密码。（注意保密，泄漏后就相当于邮箱密码泄漏了）</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210402034141089.png" alt></p><p>将授权密码赋值给程序中的mail_pw变量即可。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li>Loss出现NAN时，自动结束训练。可通过assert或if判断，否则继续训练没有意义了</li><li>可配合nohup命令使用，即使断开服务器连接也在后台继续训练</li><li>如果想定时发送邮件，可配合crontab命令</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这是篇关于炼丹期间如何摸鱼的文章～～&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="远程炼丹" scheme="http://yuanquanquan.top/tags/%E8%BF%9C%E7%A8%8B%E7%82%BC%E4%B8%B9/"/>
    
  </entry>
  
  <entry>
    <title>When Non-Convex Modeling meets Hyperspectral Remote Sensing</title>
    <link href="http://yuanquanquan.top/2021/20201108/"/>
    <id>http://yuanquanquan.top/2021/20201108/</id>
    <published>2021-03-28T07:11:48.000Z</published>
    <updated>2021-03-29T07:35:51.015Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>第一次看高光谱相关的paper，大概？由于光谱波段之间的高度相关特性，高光谱图像受到信息冗余的影响，这可能会降低在某些极端条件的情况下对材质的判别能力。此外，高光谱纬度沿着光谱域的逐渐增加，需要大存储功能和高性能计算。因此，对高光谱图像进行降维处理，具有重要的研究和实际意义。</p><p>本文从<strong>无监督、监督和半监督模型</strong>三个方面介绍基于非凸模型的高光谱图像降维方法。</p></blockquote><a id="more"></a>  <p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328150717518.png" alt></p><h2 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1. Abstract"></a><strong>1. Abstract</strong></h2><p>由于光谱波段之间的高度相关特性，高光谱图像受到信息冗余的影响，这可能会降低在某些极端条件的情况下对材质的判别能力。此外，高光谱纬度沿着光谱域的逐渐增加，需要大存储功能和高性能计算。因此，对高光谱图像进行降维处理，具有重要的研究和实际意义。</p><div class="row">    <embed src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding/Hyperspectral/2.pdf" width="100%" height="550" type="application/pdf"></div><p>本文从<strong>无监督、监督和半监督模型</strong>三个方面介绍基于非凸模型的高光谱图像降维方法。</p><h2 id="2-无监督模型"><a href="#2-无监督模型" class="headerlink" title="2. 无监督模型"></a><strong>2. 无监督模型</strong></h2><p><strong>非负矩阵分解（NMF）</strong>是一种常用的无监督学习模型，在NMF模型的基础上，Yan等人提出了图正则化正交NMF（GONMF）的高光谱降维方法，Wen等结合多个特征，对GONMF模型进行了扩展，提高了高光谱图像降维性能。Rasti等人设计了一种正交全变分成分分析（OTVCA）的高光谱特征提取方法。下面是论文中总结的常用正则项，主要有稀疏正则化、图正则化、多图正则化、全变分和低秩图正则化。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328152012870.png" alt></p><p>另外一种无监督降维方法是<strong>图嵌入</strong>，也称之为<strong>流形学习</strong>，这类方法之间的差异主要是图的构建方式。Ma等人将KNN分类器与代表性的流形学习算法进行了融合，包括局部线性嵌入、拉普拉斯特征图等。Huang等人嵌入了稀疏图结构，通过解决L1范数优化问题，实现了高光谱图像降维。He等人在稀疏图结构的基础上，提出了加权稀疏图，Hong等人则提出了一种新的空间-光谱图结构进行高光谱图像降维，称为RLMR。下面是刚才提到的稀疏图、加权稀疏图、空间-光谱图和稀疏低秩图正则项。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328145354512.png" alt></p><h2 id="3-监督模型"><a href="#3-监督模型" class="headerlink" title="3. 监督模型"></a><strong>3. 监督模型</strong></h2><p>无监督模型主要是依赖于嵌入不同的先验信息来降低高光谱图像纬度，而监督模型则是<strong>使用标签信息学习类别分离的低维表示</strong>。监督模型可以分为两大类，如下图所示，第一类是与图嵌入和流形学习相关的判别分析，使用标签信息来构建图结构，得到更具判别性的子空间。研究人员提出了稀疏图判别分析、协同图判别分析、特征空间判别分析和空间-光谱局部判别嵌入等一系列方法。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328144250420.png" alt></p><p>第二类方法是<strong>基于回归的监督降维模型</strong>，Hong等人以拉普拉斯矩阵形式学习低维空间表示，更进一步，Hong等人使用k层线性回归将这个模型扩展为一个深度模型，称之为JPlay，该模型试图通过多层线性建模打开深度网络“黑盒子”。</p><h2 id="4-半监督模型"><a href="#4-半监督模型" class="headerlink" title="4. 半监督模型"></a><strong>4. 半监督模型</strong></h2><p>由于带标签信息的数据获取成本高，联合使用带标签和无标签信息逐渐成为热门的研究方向。半监督学习一种简单的方式是<strong>结合监督和无监督模型</strong>，比如局部判别分析和局部保持投影相结合，以及半监督判别分析（SSDA）等方法，Zhao等人使用标签传播预测的伪标签，进一步提升了SSDA性能。类似的，Wu等人使用Dirichlet过程混合模型生成伪标签，学习了低维高光谱嵌入子空间。</p><p>另外一种方式是在半监督降维任务中模拟人类大脑行为，通过在图结构上自适应学习标签传播过程，提出了迭代多任务学习框架，实现了更高效和更有效的高光谱图像降维。</p><h2 id="5-对比实验分析"><a href="#5-对比实验分析" class="headerlink" title="5. 对比实验分析"></a><strong>5. 对比实验分析</strong></h2><p>下面展示了经过不同降维方法后高光谱图像分类精度。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/640-20210328144250420.png" alt></p><h2 id="6-挑战"><a href="#6-挑战" class="headerlink" title="6. 挑战"></a><strong>6. 挑战</strong></h2><p>尽管目前已经提出了许多优秀的高光谱图像降维方法，仍然存在诸多的挑战，论文总结了以下三点：</p><p><strong>Optimal Subspace Dimension：最优的子空间维度</strong>，降维后的高光谱图像维度是降维方法最关键的参数，尽管目前已经有一些子空间维度估计算法，仍然需要很多先验信息和人类的干预。</p><p><strong>Effects of Noises：噪声的影响</strong>，高光谱图像中存在复杂的噪声，在降维过程中如何避免不同类型和不同强度噪声的影响，仍然存在很大的挑战。</p><p><strong>Robustness and Generalization：鲁棒性和泛化性</strong>，复杂的噪声类型、有限的训练样本影响了降维方法的鲁棒性和泛化性，下一代降维方法应当考虑更鲁棒和智能模型。</p><p>论文地址：<a href="https://arxiv.org/abs/2103.01449" target="_blank" rel="noopener">https://arxiv.org/abs/2103.01449</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;第一次看高光谱相关的paper，大概？由于光谱波段之间的高度相关特性，高光谱图像受到信息冗余的影响，这可能会降低在某些极端条件的情况下对材质的判别能力。此外，高光谱纬度沿着光谱域的逐渐增加，需要大存储功能和高性能计算。因此，对高光谱图像进行降维处理，具有重要的研究和实际意义。&lt;/p&gt;
&lt;p&gt;本文从&lt;strong&gt;无监督、监督和半监督模型&lt;/strong&gt;三个方面介绍基于非凸模型的高光谱图像降维方法。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Hyperspectral" scheme="http://yuanquanquan.top/tags/Hyperspectral/"/>
    
  </entry>
  
  <entry>
    <title>写给憨憨姐姐的远程炼丹指南</title>
    <link href="http://yuanquanquan.top/2021/202103225/"/>
    <id>http://yuanquanquan.top/2021/202103225/</id>
    <published>2021-03-26T14:14:49.000Z</published>
    <updated>2021-03-27T14:10:44.433Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前写过<a href="https://yuanquanquan.top/2019/2019121111/">jupyter notebook如何远程炼丹</a>，今天给憨憨姐姐写一个远程炼丹指南，介绍一下PyCharm的SFTP功能</p><p>SFTP有两点要注意：</p><ul><li>配置远程连接信息。</li><li>设置本地和远程路径的映射。</li></ul><p>除了SFTP的映射外，PyCharm还支持直接使用远程的解释器，这样就多了一步：</p><ul><li>设置远程解释器。</li></ul></blockquote><a id="more"></a>  <h2 id="1-SFTP配置"><a href="#1-SFTP配置" class="headerlink" title="1. SFTP配置"></a>1. SFTP配置</h2><p>配置过程如下：</p><h3 id="1-打开配置界面"><a href="#1-打开配置界面" class="headerlink" title="1).打开配置界面"></a>1).打开配置界面</h3><ul><li>Tools-&gt;Deployment-&gt;Configuration</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327211520368.png" alt></p><h3 id="2-设置连接信息"><a href="#2-设置连接信息" class="headerlink" title="2).设置连接信息"></a>2).设置连接信息</h3><ul><li>在新建的配置界面中输入SFTP host、Port、Root path、User name、Password等。root path是可以自动检测的，在输入了其他部分后，点击test sftp connection来确认地址和用户信息是否正确，之后点击autodetect就可以自动补全root path。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327212342752.png" alt></p><h3 id="3-设置文件夹映射"><a href="#3-设置文件夹映射" class="headerlink" title="3).设置文件夹映射"></a>3).设置文件夹映射</h3><p>最后选择<code>Mappings</code>，mappings中填写你本地工程的地址和服务器那边的地址，配置好后当你在本地改程序的时候，程序就会同步到服务器上，相当于一个自动的git了~。</p><p>将本地路径与远端路径进行选择，远端路径就市部署到Linux上的路径，即本地文件上传的位置，进行保存，远端配置文件就配置完成了。</p><p>excluded path不用填</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327212450603.png" alt></p><p>到这里，SFTP就设置完毕了，接下来，再讲下如何设置远程解释器</p><h2 id="二、远程解释器配置"><a href="#二、远程解释器配置" class="headerlink" title="二、远程解释器配置"></a><strong>二、远程解释器配置</strong></h2><p>通过Files-&gt;settings路径进入配置界面，然后我们选择项目解释器<strong>Project Interpreter</strong>，选择ssh解释器已经存在的配置</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327214902257.png" alt></p><h2 id="三、将项目代码上传到远端服务器"><a href="#三、将项目代码上传到远端服务器" class="headerlink" title="三、将项目代码上传到远端服务器"></a><strong>三、将项目代码上传到远端服务器</strong></h2><p>最后还得选择上传到远端服务器的位置勾选<code>Automatically upload</code>选项就是在<code>Finish</code>之后将所有项目代码自动上传。</p><p><img src="https://cdn.jsdelivr.net/gh/Bazingaliu/clouding@master/image-20210327215815546.png" alt></p><p>上传速度很快，耐心等待后，整个项目就完全部署到远端服务器上了，这时候在本地运行，实际上代码是跑在远端服务器上的。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;之前写过&lt;a href=&quot;https://yuanquanquan.top/2019/2019121111/&quot;&gt;jupyter notebook如何远程炼丹&lt;/a&gt;，今天给憨憨姐姐写一个远程炼丹指南，介绍一下PyCharm的SFTP功能&lt;/p&gt;
&lt;p&gt;SFTP有两点要注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配置远程连接信息。&lt;/li&gt;
&lt;li&gt;设置本地和远程路径的映射。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了SFTP的映射外，PyCharm还支持直接使用远程的解释器，这样就多了一步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设置远程解释器。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="远程炼丹" scheme="http://yuanquanquan.top/tags/%E8%BF%9C%E7%A8%8B%E7%82%BC%E4%B8%B9/"/>
    
  </entry>
  
</feed>
