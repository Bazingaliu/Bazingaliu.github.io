<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yuanquanquan的个人博客 | 我愿做你光华中淡淡的一笔</title>
  
  <subtitle>我愿做你光华中淡淡的一笔</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanquanquan.top/"/>
  <updated>2019-12-30T14:12:41.751Z</updated>
  <id>http://yuanquanquan.top/</id>
  
  <author>
    <name>理科生写给世界的情书</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2019年计算机视觉综述论文汇聚</title>
    <link href="http://yuanquanquan.top/2019/20191230/"/>
    <id>http://yuanquanquan.top/2019/20191230/</id>
    <published>2019-12-30T13:23:38.000Z</published>
    <updated>2019-12-30T14:12:41.751Z</updated>
    
    <content type="html"><![CDATA[<p>​        <img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=4102261544,420722754&amp;fm=26&amp;gp=0.jpg" alt="img"></p><p>​        本文整理了2019年计算机视觉方面的综述论文，包含<strong>目标检测</strong>、<strong>图像分割(含语义/实例分割)</strong>、<strong>目标跟踪</strong>、<strong>医学图像分割</strong>、<strong>显著性目标检测</strong>、<strong>行为识别</strong>、<strong>深度估计</strong>等。可以使读者对相关领域有一个系统的了解。很适合初学者以及相关领域的研究人员。</p><p><strong><em>object detection</em></strong></p><ol><li>Imbalance Problems in Object Detection: A Reviewintro: under review at TPAMI</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1909.00169" target="_blank" rel="noopener">https://arxiv.org/abs/1909.00169</a></p><ol start="2"><li>Recent Advances in Deep Learning for Object Detectionintro: From 2013 (OverFeat) to 2019 (DetNAS)</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1908.03673" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03673</a></p><ol start="3"><li>A Survey of Deep Learning-based Object Detectionintro：From Fast R-CNN to NAS-FPN</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1907.09408" target="_blank" rel="noopener">https://arxiv.org/abs/1907.09408</a></p><ol start="4"><li>Object Detection in 20 Years: A Surveyintro：This work has been submitted to the IEEE TPAMI for possible publication</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1905.05055" target="_blank" rel="noopener">https://arxiv.org/abs/1905.05055</a></p><p><strong><em>图像分割</em></strong></p><ol><li>Deep Semantic Segmentation of Natural and Medical Images: A Reviewintro</li></ol><p>从 FCN(2014) 到 Auto-DeepLab(2019)，本综述共含179篇语义分割和医学图像分割参考文献</p><p>arXiv: <a href="https://arxiv.org/abs/1910.07655" target="_blank" rel="noopener">https://arxiv.org/abs/1910.07655</a></p><ol start="2"><li>Understanding Deep Learning Techniques for Image Segmentationintro</li></ol><p>本综述介绍了从2013年到2019年，主流的30多种分割算法（含语义/实例分割），50多种数据集，共计224篇参考文献</p><p>arXiv: <a href="https://arxiv.org/abs/1907.06119" target="_blank" rel="noopener">https://arxiv.org/abs/1907.06119</a></p><p><strong><em>目标跟踪</em></strong></p><ol><li>A Review of Visual Trackers and Analysis of its Application to Mobile Robotintro</li></ol><p>本目标跟踪综述共含185篇参考文献！从传统方法到最新的深度学习网络</p><p>arXiv: <a href="https://arxiv.org/abs/1910.09761" target="_blank" rel="noopener">https://arxiv.org/abs/1910.09761</a></p><ol start="2"><li>Deep Learning in Video Multi-Object Tracking: A Surveyintro</li></ol><p>38页目标跟踪综述，含30多种主流算法，共计174篇参考文献</p><p>arXiv: <a href="https://arxiv.org/abs/1907.12740" target="_blank" rel="noopener">https://arxiv.org/abs/1907.12740</a></p><p><strong><em>超分辨率</em></strong></p><ol><li>A Deep Journey into Super-resolution: A survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1904.07523" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07523</a></p><ol start="2"><li>Deep Learning for Image Super-resolution: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1902.06068" target="_blank" rel="noopener">https://arxiv.org/abs/1902.06068</a></p><p><strong><em>医学图像分割</em></strong></p><ol><li>Deep learning for cardiac image segmentation: A reviewintro</li></ol><p>本医学图像分割综述从FCN(2014)到Dense U-net(2019)，超过250篇的参考文献（论文中光画图的工作量就超级大）</p><p>arXiv: <a href="https://arxiv.org/abs/1911.03723" target="_blank" rel="noopener">https://arxiv.org/abs/1911.03723</a></p><ol start="2"><li>Machine  Learning Techniques for Biomedical Image Segmentation: An Overview of  Technical Aspects and Introduction to State-of-Art Applications</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1911.02521" target="_blank" rel="noopener">https://arxiv.org/abs/1911.02521</a></p><p><strong><em>显著性目标检测</em></strong></p><ol><li>Salient Object Detection in the Deep Learning Era: An In-Depth Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1904.09146" target="_blank" rel="noopener">https://arxiv.org/abs/1904.09146</a></p><p>github: <a href="https://github.com/wenguanwang/SODsurvey" target="_blank" rel="noopener">https://github.com/wenguanwang/SODsurvey</a></p><p><strong><em>行为识别</em></strong></p><ol><li>Spatio-temporal Action Recognition: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1901.09403" target="_blank" rel="noopener">https://arxiv.org/abs/1901.09403</a></p><p><strong><em>深度估计</em></strong></p><ol><li>Monocular Depth Estimation: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/abs/1901.09402" target="_blank" rel="noopener">https://arxiv.org/abs/1901.09402</a></p><p>地址连接：<a href="https://github.com/FranxYao/Deep-Generative-Models-for-Natural-Language-Processing" target="_blank" rel="noopener">https://github.com/FranxYao/Deep-Generative-Models-for-Natural-Language-Processing</a></p><p><strong><em>AutoML + NAS</em></strong></p><ol><li>Neural Architecture Search: A Survey</li></ol><p>arXiv: <a href="https://arxiv.org/pdf/1808.05377" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.05377</a></p><p><strong><em>GAN</em></strong></p><ol><li>The Six Fronts of the Generative Adversarial Networks</li></ol><p>arXiv: <a href="https://arxiv.org/pdf/1910.13076" target="_blank" rel="noopener">https://arxiv.org/pdf/1910.13076</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​        &lt;img src=&quot;https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=4102261544,420722754&amp;amp;fm=26&amp;amp;gp=0.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CV" scheme="http://yuanquanquan.top/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>leetcode 483.最小好进制</title>
    <link href="http://yuanquanquan.top/2019/2019122011/"/>
    <id>http://yuanquanquan.top/2019/2019122011/</id>
    <published>2019-12-20T03:09:20.000Z</published>
    <updated>2019-12-20T03:29:34.939Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static.leetcode-cn.com/cn-mono-assets/production/head/f2ece5fe978d056f5a425ad3387216ee.svg" alt></p><h1 id="最小好进制"><a href="#最小好进制" class="headerlink" title="最小好进制"></a>最小好进制</h1><h1 id="题目链接"><a href="#题目链接" class="headerlink" title="题目链接"></a>题目链接</h1><p>英文链接：<a href="https://leetcode.com/problems/smallest-good-base/" target="_blank" rel="noopener">https://leetcode.com/problems/smallest-good-base/</a></p><p>中文链接：<a href="https://leetcode-cn.com/problems/smallest-good-base/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/smallest-good-base/</a></p><h1 id="题目详述"><a href="#题目详述" class="headerlink" title="题目详述"></a>题目详述</h1><p>对于给定的整数 n, 如果n的k（k&gt;=2）进制数的所有数位全为1，则称 k（k&gt;=2）是 n 的一个<strong>好进制</strong>。</p><p>以字符串的形式给出 n, 以字符串的形式返回 n 的最小好进制。</p><p>   示例 1：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;13&quot;输出：&quot;3&quot;解释：13 的 3 进制是 111。</span><br></pre></td></tr></table></figure><p>示例 2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;4681&quot;输出：&quot;8&quot;解释：4681 的 8 进制是 11111。</span><br></pre></td></tr></table></figure><p>示例 3：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;1000000000000000000&quot;输出：&quot;999999999999999999&quot;解释：1000000000000000000 的 999999999999999999 进制是 11。</span><br></pre></td></tr></table></figure><p>提示：</p><ol><li><p>n的取值范围是 [3, 10^18]。</p></li><li><p>输入总是有效且没有前导 0。、</p></li></ol><h1 id="题目详解"><a href="#题目详解" class="headerlink" title="题目详解"></a>题目详解</h1><p>   ​        本题是寻找一个数最小的good base。其定义是对于一个数y，其x进制表示为全1，则称x是y的good base。应该比较好理解，其实就是将y写成1+x+x^2 +…+x^(n-1)，就是一个等比数列求和，于是我们可以将其转化为y = (x^n - 1)/(x - 1)，其中x&gt;=2, 3&lt;y&lt;10^18,为了寻找最小的x，我们可以先来确定一下n的取值范围，很明显x越小n越大，所以当x=2时，n最大为log2(y+1)。从第三个例子可以看出来，当x=y-1时，n最小为2。所以有了n的取值范围我们就可以遍历所有可能的n，然后每次循环中y和n都是确定值，在对x使用二叉搜索确定其值即可。</p><p>   这里有两个变量，一个是进制，一个是对应进制下的长度。可以固定一个变量，然后判断是否满足条件即可。进制的取值范围为 <code>[2, n - 1]</code>，范围太大，所以应该固定长度，判断是否存在满足条件的进制。进制越小，长度越长；进制越大，长度越短。我们可以从大到小枚举长度，判断是否存在满足条件的进制，这一步可以运用二分查找。</p><p>枚举长度的时间复杂度为 <code>O(logn)</code>，二分查找的时间复杂度为 <code>O(logn)</code>，辅助计算的时间复杂度为 <code>O(logn)</code>，总的时间复杂度为 <code>O(logn ^ 3)</code>。</p><p>另外一个需要注意的问题就是，因为本题中的数都比较大，所以要注意溢出问题，之前也做过一到这种题，可以使用java内置的BigInteger类进行处理。代码如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.math.BigInteger;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">smallestGoodBase</span><span class="params">(String n)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//现将字符串解析成long型数据</span></span><br><span class="line">        <span class="keyword">long</span> s = Long.parseLong(n);</span><br><span class="line">        <span class="comment">//对所有可能的指数n进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> max_e = (<span class="keyword">int</span>) (Math.log(s) / Math.log(<span class="number">2</span>)) + <span class="number">1</span>; max_e &gt;= <span class="number">2</span>; max_e--) &#123;</span><br><span class="line">            <span class="keyword">long</span> low = <span class="number">2</span>, high = s, mid;</span><br><span class="line">            <span class="comment">//进行二叉搜索，寻找最小的good base。</span></span><br><span class="line">            <span class="keyword">while</span> (low &lt;= high) &#123;</span><br><span class="line">                mid = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">                <span class="comment">//一开始没有使用BigInteger，会报错</span></span><br><span class="line">                BigInteger left = BigInteger.valueOf(mid);</span><br><span class="line">                left = left.pow(max_e).subtract(BigInteger.ONE);</span><br><span class="line">                BigInteger right = BigInteger.valueOf(s).multiply(BigInteger.valueOf(mid).subtract(BigInteger.ONE));</span><br><span class="line">                <span class="keyword">int</span> cmr = left.compareTo(right);</span><br><span class="line">                <span class="keyword">if</span> (cmr == <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">return</span> String.valueOf(mid);</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (cmr &gt; <span class="number">0</span>)</span><br><span class="line">                    high = mid - <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    low = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> String.valueOf(s - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>不过看题解的时候看到一个大佬的神仙解法，如下</p><p>有点类似于梯度下降</p><p>假设有两个变量x, y，求解方程 f(x, y) = z<br>当f(x, y)相对于x和y都是单调的时候<br>则可以先固定x，求下一个满足条件的y边界，然后固定y求下一个满足条件的x边界，</p><p>这样不断逼近最终解</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> ll = <span class="keyword">long</span> <span class="keyword">long</span>;</span><br><span class="line">    <span class="function">ll <span class="title">nextK</span><span class="params">(ll k, ll m, ll N, <span class="keyword">bool</span>&amp; match)</span> </span>&#123;</span><br><span class="line">        match = <span class="literal">false</span>;</span><br><span class="line">        ll lo = k + <span class="number">1</span>;</span><br><span class="line">        ll hi = N - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (lo &lt;= hi) &#123;</span><br><span class="line">            ll md = lo + (hi - lo) / <span class="number">2</span>;</span><br><span class="line">            ll s = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (ll i = <span class="number">0</span>; i &lt;= m; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s &gt; (N - <span class="number">1</span>) / md) &#123;</span><br><span class="line">                    s = N + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                s = s * md + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s == N) &#123;</span><br><span class="line">                match = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">return</span> md;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s &gt; N) hi = md - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> lo = md + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> lo;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">ll <span class="title">nextM</span><span class="params">(ll k, ll m, ll N, <span class="keyword">bool</span>&amp; match)</span> </span>&#123;</span><br><span class="line">        match = <span class="literal">false</span>;</span><br><span class="line">        ll lo = <span class="number">1</span>;</span><br><span class="line">        ll hi = m - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (lo &lt;= hi) &#123;</span><br><span class="line">            ll md = lo + (hi - lo) / <span class="number">2</span>;</span><br><span class="line">            ll s = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (ll i = <span class="number">0</span>; i &lt;= md; ++i) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s &gt; (N - <span class="number">1</span>) / k) &#123;</span><br><span class="line">                    s = N + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                s = s * k + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s == N) &#123;</span><br><span class="line">                match = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">return</span> md;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s &gt; N) hi = md - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> lo = md + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> hi;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">smallestGoodBase</span><span class="params">(<span class="built_in">string</span> n)</span> </span>&#123;</span><br><span class="line">        ll N = stoll(n);</span><br><span class="line">        ll k = <span class="number">2</span>;</span><br><span class="line">        ll m = N; <span class="comment">// m为N表示为k进制的最高次幂</span></span><br><span class="line">        <span class="keyword">bool</span> match = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">while</span> (k &lt; N &amp;&amp; m &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            m = nextM(k, m, N, match);</span><br><span class="line">            <span class="keyword">if</span> (match) <span class="keyword">break</span>;</span><br><span class="line">            k = nextK(k, m, N, match);</span><br><span class="line">            <span class="keyword">if</span> (match) <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> to_string(k);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://static.leetcode-cn.com/cn-mono-assets/production/head/f2ece5fe978d056f5a425ad3387216ee.svg&quot; alt&gt;&lt;/p&gt;
&lt;h1 id=&quot;最小好进制&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="算法" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="LeetCode" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="http://yuanquanquan.top/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>CSRNet：Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes</title>
    <link href="http://yuanquanquan.top/2019/2019121899/"/>
    <id>http://yuanquanquan.top/2019/2019121899/</id>
    <published>2019-12-18T11:49:36.000Z</published>
    <updated>2019-12-19T11:03:28.679Z</updated>
    
    <content type="html"><![CDATA[<h3 id="主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域，-生成高质量的人群分布密度图。"><a href="#主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域，-生成高质量的人群分布密度图。" class="headerlink" title="主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1/8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域， 生成高质量的人群分布密度图。"></a>主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1/8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持分辨率的同时扩大感知域， 生成高质量的人群分布密度图。</h3><h5 id="1、MCNN的多列设计没有显著作用："><a href="#1、MCNN的多列设计没有显著作用：" class="headerlink" title="1、MCNN的多列设计没有显著作用："></a>1、MCNN的多列设计没有显著作用：</h5><p>​    以前的拥挤场景分析工作主要基于<code>multi-scale architectures</code>。它们在该领域取得了很高的性能，但是当网络变得更深时，它们使用的设计也带来了两个显着的缺点：大量的训练时间和无效的分支结构（例如，MCNN ）。我们设计了一个实验来证明MCNN与表1中更深入的常规网络相比表现不佳。</p><p><img src="http://i1.fuimg.com/706216/912fff8efa8ec71f.png" alt></p><pre><code>如我们先前所知，MCNN的每列专用于某一级别的拥塞场景。但是，使用MCNN的有效性可能并不突出。我们在图2中展示了MCNN中三个独立列（代表大，中，小的感受野）所学习的特征，并用ShanghaiTech Part A [18]数据集进行评估。该图中的三条曲线与具有不同拥塞密度的50个测试案例共享非常相似的模式（估计的错误率），这意味着这种分支结构中的每个列学习几乎相同的特征。它违背了MCNN设计的初衷，用于学习每列的不同功能。</code></pre><p><img src="http://i1.fuimg.com/706216/c0b69c6df04e6684.png" alt></p><h5 id="2、膨胀卷积优于反卷积"><a href="#2、膨胀卷积优于反卷积" class="headerlink" title="2、膨胀卷积优于反卷积"></a>2、膨胀卷积优于反卷积</h5><p>​    已经在分割任务中证明了膨胀卷积层，其精度得到显着提高，并且它是池化层的良好替代方案。 尽管池化层（例如，最大和平均池化）被广泛用于维持不变性和控制过度拟合，但是它们还显着地降低了空间分辨率，这意味着丢失了特征映射的空间信息。 反卷积层可以减轻信息的丢失，但额外的复杂性和执行延迟可能并不适合所有情况。 膨胀卷积是一个更好的选择，它使用稀疏内核（如图3所示）来交替汇集和卷积层。 该字符在不增加参数数量或计算量的情况下扩大了感受野（例如，添加更多卷积层可以产生更大的感受野但引入更多操作）。<br>​     为了保持特征图的分辨率，与使用<code>卷积+池化+反卷积</code>的方案相比，<code>膨胀卷积</code>显示出明显的优点。我们在图4中选择一个例子用于说明。输入是人群的图像，并且它分别通过两种方法处理以产生具有相同大小的输出。在第一种方法中，输入由具有因子2的最大池化层进行下采样，然后将其传递到具有3X3 Sobel内核的卷积层。由于生成的特征映射仅是原始输入的1/2，因此需要通过解卷积层（双线性插值）<code>bilinear interpolation</code>对其进行上采样。在另一种方法中，我们尝试扩张卷积并使相同的3X3 Sobel内核适应具有因子= 2步幅的扩张内核。输出与输入共享相同的维度。最重要的是，扩张卷积的输出包含更详细的信息。</p><p><img src="http://i1.fuimg.com/706216/39f9273771846a6b.png" alt></p><h3 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h3><p>​    在本文中，我们设计了一个更深入的网络，称为<code>CSR-Net</code>，用于计算人群和生成高质量的密度图。我们的模型使用<code>纯卷积层</code>作为主干，以灵活的分辨率支持输入图像。为了限制网络的复杂性，我们在所有层中使用<code>小尺寸</code>的卷积滤波器（如3x3）。我们将VGG-16 [21]的前10层作为前端和<code>膨胀卷积层</code>作为后端部署，以扩大感受域并提取更深的特征而不会丢失分辨率（因为不使用池化层）。</p><h3 id="主要实现："><a href="#主要实现：" class="headerlink" title="主要实现："></a>主要实现：</h3><h5 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h5><p>​    此前端网络的输出大小是原始输入大小的<code>1/8</code>。如果我们继续堆叠更多卷积层和池化层（VGG-16中的基本组件），输出大小将进一步缩小，并且很难生成高质量的密度映射。 我们尝试将膨胀卷积层作为后端来提取更深层的显着性信息以及维持输出分辨率。<br>​     我们在表3中提出了四种CSRNet网络配置，它们具有相同的前端结构但后端的扩展速率不同。 关于前端，我们采用VGG-16网络（全连接层除外）并仅使用3X3内核。 根据VGG的论文，当使用相同大小的感受野时，使用具有小内核的更多卷积层比使用具有更大内核的更少层更有效。<br>​     通过移除完全连接的层，我们尝试确定需要从VGG-16使用的层数。 最关键的部分是在准确性和资源开销（包括训练时间，内存消耗和参数数量）之间进行权衡。 实验表明，在保持前十层VGG-16 [21]只有3个池化层而不是五层时，可以实现最佳权衡，以抑制由池化操作引起的对输出精度的不利影响。 由于CSRNet的输出（密度图）较小（输入尺寸的1/8），我们选择因子为8的双线性插值进行缩放，并确保<code>输出与输入图像具有相同的分辨率</code>。 使用相同的大小，CSRNet生成的结果与使用<code>PSNR</code>（峰值信噪比）和<code>SSIM</code>（图像中的结构相似性）的基础事实结果相当。</p><p><img src="http://i1.fuimg.com/706216/5a6ecfa4bb73acef.png" alt="Markdown"></p><h5 id="高斯核："><a href="#高斯核：" class="headerlink" title="高斯核："></a>高斯核：</h5><p><img src="http://i1.fuimg.com/706216/7bf155f476a47312.png" alt="Markdown"></p><h5 id="数据增强："><a href="#数据增强：" class="headerlink" title="数据增强："></a>数据增强：</h5><p>​    我们从不同位置的每个图像裁剪9个patches，原始图像的大小为1/4。 前四个patches包含四分之三的图像而没有重叠，而其他五个patches则从输入图像中随机裁剪。 之后，我们镜像patches，以便我们将训练集加倍。</p><h5 id="损失函数："><a href="#损失函数：" class="headerlink" title="损失函数："></a>损失函数：</h5><p>​    使用简单的方法将CSRNet作为端到端结构进行培训。 前10个卷积层由训练有素的VGG-16进行微调[21]。 对于其他层，初始值来自具有0.01标准偏差的高斯初始化。 随机梯度下降（SGD）在训练期间以1e-6的固定学习率应用。 此外，我们选择欧氏距离来测量地面实况与我们生成的估计密度图之间的差异，这与其他工作类似[19,18,4]。 损失函数如下：</p><p><img src="http://i1.fuimg.com/706216/aa66b1e5a19c197b.png" alt="Markdown"></p><h3 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h3><h5 id="评估指标："><a href="#评估指标：" class="headerlink" title="评估指标："></a>评估指标：</h5><p><img src="http://i1.fuimg.com/706216/f329a0f89d08cab5.png" alt="Markdown"></p><pre><code>我们还使用和来评估ShanghaiTech Part A数据集上输出密度图的质量。 为了计算和，我们遵循[5]给出的预处理，其中包括密度图调整大小（与原始输入相同的大小），对地面实况和预测密度图进行插值和归一化。     除了人群计数之外，我们在TRANCOS数据集[44]上设置了一个实验，用于车辆计数，以证明我们的方法的稳健性和一般化。 TRANCOS是一个公共交通数据集，包含由监控摄像机捕获的1244个不同拥挤交通场景的图像，其中包含46796个带注释的车辆。 此外，提供感兴趣区域（ROI）用于评估。 图像的视角不固定，图像是从非常不同的场景中收集的。 网格平均值平均绝对误差（GAME）[44]用于此测试中的估值。 GAME定义如下：</code></pre><p><img src="http://i1.fuimg.com/706216/faee7fbfe515c69a.png" alt="Markdown"></p><h5 id="代码：https-github-com-leeyeehoo-CSRNet-pytorch"><a href="#代码：https-github-com-leeyeehoo-CSRNet-pytorch" class="headerlink" title="代码：https://github.com/leeyeehoo/CSRNet-pytorch"></a>代码：<a href="https://github.com/leeyeehoo/CSRNet-pytorch" target="_blank" rel="noopener">https://github.com/leeyeehoo/CSRNet-pytorch</a></h5><p>我在根据作者的github（<a href="https://github.com/leeyeehoo/CSRNet-pytorch）" target="_blank" rel="noopener">https://github.com/leeyeehoo/CSRNet-pytorch）</a>, 构建环境时遇到了一些问题。我调试过，百度花了很长时间才解决。写这一章的目的是帮助大家学得更好，少走弯路。</p><p><img src="http://i1.fuimg.com/706216/ddc3d6661db91f8f.png" alt></p><hr><h2 id="step1-install"><a href="#step1-install" class="headerlink" title="step1. install"></a>step1. install</h2><p>For the specific installation process, you can refer to the author’s github. Here I simply show the command line of my operation.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda create -n CSRNet python=3.6</span><br><span class="line">source activate CSRNet</span><br><span class="line">unzip CSRNet-pytorch-master.zip</span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torch torchvision</span><br><span class="line">pip install decorator cloudpickle&gt;=0.2.1 dask[array]&gt;=1.0.0 matplotlib&gt;=2.0.0 networkx&gt;=1.8 scipy&gt;=0.17.0 bleach python-dateutil&gt;=2.1 decorator</span><br><span class="line">unzip ShanghaiTech_Crowd_Counting_Dataset.zip</span><br><span class="line">jupyter nbconvert --to script make_dataset.ipynb  #Convert .ipynb file to .py file</span><br></pre></td></tr></table></figure><h2 id="step2-make-dataset-py"><a href="#step2-make-dataset-py" class="headerlink" title="step2.  make_dataset.py"></a>step2.  make_dataset.py</h2><p>I just run the command to convert the <strong>make_dataset.ipynb</strong> file to a <strong>make_dataset.py</strong> file.Now you need to modify the contents of the <strong>make_dataset.py</strong> file.</p><p>Find the location where <strong>root</strong> is, add <strong>def main()</strong> in the above line</p><p><img src="http://i1.fuimg.com/706216/ffae848e61da7722.png" alt="Markdown"></p><p>Add these two lines at the end of the <strong>make_dataset.py</strong>, adjust the format of the code</p><p>!<img src="http://i1.fuimg.com/706216/c44917f78b1ebe7c.png" alt="Markdown"></p><p>There is an error in the author’s source code, you need to change the code</p><p>Replace <strong>pts = np.array(zip(np.nonzero(gt)[1], np.nonzero(gt)[0]))</strong> with <strong>pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))</strong> </p><p><img src="http://i1.fuimg.com/706216/03aebf60107f0747.png" alt="Markdown"></p><p>Then run the <strong>make_dataset.py</strong> file</p><p><img src="http://i1.fuimg.com/706216/9f9edee518e60290.png" alt="Markdown"></p><hr><p><strong>The above is just a general summary, then we will run and visualize the line-by-line code.</strong></p><p>I will use this image as an example.</p><p><img src="http://i1.fuimg.com/706216/531fb21fadbb8159.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import h5py</span><br><span class="line">import scipy.io as io</span><br><span class="line">import PIL.Image as Image</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from scipy.ndimage.filters import gaussian_filter </span><br><span class="line">import scipy</span><br><span class="line">import json</span><br><span class="line">from matplotlib import cm as CM</span><br><span class="line">from image import *</span><br><span class="line">from model import CSRNet</span><br><span class="line">import torch</span><br><span class="line">img_path=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\images\\IMG_21.jpg&apos;</span><br><span class="line">mat=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\ground_truth\\GT_IMG_21.mat&apos;</span><br><span class="line">mat = io.loadmat(mat)</span><br><span class="line">img= plt.imread(img_path)</span><br><span class="line">k = np.zeros((img.shape[0],img.shape[1]))</span><br></pre></td></tr></table></figure><p>The following is the information of <strong>k</strong></p><p><img src="http://i1.fuimg.com/706216/8f52b469b646095f.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gt = mat[&quot;image_info&quot;][0,0][0,0][0]</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/e7183bd8ca8fbcd2.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in range(0,len(gt)):</span><br><span class="line">    if int(gt[i][1])&lt;img.shape[0] and int(gt[i][0])&lt;img.shape[1]:</span><br><span class="line">        k[int(gt[i][1]),int(gt[i][0])]=1</span><br><span class="line">gt = k</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/789b9238a1f7b923.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">density = np.zeros(gt.shape, dtype=np.float32)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/1dd52b1f4c007a06.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gt_count = np.count_nonzero(gt)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/0639687d384f19e1.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/b3707c96d284a2cd.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">leafsize = 2048</span><br><span class="line"># build kdtree</span><br><span class="line">tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/7ad717c37f07d714.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distances, locations = tree.query(pts, k=4)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/2328bb5f36ae1d4d.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/75af4ca25f9e41ea.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;generate density...&apos;)</span><br><span class="line">for i, pt in enumerate(pts):</span><br><span class="line">    pt2d = np.zeros(gt.shape, dtype=np.float32)</span><br><span class="line">    pt2d[pt[1],pt[0]] = 1.</span><br><span class="line">    if gt_count &gt; 1:</span><br><span class="line">        sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1</span><br><span class="line">    else:</span><br><span class="line">        sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point</span><br><span class="line">    density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode=&apos;constant&apos;)</span><br><span class="line">print(&apos;done.&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/851af41223995264.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k = density</span><br><span class="line">with h5py.File(img_path.replace(&apos;.jpg&apos;,&apos;.h5&apos;).replace(&apos;images&apos;,&apos;ground_truth&apos;), &apos;w&apos;) as hf:</span><br><span class="line">        hf[&apos;density&apos;] = k</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/7044109c81cb61e0.png" alt="Markdown"></p><p>So far, we have generated true values for the image.  At this point I will sort the above code as follows</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">import h5py</span><br><span class="line">import scipy.io as io</span><br><span class="line">import PIL.Image as Image</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import glob</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">from scipy.ndimage.filters import gaussian_filter </span><br><span class="line">import scipy</span><br><span class="line">import json</span><br><span class="line">from matplotlib import cm as CM</span><br><span class="line">from image import *</span><br><span class="line">from model import CSRNet</span><br><span class="line">import torch</span><br><span class="line">def gaussian_filter_density(gt):</span><br><span class="line">    print(gt.shape)</span><br><span class="line">    density = np.zeros(gt.shape, dtype=np.float32)</span><br><span class="line">    gt_count = np.count_nonzero(gt)</span><br><span class="line">    if gt_count == 0:</span><br><span class="line">        return density</span><br><span class="line"></span><br><span class="line">    # pts = np.array(zip(np.nonzero(gt)[1], np.nonzero(gt)[0]))</span><br><span class="line">    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))</span><br><span class="line">    leafsize = 2048</span><br><span class="line">    # build kdtree</span><br><span class="line">    tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)</span><br><span class="line">    # query kdtree</span><br><span class="line">    distances, locations = tree.query(pts, k=4)</span><br><span class="line"></span><br><span class="line">    print(&apos;generate density...&apos;)</span><br><span class="line">    for i, pt in enumerate(pts):</span><br><span class="line">        pt2d = np.zeros(gt.shape, dtype=np.float32)</span><br><span class="line">        pt2d[pt[1],pt[0]] = 1.</span><br><span class="line">        if gt_count &gt; 1:</span><br><span class="line">            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1</span><br><span class="line">        else:</span><br><span class="line">            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point</span><br><span class="line">        density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode=&apos;constant&apos;)</span><br><span class="line">    print(&apos;done.&apos;)</span><br><span class="line">    return density</span><br><span class="line">img_path=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\images\\IMG_21.jpg&apos;</span><br><span class="line">mat=&apos;D:\\paper\\CSRNet\\CSRNet\\dataset\\Shanghai\\part_A_final\\train_data\\ground_truth\\GT_IMG_21.mat&apos;</span><br><span class="line">img_paths = []</span><br><span class="line">img_paths.append(img_path)</span><br><span class="line">    for img_path in img_paths:</span><br><span class="line">        print(img_path)</span><br><span class="line">        mat = io.loadmat(mat)</span><br><span class="line">        img= plt.imread(img_path)        </span><br><span class="line">        k = np.zeros((img.shape[0],img.shape[1]))</span><br><span class="line">        gt = mat[&quot;image_info&quot;][0,0][0,0][0]</span><br><span class="line">        for i in range(0,len(gt)):</span><br><span class="line">            if int(gt[i][1])&lt;img.shape[0] and int(gt[i][0])&lt;img.shape[1]:</span><br><span class="line">                k[int(gt[i][1]),int(gt[i][0])]=1</span><br><span class="line">        k = gaussian_filter_density(k)        </span><br><span class="line">        with h5py.File(img_path.replace(&apos;.jpg&apos;,&apos;.h5&apos;).replace(&apos;images&apos;,&apos;ground_truth&apos;), &apos;w&apos;) as hf:</span><br><span class="line">                hf[&apos;density&apos;] = k</span><br></pre></td></tr></table></figure><hr><p>And….then, let’s plot the true values of the image:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gt_file = h5py.File(img_paths[0].replace(&apos;.jpg&apos;,&apos;.h5&apos;).replace(&apos;images&apos;,&apos;ground_truth&apos;),&apos;r&apos;)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/4b4772b34da8e598.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groundtruth = np.asarray(gt_file[&apos;density&apos;])</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/138df2675cd215de.png" alt="Markdown"></p><p>plot the true values of the image</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(groundtruth,cmap=CM.jet)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/4f44077d831c59b4.png" alt="Markdown"></p><p>Calculate how many people are in this picture</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.sum(groundtruth)</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/b77eec89076ec0cf.png" alt="Markdown"></p><hr><p><strong>Based on the same operation above, I generated true values for all images in the dataset. The following operations are performed on the gpu server.</strong></p><p>That is, run the command line <strong>python make_dataset.py</strong> on the server to get the true value of all the pictures.</p><h2 id="step3-Training"><a href="#step3-Training" class="headerlink" title="step3.  Training"></a>step3.  Training</h2><p><strong>Note</strong>: if you use the python3.x</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. In model.py, change the xrange in line 18 to range</span><br><span class="line">2. In model.py, change line 19 to: list(self.frontend.state_dict().items())[i][1].data[:] = list(mod.state_dict().items())[i][1].data[:]</span><br><span class="line">3. In image.py, change line 40 to: target = cv2.resize(target,(target.shape[1]//8,target.shape[0]//8),interpolation = cv2.INTER_CUBIC)*64</span><br></pre></td></tr></table></figure><ul><li>In  part_A_train.json:change the path of images</li><li>In  part_A_val.json: change the path of images</li></ul><p>run </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py part_A_train.json part_A_val.json 0 0</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/eddfc2ba078f8a53.png" alt="Markdown"></p><h2 id="step4-Testing"><a href="#step4-Testing" class="headerlink" title="step4. Testing"></a>step4. Testing</h2><p>These are our test images. number：182</p><p><img src="http://i1.fuimg.com/706216/107a86f92d987f8e.png" alt="Markdown"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter nbconvert --to script val.ipynb</span><br></pre></td></tr></table></figure><p>Finally, the performance of this model on invisible data is tested. We will use the val.py file to verify the results. Remember to change the path to pre-train weights and images.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python val.py</span><br></pre></td></tr></table></figure><p>The average absolute error value that can be obtained by running this val.py file code</p><p>total ：182</p><p><img src="http://i1.fuimg.com/706216/1c9eda9b4fc41cd4.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/6f3f6d53bae9be0f.png" alt="Markdown"></p><p>The average absolute error value obtained is 65.96636956602663, which is very good.</p><hr><p>Now let’s examine the predicted values on a single image:</p><p>run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_single-image.py</span><br></pre></td></tr></table></figure><p><img src="http://i1.fuimg.com/706216/276e24fbb7b1ad08.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/6f3eb74c5cff3391.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/1a4cf5705f7327be.png" alt="Markdown"></p><p>another one</p><p><img src="http://i1.fuimg.com/706216/4fcbbebaee45e321.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/a9c749406c53061e.png" alt="Markdown"><img src="http://i1.fuimg.com/706216/7299a595907b7ec1.png" alt="Markdown"></p><p>The effect is not too good，maybe the model is not trained enough，i guess。</p><hr><h2 id="Reading-paper-https-arxiv-org-pdf-1802-10062-pdf"><a href="#Reading-paper-https-arxiv-org-pdf-1802-10062-pdf" class="headerlink" title="Reading paper   https://arxiv.org/pdf/1802.10062.pdf"></a>Reading paper   <a href="https://arxiv.org/pdf/1802.10062.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.10062.pdf</a></h2><p>先说数据集用的ShanghaiTech dataset，根据.jpg和.mat处理之后生成train_den文件夹下.csv文件和图片一一对应</p><p><img src="http://i1.fuimg.com/706216/048f00ca3a0fcb9e.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/e92cb793c250e9cd.png" alt="Markdown"></p><p>数据集扩充处理再补充一下：使用的是高斯模糊作用于图像中的每个人的头部。所有图像都被裁剪成9块，每块的大小是图像原始大小的1/4。</p><hr><p>空洞卷积也有的博客翻译成膨胀卷积、扩张卷积啥的，anyway，使用扩张卷积是在不增加参数的情况下扩大内核。因此，如果扩张率为1就是中间的图在整个图像上进行卷积。将膨胀率增加到2最右边图它可以替代pooling层</p><p><img src="http://i1.fuimg.com/706216/2b2b3697b32d9623.png" alt="Markdown"></p><p>接下来再说它的数学公式上怎么计算的，</p><p><img src="http://i1.fuimg.com/706216/0d271181ba1fac76.png" alt="Markdown"></p><p>由上公式得到这个([k + (k-1)<em>(r-1)] </em> [k + (k-1)*(r-1)])</p><p><img src="http://i1.fuimg.com/706216/7caceeba60d24f5a.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/0d271181ba1fac76.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/cd9855760288c342.png" alt="Markdown"></p><p><img src="http://i1.fuimg.com/706216/c5f5832af28b79ac.png" alt="Markdown"></p><p>这个过程是：首先预测出给定图像的密度图。如果没有人，像素值pixel value设为0。如果该像素对应于人，则将分配某个预定义值。所以图像中的人数就是总共有的像素值total pixel values </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;主要观点：CSRnet网络模型主要分为前端和后端网络，采用剔除了全连接层的VGG-16作为CSRnet的前端网络，输出图像的大小为原始输入图像的1-8。卷积层的数量增加会导致输出的图像变小，从而增加生成密度图的难度。所以本文采用空洞卷积神经网络作为后端网络，在保持
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Fast Online Object Tracking and Segmentation:A Unifying Approach</title>
    <link href="http://yuanquanquan.top/2019/20191213002/"/>
    <id>http://yuanquanquan.top/2019/20191213002/</id>
    <published>2019-12-12T23:41:41.000Z</published>
    <updated>2019-12-12T23:54:08.321Z</updated>
    
    <content type="html"><![CDATA[<p>用单个方法实时做到同时目标跟踪和半监督视频目标分割。SiamMask通过二值分割任务增强损失提升了全卷积Siamense目标跟踪方法的训练过程。训练完成后，SiamMask只依赖于单个边界框初始值，可以做到旋转的边界框分割未知类别目标，达到每秒55帧。</p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>展示了如何用单个方法实时做到同时目标跟踪和半监督视频目标分割。我们的方法SiamMask，通过二值分割任务增强损失提升了全卷积Siamense目标跟踪方法的训练过程。训练完成后，SiamMask只依赖于单个边界框初始值，可以做到旋转的边界框分割未知类别目标，达到每秒55帧。性能也有很大的优势【略】。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig1.png" alt></p><p>本文希望通过SiamMask拉近任意目标跟踪和视频目标分割（VOS）的关系。SiamMask是一个简单的多任务方法，可以同时解决上述两个问题。我们希望保持离线的训练能力，以及在线的速度，同时对目标的表示进行快速调节。</p><p>为了达到这个目标，我们同时在三个任务上训练一个Siamese网络，每个对应于不同的目标对象和候选区域的策略。在Bertinetto的全卷积方法中，一个任务是学习目标对象和滑动窗口中多个候选的相似性度量。它的输出是密度响应图，表示目标的未知，但是不包含空间信息。为了提炼这个信息，我们同时训练另外两个任务：用Region Proposal Network训练边界框回归，未知类别二值分割。需要注意的是：二值标签只需要在离线训练时给出，在线分割和跟踪的时候不需要。每个任务用单独的分支表示，最终的损失对三个输出求和。</p><p>在训练后，SiamMask只依赖于单个初始边界框，不需要更新即可在线运算，最后生成目标分割的mask和旋转的边界框，达到每秒55帧。在VOT-2018上达到最好的效果。另外，相同的方法在半监督VOS上也很有竞争力。</p><h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><p>为了能在线运算，速度更快，我们采用了全卷积Siamese网络。我们实验了SiamFC和SiamRPN。</p><h3 id="3-1-Fully-convolutional-Siamese-networks"><a href="#3-1-Fully-convolutional-Siamese-networks" class="headerlink" title="3.1. Fully-convolutional Siamese networks"></a>3.1. Fully-convolutional Siamese networks</h3><p><strong>SiamFC</strong> Bertinetto提出作为跟踪系统的基本块，离线训练的全卷积Siamese网络在（大的）搜索图像x中比较一个模板图像z，获得一个密集响应图。z和x分别是$w \times h$大小的中心剪切的目标对象，和更大的中心剪切的目标位置目标估计。两个输入通过相同的CNN$f_{\theta}$处理，返回两个交叉相关的特征图：</p><p>$$g_{\theta}(z, x)=f_{\theta}(z) \star f_{\theta}(x) \qquad (1)$$</p><p>我们把响应图（式1左）的每个空间元素看做候选窗的响应（ROW）。例如$g_{\theta}^{n}(z, x)$表示一个模板z和x第n个候选窗的相似性。对于SiamFC而言，目标是响应图最大值对应于在搜索区域x中的目标位置。为了让每个ROW编码更丰富的目标对象信息，我们把式1的交叉相关替换为深度方向的交叉相关，生成多通道响应图。SiamFC在几百万视频帧上用logistic损失（记作$\mathcal{L}_{s i m}$）离线训练。</p><p><strong>SiamRPN</strong> Li等通过区域提议网络（RPN提上SiamFC的性能，可以用一个不同比例的边界框预测目标位置。在SiamRPN中，每个ROW表示k个archor box提议的集合，以及对应的目标/背景分数。两个输出的分支用平滑$L_{1}$和交叉熵训练。记作$\mathcal{L}<em>{b o x}$和$\mathcal{L}</em>{\text { score }}$。</p><h3 id="3-2-SiamMask"><a href="#3-2-SiamMask" class="headerlink" title="3.2. SiamMask"></a>3.2. SiamMask</h3><p>不同于现在的依赖于低保真度目标表示的方法，我们认为生成先前帧的目标二值分割mask是很重要的。除了相似性分数和边界框坐标，ROW也包含了生成二值mask的信息。可以在现有的网络基础上增加新的分支和损失。</p><p>通过参数为$\phi$的两层神经网络$h_{\phi}$预测$w \times h$的二值mask。令$m_{n}$表示第n个ROW上预测的mask：</p><p>$$m_{n}=h_{\phi}\left(g_{\theta}^{n}(z, x)\right) \qquad (2)$$</p><p>从式2可以看出mask的预测是图像到分割x以及目标对象z的函数。这里z可以用于指导分割过程：给定一个不同的参考图像，网络会对x生成不同的分割mask。</p><p><strong>Loss function</strong> 训练的时候，每个ROW由真实二值标签$y_{n} \in{ \pm 1}$标记，同时还有$w \times h$大小的像素级真实mask$c_{n}$。令$c_{n}^{i j} \in{ \pm 1}$表示第n个候选ROW像素$(i, j)$上的标签。损失$\mathcal{L}_{\text { mask }}$是二元logistic回归损失：</p><p>$$\mathcal{L}<em>{\operatorname{mask}}(\theta, \phi)=\sum</em>{n}\left(\frac{1+y_{n}}{2 w h} \sum_{i j} \log \left(1+e^{-c_{n}^{i j} m_{n}^{i j}}\right)\right) \qquad (3)$$</p><p>因此，$h_{\phi}$的分类层包括$w \times h$个分类器，每个表示给定像素是否候选窗中的对象。$\mathcal{L}<em>{\text { mask }}$只计算正的ROW（$y</em>{n}=1$）。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig2.png" alt></p><p><strong>Mask representation</strong> </p><p>$f_{\theta}(z)$和$f_{\theta}(x)$生成的深度方向交叉相关的（$17 \times 17$）ROW之一为mask的表示。分割任务的网络$h_{\phi}$由两个$1 \times 1$卷积层表示，一个256通道，一个$63^{2}$通道。这可以让每个像素分类器利用整个ROW包含的信息，也可以对x中的候选窗有完整的视角，这对于区分目标对象是很关键的。</p><p><strong>Two variants</strong> 【略】</p><p><strong>Box generation</strong> 【略】</p><h3 id="3-3-Implementation-details"><a href="#3-3-Implementation-details" class="headerlink" title="3.3. Implementation details"></a>3.3. Implementation details</h3><p>【略】</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Evaluation-for-visual-object-tracking"><a href="#4-1-Evaluation-for-visual-object-tracking" class="headerlink" title="4.1. Evaluation for visual object tracking"></a>4.1. Evaluation for visual object tracking</h3><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig3.png" alt><br><img src="D:/BazingaliuBlog/Bazingaliu.github.io-master/blog/papers/2019/R/SiamMask-fig3.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab2.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab3-4.png" alt></p><h3 id="4-2-Evaluation-for-semi-supervised-VOS"><a href="#4-2-Evaluation-for-semi-supervised-VOS" class="headerlink" title="4.2. Evaluation for semi-supervised VOS"></a>4.2. Evaluation for semi-supervised VOS</h3><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab5-6.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig4.png" alt></p><h3 id="4-3-Further-analysis"><a href="#4-3-Further-analysis" class="headerlink" title="4.3. Further analysis"></a>4.3. Further analysis</h3><p><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-tab7.png" alt><br><img src="https://cugtyt.github.io/blog/papers/2019/R/SiamMask-fig5.png" alt></p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>我们提出了SiamMask方法，让全卷积Siamese跟踪器生成任意类别二值分割mask。我们把它成功运用到了目标跟踪和半监督视频目标分割中，速度快，性能高。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用单个方法实时做到同时目标跟踪和半监督视频目标分割。SiamMask通过二值分割任务增强损失提升了全卷积Siamense目标跟踪方法的训练过程。训练完成后，SiamMask只依赖于单个边界框初始值，可以做到旋转的边界框分割未知类别目标，达到每秒55帧。&lt;/p&gt;
&lt;h2 id
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>jupyter远程连接炼丹</title>
    <link href="http://yuanquanquan.top/2019/2019121111/"/>
    <id>http://yuanquanquan.top/2019/2019121111/</id>
    <published>2019-12-11T03:55:00.000Z</published>
    <updated>2019-12-11T04:16:43.667Z</updated>
    
    <content type="html"><![CDATA[<p><img src="data:image/jpeg;base64,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" alt></p><h1 id="一、前提："><a href="#一、前提：" class="headerlink" title="一、前提："></a>一、前提：</h1><ol><li>安装Python3</li><li>安装Anaconda</li><li>配置jupyter notebook 并启动（重点）</li></ol><h1 id="二、配置jupyter文件"><a href="#二、配置jupyter文件" class="headerlink" title="二、配置jupyter文件"></a>二、配置jupyter文件</h1><p>因为服务器已经安装好anaconda和jupyter，python版本为python3.6，以下主要讲如何配置jupyter文件</p><h2 id="1、设置jupyter-的登录密码"><a href="#1、设置jupyter-的登录密码" class="headerlink" title="1、设置jupyter 的登录密码"></a>1、设置jupyter 的登录密码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config  # 生成jupyter notebook配置文件</span><br></pre></td></tr></table></figure><p>会生成有默认配置文件 jupyter_notebook_config.py</p><h2 id="2、然后打开ipython"><a href="#2、然后打开ipython" class="headerlink" title="2、然后打开ipython"></a>2、然后打开ipython</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from notebook.auth import passwd</span><br><span class="line">passwd() #生成密码</span><br></pre></td></tr></table></figure><h2 id="3、配置文件"><a href="#3、配置文件" class="headerlink" title="3、配置文件"></a>3、配置文件</h2><p>然后会让你输入密码，确认密码，。（这里面的密码是后面在本地打开jupyter时需要输入的，要记住，如设置密码为123456）<br> 然后会输出一长串哈希密码”sha1:XXXXX”  复制这一段密码，。后面要用<br> 然后就开始配置刚才生成的jupyter_notebook_config.py文件。，<br> 使用vim打开：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><p>将以下文字复制进jupyter_notebook_config.py中，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;*&apos;</span><br><span class="line">c.NotebookApp.password = u&apos;把上面的文本粘贴到这里&apos;</span><br><span class="line">c.NotebookApp.open_browser = False</span><br><span class="line">c.NotebookApp.port =8888</span><br></pre></td></tr></table></figure><p>编辑好后按esc键，</p><p>输入:wq保存并退出。</p><h2 id="4、安装ipykernel使得jupyter能访问远程的虚拟环境。"><a href="#4、安装ipykernel使得jupyter能访问远程的虚拟环境。" class="headerlink" title="4、安装ipykernel使得jupyter能访问远程的虚拟环境。"></a>4、安装ipykernel使得jupyter能访问远程的虚拟环境。</h2><p>[1] 启动虚拟环境</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`source` `activate &lt;your virtualenv&gt;`</span><br></pre></td></tr></table></figure><p>[2] 在虚拟环境安装jupyter</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`pip ``install` `jupyter`</span><br></pre></td></tr></table></figure><p>[3] 在虚拟环境安装ipykernel</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`pip ``install` `ipykernel`</span><br></pre></td></tr></table></figure><p>[4] 配置ipykernel</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`python -m ipykernel ``install` `--user --name testenv --display-name ``&quot;Python2 (py2env)&quot;`</span><br></pre></td></tr></table></figure><p>其中，–name的参数和–display-name的参数根据配置更改。</p><p>上面就是配置服务端jupyter的以及激活虚拟环境的全过程，总结一下就是：</p><ol><li>安装jupyter，生成key，修改配置文件，按照ip:端口号登陆。</li><li>在激活的虚拟环境中安装ipykernel并配置。</li></ol><p><img src="https://ask.qcloudimg.com/http-save/4069933/qxikwzn1oj.png?imageView2/2/w/1620" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0
      
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="jupyter" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/jupyter/"/>
    
    
  </entry>
  
  <entry>
    <title>THE LOTTERY TICKET HYPOTHESIS： FINDING SPARSE, TRAINABLE NEURAL NETWORKS</title>
    <link href="http://yuanquanquan.top/2019/2019120912/"/>
    <id>http://yuanquanquan.top/2019/2019120912/</id>
    <published>2019-12-09T06:02:23.000Z</published>
    <updated>2019-12-09T06:11:50.026Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>神经网络剪枝可以减少超过90%的参数量，同时准确率没有太大影响。但是剪枝后的结构很难从头开始训练，不然就能近似提高训练性能了。</p><p>有一些子网络，在初始情况下就可以高效的训练网络，但我们发现标准的剪枝技术天然的没有包括它们。基于这些结果，我们提出了彩票假说：密集的，随机初始化的前馈网络包括一些子网络（中奖者），这些子网络独立训练时可以在相近的迭代次数达到相近的测试准确率。这些中奖者赢得了初始彩票：它们初始权重就能特别高效的训练。</p><p>我们提出了一个算法来找这些中奖者，一系列的实验也支持了我们的假说和那些偶然初始化的重要性。我们不断的在MNIST和CIFAR10数据集上发现，中奖者的大小只有全连接网络和卷积网络的10%到20%。除了尺寸，我们发现这些中奖者训练更快，测试准确率更高。</p><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><p>有一些方法可以减少90%的参数但是没有准确率影响，但是为什么开始时不用这个更小的结构来训练呢？研究发现这样做的结果比较差。</p><p>在图1中，我们从MNIST的全连接网络和CIFAR10的卷积网络中随机采样训练一个子网络。随机采样建模了非结构化剪枝。在不同的稀疏程度上，虚线表示迭代中的最小验证损失和准确率。网络越稀疏，学的越慢，最后的准确率越低。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig1.png" alt></p><p>本文中，我们展示总存在是个更小的网络，在从头训练时至少和原始网络一样快，测试准确率相近。图1实线是我们找到的网络。</p><p><strong>The Lottery Ticket Hypothesis</strong> 随机初始化的，密集的网络包括一个子网络，这个网络的初始化使得它能在单独训练时达到相似的准确率，需要的迭代次数也相近。</p><p>严格来说，考虑一个密集的前馈网络$f(x ; \theta)$，初始参数$\theta=\theta_{0} \sim \mathcal{D}<em>{\theta}$。在训练集中用SGD优化，$f$在迭代$j$次后达到最小验证损失$l$和测试准确率$a$。另外，考虑带参数mask $m \in{0,1}^{|\theta|}$训练$f(x ; m \odot \theta)$，所以初始化参数为$m \odot \theta</em>{0}$。当用SGD优化时，$f$在迭代$j‘$次后达到最小验证损失$l‘$和测试准确率$a’$。彩票假说预测，对于$\exists m$使得$j^{\prime} \leq j$，$a^{\prime} \geq a$，$|m|_{0} \ll|\theta|$参数量更少。</p><p>我们发现标准化的剪枝方法自动的略过了这样的网络。我们称这样的一个子网络为中奖者。在参数随机初始化后，$f\left(x ; m \odot \theta_{0}^{\prime}\right)$ where $\theta_{0}^{\prime} \sim \mathcal{D}_{\theta}$，它们就无法中奖了，除非初始化得好。</p><p><strong>Identifying winning tickets</strong> 我们通过训练，减去梯度最小的权重来定为这个网络。剩下的，没有剪枝的就是我们的中奖者。每个未剪枝的连接值在重新训练时在次填回原来初始化的值。这就是我们的实验核心：</p><ol><li>随机初始化网络$f\left(x ; \theta_{0}\right)\left(\text { where } \theta_{0} \sim \mathcal{D}_{\theta}\right)$</li><li>迭代j次，得到参数$\theta_{j}$</li><li>从参数$\theta_{j}$中减去$p \%$参数，生成mask $m$。</li><li>把剩下的参数值设为原来的值$\theta_{0}$，生成中奖者$f\left(x ; m \odot \theta_{0}\right)$</li></ol><p>可以看到，这个剪枝过程是one-shot：网络训练一次，$p \%$的权重剪枝，剩下的重新设置值。但是本文关注于迭代剪枝，不断的进行训练，剪枝，重设。结果显示，迭代剪枝比one-shot剪枝找到的网络更小。</p><p>结果：我们在MNIST的全连接网络和CIFAR10的卷积网络中找到了中奖者。我们使用非结构化的剪枝方法，因此这些中奖者是稀疏的。在更深的网络中，我们的基于剪枝的方法对学习率敏感，高学习率需要warmup来找中奖者。中奖者是原始网络的10-20%大小，准确率更高，迭代次数相近。随机初始化后，效果就很差了，意味着单独说结构不能解释中奖者的成功。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig2.png" alt></p><p><strong>The Lottery Ticket Conjecture</strong> 密集的随机初始化的网络相对于稀疏网络更容易训练，因为有更多可能的子网络。</p><p><strong>Contributions</strong></p><ul><li>我们证明了可以剪枝得到一个子网络，在相近的迭代后可以达到相似的测试准确率</li><li>我们展示了中奖者相比于原来学的更快，可以达到更高的准确率，泛化性更好</li><li>我们提出了彩票假说可以用来解释这个发现</li></ul><p><strong>Implications</strong> 我们可以探索：</p><ul><li><em>Improve training performance</em> 由于中奖者可以独立从头训练，所以我们可以设计一个训练方法来寻找中奖者，更容易的进行剪枝。</li><li>Design better networks* 中奖者揭示了稀疏结构和初始化对学习尤其重要。我们可以试着设计新的网络结构和初始化方案。甚至可以迁移到其他任务上。</li><li><em>Improve our theoretical understanding of neural networks</em></li></ul><h2 id="2-WINNING-TICKETS-IN-FULLY-CONNECTED-NETWORKS"><a href="#2-WINNING-TICKETS-IN-FULLY-CONNECTED-NETWORKS" class="headerlink" title="2 WINNING TICKETS IN FULLY-CONNECTED NETWORKS"></a>2 WINNING TICKETS IN FULLY-CONNECTED NETWORKS</h2><p>我们试验了MNIST上的全连接网络。使用Lenet-300-100。在随机初始化和训练网络后，我们剪枝，然后重新设置参数。我们使用了简单的修建方法，移除最小梯度的权重。</p><p><strong>Notation</strong> $P_{m}=\frac{|m|<em>{0}}{|\theta|}$表示m的稀疏性，例如$P</em>{m}=25 \%$表示75%的权重减掉了。</p><p><strong>Iterative pruning</strong> 图3表示迭代剪枝的结果。第一测修建，网络学得更快，准确率更高（图3左）。51.3%权重有最好的测试准确率，比原始网络快，但是慢于21.1%。3.6%是达到原始网络一样的性能。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig3.png" alt></p><p>图4a是每次迭代剪枝20%的情况。左侧是早停与权重比例的情况。中间是测试准确率。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/lottery-prun-fig4.png" alt></p><p>中奖者从$P-m$为100%到21%学得越来越快，最后早了38%。进一步的剪枝学习变慢，在3.6%的时候早停性能和原始网络一致。测试准确率在13.5%下提升了0.3个百分点，后续就降低了，在3.6%的时候返回原始网络的水平。</p><p><strong>Random reinitialization</strong> 为了验证中奖者初始化的重要性，我们保持了中奖者的结构，但重新初始化。在图5中对每个中奖者随机初始化3次，发现初始化是极其重要的。图3右展示了迭代剪枝的情况。</p><p>【略】</p><p><strong>One-shot pruning</strong> 图4中也展示了one-shot的实验。【略】</p><h2 id="3-WINNING-TICKETS-IN-CONVOLUTIONAL-NETWORKS"><a href="#3-WINNING-TICKETS-IN-CONVOLUTIONAL-NETWORKS" class="headerlink" title="3 WINNING TICKETS IN CONVOLUTIONAL NETWORKS"></a>3 WINNING TICKETS IN CONVOLUTIONAL NETWORKS</h2><p>【略】</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ABSTRACT&quot;&gt;&lt;a href=&quot;#ABSTRACT&quot; class=&quot;headerlink&quot; title=&quot;ABSTRACT&quot;&gt;&lt;/a&gt;ABSTRACT&lt;/h2&gt;&lt;p&gt;神经网络剪枝可以减少超过90%的参数量，同时准确率没有太大影响。但是剪枝后的结构很难从头开始
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="ICLR" scheme="http://yuanquanquan.top/tags/ICLR/"/>
    
  </entry>
  
  <entry>
    <title>艾萨克的英雄主义</title>
    <link href="http://yuanquanquan.top/2019/2019120800/"/>
    <id>http://yuanquanquan.top/2019/2019120800/</id>
    <published>2019-12-07T17:33:52.000Z</published>
    <updated>2019-12-07T18:00:39.151Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1575751702447&amp;di=043ba885e29e0e29604a1930003995af&amp;imgtype=0&amp;src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201605%2F11%2F20160511213352_42sRW.thumb.700_0.jpeg" alt></p><p>再有一个月2019年就要过去了，这一年里好像什么都发生了，又好像什么都没改变。</p><p>我的生活在发生着什么呢？大概…..我变得喜欢听汪峰、Adele、还有Johnny Cash的歌；我变得会望着车窗外飞逝的光景没来由地泪湿眼眶；我变得越来越喜欢仰望天空；我变得越来越不敢在深夜推演公式。</p><p>各种ddl日常砸脸，论文也是毫无头绪，也做不出理想的实验数据；但最难的其实是生活，作为一个人去生活。</p><p>把自己的生活打理得井井有条，敢爱，敢恨，有宽阔的胸怀。</p><p>释怀那些曾经伤害过自己的人和事。</p><p>有罗曼罗兰的英雄主义，看清生活的真相之后仍然热爱它。</p><p>就像民谣里唱的</p><p>“如果天黑之前来得及</p><p>我要忘了你的眼睛。”</p><p>有什么不是时间能冲淡的东西呢</p><p>晚年的艾萨克.牛顿在英国铸币局做了二十多年局长，帮助改革了英国的货币，后来获封勋爵，国葬在西敏寺。</p><p>涉足金融的牛顿炒股亏得一塌糊涂，他因此也留下一句名言：</p><p>我可以计算天体运行的轨道，但我无法计算疯狂的人性。</p><p>这句名言大概就像数学家费马本人证明不出费马大定理就写下一句：“我已经知道如何证明，但这里空白太少写不下”，这样略带戏谑和桀骜的托词。他杂糅着牛顿对这个世界的失望，他的傲慢，他的无奈，他的对和谐唯美自然秩序图景的渴望和混乱的人类社会现实的冲突。</p><p>而他自己的人生，在早年的贫穷、困顿、瘦弱中养成了多疑、好胜、自卑的性格，一生未娶，他并没有获得人性美好的眷顾。如果他不是自然科学之父，历史不会记住他。假设会记住他，牛顿的注脚也顶多是一个矮小怪异的加西莫多式的英格兰人，哪里会有艾丝梅拉达们给他爱情的憧憬，遑论天伦之乐。</p><p>他的这句“托词”，在过去几个月一直萦绕在我心头。也萦绕在我看向天空的视角。</p><p>苦难、贫穷、抑或孤独，它们可以塑造我们的性格，可以影响我们的外貌，可以让我们成为丑陋的加西莫多，可以让我们永远也得不到艾丝梅拉达的爱情。但它们唯一不可影响的就是我们的英雄主义。这种英雄主义不单纯基于罗曼罗兰的愿景，它更像牛顿们的勇气和韧劲，乘着思想的羽翼，去飞翔，去探索，去无畏地开疆拓土，发现未知，塑造未来，哪怕倒下，也像海中之鲸，富饶了整个群落。而每当我们看向天空，至少在那一刻，是神圣的，是唯美的，是纯粹的。</p><p>下一刻低头，便只管往前走，不回头。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://timgsa.baidu.com/timg?image&amp;amp;quality=80&amp;amp;size=b9999_10000&amp;amp;sec=1575751702447&amp;amp;di=043ba885e29e0e29604a193000
      
    
    </summary>
    
      <category term="生活" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="随笔" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活" scheme="http://yuanquanquan.top/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>初次面试-字节跳动</title>
    <link href="http://yuanquanquan.top/2019/2019120722/"/>
    <id>http://yuanquanquan.top/2019/2019120722/</id>
    <published>2019-12-06T16:28:29.000Z</published>
    <updated>2019-12-07T14:39:38.757Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容."><div class="hbe-input-container"><input type="password" id="hbePass" placeholder="您好, 这里需要密码." /><label>您好, 这里需要密码.</label><div class="bottom-line"></div></div><script id="hbeData" type="hbeData" data-hmacdigest="a296fa966cb50dff13d4fb1e220b73658b7aad0fc9132a98e531be52573f80bd">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</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      有东西被加密了, 请输入密码查看.
    
    </summary>
    
      <category term="生活" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="随笔" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="招聘" scheme="http://yuanquanquan.top/tags/%E6%8B%9B%E8%81%98/"/>
    
  </entry>
  
  <entry>
    <title>Two-armed Bandit</title>
    <link href="http://yuanquanquan.top/2019/2019120615/"/>
    <id>http://yuanquanquan.top/2019/2019120615/</id>
    <published>2019-12-06T15:40:15.000Z</published>
    <updated>2019-12-07T17:31:50.583Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdn.net/20170225171914631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY29mZmVlX2NyZWFt/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><p>RL问题有以下几个方面：</p><ul><li>不同的actions导致不同的rewards</li><li>rewards具有时延性</li><li>action的reward取决于环境状态</li></ul><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h3 id="Learning-a-Policy"><a href="#Learning-a-Policy" class="headerlink" title="Learning a Policy"></a>Learning a <em>Policy</em></h3><p>Learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones.</p><h3 id="Policy-Gradients"><a href="#Policy-Gradients" class="headerlink" title="Policy Gradients"></a>Policy Gradients</h3><p>Simple neural network learns a policy for picking actions by adjusting it’s weights through gradient descent using feedback from the environment.</p><h3 id="Value-functions"><a href="#Value-functions" class="headerlink" title="Value functions"></a>Value functions</h3><p>Instead of learning the optimal action in a given state, the agent learns to predict how good a given state or action will be for the agent to be in.</p><h3 id="e-greedy-policy"><a href="#e-greedy-policy" class="headerlink" title="e-greedy policy"></a>e-greedy policy</h3><p>This means that most of the time our agent will choose the action that corresponds to the largest expected value, but occasionally, with e probability, it will choose randomly.</p><h3 id="policy-loss-equation"><a href="#policy-loss-equation" class="headerlink" title="policy loss equation"></a>policy loss equation</h3><blockquote><p><strong>Loss = -log(π)A</strong></p></blockquote><p>A is advantage, and is an essential aspect of all reinforcement learning algorithms. Intuitively it corresponds to how much better an action was than some baseline.</p><p>π is the policy. In this case, it corresponds to the chosen action’s weight.</p><h2 id="The-Multi-armed-bandit"><a href="#The-Multi-armed-bandit" class="headerlink" title="The Multi-armed bandit"></a>The Multi-armed bandit</h2><p><a href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149" target="_blank" rel="noopener">代码来源</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">The Bandits</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Here we define our bandits. For this example we are using a four-armed bandit. The pullBandit function generates a random number from a normal distribution with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit that will give that positive reward.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#List out our bandits. Currently bandit 4 (index#3) is set to most often provide a positive reward.</span></span><br><span class="line">bandits = [<span class="number">0.2</span>,<span class="number">0</span>,<span class="number">-0.2</span>,<span class="number">-5</span>]</span><br><span class="line">num_bandits = len(bandits)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pullBandit</span><span class="params">(bandit)</span>:</span></span><br><span class="line">    <span class="comment">#Get a random number.</span></span><br><span class="line">    result = np.random.randn(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> result &gt; bandit:</span><br><span class="line">        <span class="comment">#return a positive reward.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#return a negative reward.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">The Agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment">#These two lines established the feed-forward part of the network. This does the actual choosing.</span></span><br><span class="line">weights = tf.Variable(tf.ones([num_bandits]))</span><br><span class="line">chosen_action = tf.argmax(weights,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#The next six lines establish the training proceedure. We feed the reward and chosen action into the network</span></span><br><span class="line"><span class="comment">#to compute the loss, and use it to update the network.</span></span><br><span class="line">reward_holder = tf.placeholder(shape=[<span class="number">1</span>],dtype=tf.float32)</span><br><span class="line">action_holder = tf.placeholder(shape=[<span class="number">1</span>],dtype=tf.int32)</span><br><span class="line">responsible_weight = tf.slice(weights,action_holder,[<span class="number">1</span>])</span><br><span class="line">loss = -(tf.log(responsible_weight)*reward_holder)</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">update = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Training the Agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">We will train our agent by taking actions in our environment, and recieving rewards. Using the rewards and actions, we can know how to properly update our network in order to more often choose actions that will yield the highest rewards over time.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">total_episodes = <span class="number">1000</span> <span class="comment">#Set total number of episodes to train agent on.</span></span><br><span class="line">total_reward = np.zeros(num_bandits) <span class="comment">#Set scoreboard for bandits to 0.</span></span><br><span class="line">e = <span class="number">0.1</span> <span class="comment">#Set the chance of taking a random action.</span></span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch the tensorflow graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; total_episodes:</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Choose either a random action or one from our network.</span></span><br><span class="line">        <span class="keyword">if</span> np.random.rand(<span class="number">1</span>) &lt; e:</span><br><span class="line">            action = np.random.randint(num_bandits)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = sess.run(chosen_action)</span><br><span class="line"></span><br><span class="line">        reward = pullBandit(bandits[action]) <span class="comment">#Get our reward from picking one of the bandits.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#Update the network.</span></span><br><span class="line">        _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict=&#123;reward_holder:[reward],action_holder:[action]&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Update our running tally of scores.</span></span><br><span class="line">        total_reward[action] += reward</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"Running reward for the "</span> + str(num_bandits) + <span class="string">" bandits: "</span> + str(total_reward)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"The agent thinks bandit "</span> + str(np.argmax(ww)+<span class="number">1</span>) + <span class="string">" is the most promising...."</span></span><br><span class="line"><span class="keyword">if</span> np.argmax(ww) == np.argmax(-np.array(bandits)):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"...and it was right!"</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"...and it was wrong!"</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://img-blog.csdn.net/20170225171914631?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY29mZmVlX2NyZWFt/font/5a6L5L2T/fontsiz
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="强化学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="强化学习" scheme="http://yuanquanquan.top/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>OpenAI Spinning Up Part 1： Key Concepts in RL</title>
    <link href="http://yuanquanquan.top/2019/20191206/"/>
    <id>http://yuanquanquan.top/2019/20191206/</id>
    <published>2019-12-06T15:31:10.000Z</published>
    <updated>2019-12-06T15:49:40.473Z</updated>
    
    <content type="html"><![CDATA[<p>来自OpenAI Spinning Up <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" target="_blank" rel="noopener">Introduction to RL</a></p><p>简而言之，RL是研究agent(智能体，本文保留英文描述)如何通过反复的尝试来学习。我们通过奖励或惩罚agent的行为，使其在未来能以更高的概率去重复或放弃该行为。</p><h2 id="Key-Concepts-and-Terminology"><a href="#Key-Concepts-and-Terminology" class="headerlink" title="Key Concepts and Terminology"></a>Key Concepts and Terminology</h2><p><img src="https://cugtyt.github.io/blog/rl-notes/R/spinningup-fig1.png" alt></p><p>RL的主要特征是<strong>agent</strong>和<strong>环境</strong>。环境是agent居住并与之互动的世界。在每一次互动中，agent都会对世界有一个(可能是部分的)观察，然后决定要采取的行动。环境会随着agent的互动发生变化，也可能环境会自行更改。</p><p>agent会感知来自环境的<strong>奖励</strong>信号，这个数字告诉它当前世界状态的好坏。agent的目标是最大化其累积奖励，称为<strong>回报</strong>，RL就是让agent学习更好的行为以实现这个目标。</p><p>为了更具体地讨论RL做什么，我们需要引入一些术语：</p><ul><li>状态和观察(states and observations)</li><li>行为空间(action spaces)，</li><li>策略(policies)，</li><li>轨迹(trajectories)，</li><li>不同的回报方式(different formulations of return)，</li><li>RL优化问题(the RL optimization problem)，</li><li>值函数(value functions)。</li></ul><h3 id="States-and-Observations"><a href="#States-and-Observations" class="headerlink" title="States and Observations"></a>States and Observations</h3><p>状态(states)是对世界状况的完整描述，没有隐藏信息。观察(observations)是对状态的部分描述，有一些信息被略掉了。</p><p>在深度RL中，我们几乎总是通过实值向量，矩阵或高阶张量来表示状态和观察。例如，视觉观察可以由其RGB像素矩阵表示，机器人的状态可以用它的关节角度和速度来表示。</p><p>当agent能够观察到环境的完整状态时，我们说环境被<strong>完全观察</strong>(fully observed)，当agent只能看到部分环境时，我们说环境被<strong>部分观察</strong>(partially observed)。</p><blockquote><p>强化学习符号通常把状态表示为$s$，但是表示观察$o$更合适。通常这用于讨论agent去决定采取一个行为：我们通常说动作是以状态为条件的，但是实际上，动作是以观察为条件的，因为agent没有接触到状态。</p><p>在这里，我们将遵循标准约定，但应从上下文中可以明确区分。</p><p>(这里有点绕，说白了就是agent只能观察环境，不能直接得到环境的状态)</p></blockquote><h3 id="Action-Spaces"><a href="#Action-Spaces" class="headerlink" title="Action Spaces"></a>Action Spaces</h3><p>不同的环境允许不同类型的行为，给定环境中的所有有效行为集合通常称为<strong>行为空间</strong>。某些环境(如Atari和Go)具有<strong>离散的行为空间</strong>，agent只有有限数量可采取的行为。其他环境中例如agent控制物理世界中的机器人，具有<strong>连续的行为空间</strong>，在连续空间中，行为是实值向量。</p><p>这个区别对深度RL中的方法有非常深刻的影响。一些算法族只能在某种情况下应用，而对于其他情况则必须重做。</p><h3 id="Policies"><a href="#Policies" class="headerlink" title="Policies"></a>Policies</h3><p><strong>策略</strong>是agent用来决定行为的规则。它可以是确定性的，在这种情况下通常表示为$\mu$：</p><p>$$a_t = \mu(s_t),$$</p><p>或者是随机的，表示为$\pi$：</p><p>$$a_t \sim \pi(\cdot \vert s_t).$$</p><p>策略本质上是agent的大脑，因此将“策略”一词替换为“agent”也比较常见，例如可以说“策略试图最大化奖励”。</p><p>在深度RL中，我们使用的是<strong>参数化策略</strong>：策略的输出是可计算的函数，函数依赖于一组参数(就像神经网络的权重和偏差)，我们可以通过某种优化算法来调整函数进而改变行为。</p><p>我们经常用$\theta$或$\phi$表示这种策略的参数，将其写在策略符号下标：</p><p>$$a_t = \mu_{\theta}(s_t) \ a_t \sim \pi_{\theta}(\cdot \vert s_t).$$</p><h4 id="Deterministic-Policies"><a href="#Deterministic-Policies" class="headerlink" title="Deterministic Policies"></a>Deterministic Policies</h4><p><strong>确定性策略示例</strong>。用Tensorflow在连续行为空间构建简单的确定性策略代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">obs = tf.placeholder(shape=(<span class="keyword">None</span>, obs_dim), dtype=tf.float32)</span><br><span class="line">net = mlp(obs, hidden_dims=(<span class="number">64</span>,<span class="number">64</span>), activation=tf.tanh)</span><br><span class="line">actions = tf.layers.dense(net, units=act_dim, activation=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>其中mlp是一个函数，是用多个dense层堆叠在一起表示的。</p><h4 id="Stochastic-Policies"><a href="#Stochastic-Policies" class="headerlink" title="Stochastic Policies"></a>Stochastic Policies</h4><p>深度RL中两种最常见的随机策略是<strong>类别策略</strong>(Categorical Policies)和<strong>对角高斯策略</strong>(Diagonal Gaussian Policies)。类别策略可用于离散行为空间，而对角高斯策略用于连续行为空间。使用和训练随机策略有两个关键计算：</p><ul><li>从策略中采样行为(sampling actions from the policy)</li><li>计算特定行为的log似然函数$\log \pi_\theta (a \vert s)$(computing log likelihoods of particular actions)</li></ul><blockquote><p><strong>Categorical Policies</strong> 类别策略就像离散动作的分类器。分类策略构建神经网络的方式与分类器相同：输入是观察，然后是一些层(可能是卷积或全连接，取决于输入的类型)，最后是线性层，提供每个行为的logits，接着是softmax，将logits转换为概率。</p><p><em>Sampling</em> 基于每个行为的概率，Tensorflow等框架有内置的采样工具来采样。例如tf.distributions.Categorical文档或tf.multinomial。</p><p><em>Log-Likelihood</em>。将最后一层概率表示为$P _ {\ theta}(s)$。它是一个向量，我们可以将行为视为向量的索引，然后通过向量索引来获取动作$a$的对数似然：<br>$$\log \pi_{\theta}(a \vert s) = \log \left[P_{\theta}(s)\right]_a.$$</p></blockquote><blockquote><p><strong>Diagonal Gaussian Policies</strong> 多元高斯分布(或多元正态分布)由平均向量$μ$和协方差矩阵$\Sigma$来描述。对角高斯分布是一种特殊情况，其中协方差矩阵仅在对角线上具有值，因此我们可以用向量表示它。</p><p>对角高斯策略总是用一个神经网络将观察映射到动作均值，$\mu _ {\theta}(s)$。通常有两种不同的表示协方差矩阵的方式。</p><p><em>第一种方式</em>：用一个对数标准差向量，$\log \sigma$，它不是状态函数，是独立参数。 (VPG，TRPO和PPO的实现方式是这样做的)</p><p><em>第二种方式</em>：用神经网络将状态映射到对数标准差，$\log \sigma _ {\theta}(s)$。它可以选择与均值网络共享一些层。</p><p>注意，在这两种情况下，我们都直接输出对数标准差而不是标准差。这是因为对数标准差可以容易的取到$(-  \infty，\infty)$中的任何值，而标准差必须是非负的，没有这些约束会更容易训练参数。标准差可以通过对对数标准差取幂获得，所以这种表示没有任何损失。</p><p><em>Sampling</em> 给定动作均值$\mu _ {\theta}(s)$和标准差$\sigma _ {\theta}(s)$，以及来自球形高斯的噪声向量z($z \sim \mathcal {N}(0,I)$ )，行为采样可以用如下计算获得：<br>$$a = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot z$$<br>其中$\odot$表示两个向量的元素乘积。框架都具有计算噪声向量的内置方法，例如tf.random_normal。</p><p><em>Log-Likelihood</em> 对于具有均值$\mu = \mu _ {\theta}(s)$和标准差$\sigma = \sigma _ {\theta}(s)$的对角高斯分布，k维行为a的对数似然性由下式给出：<br>$$\log \pi_{\theta}(a \vert s) = -\frac{1}{2}\left(\sum_{i=1}^k \left(\frac{(a_i - \mu_i)^2}{\sigma_i^2} + 2 \log \sigma_i \right) + k \log 2\pi \right).$$</p></blockquote><h3 id="Trajectories"><a href="#Trajectories" class="headerlink" title="Trajectories"></a>Trajectories</h3><p>轨迹$\tau$是在世界中的一系列状态和行为，</p><p>$$\tau = (s_0, a_0, s_1, a_1, …).$$</p><p>世界的第一个状态$s_0$是从<strong>起始状态分布</strong>中随机抽样的，有时用$\rho_0$表示：</p><p>$$s_0 \sim \rho_0(\cdot).$$</p><p>状态转换(在时间t的状态$s_t$和t+1的状态$s_ {t + 1}$之间发生的事情)受环境的自然规律支配，并且仅依赖于最近的行为$a_t$。它们可以是确定性的:</p><p>$$s_{t+1} = f(s_t, a_t)$$</p><p>也可以是随机的：</p><p>$$s_{t+1} \sim P(\cdot \vert s_t, a_t).$$</p><p>行动由agent根据其策略采取。</p><blockquote><p>trajectories也经常被称为episodes或rollouts。</p></blockquote><h3 id="Reward-and-Return"><a href="#Reward-and-Return" class="headerlink" title="Reward and Return"></a>Reward and Return</h3><p>奖励函数R在强化学习中至关重要。它取决于当前的世界状态，采取的行动以及世界的下一个状态：</p><p>$$r_t = R(s_t, a_t, s_{t+1})$$</p><p>虽然经常将其简化为仅依赖于当前状态$r_t = R(s_t)$或状态-行为$r_t = R(s_t，a_t)$。</p><p>agent的目标是在轨迹上最大化累积奖励，我们将用$R(\tau)$来表示所有的奖励。</p><p>一种回报是<strong>有限期未折现的回报</strong>(finite-horizon undiscounted return)，它就是在固定的步骤窗口中获得的奖励总和：</p><p>$$R(\tau) = \sum_{t=0}^T r_t.$$</p><p>另一种回报是<strong>无限期折现回报</strong>(infinite-horizon discounted return)，它是agent获得的所有奖励的总和，但是对获得奖励的距离进行折现。这种奖励形式有一个折现因子$\gamma \in(0,1)$：</p><p>$$R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t.$$</p><p>为什么要一个折现因子呢？不是要获得所有奖励吗？虽然如此，但折现因子不仅直观而且在数学也方便。在直观的层面上：当前的奖励比后续的更好。数学上：无限期的奖励总和可能不会收敛到有限值，并且很难在方程中处理，但是在折现因子以及一定的条件下，这个求和是收敛的。</p><blockquote><p>虽然这两种回报方式之间的界限非常明显，但这个界限在深度RL实践中往往没那么清晰，例如我们经常会设置算法去优化未折现的回报，但在估算值函数时却又使用折现因子。</p></blockquote><h3 id="The-RL-Problem"><a href="#The-RL-Problem" class="headerlink" title="The RL Problem"></a>The RL Problem</h3><p>无论回报方式选什么(无限期折现，还是有限期未折现)，无论策略的选择如何，RL的目标都是选择一种策略，当agent根据它采取行动时能最大化预期回报。</p><p>谈到预期回报，我们首先要讨论轨迹上的概率分布。</p><p>我们假设环境转换和策略都是随机的。 在这种情况下，T步轨迹的概率为：</p><p>$$P(\tau \vert \pi) = \rho_0 (s_0) \prod_{t=0}^{T-1} P(s_{t+1} \vert s_t, a_t) \pi(a_t \vert s_t).$$</p><p>由$J(\pi)$表示预期收益为：</p><p>$$J ( \pi ) = \int _ { \tau } P ( \tau \vert \pi ) R ( \tau ) = \underset { \tau \sim \pi } { \mathrm { E } } [ R ( \tau ) ]$$</p><p>RL中的核心优化问题可以表示为：</p><p>$$\pi^* = \arg \max_{\pi} J(\pi),$$</p><p>其中$\pi ^ *$是<strong>最优策略</strong>。</p><h3 id="Value-Functions"><a href="#Value-Functions" class="headerlink" title="Value Functions"></a>Value Functions</h3><p>了解状态或状态-动作对的<strong>值</strong>通常很有用。值指的是从该状态或状态-行动对开始的预期回报，然后一直根据特定策略行事。几乎每种RL算法都以这样或那样的方式在使用<strong>值函数</strong>。</p><p>这里有四个主要函数：</p><ul><li><p><strong>On-Policy Value Function</strong>，$V ^ {\pi}(s)$，从状态s开始并且总是根据策略$\pi$执行行为，它会给出预期的回报为：</p><p>$$V^{\pi}(s) = \underset {\tau \sim \pi}E [R(\tau)\left\vert s_0 = s \right]$$</p></li><li><p><strong>On-Policy Action-Value Function</strong>，$Q ^ {\ pi}(s，a)$，从状态s开始，采取任意行动a(可能不是来自策略),总是根据策略$\pi$执行行为,它给出预期的回报：</p><p>$$Q^{\pi}(s,a) = \underset {\tau \sim \pi}E[R(\tau)\left\vert s_0 = s, a_0 = a\right.]$$</p></li><li><p><strong>Optimal Value Function</strong>, $V ^ <em>(s)$，从状态s开始并且始终根据环境中的</em>最优*策略执行，则给出预期的回报：</p><p>$$V^*(s) = \max_{\pi} \underset {\tau \sim \pi}E[R(\tau)\left\vert s_0 = s\right.]$$</p></li><li><p><strong>Optimal Action-Value Function</strong>，$Q ^ <em>(s，a)$，从状态s开始，采取任意动作a，然后根据环境中的</em>最优*策略采取行动，给出预期回报：</p><p>$$Q^*(s,a) = \max_{\pi} \underset{\tau \sim \pi}E[R(\tau)\left\vert s_0 = s, a_0 = a\right.]$$</p></li></ul><blockquote><p>在谈论价值函数时，如果不考虑时间依赖性，那就只是在表示无限期折现回报的期望均值。有限期无折现回报的值函数需要有时间作为参数。</p></blockquote><blockquote><p>值函数和动作-值函数之间有两个关键的联系经常出现：<br>$$V^{\pi}(s) = \underset {a\sim \pi}E[Q^{\pi}(s,a)],$$<br>$$V^<em>(s) = \max_a Q^</em> (s,a).$$<br>这些关系直接来自刚刚给出的定义：你能证明它们吗？</p></blockquote><h3 id="The-Optimal-Q-Function-and-the-Optimal-Action"><a href="#The-Optimal-Q-Function-and-the-Optimal-Action" class="headerlink" title="The Optimal Q-Function and the Optimal Action"></a>The Optimal Q-Function and the Optimal Action</h3><p>最优动作-值函数$Q ^ <em>(s，a)$与最优策略选择的动作之间存在重要联系。根据定义，$Q ^ </em>(s，a)$给出了在状态s中开始,采取(任意)动作a，然后一直根据最优策略行动的预期回报。</p><p>s中的最优策略将选择从s开始最大化预期回报的行为。因此如果我们有$Q ^ <em>$，我们可以通过这个式子直接获得最优动作$a ^ </em>(s)$：</p><p>$$a^<em>(s) = \arg \max_a Q^</em> (s,a).$$</p><p>注意：可能存在多个最大化$Q ^ *(s，a)$的行为，在这种情况下，所有动作都是最优的，最优策略可以随机选择它们中的任何一个，但总存在一种确定性选择行为的最优策略。</p><h3 id="Bellman-Equations"><a href="#Bellman-Equations" class="headerlink" title="Bellman Equations"></a>Bellman Equations</h3><p>所有的四个值函数都遵循称为<strong>Bellman方程</strong>的特殊自洽方程。Bellman方程背后的基本思想是：</p><blockquote><p>起始点的值是从当前位置获得的奖励期望，在加上下次落脚点值的期望。(The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.)</p></blockquote><p>On-Policy Value Function的Bellman方程是</p><p>$$V^{\pi}(s) = \underset {a \sim \pi \ s’\sim P}E[r(s,a) + \gamma V^{\pi}(s’)], \<br>Q^{\pi}(s,a) = \underset{s’\sim P}E[r(s,a) + \gamma \underset{a’\sim \pi}E[Q^{\pi}(s’,a’)],$$</p><p>其中$s’\sim P$是$s’\sim P(\cdot \vert s，a)$的简写，表示从环境转换规则中采样的下一个状态$s’$; $a \sim \pi$是$\sim \pi(\cdot \vert s)$的简写; 而$a’\sim \pi$是$a’\sim \pi(\cdot \vert s’)$的简写。</p><p>Optimal Value Function的Bellman方程是</p><p>$$V^<em>(s) = \max_a \underset{s’\sim P}E[r(s,a) + \gamma V^</em>(s’)], \ Q^<em>(s,a) = \underset {s’\sim P}E[r(s,a) + \gamma \max_{a’} Q^</em>(s’,a’)].$$</p><p>On-Policy Value Function与Optimal Value Function的Bellman方程之间的关键区别在于在行为上有没有取$\max$,它的意思是，为了达到最优的目标，agent必须选择那个具有最高值的行为。</p><h3 id="Advantage-Functions"><a href="#Advantage-Functions" class="headerlink" title="Advantage Functions"></a>Advantage Functions</h3><p>在RL中，我们不去描述行为绝对好多少，而只需要说它比其它行为平均好多少。也就是说，我们想知道该行为的<strong>相对优势</strong>，优势函数将精确描述这个概念。</p><p>对于策略$\pi$的优势函数$A ^ {\pi}(s，a)$描述了在状态s中采取特定动作a比根据$\pi(\cdot \vert s)$随机选择动作要好多少,这里假设一直按照$\pi$采取行动。在数学上优势函数定义为：</p><p>$$A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s).$$</p><blockquote><p>我们稍后会对此进行讨论，但优势函数对于策略梯度方法至关重要。</p></blockquote><h2 id="Optional-Formalism"><a href="#Optional-Formalism" class="headerlink" title="(Optional) Formalism"></a>(Optional) Formalism</h2><p>到目前为止，我们已经以非正式的方式讨论了agent的环境，但是如果深入研究文献，你很可能会遇到一个标准数学形式：<strong>马尔可夫决策过程(MDP)</strong>。 MDP是一个5元组，$\langle S，A，R，P，\rho_0 \rangle$，其中</p><ul><li>$S$ is the set of all valid states,</li><li>$A$ is the set of all valid actions,</li><li>$R$ : $S \times A \times S \to \mathbb{R}$ is the reward function, with $r_t = R(s_t, a_t, s_{t+1})$,</li><li>$P$ : $S \times A \to \mathcal{P}(S)$ is the transition probability function, with $P(s’\vert s,a)$ being the probability of transitioning into state s’ if you start in state s and take action a,</li><li>and $\rho_0$ is the starting state distribution.</li></ul><p>马尔可夫决策过程这个名称指的是系统服从马尔可夫属性：状态转换只取决于最近的状态和行动，而不是先前的历史。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来自OpenAI Spinning Up &lt;a href=&quot;https://spinningup.openai.com/en/latest/spinningup/rl_intro.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Introducti
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="强化学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="强化学习" scheme="http://yuanquanquan.top/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Connecting Generative Adversarial Networks and Actor-Critic Methods</title>
    <link href="http://yuanquanquan.top/2019/20190112899/"/>
    <id>http://yuanquanquan.top/2019/20190112899/</id>
    <published>2019-11-28T07:28:01.000Z</published>
    <updated>2019-11-28T08:18:47.404Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>无监督的GAN和强化学习的actor-critic方法在优化困难方面声名显著。这两个领域的研究人员积累了大量的缓解不稳定和改善训练情况的策略。这里我们展示了GAN可以看作是一个特定环境中的actor-critic方法，在这个环境中actor无法影响奖励。我们对这两类模型稳定训练方法做了综述，不仅包括可以同时对二者使用的方法，还包括只针对特定模型的方法。我们也对一系列信息流更复杂的GAN和RL算法进行了综述。我们希望强调二者之间的联系来激励GAN和RL两个领域开发出通用，可扩展以及稳定的算法。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>大部分机器学习问题可以表述为对于一个目标的优化问题。但是，很多问题缺乏一个统一的代价函数，并且混合着多个模型，虽然模型间彼此传递信息，但是却试图最小化各自内部的损失函数。这对大部分学习算法的假设造成了影响，并且应用传统方法如梯度下降等通常会导致病态行为，例如波动或坍塌为退化解。尽管有这些实践中的问题，但是在具有混合或多级损失的模型中存在巨大潜力，同时不同局部损失的组合也是大脑功能的基础。</p><p>Actor-critic（AC）方法和GAN是两类多层次优化问题并且具有相似之处。在这两种情况中，信息流都是一个简单的前向传递。从一个模型，不管它接受一个Action（AC）还是生成的样本（GAN），到另一个模型来衡量前一个模型的输出质量。在这两类中，第二个模型都是唯一对环境特殊信息直接接触的模型，这个信息或许是奖励信息（AC）或是真实样本（GAN），并且第一个模型必须从第二个模型的错误信号中学习。当然二者也有其不同之处，第二部分讨论。这些模型都有稳定性方面的问题，而两个领域都形成了各自独立的稳定训练的方法。</p><h2 id="2-Algorithms"><a href="#2-Algorithms" class="headerlink" title="2 Algorithms"></a>2 Algorithms</h2><p>GAN和AC都可以看作是双层（bilevel）或双时间尺度（two-time-scale）的优化问题，一个模型通过另一个模型来优化自身：<br>$$<br>x^{<em>}=\arg \min _{x \in \mathcal{X}} F\left(x, y^{</em>}(x)\right) \qquad (1)<br>y^{*}(x)=\arg \min _{y \in \mathcal{Y}} f(x, y) \qquad (2)<br>$$<br>双层优化问题在运筹学方面已经得到广泛研究，包括AC方法，但是都是在线性或凸的设置上。</p><h3 id="2-1-Generative-Adversarial-Networks"><a href="#2-1-Generative-Adversarial-Networks" class="headerlink" title="2.1 Generative Adversarial Networks"></a>2.1 Generative Adversarial Networks</h3><p>GAN可以表述为包含两个部分的无监督问题：一个生成器G从一个分布中采样，一个判别器D分类一个样本为真或假。通常生成器是一个前向神经网络，接收一个固定噪声源$z \sim \mathcal{N}(0, I)$输入，判别器是另一个神经网络，把一个图像映射为一个二分类的概率。GAN的博弈可以表述为零和博弈，它的值表示为交叉熵损失：<br>$$<br>\min _{G} \max <em>{D} \mathbb{E}</em>{w, y}[y \log D(w)+(1-y) \log (1-D(w))]\=\min _{G} \max <em>{D} \mathbb{E}</em>{w \sim p_{\text { data }}}[\log D(w)]+\mathbb{E}<em>{z \sim \mathcal{N}(0, I)}[\log (1-D(G(z)))] \qquad (3)<br>$$<br>为了保证生成器在判别器准确率很高的情况下也能有梯度来学习，生成器的损失函数通常表示为最大化分类为真的概率，而不是最小化分类为假的概率。修改后的损失依旧是双层优化问题：<br>$$<br>F(D, G)=-\mathbb{E}</em>{w \sim p_{\text { data }}}[\log D(w)]-\mathbb{E}<em>{z \sim \mathcal{N}(0, I)}[\log (1-D(G(z)))] \qquad (4) \ f(D, G)=-\mathbb{E}</em>{z \sim \mathcal{N}(0, I)}[\log D(G(z))] \qquad (5)<br>$$</p><h3 id="2-2-Actor-Critic-Methods"><a href="#2-2-Actor-Critic-Methods" class="headerlink" title="2.2 Actor-Critic Methods"></a>2.2 Actor-Critic Methods</h3><p>AC方法在强化学习中很早就有了。大部分强化学习算法或是学习一个值函数，例如值迭代和TD学习，或直接学习一个策略，例如策略梯度方法。AC方法是同步的：actor学习策略与critic学习值函数同步进行。在很多AC方法中，critic为策略梯度方法提供了一个低方差的基准，而不用去从返回中估计值。在这种情况下，即使是一个坏的值函数估计也是有帮助的，因为无论使用什么基准策略梯度都会被修正偏置。在其他AC方法中，策略通过近似值函数来更新，这种情形与GAN很相似。如果策略通过一个不正确的值函数来优化，可能会导致一个坏的策略，从而不能完全探索整个空间，那么就不能发现好的值函数，从而出现退化解。有一系列的方法来缓解这个问题。</p><p>考虑一个RL的MDP问题，有一个状态S，行为A的集合，一个初始状态为</p><p>p_0(s)的分布，转移函数$\mathcal{P}\left(s_{t+1} \vert s_{t}, a_{t}\right)$,奖励分布$\mathcal{R}\left(s_{t}\right)$，还有折扣因子$\gamma \in[0,1]$。AC方法的主要目的是同步学习一个行为值函数$Q^{\pi}(s, a)$，来预测折扣奖励期望：<br>$$<br>Q^{\pi}(s, a)=\mathbb{E}{s{t+k} \sim \mathcal{P}, r{t+k} \sim \mathcal{R}, a{t+k} \sim \pi}\left[\sum{k=1}^{\infty} \gamma^{k} r{t+k} | s{t}=s, a{t}=a\right] \qquad (6)<br>$$<br>然后根据值函数学一个最优的策略：<br>$$<br>\pi^{*}=\arg \max <em>{\pi} \mathbb{E}</em>{s_{0} \sim p_{0}, a_{0} \sim \pi}\left[Q^{\pi}\left(s_{0}, a_{0}\right)\right]  \qquad (7)<br>$$<br>我们可以把$Q^{\pi}$表示为最小化问题的解：<br>$$<br>Q^{\pi}=\arg \min <em>{Q} \mathbb{E}</em>{s_{t}, a_{t} \sim \pi}\left[\mathcal{D}\left(\mathbb{E}_{s_{t+1}, r_{t}, a_{t+1}}\left[r_{t}+\gamma Q\left(s_{t+1}, a_{t+1}\right)\right] | Q\left(s_{t}, a_{t}\right)\right)\right] \qquad (8)<br>$$<br>其中$\mathcal{D}(\cdot | \cdot)$是任意的散度。现在AC问题可以表示为一个双层优化问题：<br>$$<br>F(Q, \pi)=\mathbb{E}_{s_{t}, a_{t} \sim \pi}\left[\mathcal{D}\left(\mathbb{E}_{s_{t+1}, r_{t}, a_{t+1}}\left[r_{t}+\gamma Q\left(s_{t+1}, a_{t+1}\right)\right]\right] | Q\left(s_{t}, a_{t}\right)\right) ] \qquad (9)<br>\ f(Q, \pi)=-\mathbb{E}_{s_{0} \sim p_{0}, a_{0} \sim \pi}\left[Q^{\pi}\left(s_{0}, a_{0}\right)\right]<br>$$<br>有很多解决这个问题的AC方法。传统的AC方法通过策略梯度来优化策略，通过TD错误来缩放策略梯度，而行为值函数通过普通的TD学习来更新。我们主要关注于确定性策略梯度（DPG），以及它在随机策略的扩展SVG（0），和连续行为的neurally-fitted Q-learning（NFQCA）。这些算法都是针对于行为和观察值为连续的情况，使用神经网络来近似行为值函数和策略。这是个早就存在的解决连续行为RL的方法，所有的方法通过传回估计值对行为的梯度来更新策略，而不是直接传入TD错误。这些方法的区别主要是训练过程。在NFQCA中，actor和critic在每个episode中以batch模式训练，而DPG和SVG（0）中，网络使用查分法在线更新。</p><h3 id="2-3-GANs-as-a-kind-of-Actor-Critic"><a href="#2-3-GANs-as-a-kind-of-Actor-Critic" class="headerlink" title="2.3 GANs as a kind of Actor-Critic"></a>2.3 GANs as a kind of Actor-Critic</h3><p>GAN和AC方法总结在图1中。在这两种情形中，模型都可以从环境（GAN的判别器和AC中的critic）中获取错误信息，而另一个模型必须从第一个模型的梯度信息中更新。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/GAN-AC-fig1.png" alt></p><p>我们可以使这种联系更加精确，把GAN看作是一种修改过的actor-critic方法来描述一个MDP。考虑这样一个MDP，其中动作为图像中的每个像素。环境随机选择显示生成器生成的图像或显示真实图像。如果环境选择真实图像，则环境奖励为1，否则为0。此MDP是无状态的，因为actor生成的图像不会影响将来的数据。</p><p>在这种环境中进行actor-critic的学习显然非常类似于GAN。但必须进行一些调整才能使其完全相同。如果actor可以获取到环境的state，它可以轻而易举地产生真实的图像，所以actor必须是“盲”的，不知道state。虽然MDP是无状态的，但这并不妨碍actor学习。均方Bellman残差通常用作critic的损失，但为了匹配GAN损失应使用交叉熵。这仍然提供了具有常量最小值的损失函数。由于actor接收值的梯度而不是RL中Bellman残差的梯度，因此要在给actor的梯度中包含与$\frac{\partial \mathcal{D}}{\partial Q}$成比例的缩放项，其中D是交叉熵（在实践中，生成器使用其他损失来细致地处理这个缩放项的复杂性）。最后，如果环境显示真实图像，那么不应更新actor的参数。在奖励为1时，可以通过让critic的梯度归零来实现。因此，GAN可以被视为在无状态MDP中“盲”actor的actor-critic方法。</p><p>此MDP有几个独特的方面会导致一些与actor-critic方法无关的行为。首先，为什么actor-critic算法导致了对抗性行为这点并不是很明显。通常，actor和critic试图优化互补损失函数，而非在不同方向上优化相同的损失。GAN中的对抗是因为其MDP是玩家不能对奖励产生影响的。从本质上讲，它是一个真实政策梯度始终为零的MDP。然而，critic不能单独从输入示例中学习游戏的因果结构，而是要朝着预测奖励的特征方向移动。actor根据critic的最佳估计向某个方向移动以增加奖励，但这种变化并不会导致真实奖励的增加，因此critic将很快学会在actor移动的方向上赋予较低的值。因此，对于actor和critic的更新而言，它们在理想情况下是正交的（如在兼容性的actor-critic中），而不是对抗。值得注意的是，这对于只能部分观察到actor的环境会产生重要影响。而对于完全可观察的MDP，最优策略始终是确定性的。然而，对于GAN，能匹配真实分布的生成器是极小极大问题的固定点。尽管GAN与RL问题之间存在这些差异，但我们认为二者有足够的相似之处，也值得研究两种情况下通用的训练技术。</p><h2 id="3-Stabilizing-Strategies"><a href="#3-Stabilizing-Strategies" class="headerlink" title="3 Stabilizing Strategies"></a>3 Stabilizing Strategies</h2><p>在回顾了GAN的基本知识，actor-critic算法及其扩展之后，我们这里讨论每个领域各自的“交易技巧”。表1总结了不同的方法。虽然并不是一个详尽的清单，我们已经包括那些我们认为已经在其领域产生最大影响或最有可能在各个领域之间交叉的东西。</p><p><img src="https://cugtyt.github.io/blog/papers/2019/R/GAN-AC-tab1.png" alt></p><h3 id="3-1-GANs"><a href="#3-1-GANs" class="headerlink" title="3.1 GANs"></a>3.1 GANs</h3><ol><li>冻结学习（Freezing learning）。通常当生成器或判别器超过对方时，GAN将陷入退化的情况。一个简单的补救措施是在一个模型开始过于强大时冻结学习。在actor-critic学习中已经成功采用了一种非常类似的方法，即当TD误差的幅度低于或高于某个阈值时，actor或critic的学习被冻结。</li><li>标签平滑（Label smoothing）。当判别器的预测非常有信心时，一种防止梯度消失的简单技巧是，用平滑标签$\epsilon / 1-\epsilon$替换0/1标签，这保证了生成器始终具有信息梯度。虽然这是针对分类的，但没有理由不能于奖励为0/1并且critic梯度消失的RL情形。</li><li>历史平均（Historical averaging）。受到游戏理论中虚构游戏的启发，历史平均为梯度下降增加了一个限制项，惩罚超过参数历史平均值的步骤。即使参数平均值不具有直观含义，此方法也可有效地防止因两个模型优化不同目标而引起的振荡。历史平均也与Polyak-Ruppert平均及其扩展密切相关。尽管Polyak-Ruppert类型的平均已经被RL领域的人严格分析过，但据我们所知，它还没有被采纳为标准的“交易工具”。在DPG中使用replay buffer在概念上也类似于虚构游戏，但仅适用于actor。</li><li>小批量判别（Minibatch discrimination）。为了防止生成器坍塌到单个样本上，小批量判别将鉴别器的作用从对单个图像分类到整个小批量分类，这有助于增加生成样本的熵。在RL中，通过向actor添加惩罚，进而鼓励更高熵策略的行为来解决类似的探索不足问题。直接计算熵对于复杂的随机策略是不切实际的，批量判别可能是鼓励在连续空间进行探索的有效替代方案。</li><li>批量标准化（Batch normalization）。批量标准化对于将GAN扩展到深度卷积网络至关重要，虚拟批量标准化对其进行了扩展，使用常量引用批处理来防止由于小批量的其他元素导致的预测相关性。批量标准化在DPG已经尝试过的大部分环境中也很有用，但尚未有研究虚拟批量标准化能否会带来更大的改进。</li></ol><h3 id="3-2-Actor-Critic"><a href="#3-2-Actor-Critic" class="headerlink" title="3.2 Actor-Critic"></a>3.2 Actor-Critic</h3><ol><li>回放缓冲（Replay buffers）。在离散和连续RL中，重放缓冲对于从训练数据中去除相关性非常有效。它们在概念上也类似于博弈论中的虚构游戏，一个玩家对另一个玩家的平均政策起到最佳响应。在虚构游戏中，两个玩家都与对手的历史平均对战，而在AC设置中，回放缓冲只能用于critic。critic可以根据actor过去选择的行动进行离线政策更新，但actor不能从过去critic的梯度中学习，因为这些梯度是针对不同的行为而采取的。类似地，对于GAN，我们可以保留先前生成图像的缓冲器，以防止判别器与当前生成器拟合太紧密。我们已经尝试使用GAN的回放缓冲，但即使对于简单的分布也无法生成渐近正确的样本。</li><li>目标网络（Target networks）。由于动作值函数在Bellman递归中出现两次，因此稳定性可能是Q函数近似中的难题。目标网络将TD更新中的一个网络固定来解决此问题。由于GAN博弈可以被视为无状态MDP，因此动作值函数的二次出现便消失了，并且判别器的学习变成普通的回归问题。因此，我们不考虑适用于GAN的目标网络。然而，将Q-learning作为子问题的其他多层优化问题可能会受益。同样值得注意的是，自相对估计在概念上与目标网络类似，因此目标网络的思想仍然可以适用于其他密度估计问题。</li><li>熵正则化（Entropy regularization）。actor-critic方法往往无法充分探索行动空间。为了解决这个问题，有时会增加额外奖励以鼓励高熵政策。当动作空间是离散的时候很容易实现，但在连续控制情况中稍微困难一些。值得注意的是，GAN经常遇到类似的问题，这时候生成器坍塌到某几种模式。在连续控制中任何鼓励进行探索的方法都可能增加GAN中的样本多样性。</li><li>兼容性（Compatibility）。actor-critic方法的理论发展之一是兼容性critic的概念。如果critic是策略梯度相对于其参数的线性函数，并且critic是最优的，那么使用它来代替预期回报的梯度真实值可以给出无偏近似。这种兼容性的critic也与预期收益的自然梯度密切相关，并可用于策略的高效自然梯度下降。虽然优雅，但仍不清楚兼容性的概念是否可以自然地扩展到GAN中。由于任何策略的真实值在GAN MDP中始终为0.5，因此真实政策梯度始终为零。通常我们更希望GAN是对抗而不是兼容。</li></ol><h2 id="4-Conclusions"><a href="#4-Conclusions" class="headerlink" title="4 Conclusions"></a>4 Conclusions</h2><p>将深度学习与多层优化相结合，可以为机器学习和人工智能中的各种问题带来巨大希望。尽管优化和探索存在固有的困难，但GAN和actor-critic方法已经对各自的领域产生了巨大的影响。我们希望通过指出两者之间的深层联系，能鼓励在不同领域之间开发和采用更通用的技术，让不同的思想自由流动。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;无监督的GAN和强化学习的actor-critic方法在优化困难方面声名显著。这两个领域的研
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="GAN" scheme="http://yuanquanquan.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>statapy——在jupyter里写stata代码</title>
    <link href="http://yuanquanquan.top/2019/2019090522/"/>
    <id>http://yuanquanquan.top/2019/2019090522/</id>
    <published>2019-11-24T11:24:11.000Z</published>
    <updated>2019-11-24T11:46:22.695Z</updated>
    
    <content type="html"><![CDATA[<p><img src="data:image/jpeg;base64,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" alt="img"></p><p>Stata是做生物统计/计量经济学的重要统计工具, 而python是做数据科学的利器, ipystata将stata和python结合在一起, 并能够在jupyter notebook中使用, 使得我们的工作效率大大提升。下面我们介绍一下, 如何安装stata,<br>如何在python中使用stata, 并进行stata的一些自动化操作。</p><p>目前来看, 在jupyternotebook中使用stata有两种方案:</p><ul><li>方案一: 使用ipystata模块, 这个模块提供了<code>%%stata</code>魔法函数, 可以把notebook的cell可以执行stata语句</li><li>方案二: 使用stata_kernel, 它实际上是一个notebook kernel, 使用stata kernel创建的notebook, 只能执行stata语句</li></ul><p>下面我们分别介绍两种方案。</p><p><strong>命令行注册</strong></p><p>这是windows的安装方法, 如果你是linux, 也是类似的道理, 需要运行stata命令来注册。</p><p>使用管理员模式打开powershell:</p><p><img src="https://mlln.cn/2018/11/01/%E5%AE%89%E8%A3%85stata%E5%B9%B6%E5%9C%A8jupyter-notebook%E4%B8%AD%E8%B0%83%E7%94%A8/powershell.png" alt></p><p>工作目录调整到stata的安装目录, 然后执行命令<code>.\StataSE-64.exe /Register</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1 PS C:\Users\syd&gt; cd c:/</span><br><span class="line">2 PS C:\&gt; cd &apos;.\Program Files (x86)\&apos;</span><br><span class="line">3 PS C:\Program Files (x86)&gt; cd .\Starth\</span><br><span class="line">4 PS C:\Program Files (x86)\Starth&gt; cd ..</span><br><span class="line">5 PS C:\Program Files (x86)&gt; cd .\Stata15\</span><br><span class="line">6 PS C:\Program Files (x86)\Stata15&gt; .\StataSE-64.exe /Register</span><br><span class="line">7 PS C:\Program Files (x86)\Stata15&gt;</span><br></pre></td></tr></table></figure><h2 id="方案一-使用stata魔法函数"><a href="#方案一-使用stata魔法函数" class="headerlink" title="方案一: 使用stata魔法函数"></a>方案一: 使用stata魔法函数</h2><h3 id="安装python模块"><a href="#安装python模块" class="headerlink" title="安装python模块"></a>安装python模块</h3><p>(假设你已经安装好了jupyter notebook)</p><p>你需要使用pip安装两个模块:</p><p>‘’’<br>pip install ipystata<br>pip install psutil<br>‘’’</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>自己随便新建一个notebook , 然后先设置stata的路径:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import ipystata  from ipystata.config import config_stata  config_stata(r&apos;C:\Program Files (x86)\Stata15\StataSE-64.exe&apos;)</span><br></pre></td></tr></table></figure><p>然后你可以使用魔法函数<code>%%stata</code>运行一个stata的输出命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display &quot;hello world&quot;</span><br></pre></td></tr></table></figure><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>把pandas.DataFrame发送给stata使用:</p><p>在python中提前定义好一个df(DataFrame), 然后:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%%stata -d df</span><br></pre></td></tr></table></figure><p>或者把数据从stata输出到python:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%%stata -o df</span><br></pre></td></tr></table></figure><p>为了调试, 运行stata的时候, 可以设置打开stata的操作界面:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%%stata -os</span><br></pre></td></tr></table></figure><p>输出图表:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%%stata -gr</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>还有很多用法请参考github地址: <a href="https://github.com/TiesdeKok/ipystata/blob/master/ipystata/Example.ipynb" target="_blank" rel="noopener">https://github.com/TiesdeKok/ipystata/blob/master/ipystata/Example.ipynb</a></p><h2 id="方案二-使用stata-kernel"><a href="#方案二-使用stata-kernel" class="headerlink" title="方案二: 使用stata kernel"></a>方案二: 使用stata kernel</h2><h3 id="安装方法"><a href="#安装方法" class="headerlink" title="安装方法"></a>安装方法</h3><p>在<code>powershell</code>中执行下面两条命令即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install stata_kernelpython -m stata_kernel.install</span><br></pre></td></tr></table></figure><p>安装输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simpleCollecting stata_kernel  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/10/5c/b0bebe1214f09d50439622ad812fb165b5a8caba1fd0f83d51b67ebc7e4f/stata_kernel-1.5.5-py3-none-any.whl (60kB)    100% |████████████████████████████████| 61kB 2.2MB/sCollecting requests&gt;=2.19.1 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)    100% |████████████████████████████████| 92kB 1.6MB/sCollecting packaging&gt;=17.1 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/89/d1/92e6df2e503a69df9faab187c684585f0136662c12bb1f36901d426f3fab/packaging-18.0-py2.py3-none-any.whlRequirement already satisfied: jupyter&gt;=1.0.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (1.0.0)Requirement already satisfied: pygments&gt;=2.2.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (2.2.0)Requirement already satisfied: jupyter-client&gt;=5.2.3 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (5.2.3)Requirement already satisfied: pywin32&gt;=223; platform_system == &quot;Windows&quot; in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (224)Collecting pexpect&gt;=4.6.0 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/89/e6/b5a1de8b0cc4e07ca1b305a4fcc3f9806025c1b651ea302646341222f88b/pexpect-4.6.0-py2.py3-none-any.whl (57kB)    100% |████████████████████████████████| 61kB 20.5MB/sCollecting pandas&gt;=0.23.4 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/0e/67/def5bfaf4d3324fdb89048889ec523c0903c5efab1a64c8dbe0ac8eec13c/pandas-0.23.4-cp36-cp36m-win_amd64.whl (7.7MB)    100% |████████████████████████████████| 7.7MB 34.2MB/sCollecting regex&gt;=2018.7.11 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/15/a5/cdb73862c207dcbb2dec5a4c64f850314910c55097dfa12cdfc533892502/regex-2018.08.29-cp36-none-win_amd64.whl (255kB)    100% |████████████████████████████████| 256kB 1.9MB/sRequirement already satisfied: ipykernel&gt;=4.8.2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from stata_kernel) (4.8.2)Collecting IPython&gt;=6.5.0 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/a0/27/29d66ed395a5c2c3a912332d446a54e2bc3277c36b0bbd22bc71623e0193/ipython-7.0.1-py3-none-any.whl (760kB)    100% |████████████████████████████████| 768kB 3.5MB/sCollecting beautifulsoup4&gt;=4.6.3 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/21/0a/47fdf541c97fd9b6a610cb5fd518175308a7cc60569962e776ac52420387/beautifulsoup4-4.6.3-py3-none-any.whl (90kB)    100% |████████████████████████████████| 92kB 452kB/sCollecting pillow&gt;=5.2.0 (from stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/bd/39/c76eaf781343162bdb1cf4854cb3bd5947a87ee44363e5acd6c48d69c4a1/Pillow-5.3.0-cp36-cp36m-win_amd64.whl (1.6MB)    100% |████████████████████████████████| 1.6MB 11.4MB/sRequirement already satisfied: certifi&gt;=2017.4.17 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from requests&gt;=2.19.1-&gt;stata_kernel) (2018.4.16)Requirement already satisfied: urllib3&lt;1.24,&gt;=1.21.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from requests&gt;=2.19.1-&gt;stata_kernel) (1.22)Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from requests&gt;=2.19.1-&gt;stata_kernel) (3.0.4)Requirement already satisfied: idna&lt;2.8,&gt;=2.5 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from requests&gt;=2.19.1-&gt;stata_kernel) (2.6)Requirement already satisfied: six in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from packaging&gt;=17.1-&gt;stata_kernel) (1.11.0)Requirement already satisfied: pyparsing&gt;=2.0.2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from packaging&gt;=17.1-&gt;stata_kernel) (2.2.0)Requirement already satisfied: notebook in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (5.5.0)Requirement already satisfied: ipywidgets in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (7.2.1)Requirement already satisfied: jupyter-console in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (5.2.0)Requirement already satisfied: nbconvert in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (5.3.1)Requirement already satisfied: qtconsole in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter&gt;=1.0.0-&gt;stata_kernel) (4.3.1)Requirement already satisfied: python-dateutil&gt;=2.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (2.7.3)Requirement already satisfied: pyzmq&gt;=13 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (17.0.0)Requirement already satisfied: traitlets in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (4.3.2)Requirement already satisfied: jupyter-core in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (4.4.0)Requirement already satisfied: tornado&gt;=4.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jupyter-client&gt;=5.2.3-&gt;stata_kernel) (5.0.2)Collecting ptyprocess&gt;=0.5 (from pexpect&gt;=4.6.0-&gt;stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whlRequirement already satisfied: pytz&gt;=2011k in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from pandas&gt;=0.23.4-&gt;stata_kernel) (2018.4)Requirement already satisfied: numpy&gt;=1.9.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from pandas&gt;=0.23.4-&gt;stata_kernel) (1.14.1)Requirement already satisfied: colorama; sys_platform == &quot;win32&quot; in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.3.9)Requirement already satisfied: decorator in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (4.3.0)Requirement already satisfied: pickleshare in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.7.4)Requirement already satisfied: simplegeneric&gt;0.8 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.8.1)Requirement already satisfied: setuptools&gt;=18.5 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (28.8.0)Requirement already satisfied: jedi&gt;=0.10 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.12.0)Collecting prompt-toolkit&lt;2.1.0,&gt;=2.0.0 (from IPython&gt;=6.5.0-&gt;stata_kernel)  Downloading https://mirrors.ustc.edu.cn/pypi/web/packages/e5/c5/f1ee6698bdcf615f171a77e81ca70293b16a6d82285f1760b388b4348263/prompt_toolkit-2.0.5-py3-none-any.whl (334kB)    100% |████████████████████████████████| 337kB 12.8MB/sRequirement already satisfied: backcall in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from IPython&gt;=6.5.0-&gt;stata_kernel) (0.1.0)Requirement already satisfied: Send2Trash in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (1.5.0)Requirement already satisfied: terminado&gt;=0.8.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.8.1)Requirement already satisfied: jinja2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (2.10)Requirement already satisfied: nbformat in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (4.4.0)Requirement already satisfied: ipython-genutils in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.2.0)Requirement already satisfied: widgetsnbextension~=3.2.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from ipywidgets-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (3.2.1)Requirement already satisfied: mistune&gt;=0.7.4 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.8.3)Requirement already satisfied: bleach in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (1.5.0)Requirement already satisfied: entrypoints&gt;=0.2.2 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.2.3)Requirement already satisfied: testpath in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.3.1)Requirement already satisfied: pandocfilters&gt;=1.4.1 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (1.4.2)Requirement already satisfied: parso&gt;=0.2.0 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jedi&gt;=0.10-&gt;IPython&gt;=6.5.0-&gt;stata_kernel) (0.2.0)Requirement already satisfied: wcwidth in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from prompt-toolkit&lt;2.1.0,&gt;=2.0.0-&gt;IPython&gt;=6.5.0-&gt;stata_kernel) (0.1.7)Requirement already satisfied: pywinpty&gt;=0.5; os_name == &quot;nt&quot; in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from terminado&gt;=0.8.1-&gt;notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.5.1)Requirement already satisfied: MarkupSafe&gt;=0.23 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from jinja2-&gt;notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (1.0)Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from nbformat-&gt;notebook-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (2.6.0)Requirement already satisfied: html5lib!=0.9999,!=0.99999,&lt;0.99999999,&gt;=0.999 in d:\mysites\deeplearning.ai-master\.env\lib\site-packages (from bleach-&gt;nbconvert-&gt;jupyter&gt;=1.0.0-&gt;stata_kernel) (0.9999999)spacy 2.0.11 has requirement regex==2017.4.5, but you&apos;ll have regex 2018.8.29 which is incompatible.jupyter-console 5.2.0 has requirement prompt-toolkit&lt;2.0.0,&gt;=1.0.0, but you&apos;ll have prompt-toolkit 2.0.5 which is incompatible.Installing collected packages: requests, packaging, ptyprocess, pexpect, pandas, regex, prompt-toolkit, IPython, beautifulsoup4, pillow, stata-kernel  Found existing installation: requests 2.18.4    Uninstalling requests-2.18.4:      Successfully uninstalled requests-2.18.4  Found existing installation: pandas 0.23.0    Uninstalling pandas-0.23.0:      Successfully uninstalled pandas-0.23.0  Found existing installation: regex 2017.4.5    Uninstalling regex-2017.4.5:      Successfully uninstalled regex-2017.4.5  Found existing installation: prompt-toolkit 1.0.15    Uninstalling prompt-toolkit-1.0.15:      Successfully uninstalled prompt-toolkit-1.0.15  Found existing installation: ipython 6.4.0    Uninstalling ipython-6.4.0:      Successfully uninstalled ipython-6.4.0  Found existing installation: Pillow 5.1.0    Uninstalling Pillow-5.1.0:      Successfully uninstalled Pillow-5.1.0Successfully installed IPython-7.0.1 beautifulsoup4-4.6.3 packaging-18.0 pandas-0.23.4 pexpect-4.6.0 pillow-5.3.0 prompt-toolkit-2.0.5 ptyprocess-0.6.0 regex-2018.8.29 requests-2.19.1 stata-kernel-1.5.5You are using pip version 18.0, however version 18.1 is available.You should consider upgrading via the &apos;python -m pip install --upgrade pip&apos; command.</span><br></pre></td></tr></table></figure><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><p>打开你的notebook:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure><p>然后, 创建一个statanotebook:</p><p><img src="https://mlln.cn/2018/11/01/%E5%AE%89%E8%A3%85stata%E5%B9%B6%E5%9C%A8jupyter-notebook%E4%B8%AD%E8%B0%83%E7%94%A8/stata.png" alt="img"></p><p>最后, 你就可以在cell中写stata命令了:</p><p><img src="https://kylebarron.github.io/stata_kernel/img/starting_jupyter_notebook.gif" alt="img"></p><ul><li>在新建的notebook中，通过下图可以初步判断是否关联成功。</li></ul><p>  <img src="https:////upload-images.jianshu.io/upload_images/17516282-c0a1880ab695006c.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200" alt="img"></p><ul><li>在命令行输入stata命令，并点击<code>运行</code>执行。如果关联成功，则会在命令的下方显示stata结果窗口的结果。具体如下图所示：</li></ul><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">dis</span> 1+3</span><br><span class="line"><span class="keyword">sysuse</span> auto,<span class="keyword">clear</span></span><br><span class="line"><span class="keyword">reg</span> price weight</span><br><span class="line"><span class="keyword">scatter</span> price weight</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/17516282-b566cbff21ac6eaf.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0
      
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="jupyter" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/jupyter/"/>
    
    
      <category term="SAS" scheme="http://yuanquanquan.top/tags/SAS/"/>
    
  </entry>
  
  <entry>
    <title>saspy——在jupyter里写sas代码</title>
    <link href="http://yuanquanquan.top/2019/2019090521/"/>
    <id>http://yuanquanquan.top/2019/2019090521/</id>
    <published>2019-11-24T04:59:15.000Z</published>
    <updated>2019-11-24T11:22:49.047Z</updated>
    
    <content type="html"><![CDATA[<p><img src="data:image/jpeg;base64,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" alt="img"></p><p>saspy, 即是一个连接sas服务器和Python解释器的一个API, 适合在jupyter Notebook里面编辑代码，如果你安装了Anaconda，即是安装了Jupyter Notebook。安装saspy API:</p><p>（1）打开Anaconda Prompt环境：pip install saspy</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131019.png" alt></p><p>（2）复制一个sascfg.py, 并重命名为sascfg_personal.py</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131030.png" alt></p><p>（3）改sascfg_personal.py里面的参数：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124135438.png" alt></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131038.png" alt></p><p>改过之后的：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131044.png" alt></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124135506.png" alt></p><p>到此saspy的配置ok了。</p><p>（4）还有一个PATH环境变量需要配置：</p><p>C:\Program Files\SASHome\SASFoundation\9.4\core\sasext</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124135821.png" alt></p><p>（5）安装sas内核，sas kernel, 回到Anaconda Prompt环境：</p><p>命令：</p><p>pip install sas_kernel<br>conda install -c conda-forge saspy<br>jupyter kernelspec list</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124135510.png" alt></p><p>看到这个，说明已经安装好了，接下来体验一下。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131049.jpg" alt></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131056.png" alt></p><p>我们先来进入Python3试试：</p><p>在sas里面成为数据集，在Python里面成为dataframe数据框，主要以pandas标准库来处理数据。由sasdata转化为Python的数据框, 然后后面就是纯Python语言处理了。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131116.jpg" alt></p><p>试试Python作图：</p><p><strong>柱状图：</strong></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.io/master/css/images/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131127.jpg" alt></p><p><strong>热力图:</strong></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131131.jpg" alt></p><p>甚至还可以画化学药物结构：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131135.png" alt></p><p>还可以批量画化学药物结构：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131139.jpg" alt></p><p>再切换至SAS服务器内核：</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131143.jpg" alt></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131151.png" alt></p><p>比如用sas<strong>作图</strong></p><p><strong>生存分析曲线</strong></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131154.png" alt></p><p><strong>比如森林图：</strong></p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/2017/R/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191124131157.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0
      
    
    </summary>
    
      <category term="日常技术" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/"/>
    
      <category term="jupyter" scheme="http://yuanquanquan.top/categories/%E6%97%A5%E5%B8%B8%E6%8A%80%E6%9C%AF/jupyter/"/>
    
    
      <category term="SAS" scheme="http://yuanquanquan.top/tags/SAS/"/>
    
  </entry>
  
  <entry>
    <title>十问 | 沉思录及其他</title>
    <link href="http://yuanquanquan.top/2019/2019091122/"/>
    <id>http://yuanquanquan.top/2019/2019091122/</id>
    <published>2019-11-22T10:35:03.000Z</published>
    <updated>2019-11-22T10:49:24.082Z</updated>
    
    <content type="html"><![CDATA[<p>​     终于完了这本霍金沉思录，也是霍金先生的遗作，这位伟大的思想家留给世界的最后的礼物。作为多年来的霍金粉丝，读这部作品是酣畅淋漓，但也不乏惆怅满腹。毕竟，我们不会再有机会读到先生的作品了。</p><p><img src="https://github.com/Bazingaliu/Bazingaliu.github.ios/blob/master/blog/ml-data/2018/R/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191122184312.jpg?raw=true" alt></p><p>​      史蒂芬 ·  霍金在2018年3月14日逝世，这天恰好是爱因斯坦的生日，霍金出生于1942年1月8日，又恰好是伽利略逝世三百周年，很多人都在感慨历史的惊人巧合；而他自己，在和脊髓侧索硬化症搏斗了55年之后离开了这蓝色的星球，结束了传奇的一生，飞向那浩渺的星空。</p><p>​      爱因斯坦曾说，“If you can’t explain it simply, you don’t really understand  it.”-  即如果你不能简单地解释一件事，那么你就没有真正理解那个东西。从这个角度说，霍金先生是真正理解他研究的宇宙的人，因为他可以用简单明了生动的方式来阐述物理，但很多职业物理学家缺乏这样的能力。这也是为什么鲜有职业物理学家涉足科普，因为实际上用文字去表达本应用数学来表达的东西本身就是一个巨大的挑战，除非你真正理解那件事并且恰好是个充满激情与想象力的思想家。显然，霍金先生正是这样的人。否则在历史上也不会有时间简史的巨大成功。</p><p>​        “十问”中的文字除了严密的科学思维和物理学家一本正经做科普的决心，还有从时间简史以来一以贯之的霍金式幽默，比如说吐槽语音合成器给他带来的美国口音，吐槽英国脱欧，又比如调侃幼年的自己曾经在二战德国V2导弹的爆炸范围内幸免于难，或者是坦率的回忆起中学同学的嘲笑：“史蒂芬·霍金将不会有所作为。”</p><p>​    当然，也有幸福与快乐的流露，比如在夕阳下看着孩子们在草坪上玩耍，又比如在国际空间站体验零重力的纯粹的快乐，那是真正摆脱轮椅束缚的自由时刻。</p><p>​       所以很明显，这本书和以前的作品相比更多的是霍金本人对于个人生命轨迹的回顾和对人类命运的关怀；对于家人和孩子们的爱，特别是对于第一任妻子简·怀尔德多年无私照料与付出的谢意；以及他自己经年累月与脊髓侧索硬化症的斗争、妥协与学会共存的心路历程；还有，不可不提的是，诸多重要物理学贡献，如黑洞熵理论、宇宙奇点、霍金辐射、包括他自己认为的最重要成就  - 宇宙无边界理论的建立过程中 - 那些无数的筚路蓝缕和灵光一现的时刻。</p><p>​      所有这些特质 -  残疾的天才物理学家、超过职业范围的好奇心、对公共事务的关心与表达，从哪个维度上说，都超越了一个物理学家的职业关切，而让霍金成为一个颇具争议性的人物。很少有人质疑他作为一个物理学家的杰出，但我相信，反对他的有关社会和外星人观点的人和赞同他的人一样多。但恐怕每个人都必须承认他热切的好奇心和探索欲，以及深思熟虑的能力，因为我们不得不惊讶于他对那些超过自己专业领域问题的见解仍然是发人深省的。</p><p>​      霍金喜欢问那些大问题，并且愿意去不懈探索它们的答案，正如这本书所展示的十个大问题。提出它们的意义不是去解决这些问题，事实上这些问题甚至我们都不知道是否是可以被解决的，遑论完成的具体步骤。但霍金对它们的思索却给了我们方向和可能性，从本质上说，他有着一种朴素的世界观，既然物理确实描述了这个世界，那么它一定包含着回答这些大问题所需要的信息，即使只是间接的提示。他清楚地知道，长期来说，我们的未来取决于我们对自然的理解程度。显然，他走在正确的道路上。并且正是由无数的霍金先生这样的巨擘不断地提出大问题，我们才越来越深入地认识这个陌生的世界，从而指引我们的生存，发展，寻得与自然的共存之道。</p><p>​       历史不会撒谎，这条道路早已显示出它的轮廓。正是因为对牛顿力学和热力学的掌握，才迎来了蒸汽机和内燃机的时代；正是因为电磁学的发展，才迎来了电力时代和信息时代；而计算机技术和量子技术的发展将带领我们进入新的未知时代，我们的计算能力将获得巨大的提升，我们将以前所未有的精度和速度去研究、模拟这个星球上的极其复杂的重要问题，比如环球大气运动、洋流、还有我们自己的细胞的运作、社会的演化等等。我不是盲目的技术乐观主义者，但我相信，这些时代的开创至少多养活了世界上大多数的人口，极大减少了来自环境的生存压力，虽然它们也带来了特定的问题，但相比于只有极少数高大强壮的人才能躲过野兽追击并存活下来的蛮荒年代，现代的依赖科学和技术维持的社会系统要宽容的多。我们可以依靠自己的思想获得生存的机会，我们不需要那样的身材去生存（虽然那样的人仍然更受异性的欢迎），生命的意义是多元化的，就像霍金先生这样只有眼和手指能动的人，同样对整个人类做出了大的贡献。</p><p>​      事实是，有很多身体残疾的杰出的人，这种东西原本不应该成为评判的一部分标准。但即便如此，你仍然无法将霍金先生的巨大贡献同他所忍受的经年之病痛割离开来。五十五年同病魔的斗争，同时还取得了正常人都难以望其项背的成就，真的是一件叹为观止的事。那个卧在轮椅上，依靠电脑合成发声的智者形象，激励了一代又一代崇尚真理的人努力奋斗，这是一种额外的津贴，或许比他的工作所产生的直接贡献更有意义。</p><p>​      这个额外的津贴，就像他在21岁被诊断患有脊髓侧索硬化症并被告知余日不多时他说的那样：“从那之后，一切都成为了额外津贴”。“我对生活的期望降到了最低，我抓紧所有的时间去做那些最要紧的事”。苹果的开创者乔布斯说，不要去患得患失，永远去做你想做的最重要的事，就好像生命的最后一天。对于霍金先生而言，这个最后一天，推迟了五十五年才来到，而他也成为了那个“不会有所作为的人”的相反面。</p><p>​       在我看来，霍金是绝世聪明的科学家没错，但绝不是同时代最聪明的，正如杨振宁先生常谈论霍金为什么拿不到诺贝尔奖。但霍金绝对是有着最好的科学直觉与远见的人之一，用英语来说，他是一个visionary的人。他晓得哪些东西对于认识世界是真正重要的，而不会囿于旁枝末节。这是一种惊人的能力，这恰恰也是爱因斯坦相比他同时代的智者们更高明的地方。再次引用杨先生的话，这是一种既能看到全景（big   picture）,又能注意到细节的微妙之处的洞察力。而霍金先生就具备这样的能力。或许他只是不够幸运，正年轻事业启航就被命运禁锢到轮椅之上；而或许他已经足够幸运，束缚于轮椅之上还能有着莎士比亚戏剧名言-“即使身处果壳之中，我也是无限空间之王”的乐观。此“王”不是统治压迫和权威的王，而是彻悟的自由。最后他能长眠于牛顿和达尔文的墓碑之间，委实也是世界对他的最高的认可了罢，诺奖之于此，也显得不那么重要了。</p><p>​       于我个人而言，霍金的人生，正如上述文字，启迪太多，我也如千千万万被他的故事感动的人们一样，虔诚地瞻仰与阅读他留给世界的回音。这回音很可能仍会在人类社会中激荡多年，因为它杰出地代表着人类最重要的特质  -  对真理的纯粹探索与理性精神。但最震撼我的，却是霍金在其博士论文封面上写下的这句话，那时的他已经逐渐无法握稳一支笔，歪歪扭扭的字体，却无比感动。完全能够想象他艰难地书写着这几个简单的单词的样子，坚毅而灵性。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/ml-data/2018/R/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191122184812.jpg" alt></p><p>This dissertation is my original work.</p><p>—- S. W. Hawking</p><p>这篇博士论文是我的原创性工作。</p><p>—-S. W. 霍金</p><p>​    其实，科学家能做的最酷的一件事，莫过于，说出这样的话了。因为每一个深刻而原创的理论都是真理在灵童耳畔的轻吟。</p><p>​       我有时候也在想，究竟是什么在支撑着一个有着残缺的躯体，但心灵却异常美丽而完整的灵魂不断地坚持探索下去。难道仅仅是因为对物理学的热爱么？可能答案不止于此。在2014年上映的电影《万物理论》中，“小雀斑”埃迪·雷德梅恩扮演的霍金本尊可能已经回答了这个问题。正是在那个他本人在书中调侃的美国口音的合成声音里，他说：</p><p>There is no boundary in human endeavors<br>However bad life seems<br>Where there is life<br>There is hope</p><p>人类的探索是没有边界的<br>无论生活看起来多糟<br>哪里有生命<br>哪里就有希望</p><p>​     我想这就是问题的答案吧。是电影的高潮，也是霍金先生留给我们最大的启示。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/ml-data/2018/R/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191122184323.jpg" alt></p><p>​     埃迪·雷德梅恩也被邀请给这本十问 | 霍金沉思录作序，在序言的末尾，他引用了奥巴马的一句话：我希望史蒂芬在灿烂的星空中玩得开心。这里快要结束这些零散文字时，我想这句话也正是我内心涌动的音符。</p><p>​     事实上，对于霍金先生而言，是否有我这样一个来自中国的粉丝的欣赏是完全没有意义的。但毕竟像我这样的不够高大强壮无法躲避野兽追击的弱者居然能够在这里欣赏这样美丽的灵魂，并且决定在太阳升起之前依然保持对明天的信心，不能不说是一种奇迹。</p><p>​       最后，也是最重要的，致敬史蒂芬，一个叹为观止的人，以及他传奇的一生。</p><p><img src="https://raw.githubusercontent.com/Bazingaliu/Bazingaliu.github.ios/master/blog/ml-data/2018/R/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20191122184337.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​     终于完了这本霍金沉思录，也是霍金先生的遗作，这位伟大的思想家留给世界的最后的礼物。作为多年来的霍金粉丝，读这部作品是酣畅淋漓，但也不乏惆怅满腹。毕竟，我们不会再有机会读到先生的作品了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/Ba
      
    
    </summary>
    
      <category term="生活" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="随笔" scheme="http://yuanquanquan.top/categories/%E7%94%9F%E6%B4%BB/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活" scheme="http://yuanquanquan.top/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>Michael Nielsen对weight初始化的解释</title>
    <link href="http://yuanquanquan.top/2019/20190911215/"/>
    <id>http://yuanquanquan.top/2019/20190911215/</id>
    <published>2019-11-21T02:50:08.000Z</published>
    <updated>2019-11-21T03:35:18.015Z</updated>
    
    <content type="html"><![CDATA[<p>当我们创建一个神经网络时，我们必须选择weights和biases的初始化方式。我们目前的方式是让weights和biases都使用独立的高斯随机初始化，标准化为均值0，标准差1。这种方式工作得挺好，也挺特别，如果我们回头审视下它，也许能找到设置weights和biase更好的方式，帮助我们的神经网络学得更快。</p><p>的确我们可以做的比标准化高斯初始化更好。为了说明原因，假设我们有1000个输入神经元的神经网络。并且我们已经用标准化高斯初始化了连接第一隐层的weights。现在我们只关注输入神经元层和第一隐层间的weights，忽略掉其他部分：</p><p><img src="https://cugtyt.github.io/blog/ml-data/2018/R/weight-initialization1.png" alt></p><p>为了简化，我们假设训练的输入x，x的一半输入神经元设为1，另一部分为0。下面的论证是普遍适用的，但是你可从这个特殊示例中明白大意。让我们假设输入到第一隐层的weights和<img src="http://latex.codecogs.com/gif.latex?%5Cinline%20z%20%3D%20%5Csum_j%20w_j%20x_j&plus;b" alt>。和中的500项消失了，因为对应的输入<img src="http://latex.codecogs.com/gif.latex?%5Cinline%20x_j" alt>是0。因此z相当于是对总共的501项标准化的高斯随机数求和，包括500项weights和1项bias。z本身的分布也是均值为1，标准差为<img src="http://latex.codecogs.com/gif.latex?%5Cinline%20%5Csqrt%7B501%7D%20%5Capprox%2022.4" alt>的高斯分布。也就是说z是非常宽的高斯分布，没有一点峰：</p><p><img src="https://cugtyt.github.io/blog/ml-data/2018/R/weight-initialization2.png" alt></p><p>我们可以从图中看出来，<code>|z|</code>会非常大，无论z&gt;&gt;1还是z&lt;&lt;-1。如果是这样的话，输出隐层神经元输出σ(z)会非常接近于1或0。这意味着隐层神经元会饱和。这种情况发生时，weights变化会非常小，在隐层的激活部分更是十分小。反过来，隐层这种非常小的改变基本上不会改变网络的其他部分了，并且损失函数的改变也十分微小。结果就是，当我们使用梯度下降算法时这些weights会学习非常慢。不幸的是，对于饱和的隐层神经元，我们改变损失函数并没有帮助。</p><p>我已经讨论了第一隐层的输入weights。当然，后面的隐层也会是类似的：如果后面的隐层使用标准化高斯初始化，那么激活部分通常会非常接近0或1，学习依旧十分慢。</p><p>有没有什么方法选择更好的初始化方法，让我们消除这种饱和，也避免学习缓慢的问题？假设我们有<img src="http://latex.codecogs.com/gif.latex?%5Cinline%20n_i_n" alt>个输入weights，然后我们使用均值为0，标准差为<img src="http://latex.codecogs.com/gif.latex?%5Cinline%201/%5Csqrt%7Bn_%7B%5Crm%20in%7D%7D" alt>的高斯随机变量初始化weights，也就是我们把高斯分布向下挤压，让它不太容易使神经元饱和。我们继续让bias使用均值为0标准差为1的高斯分布，原因我一会再说。这种选择下，weights的和<img src="http://latex.codecogs.com/gif.latex?%5Cinline%20z%20%3D%20%5Csum_j%20w_j%20x_j&plus;b" alt>再一次是均值为0的高斯随机变量的和，但是相比之前出现更加尖锐的峰。假设，就像我们刚才做的，500个输入为0,500个输入为1，很容易看出z是均值为0，标准差为<img src="http://latex.codecogs.com/gif.latex?%5Cinline%20%5Csqrt%7B3/2%7D%20%3D%201.22%5Cldots" alt>的高斯分布。比之前有一个更加尖锐的峰，即使相比之前我已经重新设置了纵轴，下图也低估了这种情形：</p><p><img src="https://cugtyt.github.io/blog/ml-data/2018/R/weight-initialization3.png" alt></p><p>这种神经元就很难饱和了，也不容易出现学习缓慢的问题。</p><p>上面我说继续像原来那样初始化biases，使用均值为0，标准差为1的高斯分布。这没有太大问题，因为这不太会让神经元饱和。事实上，我们如何初始化biases对避免饱和问题没有太大关系。一些人初始化biases为0，依靠梯度下降学习合适的biases。但是这并没有太大区别，我们依旧使用原先的方法。</p><p>对比前后（30个隐层神经元）（略去一部分不影响理解的内容，使用MNIST数据集）：</p><p><img src="https://cugtyt.github.io/blog/ml-data/2018/R/weight-initialization4.png" alt></p><p>两种情况下，我们都超过了96%的准确率。两种情况的准确率基本一样。但是新的初始化方法更快。在旧方法第一轮结束时，分类准确率只有87%，而新方法已经达到93%。新的方法让我们有更好的机制，更快地达到好的结果。100个隐层神经元也是相同的情形：</p><p><img src="https://cugtyt.github.io/blog/ml-data/2018/R/weight-initialization4.png" alt></p><p>这种情况下，两个曲线不是特别相近。但是，实验表明多训练几个周期（没有展示出来）准确率基本就一样了。在这些实验的基础上，看来除了加快速度没有其他方面的改善。但是后续我们会看到<img src="http://latex.codecogs.com/gif.latex?%5Cinline%201/%5Csqrt%7Bn_%7B%5Crm%20in%7D%7D" alt>的初始化方式会在长时间训练的神经网络上表现得更好。因此不仅是速度的提升，在最终性能上也会提升。</p><p><img src="http://latex.codecogs.com/gif.latex?%5Cinline%201/%5Csqrt%7Bn_%7B%5Crm%20in%7D%7D" alt>的初始化方式帮助神经网络学习得更好。提出的方法有很多，一些事建立在这个基本构想上面的。我不会展开，因为对于我们的目的来说已经够了。</p><hr><p>其实这种初始化方式就是Xavier初始化，虽然没有明确提出，如果乘以sqrt(2)就是He初始化了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当我们创建一个神经网络时，我们必须选择weights和biases的初始化方式。我们目前的方式是让weights和biases都使用独立的高斯随机初始化，标准化为均值0，标准差1。这种方式工作得挺好，也挺特别，如果我们回头审视下它，也许能找到设置weights和biase更
      
    
    </summary>
    
      <category term="AI" scheme="http://yuanquanquan.top/categories/AI/"/>
    
      <category term="Machine Learning &amp; Data" scheme="http://yuanquanquan.top/categories/AI/Machine-Learning-Data/"/>
    
    
      <category term="Machine Learning &amp; Data" scheme="http://yuanquanquan.top/tags/Machine-Learning-Data/"/>
    
  </entry>
  
  <entry>
    <title>send boxes to alice(easy version)</title>
    <link href="http://yuanquanquan.top/2019/20190911214/"/>
    <id>http://yuanquanquan.top/2019/20190911214/</id>
    <published>2019-11-21T02:08:14.000Z</published>
    <updated>2019-11-21T02:25:29.284Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://sta.codeforces.com/s/91655/images/codeforces-logo-with-telegram.png" alt="img"></p><p><strong>题意</strong></p><p>有$n(1\leq n\leq 10^5)$个盒子，每个盒子有$a_i(0\leq a_i \leq 1)$个糖果，你每一次可以将第$i$个盒子里的糖果放到第$i-1$或$i+1$个盒子中（如果盒子存在）。最后要使每个盒子的糖果数量都整除$k(k&gt;1)$（注意盒子可以为空），问最小操作数。</p><p><strong>分析</strong></p><p>$(1)$因为糖果是类似于平铺的形式，堆叠时，我们可以发现所有存在糖果的盒子中数量均为$k$。若存在一个盒子中有$2*k$个糖果，在平铺到堆叠的过程中，将另外$k$个糖果分在更近的盒子能得到更小的答案。</p><p>$(2)$设糖果总数为$cnt$，所有存在糖果的盒子数量均为$k$，我们又可以发现，最小的操作是将$1$~$k$、$k+1$~$2k$、……、$ik+1$~$(i+1)*k$放在一起，即将相邻的$k$个放在一堆。</p><p>$(3)$对于某$k$个糖果，需要找到一个盒子，这个盒子到这$k$个糖果的距离最小(<del>kNN算法</del>)。我们将糖果看成数轴上的点，运用高一的绝对值知识（我忘了，我向高中数学老师谢罪）。</p><ul><li>若$k$为奇数，则将该盒子设置为最中间糖果所在的盒子</li><li>若$k$为偶数，则将该盒子设置为最中间两个糖果中任意一个所在的盒子</li></ul><p>即对于$ik+1$~$(i+1)*k$来说，第$k-i/2$个盒子，设其坐标为$ave$。</p><p>$(4)$为降低时间复杂度，我们采取前缀的思想，$sum[i]$表示坐标$i$之前的糖果的坐标总和（没糖果的盒子不加），$num[i]$表示坐标$i$之前有多少糖果。</p><p>$(5)$枚举可以被$cnt$整除的$k$，模拟$(2)$的过程，设$first$为第$ ik+1$个糖果的坐标，$last$为第$(i+1)k$个糖果的坐标，那么每个循环都得加上$(num[ave] - num[first - 1])*ave-(sum[ave] - sum[first - 1])+(sum[last] - sum[ave])-(num[last] - num[ave])ave$，意思为$ave$之前的操作次数加上$ave$之后的操作次数，最后取最小值</p><p>$(6)$记得开$long\ long$，$INF$也记得开大一点。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> GCC optimize(3, <span class="meta-string">"Ofast"</span>, <span class="meta-string">"inline"</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> start ios::sync_with_stdio(false);cin.tie(0);cout.tie(0);</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ll long long</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LL long long</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> pii pair<span class="meta-string">&lt;int,int&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> int ll</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = (ll) <span class="number">1e5</span> + <span class="number">5</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> mod = <span class="number">1000000007</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> inf = <span class="number">0x3f3f3f3f3f3f3f3f</span>;</span><br><span class="line"><span class="keyword">int</span> a[maxn];</span><br><span class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> sum[maxn];</span><br><span class="line"><span class="keyword">int</span> num[maxn];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">signed</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    start;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">        <span class="keyword">int</span> x;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; x;</span><br><span class="line">        num[i] = num[i - <span class="number">1</span>] + x;<span class="comment">//前缀数量</span></span><br><span class="line">        <span class="keyword">if</span> (x) &#123;</span><br><span class="line">            a[++cnt] = i;</span><br><span class="line">            sum[i] = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i)<span class="comment">//前缀坐标和</span></span><br><span class="line">        sum[i] += sum[i - <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> ans = inf;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= cnt; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (cnt % i == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> tmp = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> k = i; k &lt;= cnt; k += i) &#123;<span class="comment">//k为最后的糖果</span></span><br><span class="line">                <span class="keyword">int</span> first = a[k - i + <span class="number">1</span>];</span><br><span class="line">                <span class="keyword">int</span> last = a[k];</span><br><span class="line">                <span class="keyword">int</span> ave = a[k - i / <span class="number">2</span>];</span><br><span class="line">                <span class="keyword">int</span> num1 = num[ave] - num[first - <span class="number">1</span>];</span><br><span class="line">                <span class="keyword">int</span> num2 = num[last] - num[ave];</span><br><span class="line">                <span class="keyword">int</span> tot1 = sum[ave] - sum[first - <span class="number">1</span>];</span><br><span class="line">                <span class="keyword">int</span> tot2 = sum[last] - sum[ave];</span><br><span class="line">                <span class="keyword">int</span> t = num1 * ave - tot1 + tot2 - num2 * ave;</span><br><span class="line">                tmp += t;</span><br><span class="line">            &#125;</span><br><span class="line">            ans = min(ans, tmp);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (ans == inf)</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; ans;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://sta.codeforces.com/s/91655/images/codeforces-logo-with-telegram.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题意&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有$n(1\
      
    
    </summary>
    
      <category term="算法" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="codeforces" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/codeforces/"/>
    
    
      <category term="算法" scheme="http://yuanquanquan.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Q-Learning Agents</title>
    <link href="http://yuanquanquan.top/2019/201907201118/"/>
    <id>http://yuanquanquan.top/2019/201907201118/</id>
    <published>2019-11-19T03:33:05.000Z</published>
    <updated>2019-11-19T01:33:43.779Z</updated>
    
    <content type="html"><![CDATA[<p><img src="data:image/png;base64,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" alt="“强化学习”的图片搜索结果"></p><p>主要内容来源于：论文以及教程（<a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course" target="_blank" rel="noopener">Thomas Simonini <em>Deep Reinforcement Learning Course with Tensorflow</em></a>， <a href="https://github.com/awjuliani/DeepRL-Agents" target="_blank" rel="noopener">Arthur Juliani <em>Simple Reinforcement Learning with Tensorflow series</em></a>），<a href="https://spinningup.openai.com/en/latest/index.html" target="_blank" rel="noopener">OpenAI Spinning Up in Deep RL</a></p><p><strong>Concepts</strong></p><h3 id="Policy-Gradient-methods"><a href="#Policy-Gradient-methods" class="headerlink" title="Policy Gradient methods"></a>Policy Gradient methods</h3><blockquote><p>which attempt to learn functions which directly map an observation to an action.</p><p><strong>observation -&gt; action</strong></p></blockquote><h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><blockquote><p>attempts to learn the value of being in a given state, and taking a specific action there.</p><p><strong>state, action -&gt; value</strong></p></blockquote><h3 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h3><blockquote><p>which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state.</p><p>$$ Q(s, a) = r + \gamma (\max (Q(s’, a’))) $$</p></blockquote><p>利用Bellman Equation可以实现Q-Table算法：</p><p><a href="https://gist.github.com/awjuliani/9024166ca08c489a60994e529484f7fe#file-q-table-learning-clean-ipynb" target="_blank" rel="noopener">代码来源</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Initialize table with all zeros</span></span><br><span class="line">Q = np.zeros([env.observation_space.n,env.action_space.n])</span><br><span class="line"><span class="comment"># Set learning parameters</span></span><br><span class="line">lr = <span class="number">.8</span></span><br><span class="line">y = <span class="number">.95</span></span><br><span class="line">num_episodes = <span class="number">2000</span></span><br><span class="line"><span class="comment">#create lists to contain total rewards and steps per episode</span></span><br><span class="line"><span class="comment">#jList = []</span></span><br><span class="line">rList = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    <span class="comment">#Reset environment and get first new observation</span></span><br><span class="line">    s = env.reset()</span><br><span class="line">    rAll = <span class="number">0</span></span><br><span class="line">    d = <span class="keyword">False</span></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    <span class="comment">#The Q-Table learning algorithm</span></span><br><span class="line">    <span class="keyword">while</span> j &lt; <span class="number">99</span>:</span><br><span class="line">        j+=<span class="number">1</span></span><br><span class="line">        <span class="comment">#Choose an action by greedily (with noise) picking from Q table</span></span><br><span class="line">        a = np.argmax(Q[s,:] + np.random.randn(<span class="number">1</span>,env.action_space.n)*(<span class="number">1.</span>/(i+<span class="number">1</span>)))</span><br><span class="line">        <span class="comment">#Get new state and reward from environment</span></span><br><span class="line">        s1,r,d,_ = env.step(a)</span><br><span class="line">        <span class="comment">#Update Q-Table with new knowledge</span></span><br><span class="line">        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])</span><br><span class="line">        rAll += r</span><br><span class="line">        s = s1</span><br><span class="line">        <span class="keyword">if</span> d == <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment">#jList.append(j)</span></span><br><span class="line">    rList.append(rAll)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Score over time: "</span> +  str(sum(rList)/num_episodes)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Final Q-Table Values"</span></span><br><span class="line"><span class="keyword">print</span> Q</span><br></pre></td></tr></table></figure><p>但是这种方法不具有扩展性，毕竟表格的容量有限。</p><h2 id="Q-Learning-with-Neural-Networks"><a href="#Q-Learning-with-Neural-Networks" class="headerlink" title="Q-Learning with Neural Networks"></a>Q-Learning with Neural Networks</h2><p><a href="https://gist.github.com/awjuliani/4d69edad4d0ed9a5884f3cdcf0ea0874#file-q-net-learning-clean-ipynb" target="_blank" rel="noopener">代码来源</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#----- Implementing the network itself -----------</span></span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="comment">#These lines establish the feed-forward part of the network used to choose actions</span></span><br><span class="line">inputs1 = tf.placeholder(shape=[<span class="number">1</span>,<span class="number">16</span>],dtype=tf.float32)</span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">16</span>,<span class="number">4</span>],<span class="number">0</span>,<span class="number">0.01</span>))</span><br><span class="line">Qout = tf.matmul(inputs1,W)</span><br><span class="line">predict = tf.argmax(Qout,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.</span></span><br><span class="line">nextQ = tf.placeholder(shape=[<span class="number">1</span>,<span class="number">4</span>],dtype=tf.float32)</span><br><span class="line">loss = tf.reduce_sum(tf.square(nextQ - Qout))</span><br><span class="line">trainer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line">updateModel = trainer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ----- Training the network -----</span></span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set learning parameters</span></span><br><span class="line">y = <span class="number">.99</span></span><br><span class="line">e = <span class="number">0.1</span></span><br><span class="line">num_episodes = <span class="number">2000</span></span><br><span class="line"><span class="comment">#create lists to contain total rewards and steps per episode</span></span><br><span class="line">jList = []</span><br><span class="line">rList = []</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">        <span class="comment">#Reset environment and get first new observation</span></span><br><span class="line">        s = env.reset()</span><br><span class="line">        rAll = <span class="number">0</span></span><br><span class="line">        d = <span class="keyword">False</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="comment">#The Q-Network</span></span><br><span class="line">        <span class="keyword">while</span> j &lt; <span class="number">99</span>:</span><br><span class="line">            j+=<span class="number">1</span></span><br><span class="line">            <span class="comment">#Choose an action by greedily (with e chance of random action) from the Q-network</span></span><br><span class="line">            a,allQ = sess.run([predict,Qout],feed_dict=&#123;inputs1:np.identity(<span class="number">16</span>)[s:s+<span class="number">1</span>]&#125;)</span><br><span class="line">            <span class="keyword">if</span> np.random.rand(<span class="number">1</span>) &lt; e:</span><br><span class="line">                a[<span class="number">0</span>] = env.action_space.sample()</span><br><span class="line">            <span class="comment">#Get new state and reward from environment</span></span><br><span class="line">            s1,r,d,_ = env.step(a[<span class="number">0</span>])</span><br><span class="line">            <span class="comment">#Obtain the Q' values by feeding the new state through our network</span></span><br><span class="line">            Q1 = sess.run(Qout,feed_dict=&#123;inputs1:np.identity(<span class="number">16</span>)[s1:s1+<span class="number">1</span>]&#125;)</span><br><span class="line">            <span class="comment">#Obtain maxQ' and set our target value for chosen action.</span></span><br><span class="line">            maxQ1 = np.max(Q1)</span><br><span class="line">            targetQ = allQ</span><br><span class="line">            targetQ[<span class="number">0</span>,a[<span class="number">0</span>]] = r + y*maxQ1</span><br><span class="line">            <span class="comment">#Train our network using target and predicted Q values</span></span><br><span class="line">            _,W1 = sess.run([updateModel,W],feed_dict=&#123;inputs1:np.identity(<span class="number">16</span>)[s:s+<span class="number">1</span>],nextQ:targetQ&#125;)</span><br><span class="line">            rAll += r</span><br><span class="line">            s = s1</span><br><span class="line">            <span class="keyword">if</span> d == <span class="keyword">True</span>:</span><br><span class="line">                <span class="comment">#Reduce chance of random action as we train the model.</span></span><br><span class="line">                e = <span class="number">1.</span>/((i/<span class="number">50</span>) + <span class="number">10</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        jList.append(j)</span><br><span class="line">        rList.append(rAll)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Percent of succesful episodes: "</span> + str(sum(rList)/num_episodes) + <span class="string">"%"</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXEAAACJCAMAAADt7/hWAAABrVBMVEX///8AAAClpaX///54eHhHR0fq6urNzc3//f/9//////zw8PBnZ
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="强化学习笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="强化学习" scheme="http://yuanquanquan.top/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY</title>
    <link href="http://yuanquanquan.top/2019/20190911191/"/>
    <id>http://yuanquanquan.top/2019/20190911191/</id>
    <published>2019-11-19T00:38:10.000Z</published>
    <updated>2019-11-28T08:09:09.181Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20190322155629388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==,size_16,color_FFFFFF,t_70" alt="img">这篇文章提出了基于权重连接重要性（saliency）的剪枝方法，能够在深度模型训练之前（初始化阶段），通过mini-batch的多次采样，决定不同权重连接的重要性，进而根据剪枝目标生成剪枝模板（prunning mask）、应用于稀疏剪枝，从而节省了相对耗时的剪枝-微调迭代周期。</p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>现有的剪枝方法是在迭代中，通过启发式剪枝策略或者增加超参数完成，削弱了可用性。我们展示了一个新方法，可以在训练前的初始化阶段一次完成剪枝。我们引入了基于连接敏感的显著性指标来定位当前任务中的重要连接。减少了预训练和剪枝策略的复杂性，对于不同的网络结构也比较鲁棒。在剪枝后，稀疏的网络按照常规训练。我们的方法可以获得极为稀疏的网络，同时准确率得到了保持。我们的方法证明了保留的连接与给定的任务相关。</p><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><p>由于剪枝是迭代优化的，现有的方法需要高昂的剪枝-再训练的循环，需要额外的超参数，实用性不好。</p><p>我们引入了显著性指标在训练前定位重要的连接，它是依赖于数据的。我们把不同尺度初始化下重要连接对损失函数的影响称为连接显著度。</p><p>我们的方法：</p><ul><li><em>Simplicity</em> 在训练前做一次即可，没有额外的参数。</li><li><em>Versatility</em> 由于显著性指标选择的是结构性重要连接，所以可以用于不同的网络结构上。</li><li><em>Interpretability</em> 我们的方法在一个小批量上就可以确定那些重要的连接。通过变化剪枝需要的小批量，我们的方法可以验证保留的连接的确是重要的。</li></ul><h2 id="2-RELATED-WORK"><a href="#2-RELATED-WORK" class="headerlink" title="2 RELATED WORK"></a>2 RELATED WORK</h2><p>【略】</p><h2 id="3-NEURAL-NETWORK-PRUNING"><a href="#3-NEURAL-NETWORK-PRUNING" class="headerlink" title="3 NEURAL NETWORK PRUNING"></a>3 NEURAL NETWORK PRUNING</h2><p>目标是找到一个稀疏的网络但是准确率要保留。</p><p>给定一个数据集$\mathcal{D}=\left{\left(\mathbf{x}<em>{i}, \mathbf{y}</em>{i}\right)\right}_{i=1}^{n}$，和一个稀疏级别$\kappa$（非0的权重），可以得到如下优化问题：</p><p>$$\min _{\mathbf{w}} L(\mathbf{w} ; \mathcal{D})=\min <em>{\mathbf{w}} \frac{1}{n} \sum</em>{i=1}^{n} \ell\left(\mathbf{w} ;\left(\mathbf{x}<em>{i}, \mathbf{y}</em>{i}\right)\right) \ s.t. \quad \mathbf{w} \in \mathbb{R}^{m}, \quad|\mathbf{w}|_{0} \leq \kappa \qquad (1)$$</p><p>$\ell(\cdot)$是损失函数，$\mathbf{W}$是参数，$m$是参数数量，$|\cdot|<em>{0}$是$L</em>{0}$正则。</p><p>传统的优化以上问题的方法是加入一个稀疏惩罚项，最近有人尝试使用随机版的映射梯度下降来解决上述受限优化问题。但是这些方法不如基于显著性的方法，因为需要大量的超参数调整。</p><p>另一方面，基于显著性的方法把上述问题看做选择性的移除荣誉参数。所以必须想到一个好的指标来定位冗余的连接。流行的指标包括权重的梯度，loss对于权重的Hessian：</p><p>$s_{j}=\left{\begin{array}{ll}{\left|w_{j}\right|}\qquad \text{for magnitude based} \ {\frac{w_{j}^{2} H_{j j}}{2}} {\text { or } \frac{w_{j}^{2}}{2 H_{j j}^{-1}} \qquad \text{for Hessian based} }\end{array}\right. \qquad (2)$</p><p>这里对于连接$j$而言，$\mathcal{S}<em>{j}$是显著性分数，$w</em>{j}$是权重，$H_{j j}$是Hessian矩阵的值，$\mathbf{H}=\partial^{2} L / \partial \mathbf{w}^{2} \in \mathbb{R}^{m \times m}$。Hessian矩阵通常不是对角化的也不是正定的，不适用于计算大型网络。</p><p>这些指标依赖于权重的大小范围，需要预训练，而且对结构选择敏感。例如不同的归一化层对权重的范围影响不同，而且剪枝和优化步骤交替很多次导致了高昂的剪枝-再训练循环。</p><p>我们设计了一个指标可以直接衡量与数据相关的连接重要性。</p><h2 id="4-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY"><a href="#4-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY" class="headerlink" title="4 SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY"></a>4 SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY</h2><p>我们希望能衡量每个连接的重要性（与权重无关），因此引入了一个指示变量$\mathbf{c} \in{0,1}^{m}$来表示参数$\mathbf{W}$的连接性。给定稀疏级别$\kappa$，公式1可以改写为：</p><p>$$\min _{\mathbf{c}, \mathbf{w}} L(\mathbf{c} \odot \mathbf{w} ; \mathcal{D})=\min <em>{\mathbf{c}, \mathbf{w}} \frac{1}{n} \sum</em>{i=1}^{n} \ell\left(\mathbf{c} \odot \mathbf{w} ;\left(\mathbf{x}<em>{i}, \mathbf{y}</em>{i}\right)\right) \ \text { s.t. } \quad \mathbf{w} \in \mathbb{R}^{m}, \mathbf{c} \in{0,1}^{m}, \quad|\mathbf{c}|_{0} \leq \kappa \qquad (3)$$</p><p>$\odot$表示Hadamard乘积。与公式1相比，我们加倍了网络要学习的参数数量，导致优化问题更加困难。但是由于我们区分了$\mathbf{W}$中要还是不要（$\mathbf{c}$控制），我们也许可以通过损失函数衡量连接的重要性。</p><p>例如，$C_{j}$表示连接$j$保留$c_{j}=1$，还是减去$c_{j}=0$。移除连接$j$的影响可以通过以下来衡量：</p><p>$$\Delta L_{j}(\mathbf{w} ; \mathcal{D})=L(\mathbf{1} \odot \mathbf{w} ; \mathcal{D})-L\left(\left(\mathbf{1}-\mathbf{e}_{j}\right) \odot \mathbf{w} ; \mathcal{D}\right) \qquad (4)$$</p><p>$\mathbf{e}_{j}$是指示向量的元素。</p><p>注意对每个$j \in{1 \ldots m}$计算$\Delta L_{j}$很昂贵，因为它需要m+1次前向传播。实际上，由于$\mathbf{C}$是二元的，$L$对于$\mathbf{C}$不是可微的，可以容易得到$\Delta L_{j}$尝试着去衡量损失函数对于连接$j$的影响。因此，放松$\mathbf{C}$的二元限制，$\Delta L_{j}$可以近似为$L$对于$c_j$的导数，记作$g_{j}(\mathbf{w} ; \mathcal{D})$。因此，连接$j$的影响可以写作：<br>$$<br>\Delta L_{j}(\mathbf{w} ; \mathcal{D}) \approx g_{j}(\mathbf{w} ; \mathcal{D})=\left.\frac{\partial L(\mathbf{c} \odot \mathbf{w} ; \mathcal{D})}{\partial c_{j}}\right|_{\mathbf{c}=1}=\lim <em>{\delta \rightarrow 0}\left.\frac{L(\mathbf{c} \odot \mathbf{w} ; \mathcal{D})-L\left(\left(\mathbf{c}-\delta \mathbf{e}</em>{j}\right) \odot \mathbf{w} ; \mathcal{D}\right)}{\delta}\right|_{\mathbf{c}=\mathbf{1}} \qquad (5)<br>$$</p><p>【TODO】</p><h2 id="6-DISCUSSION-AND-FUTURE-WORK"><a href="#6-DISCUSSION-AND-FUTURE-WORK" class="headerlink" title="6 DISCUSSION AND FUTURE WORK"></a>6 DISCUSSION AND FUTURE WORK</h2><p>SNIP方法简单可解释，可以在训练之前一次减去不相关的连接，可以在很多网络中使用无需修改。虽然SNIP产生了极其稀疏的模型，但是我们发现连接显著性衡量本身也很值得关注，因为它能在完全未训练的网络中定位出重要的连接。可以进一步探索。p</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20190322155629388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="论文笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="CVPR" scheme="http://yuanquanquan.top/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>凸集</title>
    <link href="http://yuanquanquan.top/2019/201911181/"/>
    <id>http://yuanquanquan.top/2019/201911181/</id>
    <published>2019-11-18T03:41:55.000Z</published>
    <updated>2019-11-18T08:45:46.538Z</updated>
    
    <content type="html"><![CDATA[<p><img src="data:image/jpeg;base64,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" alt="img"></p><h1 id="凸优化的笔记：序"><a href="#凸优化的笔记：序" class="headerlink" title="凸优化的笔记：序"></a>凸优化的笔记：序</h1><p>想学凸优化的伏笔应该是在大二的时候埋下的。那个时候啃着李航老师的《统计学习方法》，按书本里的先学了kNN和Decision Tree，感觉统计学习还怪有意思的，后来学到Naive Bayes时候开始有点智商捉急了，再后来的贝叶斯网络和SVM就已经开始对我幼小的心灵造成损害，看到拉格朗日对偶的推导那部分开始自闭，上网去搜，发现这是凸优化里面的理论——这是我第一次对这门学科产生印象。</p><p>本来研究炼丹术之后想划水的，奈何实验室的师兄太强了，有种不努力就会被社会抛弃的紧张感，所以想重新捡起《统计学习方法》看，争取把推导看懂吧……然后回忆起早年自闭的过程，想了想，算了，先去看看凸优化吧。</p><p>学习的参考教材是Stamford大学又帅又有魅力的Boyd教授的<strong>《Convex Optimation》</strong>（考虑到努力不给自己艰难的学习过程再下绊子，选择的是王书宁版的中译本），结合Boyd教授录制的授课视频与slides一起学习。笔记的篇章估计也会按书本的目录顺序来，除非我看到自闭打算删掉这些笔记装作一切都没发生过，否则我会努力把所有书本的重点都呈现到笔记中去。</p><p>另外，值得一提的是，现在深度学习打得火热，我也有看到有关统计学家与炼金术士之间因为神经网络非凸，效果又异常的好而相互看不起的说法出现，感觉还怪有意思的。可能有一天我会发现凸优化并没有我想象中那么好用，不过在学习过程中带来的痛并快乐的感觉确是实实在在的。不管怎么说，现在的我选择要开始这段旅程了。</p><p>是为序。</p><p>书本的<strong>第一章</strong>是绪论，没有讲太多实质性内容，合并到这里进行叙述：主要介绍了数学优化问题的通用形式，随后讲了最小二乘和线性规划的内容，并说明了它们是凸问题中比较典型的问题，而且最小二乘与线性规划问题是很容易识别的，然而凸问题很多时候非常难识别（就是说，有时候你觉得它和凸问题一点关系没有，但是稍微做个简单变换，忽然就发现变成凸问题了。书里面没有给出直接例子，但是我记得看视频的时候Boyd教授是讲过一个例子的）。最后简单总结：课程的主要目的就是为了教大家如何辨识一个凸问题并求解它——当然为了达到这个目的，首先会讲相当一部分枯燥的概念。</p><p>下面正式开始第二章：凸集的内容。也就是Boyd教授所说的枯燥的概念的部分。</p><p>除了基本的概念以外，我会努力阐述我的理解，不照着书本上面直接抄的（后者是我读不懂的时候会采取的让自己觉得自己学会了的方法）——<em>但是为了不产生误导，所有我自己的理解我都会用斜体表示。</em></p><h2 id="仿射集合和凸集"><a href="#仿射集合和凸集" class="headerlink" title="仿射集合和凸集"></a>仿射集合和凸集</h2><p><strong>直线</strong></p><blockquote><p>设x1≠x2x1≠x2是RnRn空间中的两个点，那么具有下列形式的点会构成一条穿过x1,x2x1,x2两点的直线</p><p>y = \theta x _ { 1 } + ( 1 - \theta ) x _ { 2 } , \quad \theta \in R</p></blockquote><p><strong>仿射集合</strong></p><blockquote><p>如果集合中任意两个点所构成的直线中的所有点都在这个集合中，就认为这个集合是仿射的。用数学语言表达就是，一个集合CC是仿射的当且仅当：对于∀x1,x2∈C∀x1,x2∈C以及θ∈Rθ∈R，有θx1+(1−θ)x2∈Cθx1+(1−θ)x2∈C。<br><em>（所以仿射集合中至少有两个点？）</em><br>另外书中提到，可以把结论推广到任意元素的情况，即对多个xx进行加权，约束它们的系数和为1，获得的结果也在集合CC中，那么这个集合就是仿射集合。</p></blockquote><p><strong>凸集</strong></p><blockquote><p>凸集就是把θθ的值约束了一下：仿射集合中θ∈Rθ∈R，把θθ限制到[0,1][0,1]之间，以保证每个xx前面的系数都∈[0,1]∈[0,1]，就是凸集的定义了。换句话说，如果仿射集合中的两点确定的<strong>直线</strong>中的所有点都在集合中，那么凸集中的两点确定的<strong>线段</strong>中的所有点都在集合中。我就不抄数学公式了。</p></blockquote><p>把θ1x1+θ2x2+…+θkxkθ1x1+θ2x2+…+θkxk称为是点x1,…,xkx1,…,xk的一个<strong>凸组合</strong>，其中要求θ1+θ2+…+θk=1θ1+θ2+…+θk=1，可以把凸组合看成是xx的一个加权平均。凸组合的概念可以引出下面凸包的概念。</p><p><strong>凸包</strong></p><blockquote><p>集合CC中所有的点的凸组合称为凸包。换句话说，凸包是包含CC的最小凸集。<em>回想一下算法课上学的求凸包的方法（虽然已经忘记是什么方法了），可以想象出，凸包就是把一个集合最外面的点围起来构成的一个多边形。</em></p></blockquote><p><strong>凸锥</strong></p><blockquote><p>锥的定义是和仿射集、凸集有异曲同工之妙的：锥要求所有xx的系数非负。而凸锥要求：对于∀x1,x2∈C,θ1x1+θ2x2∈C∀x1,x2∈C,θ1x1+θ2x2∈C。<br>另外，参照凸集、凸组合和凸包之间的关系，也有锥、锥组合与锥包的相对定义，在此不再赘述。</p></blockquote><p><strong>综合上面的叙述，大概总结这些概念就是：</strong></p><ul><li>不管是仿射集合与凸集，都有定义：对于x1,x2∈Cx1,x2∈C，y=θx1+(1−θ)x2∈Cy=θx1+(1−θ)x2∈C，这个式子隐性要求了x1,x2x1,x2前面的系数θiθi之和为1。仿射集与凸集定义的不同点在于θθ的取值范围</li><li>仿射集合定义里面的θ∈Rθ∈R，这样对应的yy取值是一条直线。</li><li>凸集里面的θ∈[0,1]θ∈[0,1]，对应的yy取值是一条线段。</li><li>锥的定义中不要求系数和为1，而是约束的所有的系数θi≥0,i∈Nθi≥0,i∈N</li><li>仿射集、凸集与锥的定义中只涉及了x1x1与x2x2。但是一般的，对于大于两个变量xx的形如θ1x1+…+θkxkθ1x1+…+θkxk这样的组合，可以形成对应的仿射组合/凸组合/锥组合；而使用了所有的xx的组合（即当k=nk=n时）被称为包（仿射包/凸包/锥包）</li></ul><h2 id="重要的凸集们"><a href="#重要的凸集们" class="headerlink" title="重要的凸集们"></a>重要的凸集们</h2><p>这里面主要讲了：超平面和半空间，Euclid球和椭球，范数球和范数锥，多面体，以及半正定锥这些例子，因为这些凸集会在书本后面内容中反复多次出现。</p><p>所以又是一堆概念：</p><p><strong>超平面和半空间</strong></p><blockquote><p>这两个概念放在一起讲是因为它俩之间只有一个符号的区别。<br><strong>超平面</strong>是所有满足条件的xx的集合：{x∣aTx=b}{x∣aTx=b}，其中a∈Rna∈Rn，是一个向量（注意aa表示一个向量，而AA才表示一个矩阵）。<em>所以解集xx可以看成所有与向量aTaT的点乘是常数bb的向量，想象一个空间中存在一个向量aa，它的转置与它呈90∘90∘，而向量xx与它的点积是一个常数，也就是说xx与aTaT在一个平面上。bb约束了这个平面关于该平面零点OO的距离</em>。<br>与之相对，<strong>半空间</strong>也就是把超平面中的==符号改成了≤≤或者≥≥，这样xx表征的就不是一个平面了，而是在由这个平面分割的上/下一半的空间。如果不等式中没有等号，则称为<strong>开半空间</strong>。</p></blockquote><p><strong>Euclid球与椭球</strong></p><blockquote><p><strong>Euclid球</strong>在RnRn空间中，形式定义如下：</p><p>B(xc,r)={x∣||x−xc||2≤r}={x∣(x−xc)T(x−xc)≤r2}B(xc,r)={x∣||x−xc||2≤r}={x∣(x−xc)T(x−xc)≤r2}</p><p>||.||2||.||2表征的是Euclid范数，也就是空间中所定义的’距离’。上式的含义就是与给定的圆心xcxc的距离在给定常数rr以内的所有xx向量的集合，也就是大家所理解的球。（可以考虑证明：Euclid球是个凸集-使用凸集的性质以及范数上满足的三角不等式性质）<br><strong>椭球</strong><br>ε={x∣(x−xc)TP−1(x−xc)≤1}ε={x∣(x−xc)TP−1(x−xc)≤1}，其中P=PT≻0P=PT≻0，即矩阵PP是对称正定的，≻≻表示左边的每个元素都大于右边对应的元素。PP决定了椭球从xcxc向各个方向扩展的幅度，<em>我感觉我矩阵论没学好，愣是看不懂PP是怎么决定扩展幅度的，但是我努力的把这个定义背下来了，不求甚解不求甚解……</em></p></blockquote><p><strong>范数球与范数锥</strong></p><blockquote><p>顾名思义，就是由范数定义的球和锥。其实Euclid球就是由Euclid范数定义的球，所以是范数球的一种。范数锥是集合C={(x,t)∣||x||≤t}⊆Rn+1C={(x,t)∣||x||≤t}⊆Rn+1</p></blockquote><p><strong>多面体</strong></p><blockquote><p>多面体就是由一些超平面和一些半空间的交集，也就是它们围起来的一块，想象三个aa向外的半空间，其交点所构成的三角形内部就是一个多面体。形式化的定义为：P={x∣Ax⪯b,Cx=d}P={x∣Ax⪯b,Cx=d}</p></blockquote><p>使用SnSn表示对称的n×nn×n矩阵的集合，Sn+S+n表示对称的半正定矩阵，那么更进一步，Sn++S++n表示对称的正定矩阵。半正定锥会用到这些概念。</p><p><strong>半正定锥</strong></p><blockquote><p>所谓的用到上面的概念，不如说，集合Sn+S+n就是一个凸锥，也就是半正定锥。可以看到这个概念本身就符合凸锥的定义，所以不多赘述。书中给出一个S2S2空间中 的半正定锥的定义，<em>我并不能看懂它为什么是这样</em>。<br>值得一提的是，正定锥和半正定锥的区别在于，半正定锥能在不等式中取到等号。</p></blockquote><p>上面的所有概念都是凸集，如果闲着没事，可以尝试用凸集的定义来证明它们。用翟起滨老师的话说，这样就能对这些凸集的例子手拿把下XD。</p><h2 id="保持凸性的运算"><a href="#保持凸性的运算" class="headerlink" title="保持凸性的运算"></a>保持凸性的运算</h2><p>总的来说，保持凸性的运算有以下四个：</p><ol><li><p><strong>交运算</strong>：两个集合是凸的，那么它们的交集也是凸的。这个性质可以扩展到无穷集合的交集上。作为一个例子：多面体实际上是多个超平面与半空间的交集所以它也是凸的——<strong>这是一种不从定义本身来证明一个集合是凸的的方法</strong>。</p></li><li><p>仿射函数</p><p>：对于函数</p><p>f(x)=Ax+bf(x)=Ax+b</p><p>，其中</p><p>A∈Rm×n,b∈RmA∈Rm×n,b∈Rm</p><p>，如果一个集合是凸的，那么对集合中每个元素</p><p>xx</p><p>做仿射变换</p><p>f(x)f(x)</p><p>，最终获得的另一个集合也是凸的。也就是说，线性变换不改变集合的凸性。</p><blockquote><p>两个比较典型的例子是伸缩和平移：分别是b=0b=0与A=1A=1时的特殊情况，可以想象对一个凸集进行伸缩和平移并不影响它的凸性。</p></blockquote></li><li><p><strong>透视函数</strong>：简单解释就是，首先把一个向量的每个元素都除以最后一个元素的值（当然这样操作之后最后一维元素值变为1），之后最后一维舍弃掉，得到一个新的向量，这个向量是保持原向量的凸性的。因为这个变换很像对向量进行透视，所以叫做透视函数。</p></li><li><p>线性分式函数</p><p>：线性分式函数其实是由仿射函数与透视函数复合而成的，可以看成是对凸集的一种投射。形式化表述为：</p><p>f(x)=(Ax+b)/(cTx+d),domf=(x∣cTx+d&gt;0)f(x)=(Ax+b)/(cTx+d),domf=(x∣cTx+d&gt;0)</p><p>。</p><blockquote><p>当c=0,d&gt;0c=0,d&gt;0时，可以看出ff就是一个仿射函数，所以仿射函数是特殊的线性分式函数。</p></blockquote></li></ol><h2 id="广义不等式"><a href="#广义不等式" class="headerlink" title="广义不等式"></a>广义不等式</h2><h3 id="正常锥"><a href="#正常锥" class="headerlink" title="正常锥"></a>正常锥</h3><p>要理解广义不等式的含义，首先要对<strong>正常锥</strong>进行定义。称一个锥KK是正常锥(proper cone)，如果它满足以下条件：</p><ul><li>KK是凸(convex)的，也就是KK是个凸锥</li><li>KK是闭(close)的，也就是说两端的边界要能取到</li><li>KK是实(solid)的，就是说内部必须非空。这里要提一下，大家可能会想，锥不都是非空的吗，哪有空的锥？——Boyd教授给出了一个反例：一条射线满足凸锥的定义，但是它根本不存在内部，更没有非空一说。</li><li>KK是尖(pointed)的，就是说KK不包含直线，或者说KK中要存在点x=0x=0</li></ul><h3 id="广义不等式的定义"><a href="#广义不等式的定义" class="headerlink" title="广义不等式的定义"></a>广义不等式的定义</h3><p><strong>广义不等式</strong>通过上面正常锥的定义得到：</p><p>x⪯Ky⇔y−x∈Kx⪯Ky⇔y−x∈K</p><p>当然，如果左侧的不等式中没有等号，可以说这个广义不等式是<strong>严格的</strong>，否则就是<strong>不严格的</strong>。</p><p>怎么理解这个广义不等式呢，就是说如果两个向量x,yx,y相减得到的结果y−x∈Ky−x∈K，就认为xx在正常锥KK定义的广义不等式中⪯Ky⪯Ky。<em>我的理解是，由于正常锥KK本身的定义，保证了在里面的每个向量都是相对于其中的零点（定义中是尖的那个点）x=0x=0要大的，所以如果y−x∈Ky−x∈K，说明了yy与xx相减得到的差要大于这个正常锥所定义的集合中的零点，所以也就是在KK条件下&gt;0&gt;0。因此，如果y−x∈Ky−x∈K成立，说明yy在条件KK下比xx要大，这就是广义条件下的不等式了。</em></p><p>特别地，如果K=R+K=R+时，广义不等式中的不等号⪯⪯就是我们常说的不等号≤≤。按我上面所说的理解，这是因为y−xy−x大于了R+R+中的零点——也就是0，所以有了x≤yx≤y（我的逻辑居然自洽了，真开心）。</p><h3 id="广义不等式的性质"><a href="#广义不等式的性质" class="headerlink" title="广义不等式的性质"></a>广义不等式的性质</h3><p>广义不等式具有的性质如下，我感觉都比较好理解，因为有我们数学空间R+R+中的不等号来类比它：</p><ul><li>⪯K⪯K关于加法保序：即对x⪯Kyx⪯Ky以及u⪯Kvu⪯Kv，有x+u⪯Ky+vx+u⪯Ky+v，想象1&lt;2,3&lt;41&lt;2,3&lt;4所以有1+2&lt;3+41+2&lt;3+4一样。</li><li>⪯K⪯K具有传递性，这个不多解释了</li><li>⪯K⪯K对于非负数乘是保序的：对于x⪯Kyx⪯Ky与α≥0α≥0，有αx⪯Kαyαx⪯Kαy</li><li>⪯K⪯K是自反的，就是如果x⪯Kyx⪯Ky并且y⪯Kxy⪯Kx，那么x=yx=y</li><li>⪯K⪯K对于极限运算也是保序的，这点稍微难理解一点，但是我感觉没啥用，就不生搬定义了。</li></ul><h3 id="最小元与极小元"><a href="#最小元与极小元" class="headerlink" title="最小元与极小元"></a>最小元与极小元</h3><p>我们常说的空间R+R+（其实就是第一象限）中存在着最小元x=0x=0，而我们定义的正常锥的空间中也存在这样的最小元的定义，对于整个空间KK来说，其实就是尖点x=0x=0。但是因为正常锥KK空间中的某一个集合SS并不一定包含这个尖点，所以对于集合SS，我们可以这样定义SS中的<strong>最小元xx（如果存在）</strong>：</p><p>∀y∈S,x⪯Ky∀y∈S,x⪯Ky</p><p>这里其实满足了两个条件：首先xx对所有SS中的元素必须是<strong>可比</strong>的——不像我们的数字一定是可比的，空间中的点并不一定可比。这点很好理解，比如二维空间中的两个点(0,1)(0,1)与(1,0)(1,0)，它们在空间R2+R+2上并不可比。我想起了我的导师在数据仓库里面提到的数据的skyline，感觉差不多就是这个意思。</p><p>因为SS中的数据并不一定都可比，所以不一定有最小元的存在，但是一定有极小元的存在。一个集合SS的<strong>极小元xx</strong>的定义为：如果y∈S,y⪯Kxy∈S,y⪯Kx可以推得y=xy=x，那么称xx是SS关于广义不等式≺K≺K的极小元。换句话说，如果一个xx比所有SS中它可比的yy都要小，那么它就是个极小元。所以我们可以考虑这样一种特殊情况：集合SS中有一个元素xx与其它任何元素都不可比（其实我并不确定KK的定义是否允许有这样的情况存在），那么xx一定是一个极小元（<em>其实极小元更符合skyline的定义</em>）。</p><h2 id="分离与支撑超平面"><a href="#分离与支撑超平面" class="headerlink" title="分离与支撑超平面"></a>分离与支撑超平面</h2><p>好的，经过了艰难的跋涉之后，我们来到了分离与支撑超平面(sperate and support hyperplane)的定义。我不明白这个概念放在这里有什么上下文的意义，窃以为Boyd教授可能是把所有零散的概念都放到这一章了。</p><p><strong>超平面分离定理</strong>是这样的一个定理：</p><blockquote><p>假设集合C,DC,D是两个不相交的凸集，即C∩D=∅C∩D=∅，那么存在a≠0a≠0和bb使得aTx≤b,x∈CaTx≤b,x∈C，并且aTx≥b,x∈DaTx≥b,x∈D。</p></blockquote><p><em>当然按照定义，两个等号不能同时取到，不然两个集合就相交了XD。</em></p><p><em>其实上面凸集和不相交两个约束都比较重要，如果两个集合相交那肯定不能分离它们，而如果两个集合不是凸集，想象一个被分离的太极图，如果分离得不是那么远，其实是找不到一个平面把它们分割开来的。</em></p><p>与之相对的，<strong>支撑超平面</strong>是指：</p><blockquote><p>对于凸集CC上的一点x0x0，如果有一个超平面{x∣aTx=aTx0},a≠0{x∣aTx=aTx0},a≠0（显然这个超平面和凸集交于点x0x0），对于∀x∈C∀x∈C，有aTx≤aTx0aTx≤aTx0，那么就称这个超平面在点x0x0支撑CC。</p></blockquote><p><em>换句话说，就是一个超平面和一个凸集仅有一个交点，另外超平面的参数aa需要是朝着凸集CC的反向的，准确来说是：aTaT所在的直线就是点x0x0在凸集上的切线，这种情况下aa有两种方向，一种指向凸集内部，一种指向外部，根据定义需要指向凸集的外部，以保证所有的x∈C,aTx≤aTx0x∈C,aTx≤aTx0。</em></p><h2 id="对偶锥与广义不等式"><a href="#对偶锥与广义不等式" class="headerlink" title="对偶锥与广义不等式"></a>对偶锥与广义不等式</h2><h3 id="对偶锥"><a href="#对偶锥" class="headerlink" title="对偶锥"></a>对偶锥</h3><p>来到了第二章最后的部分。从上面我们知道了，广义不等式是建立在一个正常锥KK的定义上的，而这部分Boyd教授讲述了正常锥的对偶锥，既然正常锥KK可以推出一个广义不等式，那么它的对偶锥K∗K∗想必也能推出一个广义不等式。这两个广义不等式之间应该存在着一定的关系，这个关系就是下面要讨论的。</p><p><strong>对偶锥</strong>的形式定义是：K∗={y∣xTy≥0,∀x∈K}K∗={y∣xTy≥0,∀x∈K}。要注意，锥一定有对偶锥，而正常锥的对偶锥也是正常锥。</p><p><em>怎么来理解这个对偶锥呢，可以这么来考虑：对于一个锥KK，我们考虑它的一个元素x∈Kx∈K，这个xx与锥的尖点（即零点）共同确定了一条直线。现在根据定义，我们先考虑它的转置xTxT，这个转置也确定了一个方向，和前者所指定的方向关于整个KK定义的空间是垂直的，这一点是显然的。那么，现在我们要找一个yy，令xTy≥0xTy≥0，就意味着yy的方向与xTxT的方向的夹角不会超过180∘180∘。这就是yy的含义了。那么前面整个说的是关于锥KK中的一个点xx成立，现在要求这个条件要关于整个锥KK中的所有点都成立，就把这个yy限制在了一个特定的范围内，这个范围也是一个锥，这个锥就是KK的对偶锥。</em></p><p><em>不知道失不失一般性的，我们可以这样来找一个锥KK的对偶锥：KK的边界是两条射线，那么分别经过KK的尖点做这两条射线的垂直的射线，射线所指的方向是与KK中元素夹角不超过180∘180∘的方向。这两条垂线及其&lt;180∘&lt;180∘的夹角及其内部构成了KK的对偶锥。</em></p><p>通过以上描述我们也可以知道，锥KK与其对偶锥的两个夹角是互补的，其相加为180∘180∘，特别地，一个90∘90∘的正常锥KK的对偶锥就是它自身，这种情况称为<strong>自对偶</strong>。容易知道，Rn+R+n是自对偶的。</p><p>上面扯了一大堆对对偶锥自己的理解，下面是书中所提出的对偶锥所拥有的性质：（另外，由于Mathjax与Markdown渲染机制中对*这个符号存在冲突，所以原来书中对对偶锥的记号K∗K∗，在下面统一改成K−K−）</p><ul><li>K−K−是闭凸锥</li><li>K1⊆K2K1⊆K2可以推出K−2⊆K−1K2−⊆K1−，因为对偶锥互补嘛，你包了我，那我的对偶肯定包了你，大家和和气气</li><li>如果KK有非空内部，那么K−K−是尖的</li><li>如果KK的闭包是尖的，那么K−K−具有非空内部。反正这两条意思就是尖和非空应该是能互推的。</li><li>K—K—是KK的凸包的闭包</li></ul><h3 id="广义不等式的对偶"><a href="#广义不等式的对偶" class="headerlink" title="广义不等式的对偶"></a>广义不等式的对偶</h3><p>既然正常锥确定了一个广义不等式，那么正常锥的对偶锥也确定了一个广义不等式，把后者称为前者的对偶，即：⪯K−⪯K−是广义不等式⪯K⪯K的对偶。值得注意的是，在正常锥中，有K—=KK—=K。</p><p>对应的，KK上的<strong>最小元</strong>也有对应的对偶性质。xx是SS上关于广义不等式⪯K⪯K的最小元的充要条件是（注意SS不一定需要是凸集）：对于所有的λ≻K−0λ≻K−0，xx是在z∈Sz∈S上极小化λTzλTz的唯一最优解。又是一个难以理解的定义，不过Boyd教授阐述了几何上的解释：这意味着对于∀λ≻K−0∀λ≻K−0，超平面{z∣λT(z−x)=0}{z∣λT(z−x)=0}是在xx处对SS的一个支撑超平面。我仔细考虑了一下，实在是表达不出来那种似懂非懂的感觉。</p><p>接下来是<strong>极小元</strong>的对偶性质，和最小元对偶是类似的，只是xx从极小化λTzλTz的唯一最优解变成了只要求极小化就行，<em>也就是不需要唯一（或者不需要最优？我感觉是不需要唯一比较靠谱，当然我一点都不靠谱）</em></p><h3 id="Pareto最优制造前沿"><a href="#Pareto最优制造前沿" class="headerlink" title="Pareto最优制造前沿"></a>Pareto最优制造前沿</h3><p>在章节的末尾，Boyd教授给出了一个例子，正巧我的数据仓库学习的skyline与这个例子含义十分接近，让我来根据我的数据仓库所学来篡改这个例子，以说明最小元与极小元的区别：</p><p>我们对酒店进行打分，简单来说：如果一个酒店价格更低，我们认为这个酒店更好；如果这个酒店距离外滩更近，我们也认为这个酒店更好。每个酒店都是一个二维的元组(price,dist)(price,dist)，以price为xx轴，dist为yy轴，我们能把酒店标在一个二维平面坐标系中。</p><p>那么，最小元就是这样的一个酒店：所有酒店中没有酒店离外滩的距离比它更近，没有酒店的价格比它更低；而极小元就是满足这些条件的酒店：这些酒店至少在价格或者距离任意一维上不会比任何其它酒店要差。</p><p>从这个示例中我们可以轻易的看出，最小元是不一定存在的，但是极小元一定存在。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEASABIAAD/2wBDAAcFBQYFBAcGBQYIBwcIChELCgkJChUPEAwRGBUaGRgVGBcbHichGx0lHRcYIi4iJSgpKywr
      
    
    </summary>
    
      <category term="笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="凸优化笔记" scheme="http://yuanquanquan.top/categories/%E7%AC%94%E8%AE%B0/%E5%87%B8%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="优化理论" scheme="http://yuanquanquan.top/tags/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>Kick Start 2019-Practice Round</title>
    <link href="http://yuanquanquan.top/2019/20190911174/"/>
    <id>http://yuanquanquan.top/2019/20190911174/</id>
    <published>2019-11-17T14:18:32.000Z</published>
    <updated>2019-11-17T14:53:51.545Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://img3.imgtn.bdimg.com/it/u=1916139364,2178340577&amp;fm=15&amp;gp=0.jpg" alt="img">Google的<a href="https://codingcompetitions.withgoogle.com/" target="_blank" rel="noopener">Competitions with Google</a>上有三个类型的练习：</p><ul><li><strong>Hash Code</strong>: 应该是一个组队编程训练，看起来比较偏工程向，介绍中说可以选择队伍与编程语言来完成一个engineering problem，还问了一句Are u up for the challenge? <del>Of course I am not.</del></li><li><strong>Code Jam</strong>: <del>编程果酱？？</del> 反正就是用多轮的算法问题来进行比赛，最后获胜者还有奖金，又问了一句Do u have what it takes? <del>emmmm..</del></li><li><strong>Kick Start</strong>: 今天的重头戏，介绍中说主要是磨炼自己的coding水平，受众是学生 <del>我</del> 和新接触编程的人 <del>我</del> ，也是按轮次进行的，可以只参加一轮也可以全部参加，最后的问题是What are u waiting for? 这看起来就正常多了，不像是向一个算法渣挑衅的言论，给好评。</li></ul><p>Kick Start的轮次是一个Practice Round和A~H八轮，其中除了Practice Round给了24h解题外，其它的八个轮次都是只给3h解题时间，所以介绍中所说的’可以只参加一轮’，实则是指哪一轮呢？</p><p>别问，问就是自闭。</p><h1 id="Quick-Start-Guide"><a href="#Quick-Start-Guide" class="headerlink" title="Quick-Start Guide"></a>Quick-Start Guide</h1><p>在开始解题之前，还要学习怎么使用Kick Start平台。本来不想写这部分的，后来粗粗看了看，发现解题形式和我之前常用的POJ或Leetcode <del>其实我不常用，但是常用一词会显得我在这方面非常qualified</del> 差别还是不小的，所以来这里整理一下：</p><ol><li>如果想参加的是比赛，要在contest’s home page（当然还要在比赛举行时间）进行解题，如果是练习就滚去<a href="https://code.google.com/codejam/past-contests" target="_blank" rel="noopener">Past Problems</a>，选择自己想自闭的轮次</li><li>要注意limits模块中的内容，因为很多时候不是WA，而是TLE，就像最难过的事情不是我不行，而是我本可以。</li><li>下载问题文件之后，需要生成result文件并提交。在practice中只需要提交result文件，在competition中还需要提交code文件。提交结果有三种：<br>A. Correct<br>B. Rejected：就是提交被并非对错的原因被否决了，比如传错了文件<br>C. Incorrect：回答错误</li><li>比较有意思的是，Kick Start中的设定是，先写自己的程序，写完觉得没问题了点击下载数据集，一开始只能下载Small规模的数据集，从下载后开始计时，小问题是4分钟，解答正确后才能下载大数据集，大问题是8分钟。需要在指定时间内提交答案，如果提交答案正确，结束计时；如果提交Rejected，计时会继续走，还能继续提交；如果提交答案错误，会被计一次罚时，<strong>值得注意的是，如果计时结束也没有提交或者还是Rejected，也会自动判为Incorrect，此时已经没有提交机会了</strong>。</li><li>大数据集评分只有在比赛结束后才能看到，也就是说，提交之后是看不到自己的结果的。对于大数据集，只会judge最后一次上传的结果，可以在8分钟限时内反复提交。</li><li><strong>当然，以上所有的限制在Practice mode下不会生效，这是给自闭儿童的一大福利。</strong></li></ol><hr><p>关于读取input.file并输出output.file</p><p><strong>对于C++</strong></p><p>在solution.cpp中编写解决问题的代码，输入输出使用cin与cout即可，在编译时使用以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ solution.cpp -o solution./solution &lt; input_file.txt &gt; output_file.txt</span><br></pre></td></tr></table></figure><p>第一步是为了生成名为solution的可执行文件，当然如果嫌麻烦直接省去-o参数，直接生成a.out也行。</p><p>第二步是运行这个可执行文件，并指定输入文件名为input_file.txt，输出到output_file.txt中。</p><p><strong>对于Python</strong></p><p>使用raw_input()函数读取，使用print输出以及format方法控制格式即可。在执行时输入以下指令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python2 solution.py input_file.txt output_file.txt</span><br></pre></td></tr></table></figure><p>Python的执行语句就非常的简单粗暴了。通过以上的例子可以发现，实际编写代码时和平时其它平台上的输入输出相比几乎没有区别，只是在执行代码时添加了条件。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://img3.imgtn.bdimg.com/it/u=1916139364,2178340577&amp;amp;fm=15&amp;amp;gp=0.jpg&quot; alt=&quot;img&quot;&gt;Google的&lt;a href=&quot;https://codingcompetit
      
    
    </summary>
    
      <category term="算法" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="kick start" scheme="http://yuanquanquan.top/categories/%E7%AE%97%E6%B3%95/kick-start/"/>
    
    
      <category term="算法" scheme="http://yuanquanquan.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
