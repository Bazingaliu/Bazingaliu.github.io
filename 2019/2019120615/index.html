<!DOCTYPE html>
<html lang="zh-CN">
    
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <meta name="description" content="Two-armed Bandit" />
    <meta name="hexo-theme-A4" content="v1.9.8" />
    <link rel="alternate icon" type="image/webp" href="/img/20250602032635.jpg">
    <title>Yann | æˆ‘æ„¿åšä½ å…‰åä¸­æ·¡æ·¡çš„ä¸€ç¬”</title>

    
        
<link rel="stylesheet" href="/css/highlight/style1.css">

        
<link rel="stylesheet" href="/css/reset.css">

        
<link rel="stylesheet" href="/css/markdown.css">

        
<link rel="stylesheet" href="/css/fonts.css">
 
         <!--æ³¨æ„ï¼šé¦–é¡µæ—¢ä¸æ˜¯postä¹Ÿä¸æ˜¯page-->
        
        
        
<link rel="stylesheet" href="/css/ui.css">
 
        
<link rel="stylesheet" href="/css/style.css">


        
            <!--è¿”å›é¡¶éƒ¨css-->
            
<link rel="stylesheet" href="/css/returnToTop.css">

            
<link rel="stylesheet" href="/css/unicons.css">

        
        
            <!--ç›®å½•-->
            
<link rel="stylesheet" href="/css/toc.css">

        
    

    
        
<link rel="stylesheet" href="/css/returnToLastPage.css">

    
    
   
<link rel="stylesheet" href="/css/lightgallery-bundle.min.css">


   
        
<link rel="stylesheet" href="/css/custom.css">

    

<meta name="generator" content="Hexo 5.4.2"></head>
    
    

    
    



    
        <style>
            .paper-main{
                max-width:  1200px;
            }
        </style>

    
    




    
    <style>
        :root {
            --waline-theme-color: #323e74; 
            --waline-color: #323e74; 
            --waline-border-color: #323e74; 
            --waline-white: #323e74; 
            --waline-bgcolor-light: #f2fafc;  
        }
        body {
            color: #323e74;
            background: #eaeae8;
        }
        .post-md code {
            background: #e7f7f3;
            color: #7f688d; 
        }
        .post-md pre, .post-md .highlight {
            background: #e7f7f3;
            color: #7f688d; 
        }
        pre .string, pre .value, pre .inheritance, pre .header, pre .ruby .symbol, pre .xml .cdata {
            color: #323e74;
        }
        pre .number, pre .preprocessor, pre .built_in, pre .literal, pre .params, pre .constant {
            color: #323e74;
        }
        .year-font-color {
            color: #323e74 !important;
        }
        .wl-card span.wl-nick {
            color: #323e74; 
        }
        .wl-card .wl-badge {
            border: 1px solid #323e74;
            color: #323e74; 
        }
        .wl-btn {
            border: 1px solid #323e74; 
            color:  #323e74;  
        }
        .wl-btn.primary {
            color: #f2fafc; 
        }
        .wl-header label {
            color: #323e74;
        }
        a {
            color: #7f688d;
        }

        .post-md a {
            color: #7f688d;
        }

        .nav li a {
            color: #7f688d;
        }

        .archive-main a:link {
            color: #7f688d;
        }
        .archive-main a:visited {
            color: #767c7c; 
        }

        .archive li span {
            color: #323e74;
        }

        .post-main-title {
            color: #323e74;
        }

        .post-md h1,
        .post-md h2,
        .post-md h3,
        .post-md h4,
        .post-md h5,
        .post-md h6 {
            color: #323e74;
        }

        [data-waline] p {
            color: #323e74;
        }
        [data-waline] a {
            color: #323e74;
        } 
        .wl-sort li.active {
            color: #323e74;
        }

        .wl-card .wl-meta>span {
            background: #f2fafc;
        }

        .paper {
            background: #eaeae8;
        }

        .index-main {
            background: #f2fafc;
        }

        .paper-main {
            background: #f2fafc;
        }

        .wl-panel {
            background: #f2fafc;
        }

        .archive li:nth-child(odd) {
            background: #f2fafc;
            ;
        }

        .archive li:nth-child(even) {
            background: #f2fafc;
        }

        .post-md table tr:nth-child(odd) td {
            background: #f2fafc;
        }

        .post-md table tr:nth-child(even) td {
            background: #f2fafc;
        }

    
        .progress-wrap::after {
            color: #323e74; /* ç®­å¤´çš„é¢œè‰² */
        }
        .progress-wrap svg.progress-circle path {
	        stroke: #323e74; /* è¾¹æ¡†çš„é¢œè‰² */
        }
        .progress-wrap::before {
	        background-image: linear-gradient(298deg, #7f688d, #7f688d); /* é¼ æ ‡æ»‘è¿‡çš„ç®­å¤´é¢œè‰² */
         }

        .return-to-last-progress-wrap::after {
            color: #323e74; /* ç®­å¤´çš„é¢œè‰² */
        }
        .return-to-last-progress-wrap svg.progress-circle path {
	        stroke: #323e74; /* è¾¹æ¡†çš„é¢œè‰² */
        }
        .return-to-last-progress-wrap::before {
	        background-image: linear-gradient(298deg, #7f688d, #7f688d); /* é¼ æ ‡æ»‘è¿‡çš„ç®­å¤´é¢œè‰² */
         }

         .left-toc-container::-webkit-scrollbar-thumb {
            background-color: #323e74; /* è®¾ç½®æ»šåŠ¨æ¡æ‹–åŠ¨å—çš„é¢œè‰² */
        }

        .bs-docs-sidebar .nav>.active>a,
        .bs-docs-sidebar .nav>li>a:hover,
        .bs-docs-sidebar .nav>li>a:focus {
            color: #7f688d;
            border-left-color: #7f688d;
        }
        .bs-docs-sidebar .nav>li>a {
            color:  #323e74;
        }
    </style>

    
    <style>
        body {
            background-image: url(/img/3.jpg);
            background-attachment: fixed;  /* èƒŒæ™¯å›ºå®šï¼Œä¸éšé¡µé¢æ»šåŠ¨ */
            background-repeat: no-repeat;  /* é˜²æ­¢èƒŒæ™¯å›¾ç‰‡é‡å¤ */
            background-size: cover;       /* èƒŒæ™¯è‡ªé€‚åº”å¤§å°ï¼Œè¦†ç›–æ•´ä¸ªèƒŒæ™¯ */
            background-position: center;   /* èƒŒæ™¯å±…ä¸­æ˜¾ç¤º */
        }
        .paper {
            background-image: url(/img/3.jpg);
            background-attachment: fixed;  /* èƒŒæ™¯å›ºå®šï¼Œä¸éšé¡µé¢æ»šåŠ¨ */
            background-repeat: no-repeat;  /* é˜²æ­¢èƒŒæ™¯å›¾ç‰‡é‡å¤ */
            background-size: cover;       /* èƒŒæ™¯è‡ªé€‚åº”å¤§å°ï¼Œè¦†ç›–æ•´ä¸ªèƒŒæ™¯ */
            background-position: center;   /* èƒŒæ™¯å±…ä¸­æ˜¾ç¤º */
        }  
    </style>


    
    <body>
        <script src="/js/darkmode-js.min.js"></script>
        
        <script>
            const options = {
                bottom: '40px', // default: '32px'
                right: 'unset', // default: '32px'
                left: '42px', // default: 'unset'
                time: '0.3s', // default: '0.3s'
                mixColor: '#fff', // default: '#fff'
                backgroundColor: ' #eaeae8  ',  // default: '#fff'
                buttonColorDark: '#100f2c',  // default: '#100f2c'
                buttonColorLight: '#fff', // default: '#fff'
                saveInCookies: true, // default: true,
                label: 'ğŸŒ“', // default: ''
                autoMatchOsTheme: true // default: true
            }
            const darkmode = new Darkmode(options);
            darkmode.showWidget();
        </script>
        
        
            
                <div class="left-toc-container">
                    <nav id="toc" class="bs-docs-sidebar"></nav>
                </div>
            
        
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    


<div class="header">
    <div class="header-container">
        <style>
            .header-img {
                width: 56px;
                height: auto;
                object-fit: cover; /* ä¿æŒå›¾ç‰‡æ¯”ä¾‹ */
                transition: transform 0.3s ease-in-out; 
                border-radius: 0; 
            }
            
        </style>
        <img 
            alt="^-^" 
            cache-control="max-age=86400" 
            class="header-img" 
            src="/img/20250602032635.jpg" 
        />
        <div class="header-content">
            <a class="logo" href="/">Yann</a> 
            <span class="description">äººå·¥æ™ºèƒ½ã€è®¡ç®—æœºã€æœºå™¨å­¦ä¹ ã€linuxã€ç¨‹åºå‘˜</span> 
        </div>
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="/">é¦–é¡µ</a></li>
            
        
            
                <li><a href="/list/">æ–‡ç« </a></li>
            
        
            
                <li><a href="/about/">å…³äº</a></li>
            
        
            
                <li><a href="/tags/">æ ‡ç­¾</a></li>
            
        
            
                <li><a href="/categories/">åˆ†ç±»</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--è¯´æ˜æ˜¯æ–‡ç« posté¡µé¢-->
                    
                        <div class="post-main">
    

    
        
            
                <div class="post-main-title" style="text-align: center;">
                    Two-armed Bandit
                </div>
            
        
      
    

    

        
            <div class="post-head-meta-center">
        
                
                    <span>æœ€è¿‘æ›´æ–°ï¼š2020-06-14</span> 
                
                
                    
                        &nbsp; | &nbsp;
                    
                     <span>å­—æ•°æ€»è®¡ï¼š719</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span>é˜…è¯»ä¼°æ—¶ï¼š4åˆ†é’Ÿ</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span id="busuanzi_container_page_pv">
                        é˜…è¯»é‡ï¼š<span id="busuanzi_value_page_pv"></span>æ¬¡
                    </span>
                
            </div>
    

    <div class="post-md">
        
            
                <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Concepts"><span class="post-toc-text">Concepts</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Learning-a-Policy"><span class="post-toc-text">Learning a Policy</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Policy-Gradients"><span class="post-toc-text">Policy Gradients</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Value-functions"><span class="post-toc-text">Value functions</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#e-greedy-policy"><span class="post-toc-text">e-greedy policy</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#policy-loss-equation"><span class="post-toc-text">policy loss equation</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#The-Multi-armed-bandit"><span class="post-toc-text">The Multi-armed bandit</span></a></li></ol>
            
        
        <div class=".article-gallery"><blockquote>
<p>RLé—®é¢˜æœ‰ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š</p>
<ul>
<li>ä¸åŒçš„actionså¯¼è‡´ä¸åŒçš„rewards</li>
<li>rewardså…·æœ‰æ—¶å»¶æ€§</li>
<li>actionçš„rewardå–å†³äºç¯å¢ƒçŠ¶æ€</li>
</ul>
</blockquote>
<span id="more"></span>  
<p><a target="_blank" rel="noopener" href="https://i.loli.net/2020/06/14/4GfVB92s1CwTuUq.png" class="gallery-item" style="box-shadow: none;"> <img src="https://i.loli.net/2020/06/14/4GfVB92s1CwTuUq.png" alt=""></a></p>
<h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h3 id="Learning-a-Policy"><a href="#Learning-a-Policy" class="headerlink" title="Learning a Policy"></a>Learning a <em>Policy</em></h3><p>Learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones.</p>
<h3 id="Policy-Gradients"><a href="#Policy-Gradients" class="headerlink" title="Policy Gradients"></a>Policy Gradients</h3><p>Simple neural network learns a policy for picking actions by adjusting itâ€™s weights through gradient descent using feedback from the environment.</p>
<h3 id="Value-functions"><a href="#Value-functions" class="headerlink" title="Value functions"></a>Value functions</h3><p>Instead of learning the optimal action in a given state, the agent learns to predict how good a given state or action will be for the agent to be in.</p>
<h3 id="e-greedy-policy"><a href="#e-greedy-policy" class="headerlink" title="e-greedy policy"></a>e-greedy policy</h3><p>This means that most of the time our agent will choose the action that corresponds to the largest expected value, but occasionally, with e probability, it will choose randomly.</p>
<h3 id="policy-loss-equation"><a href="#policy-loss-equation" class="headerlink" title="policy loss equation"></a>policy loss equation</h3><blockquote>
<p><strong>Loss = -log(Ï€)A</strong></p>
</blockquote>
<p>A is advantage, and is an essential aspect of all reinforcement learning algorithms. Intuitively it corresponds to how much better an action was than some baseline.</p>
<p>Ï€ is the policy. In this case, it corresponds to the chosen actionâ€™s weight.</p>
<h2 id="The-Multi-armed-bandit"><a href="#The-Multi-armed-bandit" class="headerlink" title="The Multi-armed bandit"></a>The Multi-armed bandit</h2><p><a target="_blank" rel="noopener" href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149">ä»£ç æ¥æº</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">The Bandits</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Here we define our bandits. For this example we are using a four-armed bandit. The pullBandit function generates a random number from a normal distribution with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit that will give that positive reward.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#List out our bandits. Currently bandit 4 (index#3) is set to most often provide a positive reward.</span></span><br><span class="line">bandits = [<span class="number">0.2</span>,<span class="number">0</span>,-<span class="number">0.2</span>,-<span class="number">5</span>]</span><br><span class="line">num_bandits = <span class="built_in">len</span>(bandits)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pullBandit</span>(<span class="params">bandit</span>):</span></span><br><span class="line">    <span class="comment">#Get a random number.</span></span><br><span class="line">    result = np.random.randn(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> result &gt; bandit:</span><br><span class="line">        <span class="comment">#return a positive reward.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#return a negative reward.</span></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">The Agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment">#These two lines established the feed-forward part of the network. This does the actual choosing.</span></span><br><span class="line">weights = tf.Variable(tf.ones([num_bandits]))</span><br><span class="line">chosen_action = tf.argmax(weights,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#The next six lines establish the training proceedure. We feed the reward and chosen action into the network</span></span><br><span class="line"><span class="comment">#to compute the loss, and use it to update the network.</span></span><br><span class="line">reward_holder = tf.placeholder(shape=[<span class="number">1</span>],dtype=tf.float32)</span><br><span class="line">action_holder = tf.placeholder(shape=[<span class="number">1</span>],dtype=tf.int32)</span><br><span class="line">responsible_weight = tf.<span class="built_in">slice</span>(weights,action_holder,[<span class="number">1</span>])</span><br><span class="line">loss = -(tf.log(responsible_weight)*reward_holder)</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">update = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Training the Agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">We will train our agent by taking actions in our environment, and recieving rewards. Using the rewards and actions, we can know how to properly update our network in order to more often choose actions that will yield the highest rewards over time.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">total_episodes = <span class="number">1000</span> <span class="comment">#Set total number of episodes to train agent on.</span></span><br><span class="line">total_reward = np.zeros(num_bandits) <span class="comment">#Set scoreboard for bandits to 0.</span></span><br><span class="line">e = <span class="number">0.1</span> <span class="comment">#Set the chance of taking a random action.</span></span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch the tensorflow graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; total_episodes:</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Choose either a random action or one from our network.</span></span><br><span class="line">        <span class="keyword">if</span> np.random.rand(<span class="number">1</span>) &lt; e:</span><br><span class="line">            action = np.random.randint(num_bandits)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = sess.run(chosen_action)</span><br><span class="line"></span><br><span class="line">        reward = pullBandit(bandits[action]) <span class="comment">#Get our reward from picking one of the bandits.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#Update the network.</span></span><br><span class="line">        _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict=&#123;reward_holder:[reward],action_holder:[action]&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Update our running tally of scores.</span></span><br><span class="line">        total_reward[action] += reward</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;Running reward for the &quot;</span> + <span class="built_in">str</span>(num_bandits) + <span class="string">&quot; bandits: &quot;</span> + <span class="built_in">str</span>(total_reward)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;The agent thinks bandit &quot;</span> + <span class="built_in">str</span>(np.argmax(ww)+<span class="number">1</span>) + <span class="string">&quot; is the most promising....&quot;</span></span><br><span class="line"><span class="keyword">if</span> np.argmax(ww) == np.argmax(-np.array(bandits)):</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;...and it was right!&quot;</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;...and it was wrong!&quot;</span></span><br></pre></td></tr></table></figure></div>
    </div>

    <div class="post-meta">
        <i>
        
            <span>2019-12-06</span>
            
                <span>è¯¥ç¯‡æ–‡ç« è¢« Yann</span>
            
            
                <span>æ‰“ä¸Šæ ‡ç­¾:
                    
                    
                        <a href='/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/'>
                            å¼ºåŒ–å­¦ä¹ 
                        </a>
                    
                </span>
             
             
                <span>å½’ä¸ºåˆ†ç±»:
                    
                    
                        <a href='/categories/%E7%AC%94%E8%AE%B0/'>
                            ç¬”è®°
                        </a>
                    
                </span>
            
        
        </i>
    </div>
    <br>
    
    
        
            
    
            <div class="post-footer-pre-next">
                
                    <span>ä¸Šä¸€ç¯‡ï¼š<a href='/2019/2019120722/'>åˆæ¬¡é¢è¯•-å­—èŠ‚è·³åŠ¨</a></span>
                

                
                    <span class="post-footer-pre-next-last-span-right">ä¸‹ä¸€ç¯‡ï¼š<a href="/2019/20191206/">OpenAI Spinning Up Part 1ï¼š Key Concepts in RL</a>
                    </span>
                
            </div>
    
        
    

    
        

     
</div>



                                      
                    
                    
                    <div class="footer">
    
        <span> 
            Â© 1949-2025 China 

            
                

            
        </span>
       
    
</div>



<!--è¿™æ˜¯æŒ‡ä¸€æ¡çº¿å¾€ä¸‹çš„å†…å®¹-->
<div class="footer-last">
    
            <span>ğŸŒŠçœ‹è¿‡å¤§æµ·çš„äººä¸ä¼šå¿˜è®°æµ·çš„å¹¿é˜”ğŸŒŠ</span>
            
                <span class="footer-last-span-right"><i>æœ¬ç«™ç”±<a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/index.html">Hexo</a>é©±åŠ¨ï½œä½¿ç”¨<a target="_blank" rel="noopener" href="https://github.com/HiNinoJay/hexo-theme-A4">Hexo-theme-A4</a>ä¸»é¢˜</i></span>
            
    
</div>


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

    <!--ç›®å½•-->
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tocify/1.9.0/javascripts/jquery.tocify.min.js" type="text/javascript" ></script>
        
<script src="/js/toc.js"></script>

    

    
<script src="/js/randomHeaderContent.js"></script>

    <!--å›åˆ°é¡¶éƒ¨æŒ‰é’®-->
    
        
<script src="/js/returnToTop.js"></script>

    

    
        
<script src="/js/returnToLastPage.js"></script>

    





<script src="/js/lightgallery/lightgallery.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-thumbnail.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-fullscreen.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-autoplay.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-zoom.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-rotate.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-paper.umd.min.js"></script>




<script type="text/javascript">
     
    if (typeof lightGallery !== "undefined") {
        var options1 = {
            selector: '.gallery-item',
            plugins: [lgThumbnail, lgFullscreen, lgAutoplay, lgZoom, lgRotate, lgPager], // å¯ç”¨æ’ä»¶
            thumbnail: true,          // æ˜¾ç¤ºç¼©ç•¥å›¾
            zoom: true,               // å¯ç”¨ç¼©æ”¾åŠŸ
            rotate: true,             // å¯ç”¨æ—‹è½¬åŠŸèƒ½èƒ½
            autoplay: true,        // å¯ç”¨è‡ªåŠ¨æ’­æ”¾åŠŸèƒ½
            fullScreen: true,      // å¯ç”¨å…¨å±åŠŸèƒ½
            pager: false, //é¡µç ,
            zoomFromOrigin: true,   // ä»åŸå§‹ä½ç½®ç¼©æ”¾
            actualSize: true,       // å¯ç”¨æŸ¥çœ‹å®é™…å¤§å°çš„åŠŸèƒ½
            enableZoomAfter: 300,    // å»¶è¿Ÿç¼©æ”¾ï¼Œç¡®ä¿å›¾ç‰‡åŠ è½½å®Œæˆåå¯ç¼©æ”¾
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options1); // ä¿®å¤é€‰æ‹©å™¨
    }
    
</script>


    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> 

                </div>
            
            
                <!-- å›åˆ°é¡¶éƒ¨çš„æŒ‰é’®-->  
                <div class="progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
            
                <!-- è¿”å›çš„æŒ‰é’®-->  
                <div class="return-to-last-progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
    </body>
</html>
<script src="/js/emojiHandler.js"></script>
<script>
  document.addEventListener('DOMContentLoaded', () => {
    wrapEmojis('.paper');
  });
</script>